[
  {
    "instance_id": "026d9e6e5c9644caa918247770a0fcf5",
    "figure_id": "1903.10128v1-Figure13-1",
    "image_file": "1903.10128v1-Figure13-1.png",
    "caption": " Visual results on SPMCS for 4× scaling factor.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most realistic results?",
    "answer": "GT",
    "rationale": "The GT image is the ground truth, or the original image. The other images are all results of different methods for upscaling the image. The GT image is the most realistic because it is the original image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.10128v1",
    "pdf_url": null
  },
  {
    "instance_id": "34f2dc5eef044297beb055f3f5427682",
    "figure_id": "2107.09912v2-Figure3-1",
    "image_file": "2107.09912v2-Figure3-1.png",
    "caption": " The sampler-planner (S-P) pair is compared with a random sampler (Rand) with varying levels of regularization λ. The supervised learning oracle (SL), which observes state-action pairs in the training set, is shown as baseline.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest relevance score for a fixed number of online samples?",
    "answer": "Supervised Learning (SL)",
    "rationale": "The black dashed line, which represents the SL method, is consistently higher than all other lines in the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.09912v2",
    "pdf_url": null
  },
  {
    "instance_id": "272ccb99130844a5b9dd943592a2773c",
    "figure_id": "2302.11618v2-Figure10-1",
    "image_file": "2302.11618v2-Figure10-1.png",
    "caption": " (a)Figure showing the variation of memory capacity with neuronal heterogeneity (b) Figure showing the variation of efficiency E with number of neurons NR",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of network, HRSNN or MRSNN, is more efficient?",
    "answer": "MRSNN is more efficient than HRSNN.",
    "rationale": "Figure (b) shows that the efficiency of MRSNN is higher than that of HRSNN for all values of the number of neurons NR.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.11618v2",
    "pdf_url": null
  },
  {
    "instance_id": "0173e0c5829948ef82e7242f62753cbf",
    "figure_id": "2112.07381v2-Figure2-1",
    "image_file": "2112.07381v2-Figure2-1.png",
    "caption": " Exact Match scores for given N retrieved or reranked passages on NQ development set. Rerank EM scores are from reranking only 100 retrieved passages.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does reranking passages improve the Exact Match score?",
    "answer": "Yes.",
    "rationale": "The plot shows that the Exact Match score for reranked passages is higher than the Exact Match score for retrieved passages for all values of N. This suggests that reranking passages improves the Exact Match score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.07381v2",
    "pdf_url": null
  },
  {
    "instance_id": "77f43e66c62f4d4fa0aca84e31dd0783",
    "figure_id": "2303.14969v1-Figure9-1",
    "image_file": "2303.14969v1-Figure9-1.png",
    "caption": " Performance of VTM on various shots. In general, VTM consistently improves performance as more supervision is given, and even surpasses fully supervised baselines on many tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the ZD task when only 10 shots are used for training?",
    "answer": "Ours.",
    "rationale": "The ZD plot shows that the blue line (Ours) has the lowest RMSE value at 10 shots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.14969v1",
    "pdf_url": null
  },
  {
    "instance_id": "b373a3fbfa034f1d99f00acfaf9f8a1d",
    "figure_id": "2103.14645v1-Figure8-1",
    "image_file": "2103.14645v1-Figure8-1.png",
    "caption": " Real forward-facing scene example results (PSNR in parentheses).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the image with the highest PSNR?",
    "answer": "JAXNeRF+",
    "rationale": "The PSNR values are shown in parentheses next to each method name. JAXNeRF+ has the highest PSNR of 25.50.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.14645v1",
    "pdf_url": null
  },
  {
    "instance_id": "e779a058a2034160b17390b9875ef15e",
    "figure_id": "2102.11165v1-Figure5-1",
    "image_file": "2102.11165v1-Figure5-1.png",
    "caption": " (a) AUC-ROC results of Meta-GDN and its variants; (b) Precision@100 results of Meta-GDN and its variants.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Yelp dataset in terms of AUC-ROC and Precision@100?",
    "answer": "Meta-GDN.",
    "rationale": "The figure shows that Meta-GDN has the highest AUC-ROC and Precision@100 values for the Yelp dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.11165v1",
    "pdf_url": null
  },
  {
    "instance_id": "3456dc3d585843169386ef8829b8aa3f",
    "figure_id": "2201.10122v1-Figure4-1",
    "image_file": "2201.10122v1-Figure4-1.png",
    "caption": " Dynamic simulation sequences used to learn zero-restlength spring constitutive parameters.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two different types of exercises shown in the image?",
    "answer": "Jumping jacks and calisthenics.",
    "rationale": "The image shows a person performing jumping jacks on the left and calisthenics on the right.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.10122v1",
    "pdf_url": null
  },
  {
    "instance_id": "2725e9e794e449deb59ddec1293e24cf",
    "figure_id": "2309.12415v1-Figure4-1",
    "image_file": "2309.12415v1-Figure4-1.png",
    "caption": " Box plot that shows Mean Reading Speed (MRS) measured for three sentences set “MNREAD”,“low PPL”, “high PPL”.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sentence set has the lowest mean reading speed?",
    "answer": "high_PPL",
    "rationale": "The box plot for high_PPL has the lowest median value, as indicated by the horizontal line within the box.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.12415v1",
    "pdf_url": null
  },
  {
    "instance_id": "4883d72b03944e35970dd293a9a25adf",
    "figure_id": "1910.08348v2-Figure4-1",
    "image_file": "1910.08348v2-Figure4-1.png",
    "caption": " Average test performance for the first 5 rollouts of MuJoCo environments (using 5 seeds).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best on the Cheetah-Dir environment?",
    "answer": "variBAD",
    "rationale": "The figure shows the average test performance for the first 5 rollouts of MuJoCo environments. The Cheetah-Dir environment is shown in the second plot from the left. The variBAD algorithm is the highest line on the plot, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.08348v2",
    "pdf_url": null
  },
  {
    "instance_id": "770587b8f4da40f096f2b49f9bb2ffb8",
    "figure_id": "2012.14768v2-Figure1-1",
    "image_file": "2012.14768v2-Figure1-1.png",
    "caption": " Attention distribution that each decoder layer (x-axis) attending to encoder layers (y-axis).",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which decoder layer attends most to the embedding layer for the summarization task?",
    "answer": "Decoder layer 6.",
    "rationale": "The figure shows the attention distribution of each decoder layer to the encoder layers. In the summarization task (b), the color of the cell in the 6th row and 1st column is the darkest, which indicates the highest attention weight.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.14768v2",
    "pdf_url": null
  },
  {
    "instance_id": "f66aa3a4143d4e079d2a852eb19022ca",
    "figure_id": "2304.00464v2-Figure7-1",
    "image_file": "2304.00464v2-Figure7-1.png",
    "caption": " Qualitative Grasping Trajecoties. We provide several grasping trajectories for different objects with different initial poses.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object is the most difficult for the robot to grasp?",
    "answer": "The elephant.",
    "rationale": "The elephant is the most difficult for the robot to grasp because it is the largest and most complex object. The robot has to reach around the elephant's body and grasp it in a way that is stable and secure. This is more difficult than grasping the other objects, which are smaller and simpler in shape.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.00464v2",
    "pdf_url": null
  },
  {
    "instance_id": "527a93742208418ca502af09c6f6005a",
    "figure_id": "2303.13839v1-Figure4-1",
    "image_file": "2303.13839v1-Figure4-1.png",
    "caption": " Statical results on parent ratio of different document semantic units of HRDoc dataset.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which document semantic unit has the highest parent ratio?",
    "answer": "Root",
    "rationale": "The figure shows the parent ratio of different document semantic units. The Root node has the highest parent ratio, as indicated by the darkest blue color.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.13839v1",
    "pdf_url": null
  },
  {
    "instance_id": "2b2c044247a847e3a5fe949510d435b4",
    "figure_id": "2101.00408v2-Figure3-1",
    "image_file": "2101.00408v2-Figure3-1.png",
    "caption": " Effect of amount of training data on retrieval accuracy when evaluated on NQ test set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which initialization method performs the best on the NQ test set when only a small fraction of the training data is used?",
    "answer": "Masked Salient Spans Init",
    "rationale": "The figure shows that the Masked Salient Spans Init method consistently achieves the highest top-20 accuracy across all fractions of the training data, particularly excelling when only a small portion of the data is available.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.00408v2",
    "pdf_url": null
  },
  {
    "instance_id": "e6d014354d524594bfc4189cf0baa240",
    "figure_id": "1907.10154v5-Figure2-1",
    "image_file": "1907.10154v5-Figure2-1.png",
    "caption": " Test AUROC for predicting employee access in a new department, using training data from 4 departments",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampling method performed the best in terms of test AUROC?",
    "answer": "Mix&MatchCH",
    "rationale": "The Mix&MatchCH line is consistently higher than the other lines, indicating that it has the highest test AUROC for all SGD iteration budgets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.10154v5",
    "pdf_url": null
  },
  {
    "instance_id": "33d03a18726d49b4b748ee43a4aa9632",
    "figure_id": "2109.07557v3-Figure8-1",
    "image_file": "2109.07557v3-Figure8-1.png",
    "caption": " Illustration of trade-off between invalidity and sparsity across six datasets (methods at the bottom left are preferable).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the lowest sparsity while maintaining a low invalidity?",
    "answer": "CounterNet",
    "rationale": "The figure shows the trade-off between invalidity and sparsity for six different methods. The method with the lowest sparsity and lowest invalidity is CounterNet, which is represented by the black star in the bottom left corner of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.07557v3",
    "pdf_url": null
  },
  {
    "instance_id": "0d66479d512e40a48c953a115e49de1b",
    "figure_id": "2002.08155v4-Figure4-1",
    "image_file": "2002.08155v4-Figure4-1.png",
    "caption": " Learning curve of different pre-trained models in the fine-tuning step. We show results on Python and Java.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pre-trained model performed the best on the Python dataset?",
    "answer": "Roberta",
    "rationale": "The figure shows the learning curves of three different pre-trained models on the Python dataset. The Roberta model has the highest accuracy throughout the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.08155v4",
    "pdf_url": null
  },
  {
    "instance_id": "6112d03a4e7a487d98874a273cda3512",
    "figure_id": "1906.04726v2-Figure7-1",
    "image_file": "1906.04726v2-Figure7-1.png",
    "caption": " In how many languages are the intents in Europarl translated? (intents from ill-fitting turns included in 100%, but not plotted)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of intents are translated into 10 languages?",
    "answer": "10%",
    "rationale": "The figure shows the percentage of intents that are translated into different numbers of languages. The bar for 10 languages is at the 10% mark on the y-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04726v2",
    "pdf_url": null
  },
  {
    "instance_id": "8778338e834247f8ae5ee1f168c76c66",
    "figure_id": "2006.01959v2-Figure5-1",
    "image_file": "2006.01959v2-Figure5-1.png",
    "caption": " Decoded goals (left) and sequence segmentation (right) learned for a 6-goal visual trajectory of a PR2 robot. The sequence shows 33 equally spaced frames of a 100-frame demonstration.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " How many frames are shown in the sequence?",
    "answer": " 33 frames",
    "rationale": " The caption states that the sequence shows 33 equally spaced frames of a 100-frame demonstration.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.01959v2",
    "pdf_url": null
  },
  {
    "instance_id": "91b1682a23e24887afbfb3850cede74e",
    "figure_id": "2206.15477v6-Figure3-1",
    "image_file": "2206.15477v6-Figure3-1.png",
    "caption": " Categorization of information learned and removed by various methods with distinct formulations.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the only one that can be used to learn both the reward and the dynamics of the environment?",
    "answer": "Reconstruction-Based RL",
    "rationale": "The figure shows that all methods except Reconstruction-Based RL either reduce or do not integrate the information about the reward and the dynamics of the environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.15477v6",
    "pdf_url": null
  },
  {
    "instance_id": "da7c980817ed454fb93f843980100e67",
    "figure_id": "2008.09061v1-Figure1-1",
    "image_file": "2008.09061v1-Figure1-1.png",
    "caption": " Test performance on Yahoo! LETOR set 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the Yahoo! LETOR set 1?",
    "answer": "DLCMinit",
    "rationale": "The figure shows the test performance of different methods on the Yahoo! LETOR set 1. The DLCMinit method has the highest ndCG@10 score, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.09061v1",
    "pdf_url": null
  },
  {
    "instance_id": "6aa3ebb78aab4b739e3d868aa245825f",
    "figure_id": "1906.10197v3-Figure1-1",
    "image_file": "1906.10197v3-Figure1-1.png",
    "caption": " The mutual exclusivity task used in cognitive development research [1]. Children tend to associate the novel word (“dax”) with the novel object (right).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object in the image is most likely to be the \"dax\"?",
    "answer": "The pink, bone-shaped object on the right.",
    "rationale": "The caption states that children tend to associate the novel word (\"dax\") with the novel object. In the image, the mug is a familiar object, while the pink, bone-shaped object is novel.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.10197v3",
    "pdf_url": null
  },
  {
    "instance_id": "face5211020242e2980c947cafbcf69e",
    "figure_id": "2205.09273v2-Figure4-1",
    "image_file": "2205.09273v2-Figure4-1.png",
    "caption": " Effects of iterations on dev. performance. Iteration 0 refers to the initial decoding from f . Every iteration consists of g’s decoding with f ’s guidance followed by f ’s decoding with g’s guidance. The values of λs are kept the same over all iterations for simplicity. Initially, we explored gradually increasing the λs as f and g’s outputs become closer, but we found no substantial performance gain.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the most improvement in performance over the course of the iterations?",
    "answer": "WMT20 ZH-EN",
    "rationale": "The figure shows the performance of the model on three different datasets, Medicine, WMT20 ZH-EN, and SciTLDR. The y-axis shows the performance metric, and the x-axis shows the number of iterations. The WMT20 ZH-EN dataset shows the largest increase in performance, from around 35 to around 37.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.09273v2",
    "pdf_url": null
  },
  {
    "instance_id": "f901df90a0764c599a5f76f70d9b115b",
    "figure_id": "2205.06130v1-Figure3-1",
    "image_file": "2205.06130v1-Figure3-1.png",
    "caption": " Task-wise mean SHAP values of different features for the Group Lasso model trained on XLMR zero-shot performance data. Higher value implies stronger effect.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature has the strongest positive effect on XLMR zero-shot performance for the XCOPA task?",
    "answer": "SIZE(t)",
    "rationale": "The color scale on the right of the figure shows that red indicates a strong positive effect, while dark blue indicates a strong negative effect. Looking at the row for XCOPA, we see that the SIZE(t) feature has the highest value (0.63), and its cell is the most red. This means that this feature has the strongest positive effect on XLMR zero-shot performance for the XCOPA task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.06130v1",
    "pdf_url": null
  },
  {
    "instance_id": "f25e325211054103ad04efa8e4657a69",
    "figure_id": "2004.04400v1-Figure6-1",
    "image_file": "2004.04400v1-Figure6-1.png",
    "caption": " Qualitative results on 5 different datasets. Failure cases are highlighted in magenta which specifically occur in presence of multi-level inter-limb occlusion (see LSP failure case) and very rare, athletic poses (see YTube failure case). However, the model faithfully attends to single-level occlusions, enabled by the depth-aware part representation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the most failure cases?",
    "answer": "YouTube dataset.",
    "rationale": "The caption states that failure cases are highlighted in magenta. There are two highlighted examples in the YouTube dataset, while there is only one highlighted example in the LSP dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.04400v1",
    "pdf_url": null
  },
  {
    "instance_id": "92b651ccaca04b37bb05dce0054b9b63",
    "figure_id": "2308.11072v1-Figure7-1",
    "image_file": "2308.11072v1-Figure7-1.png",
    "caption": " XD-Violence classwise AUC performance comparison between raw and anonymized videos.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of anomaly has the largest difference in AUC performance between raw and anonymized videos?",
    "answer": "Explosion",
    "rationale": "The figure shows that the difference in AUC performance between raw and anonymized videos is largest for the Explosion category. The AUC for raw videos is about 80%, while the AUC for anonymized videos is about 65%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.11072v1",
    "pdf_url": null
  },
  {
    "instance_id": "a1834f3e91c449a6bf6e86e909b039d8",
    "figure_id": "2310.16516v1-Figure3-1",
    "image_file": "2310.16516v1-Figure3-1.png",
    "caption": " Comparison among PFG, Ada-GWG, SVGD in conditioned diffusion example.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges the fastest in the conditioned diffusion example?",
    "answer": "Ada-GWG",
    "rationale": "The figure shows the log MMD (Minimum Mean Discrepancy) of the three algorithms as a function of the number of iterations. Ada-GWG has the lowest log MMD at the end of the training process, which indicates that it has converged the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.16516v1",
    "pdf_url": null
  },
  {
    "instance_id": "45a3702c59bf4eee9b91d330d4495565",
    "figure_id": "1909.01264v2-Figure15-1",
    "image_file": "1909.01264v2-Figure15-1.png",
    "caption": " Downstream performance vs. 1/(1 − ∆1) and ∆2 (GloVe embeddings). We plot the performance of compressed GloVe embeddings on question answering (SQuAD, left column) and sentiment analysis (SST-1, right column), in terms of the 1/(1 − ∆1) and ∆2 measures of compression quality. We can see that there are compressed embeddings with large values 1",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which compression method achieves the best F1 score on SQuAD?",
    "answer": "DCC.",
    "rationale": "The figure shows that DCC achieves the highest F1 score on SQuAD, across all values of 1/(1 − ∆1) and ∆2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.01264v2",
    "pdf_url": null
  },
  {
    "instance_id": "dc21d25d03a54a21b023ead25ff69ddd",
    "figure_id": "1910.09796v4-Figure3-1",
    "image_file": "1910.09796v4-Figure3-1.png",
    "caption": " Attention Weight Entropy on Evidence Graph, from KGAT and GAT, of graph edges and nodes. Uniform weights’ entropy is also shown for comparison. Less entropy shows more concentrated attention.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of attention mechanism (KGAT, GAT, or Uniform) leads to the most concentrated attention on the graph edges?",
    "answer": "KGAT.",
    "rationale": "The figure shows the entropy of the attention weights for each type of attention mechanism. Lower entropy indicates more concentrated attention. In the case of edge attention, KGAT has the lowest entropy (5.164), followed by GAT (5.707) and then Uniform (6.944). This means that KGAT assigns more weight to fewer edges compared to the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.09796v4",
    "pdf_url": null
  },
  {
    "instance_id": "c6addf9a098c433a8b3222a353a860cb",
    "figure_id": "2207.11388v2-Figure1-1",
    "image_file": "2207.11388v2-Figure1-1.png",
    "caption": " ERLE curves on the synthetic DT-EPC subset. Abrupt echo path change occurs at the shaded region.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is most robust to abrupt echo path changes?",
    "answer": "PNLMS",
    "rationale": "The ERLE curves for each algorithm show how well they perform in reducing echo. The PNLMS algorithm has the highest ERLE throughout the experiment, even during the abrupt echo path change, indicating it is the most robust to these changes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.11388v2",
    "pdf_url": null
  },
  {
    "instance_id": "716b01aec9c5475caeedd219c94c7cc1",
    "figure_id": "2205.00130v1-Figure1-1",
    "image_file": "2205.00130v1-Figure1-1.png",
    "caption": " Local model explanations need to be both correct and easily understandable. While much prior work (e.g., Zhou et al., 2022) has studied the former property, this paper focuses on the latter, which has thus far been largely ignored.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What are the two main properties of local model explanations that are important for model understanding? ",
    "answer": " Correctness and understandability. ",
    "rationale": " The figure shows that local model explanations need to be both correct and understandable in order for a user to understand the model. The figure also shows that correctness has been studied more extensively than understandability. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.00130v1",
    "pdf_url": null
  },
  {
    "instance_id": "44143fe05ca5493d942f98e5cb327fbb",
    "figure_id": "1903.02020v2-Figure5-1",
    "image_file": "1903.02020v2-Figure5-1.png",
    "caption": " Comparisons of different reward functions for selected tasks",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which reward function performs better for Task 14?",
    "answer": "ExtOnly performs better for Task 14.",
    "rationale": "The figure shows that the blue line (ExtOnly) is consistently higher than the orange line (Ext + Lang) for Task 14. This means that the ExtOnly reward function results in more successful episodes than the Ext + Lang reward function.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.02020v2",
    "pdf_url": null
  },
  {
    "instance_id": "23ac055f598c49c88de04b58c64661f6",
    "figure_id": "2302.05328v3-Figure9-1",
    "image_file": "2302.05328v3-Figure9-1.png",
    "caption": " Popularity distributions on Tencent.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which item group is the most popular in the training set?",
    "answer": "Item group 0.",
    "rationale": "The bar for item group 0 is the tallest in the training set plot, indicating that it has the highest average popularity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.05328v3",
    "pdf_url": null
  },
  {
    "instance_id": "98059c6783d846bd8679b670ad85841f",
    "figure_id": "2111.11297v2-Figure25-1",
    "image_file": "2111.11297v2-Figure25-1.png",
    "caption": " Third step of the tutorial solving with AI help",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the passage say about the Blue Paul Terrier?",
    "answer": "The Blue Paul Terrier is an extinct breed of dog.",
    "rationale": "The first sentence of the passage explicitly states this fact.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.11297v2",
    "pdf_url": null
  },
  {
    "instance_id": "8c93b7d980d646c2b74fe435cd2d0bf6",
    "figure_id": "2106.15147v2-Figure2-1",
    "image_file": "2106.15147v2-Figure2-1.png",
    "caption": " Top: Win matrices comparing pre-training methods against each other, and their improvement to existing solutions. Bottom: Box plots showing the relative improvement of different pre-training methods over baselines (y-axis is zoomed in). We see that SCARF pre-training adds value even when used in conjunction with known techniques.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pre-training method consistently outperforms the others?",
    "answer": "SCARF AE.",
    "rationale": "The box plots in the bottom row of the figure show that SCARF AE consistently achieves the highest relative gain across all conditions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15147v2",
    "pdf_url": null
  },
  {
    "instance_id": "371b7b36ed3946d9b84d075a8fa24d15",
    "figure_id": "2306.02592v1-Figure5-1",
    "image_file": "2306.02592v1-Figure5-1.png",
    "caption": " Additional analysis. (a) Significant test. The p-values are calculated using one-sided pair-wise T-tests (the alternative hypothesis is that the model performs worse than the model below it or on the right side of it). (b) Ablation studies. The metrics are ROC-AUC for Search-CTR, and macro-F1 for ESCvsI and Query2PT.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Query2PT task?",
    "answer": "GALM*rgat+",
    "rationale": "The bar chart in Figure (b) shows the performance of different models on three tasks: Search-CTR, ESCvsI, and Query2PT. The height of each bar represents the performance of the corresponding model on the corresponding task. For the Query2PT task, the bar for GALM*rgat+ is the highest, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.02592v1",
    "pdf_url": null
  },
  {
    "instance_id": "b81bcd01b2ed4841a28191c1a32ef597",
    "figure_id": "2209.04007v1-Figure2-1",
    "image_file": "2209.04007v1-Figure2-1.png",
    "caption": " Age classification accuracy (FairFace[27]) as a function of representation dimension k. (5 clients, 7 domains, heterogeneity parameter α = 1)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better, FedDAR-SA or FedDAR-WA?",
    "answer": "FedDAR-SA performs better.",
    "rationale": "The figure shows that the average domain test accuracy for FedDAR-SA is consistently higher than that of FedDAR-WA across all values of k.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.04007v1",
    "pdf_url": null
  },
  {
    "instance_id": "84079cf06a3145d3bb2d7192204248d5",
    "figure_id": "1805.03081v1-Figure4-1",
    "image_file": "1805.03081v1-Figure4-1.png",
    "caption": " The view prediction comparison against the baselines (Random and Farthest) and ShapeNet. (a): IoU values (the higher and the better) over the number of views. (b): Information gain (the lower and the better) as the decrease of Shannon Entropy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest Mean IoU for all numbers of views?",
    "answer": "\"Ours\"",
    "rationale": "The red line in Figure (a) is consistently higher than all other lines, indicating that \"Ours\" achieves the highest Mean IoU for all numbers of views.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.03081v1",
    "pdf_url": null
  },
  {
    "instance_id": "6952790943514c0d8d5dae771fae0e10",
    "figure_id": "2210.06201v2-Figure7-1",
    "image_file": "2210.06201v2-Figure7-1.png",
    "caption": " SID metric for different versions of DiffAN.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest SID values for the 20ER1 and 20SF1 experiments?",
    "answer": "DiffAN",
    "rationale": "The violin plot for DiffAN in the 20ER1 and 20SF1 experiments has the lowest median and the smallest range of values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06201v2",
    "pdf_url": null
  },
  {
    "instance_id": "0d561228555f45568fc6cb59d78c0325",
    "figure_id": "2110.06389v2-Figure6-1",
    "image_file": "2110.06389v2-Figure6-1.png",
    "caption": " Illustration of a synthetic pathway as a synthetic tree (A) and reaction templates (B & C). (A) is the synthetic tree of remdesivir, a drug authorized for emergency use to treat COVID-19. Different color box labels indicate different types of chemical nodes. (B) and (C) are examples of reaction templates for uni- and bi-molecular reactions, where SMARTS is a specific syntax for encoding reaction transforms.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two main types of reactions used in the synthesis of remdesivir?",
    "answer": "Uni-molecular and bi-molecular reactions.",
    "rationale": "The figure shows two reaction templates, one for a uni-molecular reaction (B) and one for a bi-molecular reaction (C). These templates represent the general types of reactions used in the synthesis of remdesivir, as shown in the synthetic tree (A).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.06389v2",
    "pdf_url": null
  },
  {
    "instance_id": "a78a4c41b62c4ceb98ba5fbc24a024a4",
    "figure_id": "2208.10547v1-Figure2-1",
    "image_file": "2208.10547v1-Figure2-1.png",
    "caption": " Comparison of attention scheme for a particular instance query (red) for recent transformer-based offline VIS and ours. Active objects (green) send information to the instance query (red) via self-/cross-attention or any combinations, while passive objects (grey) remain idle. Note that InstanceFormer drastically sparsifies Spatio-temporal attention by stressing the valuable past and the current information.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the approaches depicted in the figure uses a memory module to store past information?",
    "answer": "InstanceFormer.",
    "rationale": "The figure shows that InstanceFormer has a memory module (represented by the cylinder) in addition to the instance queries and frame. This memory module stores past information that can be used to attend to the current instance query.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.10547v1",
    "pdf_url": null
  },
  {
    "instance_id": "72496c7a959a42ab91298c835cc997e5",
    "figure_id": "2112.06733v4-Figure1-1",
    "image_file": "2112.06733v4-Figure1-1.png",
    "caption": " Plotting context and target word biases from BERT (black) and humans (blue) across popular context-aware lexical semantic datasets. The green shade and the yellow shade roughly indicate the areas for high target word bias and high context bias (>0.8). We would ideally want a dataset to lie towards the bottom left corner which is bias-free. The dashed red lines indicate 1.0 context (right) and 1.0 target word bias (top), implying a dataset is in effect dealt with by relying on context alone or target words alone.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following datasets is most likely to be dealt with by relying on context alone?",
    "answer": "WikiMed",
    "rationale": "The dashed red line on the right side of the figure indicates 1.0 context bias. This means that any dataset that falls on this line or to the right of it is likely to be dealt with by relying on context alone. WikiMed is the only dataset that falls on this line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.06733v4",
    "pdf_url": null
  },
  {
    "instance_id": "9bf3ae0a4a264c5fbf77317208507243",
    "figure_id": "1905.03329v1-Figure1-1",
    "image_file": "1905.03329v1-Figure1-1.png",
    "caption": " Random networks: Learned Wasserstein embeddings achieve lower distortion than Euclidean/hyperbolic embeddings. Hyperbolic embeddings outperform specifically on random trees.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of network exhibits the lowest distortion for all embedding types?",
    "answer": "Random small-world networks.",
    "rationale": "In the figure, we can see that the distortion for random small-world networks is consistently lower than the distortion for other types of networks, regardless of the embedding type used.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.03329v1",
    "pdf_url": null
  },
  {
    "instance_id": "96212a472e6c4b1ba560e590e90c3a50",
    "figure_id": "2302.07145v2-Figure3-1",
    "image_file": "2302.07145v2-Figure3-1.png",
    "caption": " Impacts of Accessible Target Data (p).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the tested methods performed the best with a limited amount of data?",
    "answer": "LightGCN",
    "rationale": "The plot shows the performance of different methods on the task of recommendation with different amounts of available data. LightGCN consistently outperforms the other methods when the amount of data is limited (20% and 40%).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.07145v2",
    "pdf_url": null
  },
  {
    "instance_id": "f86c9a04c8d8483894f1493ccc47f7cd",
    "figure_id": "2305.16834v1-Figure1-1",
    "image_file": "2305.16834v1-Figure1-1.png",
    "caption": " ZS-XLT: SRC-DEV vs. CA across various learning rates without a scheduler.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, SRC-DEV or CA, achieves higher NER scores with a learning rate of 2e-5?",
    "answer": "CA",
    "rationale": "The figure shows the NER scores for both SRC-DEV and CA across various learning rates. At a learning rate of 2e-5, CA achieves a higher NER score than SRC-DEV.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16834v1",
    "pdf_url": null
  },
  {
    "instance_id": "15445f502be945c29a78f04cb2645ffa",
    "figure_id": "2211.10287v1-Figure4-1",
    "image_file": "2211.10287v1-Figure4-1.png",
    "caption": " LPIPS versus SNR for different approaches. Our methods’ compression ratio are 1/3072 and 10/3072, while the compression ratio of Deep JSCC is 1/24",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach performs best at a high SNR (e.g., 20)?",
    "answer": "Deep JSCC.",
    "rationale": "At an SNR of 20, Deep JSCC has the lowest LPIPS score, indicating better performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.10287v1",
    "pdf_url": null
  },
  {
    "instance_id": "22ef906f805b494cb1052f0f86921320",
    "figure_id": "2006.06743v2-Figure4-1",
    "image_file": "2006.06743v2-Figure4-1.png",
    "caption": " Large dataset results across ε. We show the performance of SNG-DBSCAN and DBSCAN on five large datasets. SNG-DBSCAN values are averaged over 10 runs, and 95% confidence intervals (standard errors) are shown. We ran these experiments on a cloud environment and plot the adjusted RAND index score, adjusted mutual information score, runtime, and RAM used across a wide range of ε. DBSCAN was run with MinPts “ 10, and SNG-DBSCAN was run with MinPts “ maxp2, t10 ¨ suq for all datasets. No data is given for DBSCAN for ε settings requiring more than 750GB of RAM as it wasn’t possible for DBSCAN to run on these machines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better in terms of score and runtime across all datasets?",
    "answer": "SNG-DBSCAN",
    "rationale": "The plots show that SNG-DBSCAN consistently achieves higher scores than DBSCAN across all datasets and values of epsilon. Additionally, SNG-DBSCAN has a lower runtime than DBSCAN for most datasets and values of epsilon.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06743v2",
    "pdf_url": null
  },
  {
    "instance_id": "93fbe676809743b5897964bd42c946db",
    "figure_id": "1711.05345v3-Figure2-1",
    "image_file": "1711.05345v3-Figure2-1.png",
    "caption": " The performance of QACNN and MemN2N on different types of questions in TOEFL-manual with and without pre-training on MovieQA. ‘No’ in the parenthesis indicates the models are not pre-trained, while ‘Yes’ indicates the models are pre-trained on MovieQA.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on Type 3 questions when pre-trained on MovieQA?",
    "answer": "MemN2N",
    "rationale": "The figure shows that the testing accuracy of MemN2N (yes) is higher than QACNN (yes) for Type 3 questions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.05345v3",
    "pdf_url": null
  },
  {
    "instance_id": "0775bad5769249c089842dd7b8694dab",
    "figure_id": "2012.13841v1-Figure17-1",
    "image_file": "2012.13841v1-Figure17-1.png",
    "caption": " Learning curves for various Atari games with λ = 0.00000001.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which game shows the most consistent performance across epochs?",
    "answer": "Pong.",
    "rationale": "The plot for Pong shows a relatively smooth learning curve, with the score increasing steadily over time and with less variance than the other games.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.13841v1",
    "pdf_url": null
  },
  {
    "instance_id": "25568128490d466bb1ff4398cf2b4f06",
    "figure_id": "1805.07489v3-Figure6-1",
    "image_file": "1805.07489v3-Figure6-1.png",
    "caption": " Left: Contour entropy and query cost during the iterations of the CLoVER algorithm. Right: Reduction in contour entropy per unit query cost at every iteration. CLoVER explores IS1 to decrease the uncertainty about the location of the bifurcation before using evaluations of expensive IS0.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which information source (IS) does CLoVER explore first to decrease the uncertainty about the location of the bifurcation?",
    "answer": "IS1",
    "rationale": "The right plot shows that CLoVER explores IS1 before using evaluations of expensive IS0. This is because IS1 has a higher reduction in contour entropy per unit query cost than IS0 in the early iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.07489v3",
    "pdf_url": null
  },
  {
    "instance_id": "91e7cd39568e4acb861ffa4827081685",
    "figure_id": "2002.11537v4-Figure5-1",
    "image_file": "2002.11537v4-Figure5-1.png",
    "caption": " Further transfer learning — the dataset/configuration combo are reported in the captions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset and configuration combination results in the lowest CDSM objective value?",
    "answer": "CIFAR100 - ConvMLP-50",
    "rationale": "The CDSM objective value is the lowest for the CIFAR100 - ConvMLP-50 configuration, as shown in figure (g).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11537v4",
    "pdf_url": null
  },
  {
    "instance_id": "676df61db96f4f628d0e0aae4ae9964c",
    "figure_id": "1810.07917v2-Figure14-1",
    "image_file": "1810.07917v2-Figure14-1.png",
    "caption": " Throughput comparison (higher is better)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the Twitter-Higgs dataset when k is set to 50?",
    "answer": "HistApprox",
    "rationale": "The figure shows that HistApprox has the highest throughput when k is set to 50 on the Twitter-Higgs dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.07917v2",
    "pdf_url": null
  },
  {
    "instance_id": "f02c0d407d674837a3ba284b8b22a90c",
    "figure_id": "1907.00960v2-Figure1-1",
    "image_file": "1907.00960v2-Figure1-1.png",
    "caption": " Lean Point Networks (LPNs) can achieve higher point cloud segmentation accuracy while operating at substantially lower memory and inference time. (Top) Memory footprint and inference speed of LPN variants introduced in this work compared to the PointNet++ (PN++) baseline. (Middle) Improvements in accuracy for three segmentation benchmarks of increasing complexity. On the –most complex– PartNet dataset our deep network outperforms the shallow PointNet++ baseline by 3.4%, yielding a 9.7% relative increase. (Bottom) Part Segmentation by PointNet++ and Deep LPN.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture achieves the highest accuracy on the PartNet dataset?",
    "answer": "Deep LPN",
    "rationale": "The bar chart in the middle of the figure shows the accuracy of different network architectures on three datasets. The Deep LPN bar is the tallest for the PartNet dataset, indicating that it achieves the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.00960v2",
    "pdf_url": null
  },
  {
    "instance_id": "a811df59787b4d5489e65b796f4e33cd",
    "figure_id": "2002.07729v2-Figure4-1",
    "image_file": "2002.07729v2-Figure4-1.png",
    "caption": " Left: Cumulative distribution function of the normalized MSE for all conditions, Middle: Pairwise comparison matrix P for the methods, over all conditions. Element Pij denotes the percentage of times that method i outperforms method j. The last row shows the column average for each method, the lower the better. Right: Learning Curve for the Hybrid domain.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best according to the pairwise comparison matrix?",
    "answer": "WDR",
    "rationale": "The pairwise comparison matrix shows the percentage of times that one method outperforms another. The last row shows the column average for each method, and the lower the number, the better the performance. In this case, WDR has the lowest column average, indicating that it performs the best overall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.07729v2",
    "pdf_url": null
  },
  {
    "instance_id": "b68c3f6ffe164438b3423eb239f9269c",
    "figure_id": "2006.16531v3-Figure4-1",
    "image_file": "2006.16531v3-Figure4-1.png",
    "caption": " RBM GOF Test with different levels of perturbation noise. The black vertical line indicates the perturbation level at 0.01.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which test statistic has the highest power to reject the null hypothesis at a perturbation level of 0.01?",
    "answer": "Cauchy RFF",
    "rationale": "The figure shows the null rejection rate for different test statistics as a function of the perturbation level. The black vertical line indicates the perturbation level at 0.01. At this perturbation level, the Cauchy RFF test statistic has the highest null rejection rate, which means it has the highest power to reject the null hypothesis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.16531v3",
    "pdf_url": null
  },
  {
    "instance_id": "f0eb7d6546b94cac97ad36da7e357e0d",
    "figure_id": "2010.05862v1-Figure9-1",
    "image_file": "2010.05862v1-Figure9-1.png",
    "caption": " Visualizing samples generated on CIFAR-10 dataset corrupted with uniform noise outliers.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is more robust to noise?",
    "answer": "Robust WGAN.",
    "rationale": "The figure shows that the images generated by Robust WGAN are more realistic and less noisy than the images generated by DCGAN and WGAN. This suggests that Robust WGAN is more robust to noise.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.05862v1",
    "pdf_url": null
  },
  {
    "instance_id": "e2b0d8c0bbaf4261a8d8375253dfa67b",
    "figure_id": "2110.05064v3-Figure6-1",
    "image_file": "2110.05064v3-Figure6-1.png",
    "caption": " Potential energy surface scan of the nitrogen molecule. PESNet yields very similar but slightly higher (≈ 0.37 mEh) energies than FermiNet. Without the MetaGNN the accuracy drops significantly by ≈ 4.3 mEh on average. Reference data is taken from Le Roy et al. (2006); Gdanitz (1998); Pfau et al. (2020).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate results compared to the experimental data?",
    "answer": "FermiNet",
    "rationale": "The figure shows that the FermiNet curve is closest to the experimental data curve.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.05064v3",
    "pdf_url": null
  },
  {
    "instance_id": "8fa723f8cf2f4de39ea5b338acfb9c50",
    "figure_id": "2204.07931v1-Figure3-1",
    "image_file": "2204.07931v1-Figure3-1.png",
    "caption": " BEGIN and VRM breakdown of gold responses from CMU-DOG and TOPICALCHAT. The inner circle shows the breakdown of BEGIN classes and the outer shows the VRM types in each BEGIN type: Hallucination (red), Entailment (green), Partial Hallucination (yellow), Generic (pink), and Uncooperative (blue).",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common type of response generated by CMU-DOG, according to the figure?",
    "answer": "Hallucination",
    "rationale": "The figure shows that 61.4% of CMU-DOG responses are classified as Hallucination, which is the largest proportion of any response type.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.07931v1",
    "pdf_url": null
  },
  {
    "instance_id": "875c98e4e9d44bf6a7ec6d892ca4a43b",
    "figure_id": "2112.02862v1-Figure9-1",
    "image_file": "2112.02862v1-Figure9-1.png",
    "caption": " The visualization of features (left column is heatmap and right column is heatmap mixed with original images) extracted by ResNet-50 trained with baseline model, all and SelectAugment using Grad-Cam(Selvaraju et al. 2017). For images with complex backgrounds, SelectAugment can better help target network to locate foreground objects. Meanwhile, target network focus on more parts of the foreground object with the help of SelectAugment. In short, SelectAugment tends to help the target network focus on more discriminative areas.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which data augmentation technique helped the model focus on more discriminative areas of the foreground object?",
    "answer": "SelectAugment.",
    "rationale": "The figure shows that the models trained with SelectAugment (last column) have heatmaps that are more focused on the foreground object than the models trained with the baseline model (second column) or Mixup (third and fourth columns). This suggests that SelectAugment helps the model to learn more discriminative features of the foreground object.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.02862v1",
    "pdf_url": null
  },
  {
    "instance_id": "b9ca8f455d7d4f098a086abfb3912104",
    "figure_id": "1805.09921v4-Figure6-1",
    "image_file": "1805.09921v4-Figure6-1.png",
    "caption": " Results for ShapeNet view reconstruction for unseen objects from the test set (shown left). The model was trained to reconstruct views from a single orientation. Top row: images/views generated by a C-VAE model; middle row images/views generated by VERSA; bottom row: ground truth images. Views are spaced evenly every 30 degrees in azimuth.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model generated the images/views in the middle row?",
    "answer": "VERSA.",
    "rationale": "The caption states that the middle row of images/views was generated by VERSA.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.09921v4",
    "pdf_url": null
  },
  {
    "instance_id": "bb7a514faf104d1b80d09a85d7436c18",
    "figure_id": "2103.11320v2-Figure1-1",
    "image_file": "2103.11320v2-Figure1-1.png",
    "caption": " Negative and positive regard and sentiment results from ConceptNet and GenericsKB. We find outlier target groups with high regard and sentiment percentages that show the severity of overgeneralization issues. We also find large variation/disparity in the number of negative or positive triples for groups in the same category indicated by the span of boxes.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which group has the highest percentage of positive regard according to ConceptNet?",
    "answer": " Profession",
    "rationale": " The box plot for Profession in the ConceptNet Regard figure shows the highest median and upper quartile values, indicating that this group has the highest percentage of positive regard according to ConceptNet.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.11320v2",
    "pdf_url": null
  },
  {
    "instance_id": "9c38bf37bde64fa398cae57f412ff4d8",
    "figure_id": "2305.15871v3-Figure3-1",
    "image_file": "2305.15871v3-Figure3-1.png",
    "caption": " Posteriors obtained from NPE, RNPE, and our NPE-RS method for the Ricker model. We perform similar to NPE in the well-specified case, unlike RNPE. Under misspecification, NPE and RNPE posteriors drift away from θtrue, going even beyond the prior range (denoted by dashed gray lines) in the case of NPE. Our method is robust to model misspecification.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most robust to model misspecification?",
    "answer": "NPE-RS",
    "rationale": "The figure shows that as the degree of model misspecification increases (from left to right), the NPE and RNPE methods become increasingly biased, while NPE-RS remains relatively close to the true parameter values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.15871v3",
    "pdf_url": null
  },
  {
    "instance_id": "b7dab5180fa14bcb820b70add39dfacf",
    "figure_id": "2205.02456v1-Figure3-1",
    "image_file": "2205.02456v1-Figure3-1.png",
    "caption": " Accuracy breakdown over question semantic types on the GQA dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best for \"Global\" questions?",
    "answer": "DPT(MLM & ITM)",
    "rationale": "The bar for DPT(MLM & ITM) is the highest for \"Global\" questions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.02456v1",
    "pdf_url": null
  },
  {
    "instance_id": "b35f3ace8a1344b6bad692c092aa2120",
    "figure_id": "1809.06709v2-Figure1-1",
    "image_file": "1809.06709v2-Figure1-1.png",
    "caption": " (a, b, c): PPL (200 topics) by iDocNADE and DocNADE for each of the 50 held-out documents. The filled circle and symbols (T, R and S point to the document for which PPL differs by maximum, each for 20NewsGroups, Reuters21758 and SiROBs datsets, respectively. (d, e, f): NLL of each of the words in documents marked by T, R and S, respectively due to iDocNADE and DocNADE.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the largest difference in perplexity (PPL) between iDocNADE and DocNADE for a single document?",
    "answer": "SiROBs",
    "rationale": "The filled circle in each plot represents the document with the largest difference in PPL between iDocNADE and DocNADE. The plot for SiROBs shows the largest difference in PPL, with the filled circle located far away from the diagonal line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.06709v2",
    "pdf_url": null
  },
  {
    "instance_id": "6be65e8dbe464ce9ad36623ed87edcfd",
    "figure_id": "2106.13799v2-Figure6-1",
    "image_file": "2106.13799v2-Figure6-1.png",
    "caption": " Calibration on CIFAR10: Calibration plot of different ensembles of 100 ResNet18 trained on CIFAR10. The error bar represents one bootstrapping standard deviation (most are extremely small). The estimated CACE for each scenario is shown in Table 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which ensemble has the highest accuracy?",
    "answer": "AllDiff",
    "rationale": "The AllDiff plot has the highest accuracy for all confidence levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.13799v2",
    "pdf_url": null
  },
  {
    "instance_id": "bf2d1c00f498485996ba6ac7565d83e7",
    "figure_id": "1811.07579v2-Figure3-1",
    "image_file": "1811.07579v2-Figure3-1.png",
    "caption": " Active learning curves for CIFAR-100 dataset using various query functions, (a) softmax response, (b) MC-dopout, (c) coreset. In black (solid) – Active-iNAS (ours), blue (dashed) – Resnet-18 fixed architecture, and red (dashed) – A(Br, 1, 2) fixed.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three query functions, Softmax response, MC-dropout, or Coreset, has the highest test accuracy for the Active-iNAS model?",
    "answer": "Coreset",
    "rationale": "The plot in (c) shows that the black line (Active-iNAS) is higher than the other two lines for the majority of the data points, indicating that it has the highest test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.07579v2",
    "pdf_url": null
  },
  {
    "instance_id": "2f87a537462043b984781c515b09511c",
    "figure_id": "2306.04597v1-Figure3-1",
    "image_file": "2306.04597v1-Figure3-1.png",
    "caption": " Frequency of gender words on the StereoSet dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which gender is mentioned more frequently in the StereoSet dataset?",
    "answer": "Male",
    "rationale": "The word cloud shows that the words \"he,\" \"him,\" \"his,\" and \"man\" are all larger than the words \"she,\" \"her,\" and \"woman,\" indicating that they appear more frequently in the dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.04597v1",
    "pdf_url": null
  },
  {
    "instance_id": "e842ee093d8440288704b7fec6c7043e",
    "figure_id": "2208.14698v5-Figure4-1",
    "image_file": "2208.14698v5-Figure4-1.png",
    "caption": " Efficiency loss paths (i.e., regret plots) of BOCA compared to the results from Weissteiner et al. (2022a) of MVNNMLCA and NN-MLCA without any notion of uncertainty. Shown are averages with 95% CIs over 50 CA instances.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest efficiency loss?",
    "answer": "BOCA",
    "rationale": "The figure shows that the green line, which represents BOCA, is consistently below the other two lines, which represent MVNN-MLCA and NN-MLCA. This indicates that BOCA has the lowest efficiency loss.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.14698v5",
    "pdf_url": null
  },
  {
    "instance_id": "9e789fc560474b29b8c9b41e3cf3ade9",
    "figure_id": "2304.01199v2-Figure2-1",
    "image_file": "2304.01199v2-Figure2-1.png",
    "caption": " Class-wise performance on AVA: We show the performance of JMRN [53] and LART-pose on 60 AVA classes (average precision and relative gain). For pose based classes such as standing, sitting, and walking our 3D pose model can achieve above 60 mAP average precision performance by only looking at the 3D poses over time. By modeling multiple trajectories as input our model can understand the interactions among people. For example, activities such as dancing (+30.1%), martial art (+19.8%) and hugging (+62.1%) have large relative gains over state-of-the-art pose only model. We only plot the gains if it is above or below 1 mAP.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activity has the largest relative gain over the state-of-the-art pose-only model?",
    "answer": "Hugging",
    "rationale": "The figure shows the performance of JMRN and LART-pose on 60 AVA classes, with the relative gain indicated by the numbers above the bars. The activity with the largest relative gain is hugging, which has a gain of +62.1%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.01199v2",
    "pdf_url": null
  },
  {
    "instance_id": "15be432fc1e04a88b6ea1fb1a6de2c66",
    "figure_id": "2304.02786v1-Figure5-1",
    "image_file": "2304.02786v1-Figure5-1.png",
    "caption": " Loss values during the optimization process.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function decreases the most quickly during the optimization process?",
    "answer": "The misclassification loss.",
    "rationale": "The misclassification loss plot shows a sharp decrease in loss values within the first 20 epochs, while the other loss functions decrease more gradually.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.02786v1",
    "pdf_url": null
  },
  {
    "instance_id": "f836c5dc40b24b1ba68397ec647574c3",
    "figure_id": "1804.09081v4-Figure2-1",
    "image_file": "1804.09081v4-Figure2-1.png",
    "caption": " Progress of the Pareto front of LEMONADE during architecture search. The Pareto front gets more and more densely settled over the course of time. Very large models found (e.g., in generation 25) are discarded in a later generation as smaller, better ones are discovered. Note: generation 1 denotes the generation after one iteration of LEMONADE.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the number of parameters of the models as the validation error increases?",
    "answer": "The number of parameters decreases.",
    "rationale": "The plot shows that the number of parameters of the models decreases as the validation error increases. This is because the Pareto front is getting more and more densely settled over the course of time. This means that the algorithm is finding models with fewer parameters that perform just as well as the models with more parameters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.09081v4",
    "pdf_url": null
  },
  {
    "instance_id": "b061413edad94b098e3418fa0bff98d1",
    "figure_id": "2010.10677v2-Figure2-1",
    "image_file": "2010.10677v2-Figure2-1.png",
    "caption": " Average subjective score differences on VCTK, relative to clean audio, together with 95% confidence intervals. First column is the mean of each model over all audio clips.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which audio processing technique performs the best in terms of subjective score differences?",
    "answer": "Offline SEANet",
    "rationale": "The plot shows the average subjective score differences for different audio processing techniques. The Offline SEANet line is consistently higher than the other lines, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.10677v2",
    "pdf_url": null
  },
  {
    "instance_id": "2d54520ddaa84784b5ac871736264642",
    "figure_id": "2008.13363v2-Figure3-1",
    "image_file": "2008.13363v2-Figure3-1.png",
    "caption": " Comparing different gradient-based measures for the simple case of having two samples from the same class where a = ∇`1 and b = ∇`2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four measures is most sensitive to changes in the alignment of the two gradients?",
    "answer": "Alignment (Ours)",
    "rationale": "The Alignment (Ours) plot shows the greatest change in color across the range of cos(θa,b) values, indicating that it is the most sensitive to changes in the alignment of the two gradients. The other plots show less change in color across the range of values, indicating that they are less sensitive to changes in alignment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.13363v2",
    "pdf_url": null
  },
  {
    "instance_id": "ad3e9a4b1b2b440baa2ed671e76e36d4",
    "figure_id": "2110.01543v1-Figure8-1",
    "image_file": "2110.01543v1-Figure8-1.png",
    "caption": " Training deep neural networks for 80,120,160 epochs. We report the final test accuracy of AdaSAM for training 80,120,160 epochs at nearby point in the nested figure. The final test accuracy of SGDM for training 160 epochs is also reported for comparison.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer achieves the highest test accuracy on CIFAR100/ResNet18?",
    "answer": "AdaSAM with 160 epochs.",
    "rationale": "The test accuracy for AdaSAM with 160 epochs is 95.17%, which is higher than the test accuracy for all other optimizers and epoch lengths. This can be seen in the plot in figure (b).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.01543v1",
    "pdf_url": null
  },
  {
    "instance_id": "bf9f2c1a7f8e465fa5bf9d50ccb7c99d",
    "figure_id": "2011.00791v1-Figure3-1",
    "image_file": "2011.00791v1-Figure3-1.png",
    "caption": " Ablation study on two tasks: Walker2d and Swimmer.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best on the Walker2d-v2 task?",
    "answer": "C3PC-LM",
    "rationale": "The figure shows the average return for each algorithm on the Walker2d-v2 task. C3PC-LM has the highest average return, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.00791v1",
    "pdf_url": null
  },
  {
    "instance_id": "c077d072bd4c4a8eb0a0ac6d9a634862",
    "figure_id": "2309.04810v3-Figure9-1",
    "image_file": "2309.04810v3-Figure9-1.png",
    "caption": " Comparison between BE1 (in red) and BS1 (in blue) inside E2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approximation, BE1 or BS1, is more accurate within the domain of E2?",
    "answer": "BE1 is more accurate within the domain of E2.",
    "rationale": "The figure shows that BE1 (in red) is closer to the true function (in green) than BS1 (in blue) within the domain of E2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.04810v3",
    "pdf_url": null
  },
  {
    "instance_id": "5d5ddf6955a94c9088bdfe42b31fb592",
    "figure_id": "1910.12521v1-Figure1-1",
    "image_file": "1910.12521v1-Figure1-1.png",
    "caption": " Performance of stochastic EM methods for fitting a GMM. (Left) Precision (|µ(k) − µ?|2) as a function of the epoch elapsed. (Right) Number of iterations to reach a precision of 10−3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms converges to a solution with a precision of 10−3 the fastest?",
    "answer": "fiEM",
    "rationale": "The right panel of the figure shows the number of iterations needed to reach a precision of 10−3 for each algorithm. fiEM is the only algorithm that reaches this precision in less than 1000 iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.12521v1",
    "pdf_url": null
  },
  {
    "instance_id": "050589fd0a054e77aa225f14cbbddc4e",
    "figure_id": "2206.00706v2-Figure7-1",
    "image_file": "2206.00706v2-Figure7-1.png",
    "caption": " Empirical comparison of concentration bounds for beta distribution with parameters α and β and with the number of samples n = 100 and n = 1000. For p ∈ [0, 0.5], we take β = 5 and α ∈ [0.01, 5] while for p ∈ [0.5, 1], we take α = 5 and β ∈ [0.01, 5].",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which bound is the tightest for n = 100? ",
    "answer": " The kl bound.",
    "rationale": " The kl bound is the lowest of all the bounds for n = 100. This means that it provides the tightest concentration inequality for the beta distribution with these parameters. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.00706v2",
    "pdf_url": null
  },
  {
    "instance_id": "79bfde5f2fdf478bbf711ac21419ddb5",
    "figure_id": "2110.04844v3-Figure2-1",
    "image_file": "2110.04844v3-Figure2-1.png",
    "caption": " Movielens-1M dataset with FM and DeepFM model. CF-SGD significantly outperforms standard SGD, and is highly competitive against Adam, Adagrad.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer achieves the lowest training loss for the DeepFM model on the Movielens-1M dataset?",
    "answer": "Adam",
    "rationale": "The training loss for the DeepFM model is shown in Figure (c). The Adam optimizer (green line) achieves the lowest training loss, followed by CF-SGD (red line), SGD (orange line), and Adagrad (blue line).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.04844v3",
    "pdf_url": null
  },
  {
    "instance_id": "be23a6f6184844ef81aebc8c149f7ea6",
    "figure_id": "2205.01549v1-Figure3-1",
    "image_file": "2205.01549v1-Figure3-1.png",
    "caption": " Learned rational activation functions differ according to their place within the network and to the task they are trained for. Right: activation functions at different layers within adapters trained on the QNLI task. Left: activation functions trained at layer 2 of adapters trained on different tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task has the activation function with the largest value at x = 2?",
    "answer": "The `wnli` task.",
    "rationale": "The right panel of the figure shows the activation functions for different tasks at layer 2. The `wnli` activation function is the green line, and it has the highest value at x = 2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.01549v1",
    "pdf_url": null
  },
  {
    "instance_id": "29bf8f368f154ef8a1d9c96ca02bdcf8",
    "figure_id": "1904.03377v2-Figure10-1",
    "image_file": "1904.03377v2-Figure10-1.png",
    "caption": " SR results of the real image “Chip” with SR factor 4. The hand-craft kernel width suggested by SRMD is 1.5.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the sharpest image?",
    "answer": "The IKC method produced the sharpest image.",
    "rationale": "The IKC method produced the image with the most detail and the least amount of blur. This is evident when comparing the zoomed-in portions of the images. The LR image is very blurry, the ZSSR image is somewhat blurry, the SRMD image is slightly blurry, and the IKC image is the sharpest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03377v2",
    "pdf_url": null
  },
  {
    "instance_id": "d0b28796d19349beac91fc2a0d6460cb",
    "figure_id": "1809.07845v2-Figure13-1",
    "image_file": "1809.07845v2-Figure13-1.png",
    "caption": " Performance of trackers on each attribute using precision under protocol II. Best viewed in color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tracker performs best on the Aspect Change attribute?",
    "answer": "3:13-SiamRPN++",
    "rationale": "The precision plot for the Aspect Change attribute shows that 3:13-SiamRPN++ has the highest precision for all location error thresholds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.07845v2",
    "pdf_url": null
  },
  {
    "instance_id": "a84c09ec321f448bb2d9de64774780c7",
    "figure_id": "2309.14972v1-Figure7-1",
    "image_file": "2309.14972v1-Figure7-1.png",
    "caption": " We present qualitative examples of our method and PLAD [15]. SIRI outperforms PLAD and test-time rewriting improves it further.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method appears to be the most accurate at reconstructing the original object?",
    "answer": "SIRI + TTR",
    "rationale": "The \"Ground Truth\" column shows the original object, and the \"SIRI + TTR\" column shows the closest approximation to the original object.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.14972v1",
    "pdf_url": null
  },
  {
    "instance_id": "2f2575e472db434f95870b29ff704ab4",
    "figure_id": "2107.07746v3-Figure16-1",
    "image_file": "2107.07746v3-Figure16-1.png",
    "caption": " Visulization results of COS algorithm on class unicycle.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many unicycles are there in each image?",
    "answer": "There is one unicycle in each image.",
    "rationale": "The figure shows a collection of images of unicycles. Each image contains only one unicycle.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.07746v3",
    "pdf_url": null
  },
  {
    "instance_id": "773c30842cc146bb9f824147888fd157",
    "figure_id": "1812.00733v2-Figure13-1",
    "image_file": "1812.00733v2-Figure13-1.png",
    "caption": " Examples of object detection results on PASCAL VOC. The box colors indicate class categories.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object detection model performs the best on the PASCAL VOC dataset?",
    "answer": "Our model.",
    "rationale": "The figure shows examples of object detection results on the PASCAL VOC dataset for four different models: Clean, Distorted, RL-Restore, and Ours. The results show that our model is able to correctly identify more objects than the other models, and it also produces more accurate bounding boxes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.00733v2",
    "pdf_url": null
  },
  {
    "instance_id": "ca4a715e12524993a7adc7380c7da6f9",
    "figure_id": "2002.00652v2-Figure3-1",
    "image_file": "2002.00652v2-Figure3-1.png",
    "caption": " Different methods to incorporate recent h questions [xi−h, ...,xi−1]. (a) CONCAT: concatenate recent questions with xi as input; (b) TURN: employ a turn-level encoder to capture the inter-dependencies among questions in different turns; (c) GATE: use a gate mechanism to compute the importance of each question.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method allows the model to learn the inter-dependencies among questions in different turns?",
    "answer": "TURN",
    "rationale": "The TURN method uses a turn-level encoder to capture the inter-dependencies among questions in different turns. This is shown in the figure by the arrows connecting the encoder for each turn.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.00652v2",
    "pdf_url": null
  },
  {
    "instance_id": "f147db6449854fc88f2801f3a48bd7bb",
    "figure_id": "2110.12301v2-Figure6-1",
    "image_file": "2110.12301v2-Figure6-1.png",
    "caption": " Top-down views of the stimuli used in Experiment1. A. Practice stimuli. B. Test stimuli. ‘R’ indicates reflected. The red square is a floor mat indicating the starting location. The red diamonds are the rewards.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the difference between the environments labeled \"Env1\" and \"Env1R\"?",
    "answer": "Env1R is a mirror image of Env1.",
    "rationale": "The figure shows that the layout of the maze in Env1R is the same as the layout of the maze in Env1, but flipped horizontally.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.12301v2",
    "pdf_url": null
  },
  {
    "instance_id": "6929bc0323b34d179ff6f7e967821fa3",
    "figure_id": "2204.08781v1-Figure7-1",
    "image_file": "2204.08781v1-Figure7-1.png",
    "caption": " Train loss of NRDE3 and LORD2→3",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model, NRDE3 or LORD2→3, generally has a lower train loss for the BIDMC32HR dataset?",
    "answer": "LORD2→3",
    "rationale": "The figure shows the train loss for both models on the BIDMC32HR dataset for different values of P. In all cases, the blue line representing LORD2→3 is lower than the red line representing NRDE3, indicating that LORD2→3 has a lower train loss.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.08781v1",
    "pdf_url": null
  },
  {
    "instance_id": "c496553e11da4ee297be531080dec0c3",
    "figure_id": "2203.04450v3-Figure8-1",
    "image_file": "2203.04450v3-Figure8-1.png",
    "caption": " Ablation on (a) initial learning rate and (b) temperature. The results are based on CIFAR-100 (ID) averaged over 5 OOD test sets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which initial learning rate resulted in the highest FPR95?",
    "answer": "0.6",
    "rationale": "The figure shows the FPR95 for different initial learning rates. The highest FPR95 is achieved when the initial learning rate is 0.6.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.04450v3",
    "pdf_url": null
  },
  {
    "instance_id": "18e9608b19ff4f4fa68cae1d94485763",
    "figure_id": "2004.07453v2-Figure5-1",
    "image_file": "2004.07453v2-Figure5-1.png",
    "caption": " Instances with different labels are predicted with different degrees of confidence.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the least variation in confidence level across different BERT-large layers?",
    "answer": "MNLI.",
    "rationale": "The MNLI plot has the smallest range of values on the y-axis (confidence level), indicating that the confidence level does not vary as much across different BERT-large layers compared to the other datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.07453v2",
    "pdf_url": null
  },
  {
    "instance_id": "721a8fe100c646fd858eae3a594a5ece",
    "figure_id": "2305.19366v2-FigureD.2-1",
    "image_file": "2305.19366v2-FigureD.2-1.png",
    "caption": "Figure D.2: (a-b) Comparison of JSP-GFN with other Bayesian structure learning methods in terms of the expected-SHD to the ground truth graphs G⋆ used for data generation. (c-d) Comparison in terms of Area Under the ROC curve (AUROC) to the ground truth graphs G⋆.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of expected-SHD for linear Gaussian data?",
    "answer": "M-MC^3.",
    "rationale": "The boxplot for M-MC^3 in Figure D.2(a) has the lowest median and the smallest interquartile range, indicating that it has the lowest expected-SHD for linear Gaussian data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19366v2",
    "pdf_url": null
  },
  {
    "instance_id": "edc0d2143de2458dbd17ee49be13c7ab",
    "figure_id": "2011.15084v2-Figure4-1",
    "image_file": "2011.15084v2-Figure4-1.png",
    "caption": " Visualization of predictions from various models on a single scene from the Forking Paths dataset. The red-yellow heatmap corresponds to the visited state density from the 20 predicted trajectories from each model; yellow indicates higher density. The green lines are the ground-truth human annotated futures. LDS produces diverse forecasts that cover the diverse futures, while the other two methods appear to have collapsed to a single output.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models produces the most diverse forecasts?",
    "answer": "LDS.",
    "rationale": "The figure shows that the red-yellow heatmap for LDS is more spread out than the heatmaps for the other two models, indicating that LDS produces a wider range of possible future trajectories.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.15084v2",
    "pdf_url": null
  },
  {
    "instance_id": "c657c353483a47ba82e60d967a57f01a",
    "figure_id": "2112.00029v2-Figure12-1",
    "image_file": "2112.00029v2-Figure12-1.png",
    "caption": " Sparsity pattern candidate components: Local corresponds to local interaction of neighboring elements; Global (low-rank) involves the interaction between all elements and a small subset of elements; Butterfly captures the interaction between elements that are some fixed distance apart; Random is common in the pruning literature.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sparsity pattern candidate component captures the interaction between elements that are some fixed distance apart?",
    "answer": "Butterfly",
    "rationale": "The Butterfly sparsity pattern has non-zero entries only at specific distances from the diagonal, which indicates that it captures interactions between elements that are some fixed distance apart.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.00029v2",
    "pdf_url": null
  },
  {
    "instance_id": "f8f7f47fa2b84eeeaff1838433a3e588",
    "figure_id": "2206.13100v1-Figure1-1",
    "image_file": "2206.13100v1-Figure1-1.png",
    "caption": " Illustrations of A-stability, BIBO stability, and zero stability. (a) Blue lines denote an A-stable method: Regardless of the step size, the method approaches the exact solution (the solid green curve). Orange lines represent a non-A-stable method, which can only approach the exact solution if the step size is small. Note that dotted lines have a large step size. (b) The light shade represents the bound of the input; The dark shade represents the bound of the output. (c) The shades represent possible ranges of the difference magnitude between features with different initial values. It means that similar inputs generate similar outputs. In this work, we focus on zero stability and connect it with robustness and generalization.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stability concept focuses on the boundedness of the output?",
    "answer": "BIBO stability.",
    "rationale": "The figure shows that the output of a BIBO stable method is always bounded, regardless of the input. This is in contrast to A-stability and zero stability, which focus on the convergence of the method to the exact solution and the similarity of outputs for similar inputs, respectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.13100v1",
    "pdf_url": null
  },
  {
    "instance_id": "e7702c9e05604071b2adbedad1dd7180",
    "figure_id": "2110.14508v1-Figure9-1",
    "image_file": "2110.14508v1-Figure9-1.png",
    "caption": " Partial dependence plot of random forest outcome model in diabetes experiment",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which variable has the strongest relationship with the outcome variable?",
    "answer": "Treatment date.",
    "rationale": "The partial dependence plot for treatment date shows the largest change in the outcome variable as the value of treatment date changes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14508v1",
    "pdf_url": null
  },
  {
    "instance_id": "59727714d27d4b8ea3ccb593b2563a7c",
    "figure_id": "2106.12997v2-Figure3-1",
    "image_file": "2106.12997v2-Figure3-1.png",
    "caption": " Constrained multi-objective Bayesian Optimization tasks. MTGPs outperform batch models in both the (a) small batch (q = 2) and the (b) large batch (q = 10) setting. The latter was previously computationally infeasible for MTGPs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in the small batch setting (q = 2)?",
    "answer": "qEHVI-MTGP",
    "rationale": "In the small batch setting (q = 2), the qEHVI-MTGP method has the lowest log hypervolume difference, which indicates that it is performing the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.12997v2",
    "pdf_url": null
  },
  {
    "instance_id": "d3773f9ed39543a8bf6537bffee1942b",
    "figure_id": "2207.11122v1-Figure5-1",
    "image_file": "2207.11122v1-Figure5-1.png",
    "caption": " Per-service statistics at daily Peaks in real traces",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which service has the highest CPU core usage at peak?",
    "answer": "Service 07.",
    "rationale": "Figure (a) shows the mean CPU core usage at peaks for each service. Service 07 has the highest mean CPU core usage at peak, as shown by the red line in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.11122v1",
    "pdf_url": null
  },
  {
    "instance_id": "8f99e062fc5240ec8e41255f21585e27",
    "figure_id": "2306.00315v1-Figure4-1",
    "image_file": "2306.00315v1-Figure4-1.png",
    "caption": " The illustration of credit card scenario.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which treatment offered the largest cash coupon?",
    "answer": "Cash Coupons",
    "rationale": "The image shows that the Cash Coupons treatment offered a 2.00 yuan cash coupon, while the Interest-Free Coupons treatment offered a 2000 yuan interest-free coupon, and the Multiple Treatments treatment offered a 15000 yuan interest-free coupon and a 500 yuan cash coupon.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.00315v1",
    "pdf_url": null
  },
  {
    "instance_id": "3ceb1c1dee0e4531baaf71e260243a6b",
    "figure_id": "2108.11345v4-Figure3-1",
    "image_file": "2108.11345v4-Figure3-1.png",
    "caption": " A classification of the risk functionals that satisfy Lemma 2.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following risk functionals satisfies Lemma 2 but is neither DRF nor Con?",
    "answer": "Low",
    "rationale": "The figure shows that the set of risk functionals that satisfy Lemma 2 is a larger set that includes both DRF and Con.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.11345v4",
    "pdf_url": null
  },
  {
    "instance_id": "13acb6f1708043ab85596cee9cbbdcbf",
    "figure_id": "1912.01417v2-Figure3-1",
    "image_file": "1912.01417v2-Figure3-1.png",
    "caption": " `1 estimation error ∑ v∈V ‖xv − x?v‖1 (log10 scale) against number of agents for Total Variation Basis Pursuit Denoising solved using SPGL1 Python package (Yellow), group Lasso (blue) and Dirty Model of [24] (Green). Left: Path topology. Right: Balanced tree topology height 2 branching rate {2, 3, 4, 5, 6}. The same i.i.d. standard Gaussian matrix was associated to each node with Nv = 200 for v ∈ V , with parameters were d = 29, s = 25 and s′ = 4. Signal at the root x?1 and differences {x?v − x?w}(v,w)∈E sampled from {+1,−1} with no overlap in supports.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest L1 error for a fixed number of agents?",
    "answer": "Group Lasso",
    "rationale": "The blue line in the figure represents the L1 error for Group Lasso, and it is consistently lower than the other two lines (yellow and green) for all values of the number of agents.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.01417v2",
    "pdf_url": null
  },
  {
    "instance_id": "4bcec81f359749ce915e0b5da09ebc41",
    "figure_id": "1904.08918v1-Figure9-1",
    "image_file": "1904.08918v1-Figure9-1.png",
    "caption": " Performance vs. Resources for MobileNet: Average relative drop as a function of the number of parameters (left), and multiplyadds (right), for various points of operation of our method. Different backbones are indicated with different colors. MobileNet with our modulation is able to reach R-50 results for standard multi-tasking, by using much less parameters and computation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest relative drop for a fixed number of parameters when using the R-50 backbone?",
    "answer": "Adv SE RA",
    "rationale": "The left plot shows the relative drop as a function of the number of parameters for different methods and backbones. For the R-50 backbone (orange line), the Adv SE RA method has the lowest relative drop for any fixed number of parameters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.08918v1",
    "pdf_url": null
  },
  {
    "instance_id": "feca4b606ddd43fc83c2cd1c7d804c59",
    "figure_id": "1710.08377v1-Figure2-1",
    "image_file": "1710.08377v1-Figure2-1.png",
    "caption": " The accuracy for the UrbanSound8k dataset—aggregated over 10-fold cross validation—is shown for each of the convolutional network architectures, both with and without multiscale input using dilated convolutions. The body of each box plot denotes the 25th and 75th percentiles, the line in the body is the median, and the whiskers mark the most extreme observations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best with multiscale input?",
    "answer": "SB-CNN",
    "rationale": "The box plot for SB-CNN with multiscale input has the highest median accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1710.08377v1",
    "pdf_url": null
  },
  {
    "instance_id": "8312aef6f23941ab83d35d2a687de8b7",
    "figure_id": "2210.02186v3-Figure10-1",
    "image_file": "2210.02186v3-Figure10-1.png",
    "caption": " Statistics of period length in experimental datasets. We conduct FFT to the raw data and select the top-6 significant frequencies for each length-96 segment. Then, we record the corresponding period lengths and plot the normalized density for each period length.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the datasets has the most diverse period lengths?",
    "answer": "The Exchange dataset.",
    "rationale": "The Exchange dataset has the most spread-out distribution of period lengths, as evidenced by the wider range of bars in its histogram compared to the other datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.02186v3",
    "pdf_url": null
  },
  {
    "instance_id": "0c01eece18e7473f996046b3da26cb2b",
    "figure_id": "2301.02363v1-Figure3-1",
    "image_file": "2301.02363v1-Figure3-1.png",
    "caption": " The posters generated by various layout prediction methods.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method seems to be the most effective in generating realistic posters?",
    "answer": "Ours(K=30)",
    "rationale": "The posters generated by Ours(K=30) are the most realistic and visually appealing. They have a clear hierarchy of information, and the text and images are well-integrated. The other methods produce posters that are either too cluttered or too simple.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.02363v1",
    "pdf_url": null
  },
  {
    "instance_id": "6bb2a60db388456cb451167a40cc0552",
    "figure_id": "2009.08366v4-Figure4-1",
    "image_file": "2009.08366v4-Figure4-1.png",
    "caption": " MRR score on the validation dataset of Ruby for code search with varying length of input sequence.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on long sequences?",
    "answer": "GraphCodeBERT",
    "rationale": "The figure shows that GraphCodeBERT achieves the highest MRR score for all sequence lengths, and its performance continues to improve as the sequence length increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.08366v4",
    "pdf_url": null
  },
  {
    "instance_id": "da3e937f7cf942e2ac9f47d8c65879dd",
    "figure_id": "2102.01813v1-Figure2-1",
    "image_file": "2102.01813v1-Figure2-1.png",
    "caption": " Generating Area Multiple areas can be generated by combining adjacent items in a continuous memory block. For a 3x3 memory block, if we set the max area size to 2x2, the memory block can be divided into 1x1,1x2,2x1 and 2x2.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the maximum area size that can be generated from a 3x3 memory block, if the max area size is set to 2x2?",
    "answer": " 2x2",
    "rationale": " The figure shows that a 3x3 memory block can be divided into areas of different sizes, including 1x1, 1x2, 2x1, and 2x2. The largest area size is 2x2, which is the maximum area size that can be generated from a 3x3 memory block if the max area size is set to 2x2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.01813v1",
    "pdf_url": null
  },
  {
    "instance_id": "9086052e7e344f9e8cc45fe03444d327",
    "figure_id": "2010.01736v2-Figure16-1",
    "image_file": "2010.01736v2-Figure16-1.png",
    "caption": " Comparisons of different networks (VGG-13, Small CNN and ResNet-18) which GAIRAT and AT use on CIFAR-10 dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture achieved the highest standard test accuracy on natural test data?",
    "answer": "ResNet-18",
    "rationale": "The figure shows the standard test accuracy for each network architecture on natural test data. The ResNet-18 network achieved the highest accuracy, reaching approximately 80% after 100 epochs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.01736v2",
    "pdf_url": null
  },
  {
    "instance_id": "6eb79d63106d40738d7a86b0b3c28291",
    "figure_id": "2003.08752v1-Figure1-1",
    "image_file": "2003.08752v1-Figure1-1.png",
    "caption": " Visualizing diversity of image batch w.r.t different-level features. HMGAN1 refers λ(i) = 0 for ∀i, and HMGAN2 refers λ(i) = 1. We measured the diversity with LPIPS which uses Alexnet as feature decoder, it provides outputs from 5 layers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves higher diversity according to LPIPS?",
    "answer": "HMGAN2",
    "rationale": "The plot shows that the orange line, which represents HMGAN2, is consistently higher than the blue line, which represents HMGAN1, for all five diversity levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.08752v1",
    "pdf_url": null
  },
  {
    "instance_id": "5d97d3b3f4ea41608da63ff64d52ae78",
    "figure_id": "2211.03232v2-Figure3-1",
    "image_file": "2211.03232v2-Figure3-1.png",
    "caption": " The graph G used for Theorem D.1, showing the disconnected pieces G1, G ′ 1, G2, G ′ 2, . . . , Gm, G ′ m. Note that for all k, Gk and G′k are isomorphic.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the graphs G_k and G'_k?",
    "answer": "G_k and G'_k are isomorphic.",
    "rationale": "The caption states that \"for all k, G_k and G'_k are isomorphic.\" This means that the two graphs have the same structure, even though the labels on the nodes may be different.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.03232v2",
    "pdf_url": null
  },
  {
    "instance_id": "53a609e3b1df424789cf51128d5a50db",
    "figure_id": "2112.00305v1-Figure3-1",
    "image_file": "2112.00305v1-Figure3-1.png",
    "caption": " Comparison of different sampling techniques using AE trained on CelebA 64x64. Left to right: samples of (1) Two-Stage VAE (Dai & Wipf, 2019) (2) SRAEGlow (Kingma & Dhariwal, 2018) (3) SRAEGMM (4) SRAE NTK-kPF using 10k latent points.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampling technique produces the most realistic images?",
    "answer": "SRAENTK-kPF",
    "rationale": "The images produced by SRAENTK-kPF are the most realistic, as they are the most detailed and have the least amount of blur.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.00305v1",
    "pdf_url": null
  },
  {
    "instance_id": "54c8c6de4c8a409fba92eaaa48494326",
    "figure_id": "2104.06392v3-Figure11-1",
    "image_file": "2104.06392v3-Figure11-1.png",
    "caption": " Top row: the initial program output shape (gray) and target shape (yellow) for each task in our goal-directed editing study. Bottom row: plots of how quickly participants were able to edit a program’s parameters to match the target shape, with 95% confidence intervals shown. The x axis is time elapsed in minutes, while the y axis is the mean of the running minimum of each participant’s corner distance to the target shape. In general, participants using ShapeMOD macros more quickly converged to the target shape and achieved a closer fit. To allow users to take breaks between tasks, time starts when the user makes their first edit for each task .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which editing method led to a closer fit to the target shape in Task 4?",
    "answer": "ShapeMOD macros",
    "rationale": "The plot for Task 4 shows that the orange line (ShapeMOD) is consistently below the blue line (Baseline), indicating that participants using ShapeMOD macros achieved a closer fit to the target shape than those using the baseline method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.06392v3",
    "pdf_url": null
  },
  {
    "instance_id": "665aa41ac6c043b2b8d3eb64873b1b67",
    "figure_id": "2005.13117v4-Figure8-1",
    "image_file": "2005.13117v4-Figure8-1.png",
    "caption": " Visualization of GA-SPIN training procedures. We show the input image and the rectified images after 3 and 5 epoches in each line, respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training dataset produces better results after 5 epochs of training with GA-SPIN?",
    "answer": "CUTE80",
    "rationale": "The rectified images after 5 epochs of training on CUTE80 are more clear and readable than the rectified images after 5 epochs of training on ICDAR2015.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.13117v4",
    "pdf_url": null
  },
  {
    "instance_id": "3f478611159043d6b6edd3016dc818c3",
    "figure_id": "2205.07246v3-Figure3-1",
    "image_file": "2205.07246v3-Figure3-1.png",
    "caption": " How FreeMatch works in STL-10 with 40 labels, compared to others. (a) Class-average confidence threshold; (b) class-average sampling rate; (c) convergence speed in terms of accuracy; (d) confusion matrix, where fading colors of diagonal elements refer to the disparity of accuracy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods, FixMatch, Dash, FlexMatch, or FreeMatch, has the highest accuracy at 500k iterations?",
    "answer": "FreeMatch.",
    "rationale": "The accuracy plot (c) shows that FreeMatch has the highest accuracy at 500k iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.07246v3",
    "pdf_url": null
  },
  {
    "instance_id": "24abc7a4a81449ddb89ff0cb1af2f53b",
    "figure_id": "2105.09384v2-Figure3-1",
    "image_file": "2105.09384v2-Figure3-1.png",
    "caption": " Efficiency comparison",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two algorithms is more efficient in terms of time per iteration as the number of nodes increases?",
    "answer": "GaSoLiNe-LR",
    "rationale": "The plot shows that GaSoLiNe-LR takes less time per iteration than GaSoLiNe for all numbers of nodes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.09384v2",
    "pdf_url": null
  },
  {
    "instance_id": "0d56f358a1f74de2aca393f726cd2c94",
    "figure_id": "2102.07074v4-Figure5-1",
    "image_file": "2102.07074v4-Figure5-1.png",
    "caption": " Left: training dynamic with training epochs for both TransGAN and MSG-GAN on CelebA-HQ (256× 256). Right: Interpolation on latent space produced by TransGAN.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two models, TransGAN or MSG-GAN, produces more realistic faces at earlier epochs?",
    "answer": "TransGAN",
    "rationale": "The images generated by TransGAN at epochs 15 and 40 are more realistic than the images generated by MSG-GAN at the same epochs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.07074v4",
    "pdf_url": null
  },
  {
    "instance_id": "b5096316f9884a078b96b52004ca443e",
    "figure_id": "1910.09652v4-Figure3-1",
    "image_file": "1910.09652v4-Figure3-1.png",
    "caption": " Test accuracy and Training accuracy for classification on Cifar10 (top) and Cifar100 (bottom) in both the ill-conditioned case (left side) and well-conditioned case (right side) for different optimization methods. on Cifar10 Results are averaged over 5 independent runs except for KFAC and eKFAC.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization method achieves the highest test accuracy on Cifar100 in the ill-conditioned case?",
    "answer": "KFAC",
    "rationale": "The figure shows the test accuracy for different optimization methods on Cifar100 in the ill-conditioned case. The KFAC curve is the highest, indicating that it achieves the highest test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.09652v4",
    "pdf_url": null
  },
  {
    "instance_id": "73db27937e67480fb30fc7a2500517f8",
    "figure_id": "2305.19753v2-Figure19-1",
    "image_file": "2305.19753v2-Figure19-1.png",
    "caption": " In and out of distribution linear probing performance for VGG-19 trained on CIFAR-100. The shaded area depicts the tunnel, the red dashed line depicts the numerical rank and the blue curve depicts linear probing accuracy (in and out of distribution) respectively. Out-of-distribution performance is computed on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer of the VGG-19 network trained on CIFAR-100 has the highest linear probing accuracy in distribution?",
    "answer": "Layer 10.",
    "rationale": "The blue curve in Figure (a) shows the linear probing accuracy for each layer of the network in distribution. The highest point on the curve is at Layer 10, which corresponds to an accuracy of approximately 0.7.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19753v2",
    "pdf_url": null
  },
  {
    "instance_id": "402ee669a8be4095bdaa56bdadc75487",
    "figure_id": "2301.12623v2-Figure1-1",
    "image_file": "2301.12623v2-Figure1-1.png",
    "caption": " Comparison of FedPass with baseline defense methods against Model Inversion attack [He et al., 2019] (the first line), CAFE attack [Jin et al., 2021] (the second line), and Model Completion attack [Fu et al., 2022a] (the third line) in terms of their main task accuracy (the higher the better, green) and data (feature or label) recovery error (the higher the better, blue) on ResNet-CIFAR10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which defense method is most effective against Model Completion attack?",
    "answer": "FedPass.",
    "rationale": "The figure shows the main task accuracy and data recovery error for different defense methods against different attacks. For the Model Completion attack (the third line), FedPass has the highest main task accuracy and the lowest label recovery error, indicating that it is the most effective defense method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.12623v2",
    "pdf_url": null
  },
  {
    "instance_id": "bf068a31aace47dd85dc45c5136684df",
    "figure_id": "2112.01388v1-Figure6-1",
    "image_file": "2112.01388v1-Figure6-1.png",
    "caption": " Average reward curves (max over steps) for an RPP-EMLP applied to the policy π only, as well as an RPP-EMLP for both the policy π and the critic Q. Mean and standard deviation taken over 4 trials shown in the shaded region. Only minor performance gains are achieved if using RPP for the policy only, however this variant is more stable and can to train on Humanoid-v2 without diverging.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best on the Humanoid-v2 environment?",
    "answer": "RPP π & Q",
    "rationale": "The figure shows that RPP π & Q achieves the highest average return on the Humanoid-v2 environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.01388v1",
    "pdf_url": null
  },
  {
    "instance_id": "a856b2ec83254812820bba35f828bab9",
    "figure_id": "1902.05454v3-Figure3-1",
    "image_file": "1902.05454v3-Figure3-1.png",
    "caption": " Empirical runtime variation for different solvers and input distributions. For given δ, each plot shows the fraction of configurations which are (ε, δ)-optimal for different values of ε; data from Hutter et al. (2014). (top) SPEAR SAT solver configurations on SWV for various δ. (bottom) SPEAR on IBM instances and CPLEX MIP solver on various distributions, for fixed values of δ.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which solver has the largest proportion of configurations that are (ε, δ)-optimal for a given value of ε?",
    "answer": "CPLEX-CORLAT.",
    "rationale": "The figure shows that the CPLEX-CORLAT solver has the steepest curve, which means that it has the largest proportion of configurations that are (ε, δ)-optimal for a given value of ε.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.05454v3",
    "pdf_url": null
  },
  {
    "instance_id": "a690410eb2b54e169ca713d0958d8aaf",
    "figure_id": "2109.05750v4-Figure10-1",
    "image_file": "2109.05750v4-Figure10-1.png",
    "caption": " The layout of a single image group example in our user study. The displaying order of the composite input and the harmonization results is randomly shuffled without annotations.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different images are shown in the figure?",
    "answer": "5",
    "rationale": "The figure shows a single image group example from a user study. It consists of five different images of the same scene, with different color and lighting variations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.05750v4",
    "pdf_url": null
  },
  {
    "instance_id": "0c2696ab873a453aab7365bb1a8a3965",
    "figure_id": "1906.04716v3-Figure5-1",
    "image_file": "1906.04716v3-Figure5-1.png",
    "caption": " Attentions from each self-attention block of Transformer trained for graph reconstruction. Code starting with ‘D’ are diagnosis codes, ‘T’ treatment codes, ‘L’ lab codes. The diagnosis code with the red background D 199 is attending to the other features. The red bars indicate the codes that are actually connected to D 199, and the blue bars indicate the attention given to all codes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which self-attention block shows the strongest attention to the diagnosis code D_199?",
    "answer": "Self-attention Block 2",
    "rationale": "The red bars in each self-attention block indicate the attention strength of the diagnosis code D_199 to other codes. The red bar in Self-attention Block 2 is the longest, indicating the strongest attention.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04716v3",
    "pdf_url": null
  },
  {
    "instance_id": "eeec764a39b445c0b87734a0992316c4",
    "figure_id": "2007.07206v4-Figure5-1",
    "image_file": "2007.07206v4-Figure5-1.png",
    "caption": " Few-shot generalization performance on the interpolation (2 left) and extrapolation (2 right) tasks. Green line shows a threshold reward. 100 steps are used for adaptation to the evaluation environments.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed better on the extrapolation tasks?",
    "answer": "HIP-BMDP",
    "rationale": "The figure shows that the HIP-BMDP algorithm achieved a higher episodic reward than the PEARL algorithm on both extrapolation tasks (Walker-Walk-v1 and Cartpole-Swingup-v0).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.07206v4",
    "pdf_url": null
  },
  {
    "instance_id": "9c4fe2f789bd49a2ae323683c0520cb2",
    "figure_id": "2003.04448v2-Figure2-1",
    "image_file": "2003.04448v2-Figure2-1.png",
    "caption": " Decomposition results of SRN (b) and Baseline (c) models over a sample input (a) in the image reconstruction task with the synthetic Circles Dataset. The SRN can successfully decompose the image.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model, SRN or Baseline, is better at decomposing the image into its constituent parts?",
    "answer": "SRN",
    "rationale": "The SRN model is able to successfully decompose the image into its constituent parts, while the Baseline model is not. This can be seen in the figure, where the SRN model produces a clear and accurate decomposition of the image, while the Baseline model produces a noisy and inaccurate decomposition.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.04448v2",
    "pdf_url": null
  },
  {
    "instance_id": "44a3614da78343c8bf56de245b351faf",
    "figure_id": "2305.11275v2-FigureB.1-1",
    "image_file": "2305.11275v2-FigureB.1-1.png",
    "caption": "Figure B.1: V1 alignment Brain-Scores for 20 different hidden layers of ResNet50. In the plot above, readout location ‘X.Y’ denotes that artificial V1 activity was evaluated from residual block ‘Y’ of residual layer ‘X’. Readout location suffixed with ‘.d’ correspond to downsampling layers of the associated residual bottleneck. Highest V1 overall score came from block 3 of residual layer 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which readout location in ResNet50 has the highest V1 Overall score?",
    "answer": "Readout location 1.3",
    "rationale": "The V1 Overall score is represented by the blue line in the plot. The highest point on the blue line corresponds to readout location 1.3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.11275v2",
    "pdf_url": null
  },
  {
    "instance_id": "4d7b4f36a9ac40788267992203c69f07",
    "figure_id": "2104.08524v1-Figure3-1",
    "image_file": "2104.08524v1-Figure3-1.png",
    "caption": " Results when training with translations obtained from two translation services: Google Translate and DeepL. Translate-to-EN standard setup.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which translation service performed better according to the plot?",
    "answer": "DeepL",
    "rationale": "The plot shows the accuracy of the translation system for different languages. The blue bars represent the accuracy when using DeepL, and the orange bars represent the accuracy when using Google Translate. For every language, the blue bar is higher than the orange bar, which indicates that DeepL performs better.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.08524v1",
    "pdf_url": null
  },
  {
    "instance_id": "812060cdfa504d23ae58aef062e9bd51",
    "figure_id": "2307.09721v1-Figure7-1",
    "image_file": "2307.09721v1-Figure7-1.png",
    "caption": " Distribution of sentence length for three datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the shortest sentences on average?",
    "answer": "WikiDiverse",
    "rationale": "The figure shows the distribution of sentence lengths for each dataset. The x-axis shows the number of words per sentence, and the y-axis shows the density of sentences with that length. The WikiDiverse dataset has the most sentences with fewer than 10 words, and the fewest sentences with more than 20 words.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.09721v1",
    "pdf_url": null
  },
  {
    "instance_id": "e6f06f2cb0d845f4a2e985c5b649bb9d",
    "figure_id": "2006.15418v1-Figure8-1",
    "image_file": "2006.15418v1-Figure8-1.png",
    "caption": " One model, many domains and applications. A single model is capable of performing these tasks over videos from many diverse domains (animal movement, physics experiments, humans manipulating objects, people exercising, child swinging) in a classagnostic manner. Please see the project webpage for videos showcasing these tasks.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following tasks can the model perform?\n\n(a) Counting repetitions of an action.\n(b) Detecting changes in the speed of an action.\n(c) Retrieving similar frames from different periods of a video.\n(d) All of the above.",
    "answer": "(d) All of the above.",
    "rationale": "The figure shows that the model can be used for a variety of tasks, including counting repetitions of an action (e.g., counting the number of times a bird flaps its wings), detecting changes in the speed of an action (e.g., detecting that a person is running faster), and retrieving similar frames from different periods of a video (e.g., retrieving frames of a child swinging on a swing, even though the child's appearance changes throughout the video).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.15418v1",
    "pdf_url": null
  },
  {
    "instance_id": "d31f0937d56a4837b37b1b1ab05bdac1",
    "figure_id": "1807.01659v1-Figure3-1",
    "image_file": "1807.01659v1-Figure3-1.png",
    "caption": " Samples in four training datasets.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset contains the most diverse set of images?",
    "answer": "The \"Bags\" dataset.",
    "rationale": "The \"Bags\" dataset contains images of various types of bags, including clutches, totes, and backpacks, while the other datasets contain images of a single type of object (e.g., handwritten digits, street view house numbers, shoes).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1807.01659v1",
    "pdf_url": null
  },
  {
    "instance_id": "f420841c01c6412c9e3075ddc49a5f25",
    "figure_id": "2108.12870v2-Figure3-1",
    "image_file": "2108.12870v2-Figure3-1.png",
    "caption": " Rouge-1 score vs. the number of selected sentences.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does blocking improve ROUGE-1 score?",
    "answer": "No, blocking does not improve ROUGE-1 score.",
    "rationale": "The figure shows that the ROUGE-1 score is higher for the \"w/o blocking\" condition than for the \"blocking\" condition for both CNN and DailyMail datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.12870v2",
    "pdf_url": null
  },
  {
    "instance_id": "0a8d7ca7d9c8437e9db97fd2cf79c585",
    "figure_id": "1905.13021v1-Figure2-1",
    "image_file": "1905.13021v1-Figure2-1.png",
    "caption": " Comparison of the test error-rates on adversarial examples calculated by PGM [34], under `2-norm constraint.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on MNIST, SVHN, and CIFAR-10 datasets?",
    "answer": "F-SSDRL performs the best on all three datasets.",
    "rationale": "The figure shows the error rate of different methods on three datasets. The error rate of F-SSDRL is lower than the error rate of other methods on all three datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.13021v1",
    "pdf_url": null
  },
  {
    "instance_id": "54ffb4cda4f34f62a94dc1ddd876a673",
    "figure_id": "2102.00411v1-Figure4-1",
    "image_file": "2102.00411v1-Figure4-1.png",
    "caption": " Visual comparison of matching results using RANSAC, CN-Net and our method. Images are taken from YFCC100M&SUN3D and COLMAP datasets. Correspondences are in green if they are inliers, and in red otherwise. Best viewed in color.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method seems to perform the best in terms of matching accuracy?",
    "answer": "Ours.",
    "rationale": "The figure shows that our method has the most green lines, which indicate inliers, and the least red lines, which indicate outliers. This suggests that our method is more accurate than the other methods in terms of matching correspondences.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.00411v1",
    "pdf_url": null
  },
  {
    "instance_id": "ef8e6314754348c8a9cef2f619628db4",
    "figure_id": "2106.01345v2-Figure6-1",
    "image_file": "2106.01345v2-Figure6-1.png",
    "caption": " Histogram of steps to reach the goal node for random walks on the graph, shortest possible paths to the goal, and attempted shortest paths generated by the transformer model. ∞ indicates the goal was not reached during the trajectory.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most likely to reach the goal in the fewest steps?",
    "answer": "The shortest path method.",
    "rationale": "The shortest path method has the highest proportion of paths that reach the goal in 1 step, and the lowest proportion of paths that reach the goal in 8 or more steps. This indicates that the shortest path method is more likely to reach the goal in fewer steps than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01345v2",
    "pdf_url": null
  },
  {
    "instance_id": "a1f91d3aa5c24182a64e360b1cb78250",
    "figure_id": "2306.07117v1-Figure3-1",
    "image_file": "2306.07117v1-Figure3-1.png",
    "caption": " The figure presents the trajectory of new offers in the two treatments. In 3a and 3c, each line represents a sequence of new offers exchanged between buyer and seller in a single negotiation. Only negotiations ending in agreement are included. Figure 3b presents the absolute differences in consecutive new offers under both treatments. Each dot represents an absolute difference in consecutive new offers within a single bargaining session.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which treatment resulted in a larger decrease in the absolute difference between consecutive new offers?",
    "answer": "Natural Language.",
    "rationale": "The figure shows that the absolute difference between consecutive new offers decreases more quickly and to a lower level in the Natural Language treatment than in the Alternating Offers treatment. This suggests that the Natural Language treatment leads to more efficient bargaining.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.07117v1",
    "pdf_url": null
  },
  {
    "instance_id": "40faf9c474b740819256fe0b872d3a09",
    "figure_id": "1905.07503v1-Figure6-1",
    "image_file": "1905.07503v1-Figure6-1.png",
    "caption": " The precision and recall comparison with graph-based multiview learning methods under PSB.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of precision and recall?",
    "answer": "3DViewGraph.",
    "rationale": "The black line, representing 3DViewGraph, is above the other lines, meaning it achieves higher precision at any given recall level.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.07503v1",
    "pdf_url": null
  },
  {
    "instance_id": "0130d2d8248e44f1a58f2c20ee96c8ef",
    "figure_id": "2102.04095v1-Figure3-1",
    "image_file": "2102.04095v1-Figure3-1.png",
    "caption": " Impact of embedding dimension.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest recall at 5?",
    "answer": "NYC",
    "rationale": "The figure shows the recall at 5 for four different datasets: Gowalla, TKY, SIN, and NYC. The NYC dataset has the highest recall at 5, as shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.04095v1",
    "pdf_url": null
  },
  {
    "instance_id": "b9a10bd4a70a491fb0c870e2c80f059f",
    "figure_id": "1811.00908v3-Figure1-1",
    "image_file": "1811.00908v3-Figure1-1.png",
    "caption": " Training data with non-Gaussian noise (blue dots), predicted median (solid line), 65% and 80% quantiles (dashed lines), aleatoric uncertainty or 95% prediction interval (gray shade, estimated by SQR Sec. 2), and epistemic uncertainty (pink shade, estimated by orthonormal certificates Sec. 3).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of uncertainty is larger in the left half of the plot?",
    "answer": "Epistemic uncertainty",
    "rationale": "The pink shaded region, which represents epistemic uncertainty, is larger in the left half of the plot than the gray shaded region, which represents aleatoric uncertainty.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.00908v3",
    "pdf_url": null
  },
  {
    "instance_id": "b1ca5ad091a2495eac9729ff1c5be3e5",
    "figure_id": "2203.11197v2-Figure17-1",
    "image_file": "2203.11197v2-Figure17-1.png",
    "caption": " Architecture diagram modified from BabyAI 1.1 [19]. For the Point Maze and Ant envs, which do not have image input or instructions, the advice is linearly embedded, concatenated with the state, and passed to MLP actor and critic models.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the learning rate of the Adam optimizer used in this architecture?",
    "answer": "The learning rate of the Adam optimizer is 1e-3.",
    "rationale": "The table in the image lists the hyperparameters used in the architecture, including the learning rate for the Adam optimizer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.11197v2",
    "pdf_url": null
  },
  {
    "instance_id": "c471c4125ab040508dbbe2c91bcbbbaf",
    "figure_id": "2205.05071v4-Figure1-1",
    "image_file": "2205.05071v4-Figure1-1.png",
    "caption": " Example usage of our proposed climate performance model card (§5), on the Hugging Face Hub.2",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How much CO2 was emitted to train the final model?",
    "answer": "15.79 kg",
    "rationale": "The figure shows a model card for a language model called \"climatebert\". The model card includes information about the model's performance, as well as its environmental impact. One of the metrics listed is the amount of CO2 emitted to train the model, which is 15.79 kg.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.05071v4",
    "pdf_url": null
  },
  {
    "instance_id": "d70912c03b644452920b342eaec8475f",
    "figure_id": "1906.10198v1-Figure9-1",
    "image_file": "1906.10198v1-Figure9-1.png",
    "caption": " Confusion matrix of the acoustic model BACO-1.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which emotion was most accurately classified by the acoustic model BACO-1?",
    "answer": "sadness",
    "rationale": "The confusion matrix shows that the acoustic model BACO-1 correctly classified 106 instances of sadness, which is the highest number of correct classifications for any emotion.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.10198v1",
    "pdf_url": null
  },
  {
    "instance_id": "f97f87e9f37940338f21812a24f3c587",
    "figure_id": "1912.13023v1-Figure1-1",
    "image_file": "1912.13023v1-Figure1-1.png",
    "caption": " Example: A Goodreads user likes three different lists (A, B, and C), each composed of a collection of books. Our proposedmodel exploits this user-list-itemhierarchical structure to recommend additional lists (e.g., X).",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the user's preferences, what book list is most likely to be recommended to the user?",
    "answer": "Book List X",
    "rationale": "The figure shows that the user has liked three book lists, A, B, and C. Book List X contains books that are similar to the books in the user's liked lists, such as \"The Lord of the Rings\" and \"The Odyssey.\" This suggests that the user is likely to enjoy the books in Book List X.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.13023v1",
    "pdf_url": null
  },
  {
    "instance_id": "bce18ab53e9c4597bc1ef9e46cf74649",
    "figure_id": "2205.10964v2-Figure5-1",
    "image_file": "2205.10964v2-Figure5-1.png",
    "caption": " Representations in layers five, seven, and nine projected linearly onto the first two LDA axes that separate languages in layer eight. Representations remained relatively unchanged along these axes as they passed through middle layers of the model. Detailed plots are included in Appendix D.1.",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which layer of the model shows the most distinct separation between language families?",
    "answer": " Layer 9.",
    "rationale": " The figure shows the representations of languages in layers 5, 7, and 9 of a neural network model, projected onto the first two LDA axes that separate languages in layer 8. In Layer 9, the different language families are more clearly separated than in the other layers. This suggests that the model is able to learn increasingly abstract representations of languages as it processes them through its layers.\n\n**Figure type:** Plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.10964v2",
    "pdf_url": null
  },
  {
    "instance_id": "a55d340fc70649d1ab876a7557539c34",
    "figure_id": "2003.01908v2-Figure12-1",
    "image_file": "2003.01908v2-Figure12-1.png",
    "caption": " Results for certifying a black-box ResNet-18 ImageNet classifier.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best when σ=0.5?",
    "answer": "Stab+MSE-resnet50",
    "rationale": "The plot in (b) shows that the Stab+MSE-resnet50 method has the highest certified accuracy for all values of the f2 radius when σ=0.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.01908v2",
    "pdf_url": null
  },
  {
    "instance_id": "a0e3c717ab3c46f7902e26683c49b4f1",
    "figure_id": "1811.09720v1-Figure2-1",
    "image_file": "1811.09720v1-Figure2-1.png",
    "caption": " Dataset debugging performance for several methods. By inspecting the training points using the representer value, we are able to recover the same amount of mislabeled training points as the influence function (right) with the highest test accuracy compared to other methods (left).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most efficient at fixing mislabeled training points?",
    "answer": "Ours.",
    "rationale": "The right plot shows that our method is able to fix the same amount of mislabeled training points as the influence function method, but with a higher test accuracy, as shown in the left plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.09720v1",
    "pdf_url": null
  },
  {
    "instance_id": "63812d5af3e341269d85780448600f09",
    "figure_id": "2010.13816v1-Figure3-1",
    "image_file": "2010.13816v1-Figure3-1.png",
    "caption": " Human judgements of target agency and meaning preservation in POWERTRANSFORMER vs. three other model variants. Selection rates >50% indicate preference towards our model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model variant was preferred for agency?",
    "answer": "PowerTransformer",
    "rationale": "The figure shows that the red bars, which represent agency, are higher for PowerTransformer than for the other model variants.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.13816v1",
    "pdf_url": null
  },
  {
    "instance_id": "1a0cac614f1e4d6790acf6c3563be219",
    "figure_id": "1906.02735v6-Figure6-1",
    "image_file": "1906.02735v6-Figure6-1.png",
    "caption": " Random samples from Residual Flow are more globally coherent. PixelCNN (Oord et al., 2016) and Flow++ samples reprinted from Ho et al. (2019).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods shown in the figure produces the most globally coherent images?",
    "answer": "Residual Flow.",
    "rationale": "The caption states that \"Random samples from Residual Flow are more globally coherent.\" This means that the images produced by Residual Flow are more realistic and look more like they could have been taken from the real world.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02735v6",
    "pdf_url": null
  },
  {
    "instance_id": "28f5e66a52f348cc8a1bc7111c012200",
    "figure_id": "2006.12792v2-Figure3-1",
    "image_file": "2006.12792v2-Figure3-1.png",
    "caption": " Average Decision Boundary Distance (ADBD) against RayS attack iterations plot for several robust models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the robust models shown in the figure has the highest ADBD against RayS attack iterations?",
    "answer": "FeatureScattering",
    "rationale": "The figure shows the ADBD against RayS attack iterations for several robust models. The FeatureScattering model has the highest ADBD, as its line is the highest on the y-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.12792v2",
    "pdf_url": null
  },
  {
    "instance_id": "72b3f6c2a6bc48cfb520ffd4f05bf808",
    "figure_id": "1906.02735v6-Figure8-1",
    "image_file": "1906.02735v6-Figure8-1.png",
    "caption": " Effect of activation functions on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activation function performed the best on the CIFAR-10 dataset?",
    "answer": "LipSwish",
    "rationale": "The plot shows the bits/dim vs. epoch for three different activation functions: Softplus, ELU, and LipSwish. The lower the bits/dim, the better the performance. LipSwish has the lowest bits/dim of the three activation functions, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02735v6",
    "pdf_url": null
  },
  {
    "instance_id": "1ffb34886a3e41b79d5ee9c732e05290",
    "figure_id": "2304.01577v3-Figure3-1",
    "image_file": "2304.01577v3-Figure3-1.png",
    "caption": " Value Patterns Distributions for Each Key Group.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of the `class` values are \"Ordinary Only\"?",
    "answer": "Approximately 40%.",
    "rationale": "The figure shows the distribution of different value patterns for each key group. The `class` key group has five different value patterns: \"Empty\", \"Others\", \"ORD\", \"Ordinary Share\", and \"Ordinary Only\". The \"Ordinary Only\" value pattern makes up about 40% of the total `class` values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.01577v3",
    "pdf_url": null
  },
  {
    "instance_id": "f7c0fa0f5e904295b0bfe7eb1e3c1618",
    "figure_id": "2308.10896v1-Figure15-1",
    "image_file": "2308.10896v1-Figure15-1.png",
    "caption": " Shadow Art with one view. Our method can be used for simple scenes with co-located light and camera (left), for more complex settings with perspective cameras that observe the shadow receiver from any direction (middle), and for complex receiver geometry (right).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the light source and the camera in the first column of the figure?",
    "answer": "The light source and the camera are co-located.",
    "rationale": "The first column of the figure shows a simple scene with a single light source and a camera. The light source is located directly above the object, and the camera is located directly in front of the object. This is evident from the top view of the scene, which shows the light source directly above the object and the camera directly in front of the object.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.10896v1",
    "pdf_url": null
  },
  {
    "instance_id": "ea58d226f7c54b34a1e3aa29b6e3bdc8",
    "figure_id": "1903.05690v2-Figure16-1",
    "image_file": "1903.05690v2-Figure16-1.png",
    "caption": " Generated poses by our pose prediction model. We show generated poses in images (first column) and voxels visualized from two different views (last two columns) for each scene. Poses are generated by our model which takes a single depth image as input.",
    "figure_type": "photograph(s) and other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of input does the pose prediction model take?",
    "answer": "A single depth image.",
    "rationale": "The caption states that the poses are generated by a model that takes a single depth image as input.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.05690v2",
    "pdf_url": null
  },
  {
    "instance_id": "cc4b6756a374472fbaceb282c86f846a",
    "figure_id": "2303.05312v2-Figure8-1",
    "image_file": "2303.05312v2-Figure8-1.png",
    "caption": " Results of our ablations. Our full model produces the fewest artifacts.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model produced the fewest artifacts?",
    "answer": "Our full model produced the fewest artifacts.",
    "rationale": "The figure shows the results of different models, and the \"Ours\" column has the fewest artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.05312v2",
    "pdf_url": null
  },
  {
    "instance_id": "b4bbe49833bf4a0e8fd8d755ae0f11b1",
    "figure_id": "2210.17167v1-Figure7-1",
    "image_file": "2210.17167v1-Figure7-1.png",
    "caption": " Composition of new negatives in each episode of ANCE with CyclicLR. New negatives are those first introduced in each episode.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which episode of ANCE (BERT) has the highest percentage of new negatives that are not previous d* neighbors?",
    "answer": "Epi-3",
    "rationale": "The figure shows the composition of new negatives in each episode of ANCE with CyclicLR. The bar chart for ANCE (BERT) shows that Epi-3 has the highest percentage of new negatives that are not previous d* neighbors. This is because the beige bar for Epi-3 is the highest among all the beige bars.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.17167v1",
    "pdf_url": null
  },
  {
    "instance_id": "a3b1c1dd1fd3479a9ec91e3dd7505233",
    "figure_id": "2211.12131v2-Figure5-1",
    "image_file": "2211.12131v2-Figure5-1.png",
    "caption": " Comparison of 3D consistency achieved by our DiffDreamer and InfNat-0 [39], where we ask the camera to fly towards the top of the hill and show the intermediate renderings at camera positions c0 to c5.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves better 3D consistency, DiffDreamer or InfNat-0?",
    "answer": "DiffDreamer.",
    "rationale": "The figure shows that the images generated by DiffDreamer are more consistent with each other as the camera moves towards the top of the hill, while the images generated by InfNat-0 show more variation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.12131v2",
    "pdf_url": null
  },
  {
    "instance_id": "bd06b239c6f845e3b9782852689b0605",
    "figure_id": "1912.11589v3-Figure4-1",
    "image_file": "1912.11589v3-Figure4-1.png",
    "caption": " Dynamic intermedium attentionmemory network (DIAMNet). Φ1 represents Eqs. (1) and (2), Φ2 represents Eqs. (4) and (5), and two types of gates are Eqs. (3) and (6).",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two types of representations used in the DIAMNet model?",
    "answer": "Pattern representation and graph representation.",
    "rationale": "The figure shows two different types of representations being fed into the DIAMNet model. The top branch shows a pattern representation, which is a set of features extracted from the input data. The bottom branch shows a graph representation, which is a set of nodes and edges that represent the relationships between the features.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.11589v3",
    "pdf_url": null
  },
  {
    "instance_id": "947ef1f4544a400ca8db4b24342319c3",
    "figure_id": "2004.12934v1-Figure1-1",
    "image_file": "2004.12934v1-Figure1-1.png",
    "caption": " Test blank accuracy of BERT-ft and Human on each reasoning type category introduced in §3.3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which reasoning type category does BERT-ft perform best on?",
    "answer": "WordMatch",
    "rationale": "The figure shows that BERT-ft achieves the highest accuracy on the WordMatch category, with an accuracy of about 90%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.12934v1",
    "pdf_url": null
  },
  {
    "instance_id": "98f1b1239e184003abfb16ac3489c411",
    "figure_id": "2307.01201v1-Figure2-1",
    "image_file": "2307.01201v1-Figure2-1.png",
    "caption": "Figure 2 | A. CSCGs allow both separation of contexts and transitive generalization. The word “bank” is wired to different clones that correspond to the different contexts it is used in. If “milk and honey”, and “bread and butter” are seen in training, transitive generalization occurs if they get wired through the same ‘and’ clone, resulting in “bread and honey” and “milk and butter” being valid sequences. B. Probabilistic branching & merging of sequences. C – F. Exemplar CSCG circuits for respectively copying a sequence, parity operation, reversing a list with exactly five elements, reversing lists with a variable number of elements. G. Rebinding to new observations: dashed gray arrows correspond to old emissions while green arrows correspond to new rebound emissions.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which subfigure shows how CSCGs can be used to reverse a list of variable length?",
    "answer": "Subfigure F.",
    "rationale": "Subfigure F shows a CSCG circuit that can reverse lists of variable length. The circuit works by first copying the input sequence to a temporary buffer, then reversing the order of the elements in the buffer, and finally outputting the reversed sequence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.01201v1",
    "pdf_url": null
  },
  {
    "instance_id": "bf2d16fedae44b969971edd3f272a02a",
    "figure_id": "2302.09178v2-Figure4-1",
    "image_file": "2302.09178v2-Figure4-1.png",
    "caption": " (a) We dive into three typical moments in model training: The model was training healthily before step-a. Then at step-b, model’s loss aroused and AUC dropped. Finally at step-c, the loss is fully diverged and AUC dropped to 0.5. (b) When checking some statistics from the top hidden layer of the model, we found that GC and AGC failed to provide small enough clipping factor. While Clippy’s clipping factor can be 2-orders of magnitude smaller than GC and AGC. Section B in Supplementary Material has the statistics for other layers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method provided the smallest clipping factor at step-b?",
    "answer": "Clippy.",
    "rationale": "Figure (b) shows the clipping factors for each method at each step. At step-b, Clippy's clipping factor is the smallest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.09178v2",
    "pdf_url": null
  },
  {
    "instance_id": "d0831c4659e944d1afac996e65127c46",
    "figure_id": "2307.00682v1-Figure36-1",
    "image_file": "2307.00682v1-Figure36-1.png",
    "caption": " Simulating a data addition attack by picking a single segment (either the 1st, 72nd, or 143rd), and adding 1 32 th of data from the same distribution (deduped Pile), or no data addition (truthful reporting). Results are shown with error bars across 5 random seeds. (Some ranges are too small to see.) From left to right: Plotting weight-changes between checkpoints, a Verifier can see a suspicious spike at the attacked segment in the middle of training; The Verifier retrains the suspicious segments and checks the distance between the reported and re-executed checkpoint weights. Distance between multiple runs of the reported data are shown as a reference for setting the tolerance ϵ.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which segment was attacked in the data addition attack?",
    "answer": "The 72nd segment.",
    "rationale": "The left plot shows a spike in weight changes at the 72nd checkpoint for the \"Add 3.1% (Pile)\" case, which is not present in the \"Baseline (Pile)\" case. This suggests that the 72nd segment was attacked.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.00682v1",
    "pdf_url": null
  },
  {
    "instance_id": "f9e3e4686031401aa0d41164162d9276",
    "figure_id": "2010.08618v1-Figure8-1",
    "image_file": "2010.08618v1-Figure8-1.png",
    "caption": " An example of a question used for human evaluation of the recipe rewrite task on Amazon Mechanical Turk.",
    "figure_type": "Other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the dietary restriction that this recipe is being evaluated against?",
    "answer": "Alcohol-free.",
    "rationale": "The title of the recipe is \"Alcohol-Free Best Chicken Stroganoff\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.08618v1",
    "pdf_url": null
  },
  {
    "instance_id": "e0a97169edfd4343b49ca160e7afc3e8",
    "figure_id": "2301.09604v2-Figure10-1",
    "image_file": "2301.09604v2-Figure10-1.png",
    "caption": " Additional results for CIFAR-100 dataset. Mean and standard deviation from experiments with 5 different random seeds. The shaded areas show the standard deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest training accuracy after 500 training rounds?",
    "answer": "FedAvg and SCAFFOLD.",
    "rationale": "The figure shows the training accuracy for four different algorithms: FedEXP, FedAvg, SCAFFOLD, and FedAdagrad. The training accuracy for FedAvg and SCAFFOLD is about 60% after 500 training rounds, which is higher than the training accuracy for the other two algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.09604v2",
    "pdf_url": null
  },
  {
    "instance_id": "cdfb48d351484d85b6185dae06aa2531",
    "figure_id": "2207.12534v3-Figure5-1",
    "image_file": "2207.12534v3-Figure5-1.png",
    "caption": " Mean JSV and test accuracy during retraining with different methods (network: MLP-7Linear, dataset: MNIST) when pruning a random model. Below each plot are, in order, the best accuracy of LR 0.01, the best accuracy of LR 0.001, and the mean JSV right after pruning (i.e., without retraining).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which of the three methods achieves the highest test accuracy on the MNIST dataset?",
    "answer": "TPP",
    "rationale": "The test accuracy of each method is shown in the bottom right corner of each plot. TPP achieves the highest test accuracy of 92.82%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.12534v3",
    "pdf_url": null
  },
  {
    "instance_id": "e57f828635c04a54bce7d5be8d98a4b9",
    "figure_id": "2210.05177v2-Figure2-1",
    "image_file": "2210.05177v2-Figure2-1.png",
    "caption": " The training curves of SGD, SAM and SSAM. The sparsity of SSAM is 50%.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer consistently performs the best across all datasets and models?",
    "answer": "SAM",
    "rationale": "The figure shows the test accuracy of different optimizers on different datasets and models. SAM consistently achieves the highest test accuracy across all datasets and models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.05177v2",
    "pdf_url": null
  },
  {
    "instance_id": "704bd63e2035479c8e2a0f85070d436f",
    "figure_id": "2112.05341v3-Figure4-1",
    "image_file": "2112.05341v3-Figure4-1.png",
    "caption": " KDE estimate of separation between ID and OOD sets based on minimum angular distance to closest class representative in the CIFAR10 setting. To avoid clutter, far-OOD datasets have been grouped: i) MNIST (AVG) contains KMNIST, MNIST and FashionMNIST. ii) SUN (AVG) contains iSUN, LSUNr and LSUNc. iii) Other (AVG) contains all other far-OOD datasets. Overlap between the test and OOD distributions can be considered erroneous samples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the largest overlap with the CIFAR10 test set?",
    "answer": "CIFAR10 (train)",
    "rationale": "The KDE estimate for CIFAR10 (train) overlaps the most with the KDE estimate for CIFAR10 (test). This means that the angular distances between the CIFAR10 test set and the CIFAR10 train set are very similar, which is to be expected since they are both drawn from the same distribution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.05341v3",
    "pdf_url": null
  },
  {
    "instance_id": "147cdf9d487a43c0b0376a7e47c217be",
    "figure_id": "2002.05715v3-Figure6-1",
    "image_file": "2002.05715v3-Figure6-1.png",
    "caption": " (Left 2 plots): self-distillation compared to early stopping for Resnet50 and CIFAR-10 using `2 and cross-entropy loss, respectively. (Right 2 plots): self-distillation with `2 loss using VGG16 Network on CIFAR-100.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the plots, which technique achieves higher test accuracy for ResNet50 and CIFAR-10: early stopping or self-distillation?",
    "answer": "Early stopping.",
    "rationale": "The leftmost plot shows the test accuracy for ResNet50 and CIFAR-10 using early stopping and self-distillation. The green line, which represents early stopping, is consistently higher than the blue line, which represents self-distillation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.05715v3",
    "pdf_url": null
  },
  {
    "instance_id": "636bd5e935224034a8ac7e93e8d5820e",
    "figure_id": "2008.01187v1-Figure2-1",
    "image_file": "2008.01187v1-Figure2-1.png",
    "caption": " Category matching in Mask-RCNN top. Each input category (left) are matched to its best substitute (right) measured by performance on training set. Categories are ordered from top to bottom by frequency.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which category is most frequently substituted for \"man\"?",
    "answer": "\"cloud\"",
    "rationale": "The figure shows that the category \"man\" is most frequently substituted for \"cloud\". This is because the line connecting \"man\" and \"cloud\" is the thickest, indicating that this substitution occurs most often.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.01187v1",
    "pdf_url": null
  },
  {
    "instance_id": "9b7d2b688dbb4b2bb8fb053d0ab9c8f7",
    "figure_id": "2302.00878v3-Figure1-1",
    "image_file": "2302.00878v3-Figure1-1.png",
    "caption": " Fitted coefficient functions from the contextual lasso for the house pricing dataset. Colored points indicate coefficient values at different locations. Grey points indicate coefficients equal to zero.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature has the most consistent effect on house prices across different locations?",
    "answer": "Renovation condition.",
    "rationale": "The renovation condition plot shows a relatively smooth gradient of coefficient values, with higher values in the center of the plot and lower values towards the edges. This suggests that the effect of renovation condition on house prices is relatively consistent across different locations. In contrast, the other plots show more variation in coefficient values, suggesting that the effects of those features on house prices are more dependent on location.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.00878v3",
    "pdf_url": null
  },
  {
    "instance_id": "a40a55c466fb4f099b4bc7488620785a",
    "figure_id": "1903.05285v1-Figure2-1",
    "image_file": "1903.05285v1-Figure2-1.png",
    "caption": " The practical runtime analysis. For clear comparison, both batch-normalization and ReLU layers are neglected since they can be merged into convolutional layer for inference. Also data feeding and preprocessing time are not considered here. Results are achieved under Caffe with mini-batch 32. They are averaged from 100 runs. (a) ShiftNet-A [37] on CPU (Intel Xeon E5-2650, atlas). (b) ShiftNet-A on GPU (TITAN X Pascal, CUDA8 and cuDNN5). (c) Shift layers in ShiftNet-A are replaced by depthwise separable convolution layers. (d) Depthwise separable convolution layers with kernel size 5 are replaced by the ones with kernel size 3. (e) ShiftNet-A with 80% shift sparsity on GPU (Shift sparsity denotes the ratio of unshifted feature maps).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which operation takes the most time on average for ShiftNet-A on CPU?",
    "answer": "Conv.",
    "rationale": "In Figure (a), the pie chart shows the breakdown of the runtime for ShiftNet-A on CPU. The largest segment of the pie chart is labeled \"Conv.\", which indicates that convolution operations take the most time on average.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.05285v1",
    "pdf_url": null
  },
  {
    "instance_id": "ac9719721407413cba19a431e7c69b08",
    "figure_id": "2305.02031v2-Figure1-1",
    "image_file": "2305.02031v2-Figure1-1.png",
    "caption": " The design of our research. At each stage (from left to right), we examine different modeling decisions in order to gain a better understanding of their impact. We start with the architectural decisions which largely impact the task performance and computational aspects of the NLG models. Following that, we compare different KD objectives, and then we focus on augmenting the training data with Pseudo-Targets (PTs). A bold border indicates the decision we made at each stage based on the average development set performance over four NLG tasks.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In the realistic setup, which KD objective performed the best?",
    "answer": "Attention-Relations KD.",
    "rationale": "The figure shows that Attention-Relations KD is highlighted with a bold border, which indicates that it achieved the best average development set performance over four NLG tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.02031v2",
    "pdf_url": null
  },
  {
    "instance_id": "daa2eeccf3e847989440bd7976f36db0",
    "figure_id": "2101.08152v2-Figure24-1",
    "image_file": "2101.08152v2-Figure24-1.png",
    "caption": " Local exploration scores achieved by RAPID and baselines on MiniWorld Maze with pure exploration.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieved the highest local exploration score in the MiniWorld Maze with pure exploration?",
    "answer": "RAPID",
    "rationale": "The plot shows the local exploration scores achieved by different algorithms over time. RAPID is the red line, and it consistently achieves the highest score throughout the experiment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.08152v2",
    "pdf_url": null
  },
  {
    "instance_id": "c8f002abb64b4bbf83f05502d03b6d80",
    "figure_id": "1811.04857v4-Figure4-1",
    "image_file": "1811.04857v4-Figure4-1.png",
    "caption": " Test accuracy for unseen classes with respect to the number of synthetic samples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest test accuracy for unseen classes when using 500 synthetic samples?",
    "answer": "CUB",
    "rationale": "The figure shows that the CUB line is the highest at 500 synthetic samples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.04857v4",
    "pdf_url": null
  },
  {
    "instance_id": "cd15df8f92084e9a848e98bf6bfb7fb2",
    "figure_id": "2109.12393v1-Figure1-1",
    "image_file": "2109.12393v1-Figure1-1.png",
    "caption": " Accuracy by number of attractors, with semantically related attractor type",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best with a single attractor?",
    "answer": "GPT2Medium",
    "rationale": "The figure shows the accuracy of different models with different numbers of attractors. The GPT2Medium model has the highest accuracy with a single attractor.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.12393v1",
    "pdf_url": null
  },
  {
    "instance_id": "8c2c7e192fc94468abec33488d4eb3c7",
    "figure_id": "1902.09566v5-Figure4-1",
    "image_file": "1902.09566v5-Figure4-1.png",
    "caption": " Comparison of models. We use 5-fold cross validation with stratified splits. Plot (a) shows the precision-recall curves on the test data with 0.1% anomalies. Plot (b) shows the F1 score as a function of the percent anomalies in the test data; the percentage of anomalies on the plot ranges from 0.1% to 25%.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest precision for a recall of 0.5?",
    "answer": "Random Forests.",
    "rationale": "At a recall of 0.5, the precision of Random Forests is the highest among all the models. This can be seen from the precision-recall curve in plot (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.09566v5",
    "pdf_url": null
  },
  {
    "instance_id": "e25bcdffbcf3441295cf6a849f08cb65",
    "figure_id": "1904.01681v3-Figure5-1",
    "image_file": "1904.01681v3-Figure5-1.png",
    "caption": " Comparison of training losses of NODEs and ResNets. Compared to ResNets, NODEs struggle to fit g(x) both in d = 1 and d = 2. The difference between ResNets and NODEs is less pronounced for the separable function.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better in terms of training loss for g(x) in d = 1 and d = 2?",
    "answer": "ResNet",
    "rationale": "The plots show that the loss for ResNet (blue line) decreases faster and reaches a lower value than the loss for Neural ODE (orange line) in both cases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.01681v3",
    "pdf_url": null
  },
  {
    "instance_id": "4ab8039452fb4dbdbb44c57fc35257ae",
    "figure_id": "2203.09711v1-Figure7-1",
    "image_file": "2203.09711v1-Figure7-1.png",
    "caption": " The accuracy of evaluation metrics to distinguish coherent/incoherent conversations in test data (yaxis) generated using baseline manipulations (x-axis).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which evaluation metric is the most accurate in distinguishing coherent/incoherent conversations in test data generated using baseline manipulations?",
    "answer": "DynaEval",
    "rationale": "The figure shows that DynaEval has the highest accuracy (0.9) for distinguishing coherent/incoherent conversations in test data generated using baseline manipulations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.09711v1",
    "pdf_url": null
  },
  {
    "instance_id": "839ace463605417dbe5992db779cf8eb",
    "figure_id": "2110.03469v3-Figure4-1",
    "image_file": "2110.03469v3-Figure4-1.png",
    "caption": " Synthetic data results. Comparison of FEDDC (a), FEDAVG with same communication (b) and same averaging period (c) for training fully connected NNs on synthetic data. We report mean and confidence accuracy per client in color and accuracy of central learning as dashed black line.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three algorithms shown in the figure achieves the highest accuracy on the synthetic data?",
    "answer": "FEDDC",
    "rationale": "The figure shows that the green line (representing FEDDC) is higher than the orange line (representing FEDAVG) for all three cases. This means that FEDDC achieves a higher accuracy than FEDAVG on the synthetic data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.03469v3",
    "pdf_url": null
  },
  {
    "instance_id": "13577521a98c49f09c25ec3a4d2368a3",
    "figure_id": "2010.01610v2-Figure4-1",
    "image_file": "2010.01610v2-Figure4-1.png",
    "caption": " Average results of five time retrain using different datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on average?",
    "answer": "Ours",
    "rationale": "The bar chart shows that \"Ours\" has the highest average result, which is approximately 95.6.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.01610v2",
    "pdf_url": null
  },
  {
    "instance_id": "85706d341dd541b894e7a654c2bbcf26",
    "figure_id": "1906.10115v3-Figure22-1",
    "image_file": "1906.10115v3-Figure22-1.png",
    "caption": " Across all models, improvements in likelihood bounds correlate strongly with improvements in posterior accuracy. Better sampling methods can improve both.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampling method consistently leads to the lowest error across all models and M values?",
    "answer": "anti-qmc",
    "rationale": "The plots in the right column show the error for each sampling method. The anti-qmc method has the lowest error for all models and M values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.10115v3",
    "pdf_url": null
  },
  {
    "instance_id": "c16bb1b9ef6b4d3ca627da8dfa4af949",
    "figure_id": "2003.09210v3-Figure5-1",
    "image_file": "2003.09210v3-Figure5-1.png",
    "caption": " Test results of model reproducibility. From top to bottom: image fusion dataset TNO, FLIR, and NIR. From left to right: the values of EN, MI, SD, SF, VIF and AG.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest values for EN, MI, and SD?",
    "answer": "The TNO dataset.",
    "rationale": "The first row of plots shows the values for EN, MI, and SD for the TNO dataset. The values are higher than those for the FLIR and NIR datasets, which are shown in the second and third rows, respectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.09210v3",
    "pdf_url": null
  },
  {
    "instance_id": "d378adb1c36543dd9c246fb80d5677e5",
    "figure_id": "1811.02146v5-Figure2-1",
    "image_file": "1811.02146v5-Figure2-1.png",
    "caption": " Our 4D Graph for a traffic sequence. (a) Icons for instances and categories are shown on the left table. (b) The instance layer of the 4D Graph with spatial edges as solid lines and temporal edges as dashed lines. (c) The category layer with temporal edges of super nodes drawn by dashed lines.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the different types of nodes in the 4D Graph?",
    "answer": "Instance nodes and super nodes.",
    "rationale": "The table in the bottom left corner of the figure shows the different types of nodes. Instance nodes represent individual objects, such as pedestrians, bicycles, and cars. Super nodes represent categories of objects, such as \"pedestrian\", \"bicycle\", and \"car\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.02146v5",
    "pdf_url": null
  },
  {
    "instance_id": "b63a070d5f18437f8ceee0d19495580b",
    "figure_id": "2310.20708v1-Figure6-1",
    "image_file": "2310.20708v1-Figure6-1.png",
    "caption": " Best objective value as a function of number of function evaluations (iterations) on three high-dimensional problems, including Eriksson and Jankowiak [19]’s SAAS prior.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which acquisition function performs best on the 1000D Embedded Hartmann6 problem?",
    "answer": "qloqei + saas",
    "rationale": "The plot on the left shows the best observed value as a function of the number of iterations for different acquisition functions. The qloqei + saas line is consistently above the other lines, indicating that it achieves the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.20708v1",
    "pdf_url": null
  },
  {
    "instance_id": "dd71cb125c704fbdaf3ab0d9bc32effe",
    "figure_id": "2110.14300v5-Figure23-1",
    "image_file": "2110.14300v5-Figure23-1.png",
    "caption": " Compare MARL with traditional control methods on a typical bus during a day for the 322-bus network. 1st row: results for summer. 2nd row: results for winter. None and limit in (a) represent the voltage with no control and the safety range respectively. P and Q in (b) indicate the PV active power and the reactive power by various methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "During which season does the voltage with no control exceed the safety range?",
    "answer": "Summer",
    "rationale": "In Figure (a), the voltage with no control (orange line) exceeds the safety range (red dashed line) in the first row, which represents the results for summer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14300v5",
    "pdf_url": null
  },
  {
    "instance_id": "2d52e607df1d493995e9581747cc69d7",
    "figure_id": "2210.13605v2-Figure8-1",
    "image_file": "2210.13605v2-Figure8-1.png",
    "caption": " Ablation study on the spatiotemporal consistency objective on SSv2 dataset. (a) accuracy of GliTr when trained using different combinations of the training objectives (b) accuracy of the teacher with the glimpses selected by the above variants. (c) accuracy of the above variants of GliTr when tested with the Uniform random strategy. We display mean±5×std from five independent runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training objective yielded the highest accuracy for GliTr when tested with the Uniform random strategy?",
    "answer": "GliTr spatiotemporal",
    "rationale": "The figure shows the accuracy of GliTr when trained with different combinations of training objectives. The highest accuracy for GliTr when tested with the Uniform random strategy is achieved with the GliTr spatiotemporal objective, as shown in the plot (c).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.13605v2",
    "pdf_url": null
  },
  {
    "instance_id": "13b22295ff8f4af995f4718bedb244e4",
    "figure_id": "2104.14138v1-Figure4-1",
    "image_file": "2104.14138v1-Figure4-1.png",
    "caption": " Evolution of the temporal difference percentage error in Exponential Pong.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, Pop-Art, DQN+TC, or Spectral DQN, has the highest temporal difference percentage error at the end of training?",
    "answer": "Spectral DQN",
    "rationale": "The figure shows the evolution of the temporal difference percentage error for the three methods over the course of training. The Spectral DQN method has the highest error at the end of training, as indicated by the red line in the rightmost plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.14138v1",
    "pdf_url": null
  },
  {
    "instance_id": "dddb2be6bb354de18b0f211395d6b712",
    "figure_id": "2203.10652v2-Figure3-1",
    "image_file": "2203.10652v2-Figure3-1.png",
    "caption": " The growing process of our model on sequence: hotel e2e rest laptop tv. The 1st layer is shown at the bottom and the 12th layer is at the top of each figure. Note that here we only depict the architecture growing process of our inserted modules: (i) Each rectangle represents a module added in that specific transformer layer. (ii) Each module is painted with the corresponding color if it is used by a task. (iii) Modules with multiple colors are shared by multiple tasks.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many modules are added in the 12th layer of the model?",
    "answer": "5",
    "rationale": "The 12th layer is the top layer of each figure. There are 5 rectangles in the top layer of each figure, which represents the 5 modules added in the 12th layer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.10652v2",
    "pdf_url": null
  },
  {
    "instance_id": "9ea80f789bca45ec9dfebe419ea237bf",
    "figure_id": "1805.04617v1-Figure3-1",
    "image_file": "1805.04617v1-Figure3-1.png",
    "caption": " Relevance accuracies of the Doc2Vec and LDA resource recommendation models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed better overall, Doc2Vec or LDA?",
    "answer": "Doc2Vec performed better overall.",
    "rationale": "The figure shows that Doc2Vec has a higher percentage of relevant recommendations for most of the test document IDs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.04617v1",
    "pdf_url": null
  },
  {
    "instance_id": "a757a86727014c1ebd3f6fc0710a2c78",
    "figure_id": "1804.09170v4-Figure5-1",
    "image_file": "1804.09170v4-Figure5-1.png",
    "caption": " Average validation error over 10 randomlysampled nonoverlapping validation sets of varying size. For each SSL approach, we re-evaluated an identical model on each randomly-sampled validation set. The mean and standard deviation of the validation error over the 10 sets are shown as lines and shaded regions respectively. Models were trained on SVHN with 1,000 labels. Validation set sizes are listed relative to the training size (e.g. 10% indicates a size-100 validation set). X-axis is shown on a logarithmic scale.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which semi-supervised learning approach performs the best on small validation sets?",
    "answer": "The Pi-Model.",
    "rationale": "The Pi-Model has the lowest average validation error across all validation set sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.09170v4",
    "pdf_url": null
  },
  {
    "instance_id": "ac2ee1315c42442a8b8663ff28e7f645",
    "figure_id": "2008.03235v2-Figure8-1",
    "image_file": "2008.03235v2-Figure8-1.png",
    "caption": " ∆AUUC (higher is better) of two IPE models, corresponding C-IPE models and Oracle model (theoretical truth). Box plots are done on 51 random splits, whiskers at 5/95 percentiles. Note how C-IPE systematically increases the AUUC of standard IPE estimators",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest median ΔAUUC?",
    "answer": "Oracle",
    "rationale": "The green triangle in each box plot represents the median ΔAUUC for each model. The green triangle for the Oracle model is the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.03235v2",
    "pdf_url": null
  },
  {
    "instance_id": "6beca13d0b684d54939c69b4711a8c0e",
    "figure_id": "2205.10226v1-Figure3-1",
    "image_file": "2205.10226v1-Figure3-1.png",
    "caption": " Correlation between human fixations and different models for SST (left) and Wikipedia (right) with respect to word predictability in equally sized bins. Word predictability scores, were calculated with a 5-gram Kneser-Ney language model. Respective bin limits are given on the x-axis. Samples for every other bin are displayed on the upper x-axis.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is the most consistent in its correlation with human fixations across different levels of word predictability?",
    "answer": "BERT",
    "rationale": "The BERT line is the most stable across the different word predictability bins in both the SST and Wikipedia plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.10226v1",
    "pdf_url": null
  },
  {
    "instance_id": "754098b5b3f445c6b4918603bd330f5f",
    "figure_id": "2001.06354v1-Figure7-1",
    "image_file": "2001.06354v1-Figure7-1.png",
    "caption": " An example of the ranking list of the image-history joint and image-only model (the numbers next to the answers are the scores which indicate the relevance of the corresponding answers to the question).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the people in the image doing?",
    "answer": "The people in the image are playing video games.",
    "rationale": "The image shows several young people standing in a living room. They are holding controllers and looking at a television screen.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.06354v1",
    "pdf_url": null
  },
  {
    "instance_id": "9af0f3d510d143e8be07a9aaf97a8242",
    "figure_id": "2210.02157v2-Figure2-1",
    "image_file": "2210.02157v2-Figure2-1.png",
    "caption": " The lazy infinite width limits of the various learning rules can be fully summarized with their initial eNTK. (a) The kernels of ρ-aligned ReLU FA and ReLU GLN for inputs separated by angle θ. (a) The kernels for varying ρ in ρ-aligned FA. Larger ρ has a sharper peak in the kernel around θ = 0. The ρ→ 0 limit recovers the NNGP kernel ΦL while the ρ→ 1 limit gives the backprop NTK. (b) Deeper networks with partial alignment ρ = 0.5. (c) ReLU-GLN kernel sharpens with depth. (d)-(e) The relative error of the infinite width Φℓ, Gℓ kernels in a width N ReLU neural network. The late layer Φℓ and early layer Gℓ kernels have highest errors since finite size effects accumulate on forward and backward passes respectively. (f) Finite width corrections to eNTK are larger for small ρ and large depth L. All square errors go as |KN −K∞|2 ∼ ON (1/N).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the kernel of ρ-aligned ReLU FA change as ρ increases?",
    "answer": "The kernel becomes sharper around θ = 0.",
    "rationale": "This can be seen in panel (a) of the figure, which shows the kernels for varying ρ. As ρ increases, the peak of the kernel becomes narrower and higher.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.02157v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd96dcb5c6d34934adf046a9b169c148",
    "figure_id": "2103.05137v2-Figure5-1",
    "image_file": "2103.05137v2-Figure5-1.png",
    "caption": " Model accuracy against adversarial perturbations (left) and noise corruptions (middle). The right panel shows a sample chair image along with its bounding box and segmentation mask.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of input is most robust to adversarial perturbations?",
    "answer": "The full image.",
    "rationale": "The left panel of the figure shows that the full image has the highest accuracy for all levels of epsilon, which is a measure of the strength of the adversarial perturbation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.05137v2",
    "pdf_url": null
  },
  {
    "instance_id": "90648855780d4b0a806a7f8456154208",
    "figure_id": "2105.10053v1-Figure1-1",
    "image_file": "2105.10053v1-Figure1-1.png",
    "caption": " Band diagrams representing the positions of the attacks (true positives) in some contexts of the BSD dataset (scenario 1). The x-axis represents the attack positions in ranked lists.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which context were there the most attacks?",
    "answer": "PE",
    "rationale": "The PE context has the most red lines, which represent true positives.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.10053v1",
    "pdf_url": null
  },
  {
    "instance_id": "318fad2438af493d8a315c860ab1f993",
    "figure_id": "2206.10926v2-Figure7-1",
    "image_file": "2206.10926v2-Figure7-1.png",
    "caption": " (a) Frequent cycles per formation period, and (b)–(e) some tactically notable cycles from the match in Fig. 1.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What formation did the team use in the first half of the game?",
    "answer": "4-2-3-1",
    "rationale": "The histograms in (a) show the top-5 frequent cycles in each formation period. In the first half, the formation used was 4-2-3-1. This can be seen from the first histogram, which shows the positions of the players in the 4-2-3-1 formation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.10926v2",
    "pdf_url": null
  },
  {
    "instance_id": "8c89cc9ab33a4e0db9de5e702329ba00",
    "figure_id": "2007.12223v2-Figure5-1",
    "image_file": "2007.12223v2-Figure5-1.png",
    "caption": " The performance of transferring IMP subnetworks between tasks. Each row is a source task S. Each column is a target task T . Let TRANSFER(S, T ) be the performance of finding an IMP subnetwork at 50% sparsity on task S and training it on task T . Each cell is TRANSFER(S, T ) minus the performance of the full network f(x; θ0) on task T . Dark cells mean transfer performance TRANSFER(S, T ) is at least as high as same-task performance TRANSFER(T , T ); light cells mean it is lower. The number on the right is the number of target tasks T for which transfer performance is at least as high as same-task performance. The last row is the performance when the pruning mask comes from directly pruning the pre-trained weights θ0.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which source task performs best when transferred to other tasks?",
    "answer": "(IMP) MLM",
    "rationale": "The last column shows the number of target tasks for which transfer performance is at least as high as same-task performance. The (IMP) MLM row has the highest number, 8, indicating that it performs best when transferred to other tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.12223v2",
    "pdf_url": null
  },
  {
    "instance_id": "f27540c962b24b6b9d1f6f55091afcf2",
    "figure_id": "2303.13508v2-Figure5-1",
    "image_file": "2303.13508v2-Figure5-1.png",
    "caption": " User Study. Users show a significant preference for our DreamBooth3D over DB+DF and L-NeRF for 3D consistency, subject fidelity and prompt fidelity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, DreamBooth3D, DB+DF, or L-NeRF, performed the best in terms of 3D consistency?",
    "answer": "DreamBooth3D",
    "rationale": "The bar graph shows that DreamBooth3D had the highest percentage of \"Left preferred\" responses for 3D consistency (77%), indicating that users preferred it over the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.13508v2",
    "pdf_url": null
  },
  {
    "instance_id": "5cf194e5c00a4a8dab23c93c4bc12310",
    "figure_id": "2106.12657v1-Figure4-1",
    "image_file": "2106.12657v1-Figure4-1.png",
    "caption": " Throughput-Recall Trade-off comparison between XR-Linear (PECOS) and DSSM +HNSW. The curve of the latter method is obtained by sweeping the HNSW inference hyper-parameters 𝑒 𝑓 𝑆 = {10, 50, 100, 250, 500}.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves higher throughput at the expense of lower recall?",
    "answer": "XR-Linear",
    "rationale": "The figure shows that the XR-Linear curve is generally above the DSSM + HNSW curve, which means that for a given recall value, XR-Linear achieves higher throughput.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.12657v1",
    "pdf_url": null
  },
  {
    "instance_id": "12ac1b66c4534b7394f827cc1264b6ac",
    "figure_id": "2206.01995v5-Figure5-1",
    "image_file": "2206.01995v5-Figure5-1.png",
    "caption": " Regrets of Algorithms Run on G2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of cumulative regret?",
    "answer": "BLM-LR",
    "rationale": "The plot shows the cumulative regret of several algorithms over the course of 2000 rounds. The algorithm with the lowest cumulative regret at the end of the experiment is BLM-LR, which means that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.01995v5",
    "pdf_url": null
  },
  {
    "instance_id": "ada6aa356aa341c3aa2bda926695f9e1",
    "figure_id": "1805.11604v5-Figure13-1",
    "image_file": "1805.11604v5-Figure13-1.png",
    "caption": " Evaluation of the training performance of `p normalization techniques discussed in Section 3.3. For both networks, all `p normalization strategies perform comparably or even better than BatchNorm. This indicates that the performance gain with BatchNorm is not about distributional stability (controlling mean and variance).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which normalization technique appears to have the best performance for the VGG network?",
    "answer": "Standard + L1",
    "rationale": "The figure shows that the Standard + L1 normalization technique achieves the highest training accuracy for the VGG network.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.11604v5",
    "pdf_url": null
  },
  {
    "instance_id": "33039013900643ac9b476300fa5a108c",
    "figure_id": "1902.03748v3-Figure3-1",
    "image_file": "1902.03748v3-Figure3-1.png",
    "caption": " We show the person behavior module given a sequence of person frames. We extract person appearance features and pose features to model the changes of a person’s behavior. See Section 3.2.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two types of features that are extracted from the person frames to model the changes in a person's behavior?",
    "answer": "Person appearance features and pose features.",
    "rationale": "The figure shows that the person behavior module takes a sequence of person frames as input. These frames are then processed by two different encoders: the person appearance encoder and the person keypoint encoder. The person appearance encoder extracts features related to the person's appearance, such as their clothing, hair color, and facial features. The person keypoint encoder extracts features related to the person's pose, such as the position of their limbs and joints. These features are then combined to model the changes in the person's behavior.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.03748v3",
    "pdf_url": null
  },
  {
    "instance_id": "6613e8a57ca34bb4bad03ed29d0864a8",
    "figure_id": "2103.11784v3-Figure8-1",
    "image_file": "2103.11784v3-Figure8-1.png",
    "caption": " Mean and standard deviation of feature maps of the VGG19 network under different thumbnail scales. It shows that with the growth of the thumbnail scale, the normalization statistics of feature maps tend to be stable.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activation function has the most stable standard deviation across different thumbnail scales?",
    "answer": "relu4_1",
    "rationale": "The standard deviation of the feature maps for relu4_1 is the most stable across different thumbnail scales, as shown by the purple line in the bottom plot. This indicates that the activation function relu4_1 is less sensitive to changes in the thumbnail scale.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.11784v3",
    "pdf_url": null
  },
  {
    "instance_id": "9091fba249644bf1ae0e57b76e7b1c1c",
    "figure_id": "2210.11689v1-Figure29-1",
    "image_file": "2210.11689v1-Figure29-1.png",
    "caption": " The LM accuracy on the polarity item more or less paradigm.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language model performs the best on the polarity item more or less paradigm for both mono-lingual and multi-lingual tasks?",
    "answer": "PanGu.",
    "rationale": "The figure shows the accuracy of different language models on the polarity item more or less paradigm. PanGu achieves the highest accuracy for both mono-lingual and multi-lingual tasks, with accuracies of 98.6 and 97.7, respectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.11689v1",
    "pdf_url": null
  },
  {
    "instance_id": "140dca1905524320aa71e09e35814abb",
    "figure_id": "1908.03195v2-Figure6-1",
    "image_file": "1908.03195v2-Figure6-1.png",
    "caption": " Dataset statistics. Best viewed digitally.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the most uniform distribution of categories per image?",
    "answer": "ADE20K.",
    "rationale": "The plot in (a) shows the distribution of category count per image for each dataset. ADE20K has the most uniform distribution, as indicated by the green line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.03195v2",
    "pdf_url": null
  },
  {
    "instance_id": "11f4028b07e84a7f8c4991ab4b6d0252",
    "figure_id": "2006.06752v3-Figure10-1",
    "image_file": "2006.06752v3-Figure10-1.png",
    "caption": " CLIC 2020 human rankings according to ELO, plotted against ranking according to various perceptual quality metrics. Each dot represents one image compression method competing in CLIC 2020.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which perceptual quality metric has the strongest correlation with ELO?",
    "answer": "Butteraugli",
    "rationale": "The plot of Butteraugli vs. ELO has the steepest slope and the tightest confidence interval, indicating a strong negative correlation between the two variables.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06752v3",
    "pdf_url": null
  },
  {
    "instance_id": "cad96a9ce75a4c938115d05c7a8add1c",
    "figure_id": "2206.08871v2-Figure4-1",
    "image_file": "2206.08871v2-Figure4-1.png",
    "caption": " Evaluations on the CdSprites dataset with rid ∈ {0.25, 0.5, 0.75, 1.0}. We report shape classification accuracy using OOD-trained linear head (acco(f, co)), shift sensitivity s, and linear head bias b. Results are shown for individual models from the class of AE (blue), SSL (green), and SL (grey) algorithms. The black horizontal line denotes the random baseline (33.3% for three classes).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in terms of shift sensitivity?",
    "answer": "SimCLR",
    "rationale": "The plot in (b) shows that SimCLR has the highest shift sensitivity for all values of rid.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.08871v2",
    "pdf_url": null
  },
  {
    "instance_id": "8d55eb1635cc45bda6277f963cc3849b",
    "figure_id": "2012.13023v3-Figure7-1",
    "image_file": "2012.13023v3-Figure7-1.png",
    "caption": " PFOEqueries andhyperbolic distance in a Poincaré geodisc.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which panel of the figure shows the union of two shapes?",
    "answer": "Panel (c)",
    "rationale": "Panel (c) shows two shapes that are combined to form a larger shape. This is the definition of a union.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.13023v3",
    "pdf_url": null
  },
  {
    "instance_id": "ee0c7c3336664ced9bc4fcee63ee2e00",
    "figure_id": "2303.11305v4-Figure17-1",
    "image_file": "2303.11305v4-Figure17-1.png",
    "caption": " Additional analysis of Cut-Mix data augmentation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which augmentation technique was used to create the image of the astronaut riding a horse?",
    "answer": "Cut-Mix",
    "rationale": "The figure shows that the image of the astronaut riding a horse was created by combining the image of a horse with the image of an astronaut. This is an example of the Cut-Mix data augmentation technique.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.11305v4",
    "pdf_url": null
  },
  {
    "instance_id": "42e54ad25b504f3c94aa187b3893c1cb",
    "figure_id": "2107.09598v4-Figure5-1",
    "image_file": "2107.09598v4-Figure5-1.png",
    "caption": " Example behaviour of altruistic agents (blue) that learned to actively defend the leader agent (green) from the adversaries (red) in Tag. Obstacles are black. The trajectories taken by some of the agents in the last 10 steps are shown as dotted lines.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What color are the agents that learned to defend the leader agent?",
    "answer": "Blue",
    "rationale": "The caption states that the blue agents are the altruistic agents that learned to defend the leader agent.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.09598v4",
    "pdf_url": null
  },
  {
    "instance_id": "e802b78fb826460db9008edb42a55261",
    "figure_id": "1902.08412v1-Figure9-1",
    "image_file": "1902.08412v1-Figure9-1.png",
    "caption": " Change in accuracy of Deepwalk on CITESEER.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most robust to changes in the graph structure?",
    "answer": "A-Meta-Both.",
    "rationale": "As the percentage of edges changed increases, the accuracy of all methods decreases. However, A-Meta-Both consistently has the highest accuracy for all levels of edge changes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.08412v1",
    "pdf_url": null
  },
  {
    "instance_id": "cb6b3caba658429da307c0f6d362e9a4",
    "figure_id": "1912.00426v2-Figure7-1",
    "image_file": "1912.00426v2-Figure7-1.png",
    "caption": " Times of redundant validations of area queries with various query sizes",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach is more efficient in terms of reducing redundant validations, the traditional approach or our approach?",
    "answer": "Our approach is more efficient.",
    "rationale": "The figure shows that for all query sizes, the number of redundant validations is lower for our approach than for the traditional approach. This indicates that our approach is more efficient in reducing redundant validations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.00426v2",
    "pdf_url": null
  },
  {
    "instance_id": "98d96021f24b484b846f4c248c966e43",
    "figure_id": "2004.03967v1-Figure19-1",
    "image_file": "2004.03967v1-Figure19-1.png",
    "caption": " Simplified affordances in the 3DSSG dataset, sorted by occurrence, presented in logarithmic scale. Please note that for visualization purposes nouns and prepositions are removed such that e.g. hanging in or hanging on are combined into hanging.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which affordance is the most frequent in the 3DSSG dataset?",
    "answer": "Carrying.",
    "rationale": "The bar for \"carrying\" is the longest, which indicates that it is the most frequent affordance in the dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.03967v1",
    "pdf_url": null
  },
  {
    "instance_id": "364e7ae517c343b9bb183dc8442dae00",
    "figure_id": "2204.11396v1-Figure5-1",
    "image_file": "2204.11396v1-Figure5-1.png",
    "caption": " Visual comparisons of our proposed method with various state-of-the-art methods on the Vimeo90K.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods produced the most blurry and distorted results?",
    "answer": "Sepconv-L1 and R-SepConv.",
    "rationale": "The figure shows the results of different video frame interpolation methods. Sepconv-L1 and R-SepConv produced the most blurry and distorted results, as can be seen in the examples of the boy in the classroom, the car on the highway, and the man by the window.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.11396v1",
    "pdf_url": null
  },
  {
    "instance_id": "16c67092df164796af20284e7515e3f8",
    "figure_id": "2311.00664v1-Figure9-1",
    "image_file": "2311.00664v1-Figure9-1.png",
    "caption": " Performance comparison between different encoders and data modalities on the N24News multimodal dataset. On the right, the accuracy of models trained end-to-end on a single data modality (Score) and their average norm (Scale). On the left the stitching performance between pairs of encoders and decoder. This shows the importance of translating from good encoders, that can even improve unimodal decoder performances. Results obtained with 2000 anchors and SVD, with a MLP as classification head.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which encoder performed the best on the N24News multimodal dataset?",
    "answer": "ViT-base-224",
    "rationale": "The figure shows the performance of different encoders on the N24News multimodal dataset. The encoder with the highest score is ViT-base-224, which achieved a score of 0.46.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2311.00664v1",
    "pdf_url": null
  },
  {
    "instance_id": "e6f142b1cfb44e12b6094f538842b532",
    "figure_id": "1812.02713v1-Figure16-1",
    "image_file": "1812.02713v1-Figure16-1.png",
    "caption": " Hierarchical instance-level segmentation visualization (2/3). We present visualization for example hierarchical instancelevel segmentation annotations for dishwasher, laptop, display, trash can, door (door set), earphone, vase (pot), and keyboard. The lamp examples are shown in the main paper. The And-nodes are drawn in solid lines and Or-nodes in dash lines.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two types of nodes used in the hierarchical instance-level segmentation visualization?",
    "answer": "And-nodes and Or-nodes.",
    "rationale": "The caption states that \"The And-nodes are drawn in solid lines and Or-nodes in dash lines.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.02713v1",
    "pdf_url": null
  },
  {
    "instance_id": "d9a7cbb4852d4923bfafacd1bdbe3be0",
    "figure_id": "2202.04557v2-Figure6-1",
    "image_file": "2202.04557v2-Figure6-1.png",
    "caption": " Comparison of auto- vs. heteroassociative MCHN and HNs on retrieval task. For both, given a corrupted image, the heteroassociative task was to retrieve only the bottom half. The MCHN was queried with CIFAR10 images corrupted with Gaussian noise of variance 0.5. The HN was tested on binarized MNIST images where the query was the top half of the image. Error bars are the standard deviations of the retrieval capacity over 10 runs. The performance of the HN is extremely poor due to interference between memories caused by its identity separation function. In both cases, the differences between auto- and heteroassociative capacity are negligible",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network performs better on the retrieval task, the Classical Hopfield Network or the MCHN?",
    "answer": "The MCHN performs better on the retrieval task.",
    "rationale": "The figure shows that the MCHN has a higher fraction of correctly retrieved images than the Classical Hopfield Network for both autoassociative and heteroassociative tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.04557v2",
    "pdf_url": null
  },
  {
    "instance_id": "d59cada9f1c44c4696a57bb662aa05d3",
    "figure_id": "2305.11759v1-Figure3-1",
    "image_file": "2305.11759v1-Figure3-1.png",
    "caption": " The change in fractional extraction rates against prompt length (3-A1, 3-A2), suffix size (3-B1, 3-B2), prefix size (3-C1, 3-C2) and beam size (3-D1, 3-D2). Top panels show the GPT-Neo-125M results while the bottom panels show GPT-Neo-1.3B results. The transparent polygons about each line represent 95% confidence intervals across the points.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model (GPT-Neo-125M or GPT-Neo-1.3B) shows the most improvement in fractional extraction rate when using the CLM attack with a prompt length of 150 compared to the baseline attack?",
    "answer": "GPT-Neo-1.3B",
    "rationale": "Comparing panels A1 and A2, we can see that for a prompt length of 150, the CLM attack results in a fractional extraction rate of approximately 0.85 for GPT-Neo-1.3B, while the baseline attack only achieves a rate of around 0.7. This represents a significant improvement for the larger model. In contrast, for GPT-Neo-125M, the CLM attack with a prompt length of 150 achieves a rate of approximately 0.45, while the baseline attack achieves a rate of around 0.4, indicating a much smaller improvement.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.11759v1",
    "pdf_url": null
  },
  {
    "instance_id": "c1c1283dbac84cb9a74f960e3674b829",
    "figure_id": "2010.13997v3-Figure4-1",
    "image_file": "2010.13997v3-Figure4-1.png",
    "caption": " (a)-(b) Average cumulative regret against wall clock time for different algorithms on benchmark functions. (d) Computation time (in seconds) for 1000 samples for different algorithms.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest regret for the Branin function?",
    "answer": "IGP UCB",
    "rationale": "The IGP UCB line is the highest in the plot for the Branin function.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.13997v3",
    "pdf_url": null
  },
  {
    "instance_id": "af1186f89dd04d42bce6fc8dcf93373d",
    "figure_id": "1806.09186v3-Figure2-1",
    "image_file": "1806.09186v3-Figure2-1.png",
    "caption": " Average detection accuracy for detectors against FGSM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest average detection accuracy?",
    "answer": "RBF-SVM",
    "rationale": "The figure shows the average detection accuracy for different methods against FGSM. The RBF-SVM method has the lowest average detection accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.09186v3",
    "pdf_url": null
  },
  {
    "instance_id": "9a1be8b82f9d4028962608cb0a079304",
    "figure_id": "2006.03158v2-Figure1-1",
    "image_file": "2006.03158v2-Figure1-1.png",
    "caption": " Task loss (solid) and perplexity (dashed) as α varies.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of perplexity?",
    "answer": "MGS.",
    "rationale": "The plot shows that MGS has the lowest perplexity across all values of α.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.03158v2",
    "pdf_url": null
  },
  {
    "instance_id": "65a76341d0854accb0c376b19f47abfa",
    "figure_id": "2301.11494v3-Figure15-1",
    "image_file": "2301.11494v3-Figure15-1.png",
    "caption": " Error analysis on a second synthetic video. The top row plots the inference errors of velocity, vorticity, and compressibility. The bottom row plots the future prediction errors, which consider both the dynamic error of the velocity and the perceptual error of the generated image sequence.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest future prediction error for image RMSE?",
    "answer": "Ours",
    "rationale": "The bottom right plot shows the future prediction errors for image RMSE. The line labeled \"Ours\" is the lowest line on the plot, indicating that this method has the lowest future prediction error for image RMSE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.11494v3",
    "pdf_url": null
  },
  {
    "instance_id": "84cdcbf54ba541a4b489554af4ee1c66",
    "figure_id": "1908.01801v2-Figure1-1",
    "image_file": "1908.01801v2-Figure1-1.png",
    "caption": " We propose the PReFIL algorithm for chart question answering (CQA). PReFIL surpasses the prior state-of-the-art (SoTA) and human baselines on DVQA and FigureQA datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the FigureQA dataset?",
    "answer": "Ours (PReFIL)",
    "rationale": "The figure shows the accuracy of different algorithms on the FigureQA dataset. The green bar, which represents Ours (PReFIL), is the highest, indicating that this algorithm performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.01801v2",
    "pdf_url": null
  },
  {
    "instance_id": "9738e47325924d23a4e947b8fbcd389f",
    "figure_id": "2109.09717v1-Figure7-1",
    "image_file": "2109.09717v1-Figure7-1.png",
    "caption": " Beach bar 2D: Training set",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the shape of the training set?",
    "answer": "Square.",
    "rationale": "The figure shows a square shape in the center of the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.09717v1",
    "pdf_url": null
  },
  {
    "instance_id": "00a58d76bdfd4c98b96c5791a967e029",
    "figure_id": "1907.10178v1-Figure10-1",
    "image_file": "1907.10178v1-Figure10-1.png",
    "caption": " The average log likelihood in dependence of the k̄ used for the transformation in (9).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of k̄ maximizes the average log likelihood for t = 1?",
    "answer": "k̄ = 1.0",
    "rationale": "The figure shows that the average log likelihood for t = 1 is maximized at k̄ = 1.0. This can be seen by following the blue line, which represents the average log likelihood for t = 1, to its peak.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.10178v1",
    "pdf_url": null
  },
  {
    "instance_id": "5dfe801f7fda4c22a1a199b23bc2df38",
    "figure_id": "1901.10879v6-Figure2-1",
    "image_file": "1901.10879v6-Figure2-1.png",
    "caption": " The P-R curve of different Open IE systems on OIE2016",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which Open IE system has the best precision at a recall of 0.5?",
    "answer": "BIO",
    "rationale": "The P-R curve shows that BIO has the highest precision at a recall of 0.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.10879v6",
    "pdf_url": null
  },
  {
    "instance_id": "63223e849d924dfd95262fdf059911d5",
    "figure_id": "2202.00914v2-Figure16-1",
    "image_file": "2202.00914v2-Figure16-1.png",
    "caption": " Average normalized state difference ‖sT − s0‖ of skill discovery methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which skill discovery method has the lowest average normalized state difference for the Ant environment with continuous skills?",
    "answer": "DADs (d=5)",
    "rationale": "The bar for DADs (d=5) is the shortest in the Ant environment with continuous skills.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.00914v2",
    "pdf_url": null
  },
  {
    "instance_id": "d64c7c8d7a0b4d1693345d5f27112b3d",
    "figure_id": "2003.11236v1-Figure5-1",
    "image_file": "2003.11236v1-Figure5-1.png",
    "caption": " Top-1 accuracy histogram of 30 systematically sampled paths from 1000 paths searched by evolutionary algorithm after trained from scratch.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which supernet method achieves the highest top-1 accuracy?",
    "answer": "Greedy supernet.",
    "rationale": "The greedy supernet has a taller bar than the uniform supernet at the highest top-1 accuracy bin (around 74.5%). This means that more greedy supernet paths achieved this high accuracy compared to uniform supernet paths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.11236v1",
    "pdf_url": null
  },
  {
    "instance_id": "814e3debe2cb4ea38e8aacfb893992cd",
    "figure_id": "1805.04514v2-Figure6-1",
    "image_file": "1805.04514v2-Figure6-1.png",
    "caption": " Return vs. training episodes on drifting mountain car for a fixed initial alpha value of 2−10 with best µ value for each tuning method based on average return over the final 100 episodes. Each curve shows the average of 20 repeats, smoothed over 40 episodes. While all tuning methods improve on the baseline to some degree, mixed Metatrace is generally best.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tuning method performs best for a drift rate of 4 × 10^-6?",
    "answer": "Mixed Metatrace",
    "rationale": "The figure shows that the Mixed Metatrace line is the highest for a drift rate of 4 × 10^-6. This indicates that Mixed Metatrace has the highest average return for this drift rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.04514v2",
    "pdf_url": null
  },
  {
    "instance_id": "e76b99e0f7f442c291aebd440d621546",
    "figure_id": "2004.09750v3-Figure4-1",
    "image_file": "2004.09750v3-Figure4-1.png",
    "caption": " Visual comparison between MiniSeg and other methods. Red: true positive; Green: false negative; Blue: false positive.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method appears to have the most false negatives?",
    "answer": "Attention UNet",
    "rationale": "The figure shows the results of different methods for segmenting lungs in CT slices. False negatives are shown in green. Attention UNet appears to have the most green pixels, indicating that it missed more lung tissue than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.09750v3",
    "pdf_url": null
  },
  {
    "instance_id": "44204824386d46feb1c4e215bd1bd0ae",
    "figure_id": "2210.10695v1-Figure2-1",
    "image_file": "2210.10695v1-Figure2-1.png",
    "caption": " Average time in milliseconds (in log scale) for retrieval (BM25 and BM25-QE) and re-ranking (kNN and CE) 1000 documents. Average over all queries in the test sets. For the Cross-Encoder we separate the time for fine-tuning and re-ranking.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest for retrieval?",
    "answer": "BM25",
    "rationale": "The figure shows the average time for each method, and BM25 has the lowest bar for all values of k.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.10695v1",
    "pdf_url": null
  },
  {
    "instance_id": "9f67e51ccaad4cd083b8c7add1f24605",
    "figure_id": "2305.16379v2-Figure23-1",
    "image_file": "2305.16379v2-Figure23-1.png",
    "caption": " Illustration of realistic observations captured in CARLA under various weather conditions.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which weather condition is the most challenging for autonomous driving?",
    "answer": "HardRainSunset",
    "rationale": "The HardRainSunset condition has the most rain and the least amount of light, which can make it difficult for cameras and sensors to see the road and other vehicles.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16379v2",
    "pdf_url": null
  },
  {
    "instance_id": "58075b632e134155a1df8da989a435e2",
    "figure_id": "2305.15790v2-Figure2-1",
    "image_file": "2305.15790v2-Figure2-1.png",
    "caption": " (Example 4.6) ApproxCC returns the ordering ⟨𝑡3, 𝑡2, 𝑡1, 𝑡4, 𝑡5, 𝑡6⟩",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the maximum flow from t1 to t6 after the second iteration of the algorithm?",
    "answer": "0.8",
    "rationale": "The maximum flow from t1 to t6 is the maximum flow through the path t1 -> t3 -> t4 -> t6. The capacity of this path is limited by the edge between t1 and t3, which has a capacity of 0.8. Therefore, the maximum flow from t1 to t6 after the second iteration is 0.8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.15790v2",
    "pdf_url": null
  },
  {
    "instance_id": "666d90a8cebf4260a61bb8985413a20b",
    "figure_id": "2109.12814v2-Figure4-1",
    "image_file": "2109.12814v2-Figure4-1.png",
    "caption": " Pattern-level F1 on different English datasets. Noted that we train NFC based on 3-gram pattern in English. There is no direct supervision signal for 2- gram pattern.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on the PTB dataset when measured by 3-gram pattern F1 score?",
    "answer": "Kitaev and Klein (2018)",
    "rationale": "The figure shows the F1 scores of two models, Kitaev and Klein (2018) and NFC, on different datasets when measured by 3-gram pattern F1 score. The bar for Kitaev and Klein (2018) is higher than the bar for NFC on the PTB dataset, indicating that Kitaev and Klein (2018) performs better.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.12814v2",
    "pdf_url": null
  },
  {
    "instance_id": "82aaae7aa1f54673bcedaffbe8dc43f5",
    "figure_id": "2001.07444v1-Figure6-1",
    "image_file": "2001.07444v1-Figure6-1.png",
    "caption": " Class activation maps for local and full face ResNet for the Proposed Network.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which ResNet model seems to be most sensitive to the eyes?",
    "answer": "ResNet_3.",
    "rationale": "The class activation maps for ResNet_3 show the highest activation in the eye region, regardless of the compression level.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.07444v1",
    "pdf_url": null
  },
  {
    "instance_id": "2458a3899f5c4d798e46206d8601556d",
    "figure_id": "2010.05654v1-Figure13-1",
    "image_file": "2010.05654v1-Figure13-1.png",
    "caption": " Customized VIA project to support the labeling of active objects. Annotators were presented with a panel which allowed them to identify object classes through their thumbnails.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the purpose of the VIA project shown in the image?",
    "answer": "The VIA project is used to label active objects.",
    "rationale": "The figure shows a person using the VIA project to label objects in a video. The object class is highlighted in red, and the object bounding box is shown in yellow.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.05654v1",
    "pdf_url": null
  },
  {
    "instance_id": "57187fe2ad2c4e48bed73d185d0f97aa",
    "figure_id": "2303.17264v1-Figure11-1",
    "image_file": "2303.17264v1-Figure11-1.png",
    "caption": " The Koopman matrix spectrum of different models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the most eigenvalues that are close to the unit circle?",
    "answer": "KAE",
    "rationale": "The eigenvalues of a Koopman matrix are shown as dots in the figure. The closer the eigenvalues are to the unit circle, the better the model is at capturing the dynamics of the system. In the figure, the KAE model has the most eigenvalues that are close to the unit circle, which suggests that it is the best model at capturing the dynamics of the system.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.17264v1",
    "pdf_url": null
  },
  {
    "instance_id": "366740f0658d41cdaffe4f303fa38f57",
    "figure_id": "2107.06196v2-Figure3-1",
    "image_file": "2107.06196v2-Figure3-1.png",
    "caption": " Meta-learning of a highly rewarding digit 1.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods is able to learn the most accurate representation of the digit 1?",
    "answer": "AdaTS",
    "rationale": "The AdaTS method produces the most accurate representations of the digit 1, as can be seen in the bottom row of the figure. The other methods produce less accurate representations, with the TS method producing the least accurate representations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.06196v2",
    "pdf_url": null
  },
  {
    "instance_id": "6849eae5cd91446696f727b00fe90d35",
    "figure_id": "2110.14625v1-Figure8-1",
    "image_file": "2110.14625v1-Figure8-1.png",
    "caption": " Independent Gaussian target (d = 10000). Minimum (8a), mean (8b) and median (8c) effective sample size of q 7→ qi per second. Average acceptance rates in 8d and estimates of the eigenvalues of DL in 8e. Condition number of transformed Hessian C>Σ−1C in 8f.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampler achieves the highest median effective sample size per second (ESS/sec)?",
    "answer": "NUTS",
    "rationale": "Subfigure (c) shows the median ESS/sec for each sampler. The NUTS sampler has the highest median ESS/sec for all leapfrog steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14625v1",
    "pdf_url": null
  },
  {
    "instance_id": "b49a4f39621d4364bd656a9a78dfbfe6",
    "figure_id": "2003.01908v2-Figure17-1",
    "image_file": "2003.01908v2-Figure17-1.png",
    "caption": " The certification results of the Google Cloud Vision API using STAB+MSE (across various surrogate models) and MSE denoisers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which denoiser, Stab+MSE or MSE, consistently provides higher certified accuracy across all three ResNet architectures and for all values of r_2 radius?",
    "answer": "Stab+MSE",
    "rationale": "The plots show that the solid lines representing Stab+MSE are consistently above the dashed lines representing MSE, indicating higher certified accuracy for Stab+MSE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.01908v2",
    "pdf_url": null
  },
  {
    "instance_id": "3cbe58797f68453494a7489fda39e80a",
    "figure_id": "2211.08540v4-Figure13-1",
    "image_file": "2211.08540v4-Figure13-1.png",
    "caption": " Qualitative results for human reposing",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most accurate for human reposing?",
    "answer": "Our method.",
    "rationale": "The figure shows that our method produces results that are most similar to the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.08540v4",
    "pdf_url": null
  },
  {
    "instance_id": "23f2a98a4c1143f483b2ca6b40d3d1c5",
    "figure_id": "2002.00317v3-Figure5-1",
    "image_file": "2002.00317v3-Figure5-1.png",
    "caption": " Upper and lower bounds of BLEU for different choices of α.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " How does the BLEU score change as α increases? ",
    "answer": " The BLEU score increases as α increases. ",
    "rationale": " The figure shows that the upper bound of BLEU increases as α increases. This is because a higher value of α gives more weight to longer n-grams, which are more likely to be present in the reference translation. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.00317v3",
    "pdf_url": null
  },
  {
    "instance_id": "aa0b4b0fe86745e590238b8e7fe3f20a",
    "figure_id": "2107.14762v2-Figure1-1",
    "image_file": "2107.14762v2-Figure1-1.png",
    "caption": " Baseline and improved performance of five popular small models. The gray boxes represent the SEED distillation results with ResNet50 as teacher network.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieved the highest accuracy when trained with the improved method for 800 epochs?",
    "answer": "ResNet50",
    "rationale": "The figure shows the Top-1 accuracy of five popular small models trained with different methods. The improved method with 800 epochs is represented by the orange bars. The ResNet50 model has the highest orange bar, indicating it achieved the highest accuracy with this training method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.14762v2",
    "pdf_url": null
  },
  {
    "instance_id": "0d4c6a69bdd14464a777c9df5aa97152",
    "figure_id": "2201.02331v1-Figure3-1",
    "image_file": "2201.02331v1-Figure3-1.png",
    "caption": " TNR vs |V(x)| reported by iDECODe on CIFAR-10 as iD.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest TNR?",
    "answer": "Places365",
    "rationale": "The TNR for Places365 is the highest at approximately 100%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.02331v1",
    "pdf_url": null
  },
  {
    "instance_id": "4be3888e8316474ca9bb157df2beb747",
    "figure_id": "1906.11301v1-Figure5-1",
    "image_file": "1906.11301v1-Figure5-1.png",
    "caption": " The representation of the BIGISSUES vector derived by this user’s decisions on big issues. Here, the user is CON for ABORTION and AFFIRMATIVE ACTION issues and PRO for the WELFARE issue.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the user's opinion on affirmative action?",
    "answer": "The user is against affirmative action.",
    "rationale": "The figure shows that the user's BIGISSUES vector has a value of 1 for \"Con\" and 0 for \"Pro\" and \"N/O\" for the affirmative action issue. This indicates that the user is against affirmative action.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.11301v1",
    "pdf_url": null
  },
  {
    "instance_id": "96803f4855e0476ab893d38b0d4025a6",
    "figure_id": "2001.00745v1-Figure8-1",
    "image_file": "2001.00745v1-Figure8-1.png",
    "caption": " Additional cases on 2D regression.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the following functions is most likely to be represented by the \"Ripple\" plot?",
    "answer": " A function with multiple local minima and maxima.",
    "rationale": " The \"Ripple\" plot shows a function with multiple peaks and valleys, which suggests that it has multiple local minima and maxima. This is a characteristic of functions that are not smooth, such as functions with sharp corners or discontinuities.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.00745v1",
    "pdf_url": null
  },
  {
    "instance_id": "133712c1a8c8462aa994608af00f344e",
    "figure_id": "1908.00672v1-Figure1-1",
    "image_file": "1908.00672v1-Figure1-1.png",
    "caption": " Alpha mattes of different models. From left to right, Deeplabv3+ [4], RefineNet [30], Deep Matting [49] and Ours. Bilinear upsampling fails to recover subtle details, but unpooling and our learned upsampling operator can produce much clear mattes with good local contrast.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model produced the clearest alpha matte with the best local contrast?",
    "answer": "Ours",
    "rationale": "The figure shows the alpha mattes produced by four different models: Deeplabv3+, RefineNet, Deep Matting, and Ours. The alpha matte produced by Ours is the clearest and has the best local contrast.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.00672v1",
    "pdf_url": null
  },
  {
    "instance_id": "0aedfb5b76954f31870807f1676a9b57",
    "figure_id": "2006.07882v2-Figure5-1",
    "image_file": "2006.07882v2-Figure5-1.png",
    "caption": " Histograms showing the mean across-cohort variability as a function of the distance to an event. The x-axis shows the time steps prior to (negative) or after (positive) an event boundary, while their y-axis depicts across-cohort variability. Please refer to Section 5.2.2 for more details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which brain mask shows the highest variability before the event boundary?",
    "answer": "The whole-brain mask.",
    "rationale": "The whole-brain mask (a) shows the highest variability before the event boundary, with a peak at -3 time steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.07882v2",
    "pdf_url": null
  },
  {
    "instance_id": "151360e6257c43d5bbbf094ed1e06ef3",
    "figure_id": "2004.14070v4-Figure3-1",
    "image_file": "2004.14070v4-Figure3-1.png",
    "caption": " Fig. (a) shows that FROMP outperforms weight-regularisation methods (see App. F.3 for numerical values). ‘Tx’ means Task x. Figs. (b) and (c) show average accuracy with respect to the number of memorable examples. A careful selection of memorable examples in FROMP gives better results than random examples in FRORP, especially when the memory size is small. For MNIST, the kernel in FROMP improves performance over FROMP-L2, which does not use a kernel.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the performance of FROMP compare to other methods on Split CIFAR and Permuted MNIST datasets?",
    "answer": "FROMP outperforms other methods on both datasets.",
    "rationale": "In Fig. (a), FROMP achieves higher validation accuracy than other methods on all tasks and the average of all tasks for Split CIFAR. In Figs. (b) and (c), FROMP consistently achieves higher validation accuracy than other methods as the number of examples increases for both Split CIFAR and Permuted MNIST datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14070v4",
    "pdf_url": null
  },
  {
    "instance_id": "da5f0b94d39d4e23a6efc7926044effd",
    "figure_id": "2009.04324v4-Figure6-1",
    "image_file": "2009.04324v4-Figure6-1.png",
    "caption": " Usefulness of Laplacian regularization. We illustrate the reconstruction based on our spectral filtering techniques based on the sole use of the covariance matrix Σ on the left, and on the sole use of the Laplacian matrix L on the right. We see that the covariance matrix does not capture the geometry of the problem, which contrasts with the use of Laplacian regularization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which matrix better captures the geometry of the problem, the covariance matrix or the Laplacian matrix?",
    "answer": "The Laplacian matrix.",
    "rationale": "The figure shows that the reconstruction based on the Laplacian matrix (right) better captures the geometry of the problem than the reconstruction based on the covariance matrix (left). This is because the Laplacian matrix takes into account the local neighborhood of each point, while the covariance matrix does not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.04324v4",
    "pdf_url": null
  },
  {
    "instance_id": "195d285cf0604696b42c029a0ee9d7ff",
    "figure_id": "2007.02747v2-Figure5-1",
    "image_file": "2007.02747v2-Figure5-1.png",
    "caption": " In-depth results of streaming session-based recommendation performance on Gowalla.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four algorithms (GAG, FGNN, GAG-50, and SSRM) performs the best in terms of Recall@5 on the Gowalla dataset?",
    "answer": "FGNN.",
    "rationale": "Figure (a) shows the Recall@5 performance of the four algorithms on the Gowalla dataset. The FGNN line is consistently above the other three lines, indicating that it has the highest Recall@5 score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.02747v2",
    "pdf_url": null
  },
  {
    "instance_id": "742f24251eb6434ca5b3faa15d972730",
    "figure_id": "2303.05952v1-Figure7-1",
    "image_file": "2303.05952v1-Figure7-1.png",
    "caption": " Visualization of Two-tower-based methods (e.g. CLIP) performance. Each axis represents the performance on a dataset with a certain metric. Each color represents different approach. The larger area that one approach covers, the better overall performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the image, which method has the best overall performance?",
    "answer": "CLIP has the best overall performance.",
    "rationale": "CLIP is the method with the blue line in the image. This line encloses the largest area, which indicates that CLIP has the best performance across all datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.05952v1",
    "pdf_url": null
  },
  {
    "instance_id": "8f679f339780480abfbcfd5379da73d0",
    "figure_id": "2012.06979v1-Figure10-1",
    "image_file": "2012.06979v1-Figure10-1.png",
    "caption": " AFS vs baseline: MNIST: 4 vs 6, k = 20. Top: full experiment. Bottom: Zoom in.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the smallest mutual information gap at a budget of 100?",
    "answer": "AFS",
    "rationale": "The bottom plot shows that the AFS curve (green triangles) is below the other curves at a budget of 100.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.06979v1",
    "pdf_url": null
  },
  {
    "instance_id": "23f7e2c85c2748b885bf4577c078c4fb",
    "figure_id": "1911.00731v1-Figure2-1",
    "image_file": "1911.00731v1-Figure2-1.png",
    "caption": " An illustration of grid G and cube Cs centered at point s for d = 2. The point p belongs to G̃2 s and p′ is the parent of p.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which point in the figure is the parent of p?",
    "answer": "p'",
    "rationale": "The figure shows that p is in G̃2 s, which means that it is a child of p'.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.00731v1",
    "pdf_url": null
  },
  {
    "instance_id": "ea251e0eed694de9bd0f0bb4f3b73815",
    "figure_id": "2006.06068v4-Figure1-1",
    "image_file": "2006.06068v4-Figure1-1.png",
    "caption": " Example 1. Decay of Error of O-LMC (left) and U-LMC (right) with and without RCAD.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the fastest rate of error decay for both O-LMC and U-LMC?",
    "answer": "RCAD-O-LMC and RCAD-U-LMC.",
    "rationale": "The slopes of the lines in the plots indicate the rate of error decay. The steeper the slope, the faster the error decays. The lines for RCAD-O-LMC and RCAD-U-LMC have steeper slopes than the lines for RCD-O-LMC and RCD-U-LMC, respectively. This indicates that the RCAD methods have a faster rate of error decay than the RCD methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06068v4",
    "pdf_url": null
  },
  {
    "instance_id": "f15f5072477a4a1cae8a3c141b89cbb2",
    "figure_id": "2205.07857v1-Figure12-1",
    "image_file": "2205.07857v1-Figure12-1.png",
    "caption": " The entropy of the distribution decays when query goes on.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the fastest decrease in entropy?",
    "answer": "Karel.",
    "rationale": "The figure shows that the entropy of Karel decreases the most rapidly, followed by List-D2, and then List-D1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.07857v1",
    "pdf_url": null
  },
  {
    "instance_id": "072de12a403c49f4bfdd0108aab91763",
    "figure_id": "2203.08887v1-Figure15-1",
    "image_file": "2203.08887v1-Figure15-1.png",
    "caption": " Box-and-whisker plots showing the distribution of the operation distribution in NB201 benchmark on (a) CIFAR-10, (b) CIFAR-100 and (c) ImageNet16-120 datasets. The gray shaded areas denote the noise standard deviation which differs in each dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which operation has the highest importance on CIFAR-100?",
    "answer": "c3",
    "rationale": "The boxplot for c3 on CIFAR-100 is higher than the boxplots for the other operations, indicating that it has the highest importance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.08887v1",
    "pdf_url": null
  },
  {
    "instance_id": "b7153a7b18be4753be00f90274966c81",
    "figure_id": "1810.05997v6-Figure6-1",
    "image_file": "1810.05997v6-Figure6-1.png",
    "caption": " Accuracy of APPNP with propagation used only during training/inference. Best results are achieved with full propagation, but propagating only during inference also achieves good results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which propagation setting achieves the best results on the Cora-ML dataset?",
    "answer": "Inference & Training",
    "rationale": "The figure shows that the accuracy of APPNP on the Cora-ML dataset is highest when propagation is used during both inference and training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.05997v6",
    "pdf_url": null
  },
  {
    "instance_id": "b90d85d1f1694853bf6256378677ffe0",
    "figure_id": "2002.04932v2-Figure4-1",
    "image_file": "2002.04932v2-Figure4-1.png",
    "caption": " T-SNE visualization of the features learned byM3,M5, andM7, which are respectively shown from left to right in the first row. Typical example images from IDs #9,#11,#12,#14 are presented in the bottom row.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the IDs #9, #11, #12, and #14 is most likely to be an outlier?",
    "answer": "ID #14.",
    "rationale": "The T-SNE visualization shows that ID #14 is located far away from the other IDs, which suggests that it is an outlier. This is further supported by the fact that the images of ID #14 are different from the images of the other IDs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.04932v2",
    "pdf_url": null
  },
  {
    "instance_id": "f2d09680701d4b50a9bd8187f1a7b3e9",
    "figure_id": "2310.15165v1-Figure3-1",
    "image_file": "2310.15165v1-Figure3-1.png",
    "caption": " Weight divergence with respect to models’ centralized version on CIFAR-10. The use of BN leads to a larger weight divergence.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which model architecture has the highest weight divergence when using Batch Normalization (BN)?",
    "answer": " ResNet-BN+",
    "rationale": " The figure shows the weight divergence for different model architectures with and without BN. The bars representing models with BN (indicated by \"+\") are consistently higher than those without BN. Among the models with BN, ResNet-BN+ has the tallest bar, indicating the highest weight divergence. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.15165v1",
    "pdf_url": null
  },
  {
    "instance_id": "e85897918be943728b4698f3b17dc231",
    "figure_id": "2111.03628v1-Figure5-1",
    "image_file": "2111.03628v1-Figure5-1.png",
    "caption": " accuracy averaged over the 20 unseen tasks",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest accuracy when using 20 checkpoints?",
    "answer": "MMI",
    "rationale": "The figure shows that the red line (MMI) is above the green line (peak) and the blue line (random) at the point where k = 20.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.03628v1",
    "pdf_url": null
  },
  {
    "instance_id": "c7f5852f54744e0694e76ababb37da45",
    "figure_id": "2209.07007v2-Figure4-1",
    "image_file": "2209.07007v2-Figure4-1.png",
    "caption": " Histograms of the differences in MNIST (LeCun et al., 1998). Each histogram consists of 10,000 samples of (∆x,∆z), where ∆x (vertical) and ∆z (horizontal) respectively denote the differences ∆x = dX (x,x′) and ∆z = dZ(z, z′) of two generative samples (x, z), (x′, z′) ∼ pθ(x, z). In all reported results including FactorVAE (Kim & Mnih, 2018) (γ=3), WAE (Tolstikhin et al., 2018), and GWAE (NP, λD=1, λW=1, λH=1), the latent dimension L was set to L = 16, and their priors were set to the standard Gaussian.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which VAE model seems to generate samples with the most diversity?",
    "answer": "GWAE.",
    "rationale": "The GWAE plot shows the widest distribution of samples, indicating that the model is generating a wider variety of images than the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.07007v2",
    "pdf_url": null
  },
  {
    "instance_id": "918f146c8c964c3ea29bf2849f9faa14",
    "figure_id": "2106.04489v1-Figure3-1",
    "image_file": "2106.04489v1-Figure3-1.png",
    "caption": " Visualization of learned task embeddings by HYPERFORMER++BASE.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task is most similar to MNLI in terms of its embedding?",
    "answer": "SciTail",
    "rationale": "MNLI is represented by the point labeled \"mn\" in the figure, and SciTail is represented by the point labeled \"sc\". These two points are close together in the embedding space, indicating that the tasks are similar in terms of their learned representations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04489v1",
    "pdf_url": null
  },
  {
    "instance_id": "4a909abe60dd485897808beca2aae532",
    "figure_id": "2206.13289v1-Figure7-1",
    "image_file": "2206.13289v1-Figure7-1.png",
    "caption": " Example clusters: (a) LIWC:cause, (b) WORDNET:verb.cognition, (c) WORDNET:noun.artifact",
    "figure_type": "Other (Word clouds)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which cluster represents words related to clothing?",
    "answer": "Cluster (c)",
    "rationale": "Cluster (c) contains words such as \"shirt,\" \"jeans,\" \"shoes,\" \"boots,\" and \"bikini,\" which are all related to clothing.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.13289v1",
    "pdf_url": null
  },
  {
    "instance_id": "8b81a48cc3f844599361cf9682b0aeac",
    "figure_id": "1901.10946v3-Figure7-1",
    "image_file": "1901.10946v3-Figure7-1.png",
    "caption": " Comparison of imputed basketball trajectories. Black dots represent known observations (10 in first row, 5 in second). Overall, NAOMI produces trajectories that are the most consistent and have the most realistic player velocities and speeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most realistic player velocities and speeds?",
    "answer": "NAOMI.",
    "rationale": "The caption states that \"Overall, NAOMI produces trajectories that are the most consistent and have the most realistic player velocities and speeds.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.10946v3",
    "pdf_url": null
  },
  {
    "instance_id": "a6f208559ed0403089493c62ffc8cd76",
    "figure_id": "2205.06262v2-Figure4-1",
    "image_file": "2205.06262v2-Figure4-1.png",
    "caption": " Score ∆ by target task type. Lines show the average score ∆ when the target task is of the specified task type, computed as a best-fit linear interpolation of the data with a 95% confidence interval. The number of samples for an individual task are fixed, but source/target ratios vary depending on which task pair is used.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task type is most affected by the source/target sample ratio?",
    "answer": "Span Extraction.",
    "rationale": "The figure shows that the score ∆ for Span Extraction changes the most as the source/target sample ratio increases. The other task types are less affected by the source/target sample ratio.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.06262v2",
    "pdf_url": null
  },
  {
    "instance_id": "75faf9f4be57487684f1530c50e85d47",
    "figure_id": "2305.16816v1-Figure7-1",
    "image_file": "2305.16816v1-Figure7-1.png",
    "caption": " Error bar charts of comparative study of different prompt forms for controlling of different aspects. ref-: results in the tgt-const setting. src-: results in the src-const setting.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which prompt form performed the best for controlling length?",
    "answer": "dec-pref",
    "rationale": "The figure shows that the dec-pref prompt form achieved the highest BLEU score for tgt-BLEU in the Length Control Comparison section.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16816v1",
    "pdf_url": null
  },
  {
    "instance_id": "16d4dfc4353c4813bd57478c53fe36ce",
    "figure_id": "2002.11318v5-Figure8-1",
    "image_file": "2002.11318v5-Figure8-1.png",
    "caption": " On MNIST, (a-b) Aug - R StdCNN, (c-d) Aug - R GCNN, (e-f) Aug - T StdCNN, (g-h) Aug - T GCNN, (i-l) Aug - RT StdCNN, (m-p) Aug - RT GCNN, invariance profiles of StdCNN/GCNN models and corresponding robustness profiles.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is most robust to translation?",
    "answer": "Aug - RT GCNN is most robust to translation.",
    "rationale": "The plots in the figure show the rate of invariance and 1-to-ping rate as a function of the translation range for different models. The Aug - RT GCNN model has the highest rate of invariance and the lowest 1-to-ping rate for all translation ranges, indicating that it is the most robust to translation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11318v5",
    "pdf_url": null
  },
  {
    "instance_id": "0a4c1cfd3eff4f8c898578604086103d",
    "figure_id": "2302.07317v3-Figure5-1",
    "image_file": "2302.07317v3-Figure5-1.png",
    "caption": " SVHN, Balanced Accuracy",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the SVHN dataset, as measured by balanced accuracy?",
    "answer": "TAILOR Div (ours, AL algs)",
    "rationale": "The figure shows the balanced accuracy of different methods on the SVHN dataset. The TAILOR Div (ours, AL algs) method has the highest balanced accuracy of all the methods shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.07317v3",
    "pdf_url": null
  },
  {
    "instance_id": "450f5b30f062452e80aa0b6bf9a9177e",
    "figure_id": "2212.04663v1-Figure1-1",
    "image_file": "2212.04663v1-Figure1-1.png",
    "caption": " The relative L2 error in time (defined in (22)) for 1D Allen Cahn (16). The networks with “CONT\" refer to networks trained by using (5) whereas the others are trained by using (4). The prefix “TL\" means tuned by transfer learning. See implementation details in Appendix C.3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network has the lowest relative L2 error?",
    "answer": "FNO",
    "rationale": "The FNO line is the lowest on the plot, indicating that it has the lowest relative L2 error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.04663v1",
    "pdf_url": null
  },
  {
    "instance_id": "7e87b9c87acc42e898f5203425cd029e",
    "figure_id": "2206.01451v3-Figure10-1",
    "image_file": "2206.01451v3-Figure10-1.png",
    "caption": " Communication overhead for CTDE (a) grows linearly during training and (b) can have negative effects on the packet transmission latency of the whole networking system.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does the communication overhead increase or decrease with the number of agents?",
    "answer": "Increase.",
    "rationale": "Figure (a) shows that the communication overhead (measured in MiB) increases with the number of episodes. This is true for both 2 agents and 6 agents. The slope of the line for 6 agents is steeper than the slope of the line for 2 agents, indicating that the communication overhead increases more quickly with more agents.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.01451v3",
    "pdf_url": null
  },
  {
    "instance_id": "cdcb2935203b47ca8b13cb285815cd3a",
    "figure_id": "1805.08524v1-Figure3-1",
    "image_file": "1805.08524v1-Figure3-1.png",
    "caption": " The GMV increase with respect to rerank size.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most effective for increasing GMV?",
    "answer": "miRNN + attention",
    "rationale": "The figure shows that miRNN + attention has the highest GMV increase over baseline for all rerank sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.08524v1",
    "pdf_url": null
  },
  {
    "instance_id": "4f81e5deec7840f1b337b4c986f12b66",
    "figure_id": "2304.05947v1-Figure17-1",
    "image_file": "2304.05947v1-Figure17-1.png",
    "caption": " Isolating the impact of geometric fidelity by combining real images with geometry from the Internet models. We show cumulative histograms of the mean DCRE, as a percentage of the image diagonal, over all query images in a scene for the ground truth poses obtained via local refinement (LR).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of geometric fidelity on the mean DCRE?",
    "answer": "Geometric fidelity reduces the mean DCRE.",
    "rationale": "The cumulative histograms show that the mean DCRE is lower for the ground truth poses obtained via local refinement (LR) than for the poses obtained from the Internet models. This indicates that geometric fidelity improves the accuracy of pose estimation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.05947v1",
    "pdf_url": null
  },
  {
    "instance_id": "a51448613461424b84ab91844112397f",
    "figure_id": "2103.01453v1-Figure4-1",
    "image_file": "2103.01453v1-Figure4-1.png",
    "caption": " Histogram of simulated expected reward (CTR) for 200 ad creatives",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which CTR range has the most creatives?",
    "answer": "0.022 to 0.024",
    "rationale": "The figure shows that the highest bar in the histogram is in the range of 0.022 to 0.024. This means that the most creatives have a CTR in this range.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.01453v1",
    "pdf_url": null
  },
  {
    "instance_id": "aeb50026e94740e6af5a6260aae75165",
    "figure_id": "2308.10027v1-Figure6-1",
    "image_file": "2308.10027v1-Figure6-1.png",
    "caption": " The visual comparison of different settings involved in the ablation study.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which setting results in the most realistic image of the rock face?",
    "answer": " Ours",
    "rationale": " The image labeled \"Ours\" most closely resembles the ground truth (GT) image, which is the original photograph of the rock face. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.10027v1",
    "pdf_url": null
  },
  {
    "instance_id": "0898fbf6d76a41a99292eff8e449794f",
    "figure_id": "1901.08291v2-FigureD.1-1",
    "image_file": "1901.08291v2-FigureD.1-1.png",
    "caption": "Figure D.1: [d = 2] The results for the decision-maker with a = 0.2: The shaded regions in (a) denotes the average DP ± std. The shaded regions in (b), (c), and (d) denote 95% confidence intervals. The dotted line in (b), (c), and (d) denotes the significance level 0.05.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, case-control or stealth, has a higher average DP?",
    "answer": "Case-control.",
    "rationale": "The average DP for the case-control method is shown by the red dashed line in Figure D.1(a), while the average DP for the stealth method is shown by the blue solid line. The red dashed line is consistently higher than the blue solid line, indicating that the case-control method has a higher average DP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.08291v2",
    "pdf_url": null
  },
  {
    "instance_id": "771d8361898b4c128619371d12bd761a",
    "figure_id": "2306.10028v1-Figure9-1",
    "image_file": "2306.10028v1-Figure9-1.png",
    "caption": " Three clusters visualization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many clusters are visualized in the figure?",
    "answer": "Three",
    "rationale": "The figure shows three distinct groups of data points, which are colored differently.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.10028v1",
    "pdf_url": null
  },
  {
    "instance_id": "6a34380a9f674de0a05274e83a9a9f29",
    "figure_id": "2209.02535v2-Figure6-1",
    "image_file": "2209.02535v2-Figure6-1.png",
    "caption": " Average Simk(x̂, ŷ) for k = 100 by layer, where blue is when matching pairs are aligned, and orange is when pairs are shuffled within the layer. Top Left: FF keys and FF values. Top Right: The subheads of WO and WV . Bottom: The subheads of WQ and WK .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the highest average Simk(x̂, ŷ) for k = 100 when matching pairs are aligned?",
    "answer": "The last layer (layer 23)",
    "rationale": "The blue bars in the plots represent the average Simk(x̂, ŷ) for k = 100 when matching pairs are aligned. The height of the blue bars increases as the layer number increases, indicating that the average Simk(x̂, ŷ) is highest for the last layer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.02535v2",
    "pdf_url": null
  },
  {
    "instance_id": "dae3578045c249c694188d8045024213",
    "figure_id": "2104.09068v1-Figure5-1",
    "image_file": "2104.09068v1-Figure5-1.png",
    "caption": " Visual comparisons of different methods. Masked regions are visualized in red. Our method reconstructs coherent structures with fewer color artifacts. Zoom in for details.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most accurate reconstruction of the masked regions?",
    "answer": "Our method.",
    "rationale": "The figure shows that our method produced a more accurate reconstruction of the masked regions than the other methods. This is evident in the fact that our method produced fewer color artifacts and more coherent structures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.09068v1",
    "pdf_url": null
  },
  {
    "instance_id": "07f98fdda6d94e3ba9d135ffcbbe334e",
    "figure_id": "2305.04346v1-Figure3-1",
    "image_file": "2305.04346v1-Figure3-1.png",
    "caption": " Nodes in G originating from generation actions (yellow) and from pointer actions (blue).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which node in the figure represents a pointer action?",
    "answer": "The blue node.",
    "rationale": "The caption states that nodes originating from pointer actions are blue.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.04346v1",
    "pdf_url": null
  },
  {
    "instance_id": "bf3a72b88e0a464082b4a944fe1343c0",
    "figure_id": "2012.06046v2-Figure2-1",
    "image_file": "2012.06046v2-Figure2-1.png",
    "caption": " Test set AUC of end classifiers vs. number of iterations. IWS-LSE is compared to active learning, Snuba, and to using all training ground truth labels. Note that one iteration in this plot corresponds to one expert label. A comparison of true user effort needed to answer each type of query (label for one sample vs. label for one LF) will vary by application.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method requires the least amount of user effort to achieve a high AUC?",
    "answer": "Using all training ground truth labels.",
    "rationale": "The plot shows that using all training ground truth labels achieves the highest AUC with the least amount of user effort. This is because it does not require any active learning or expert queries.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.06046v2",
    "pdf_url": null
  },
  {
    "instance_id": "893313345c6c4117a986175edf04fed7",
    "figure_id": "2304.00295v1-Figure5-1",
    "image_file": "2304.00295v1-Figure5-1.png",
    "caption": " CelebA. The trade-off between AP and ∆DP / ∆EO. Fair-CDA outperforms other methods across tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of AP for the Smiling task?",
    "answer": "Fair-CDA",
    "rationale": "The figure shows that Fair-CDA has the highest AP for all values of ΔDP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.00295v1",
    "pdf_url": null
  },
  {
    "instance_id": "7f52a091b7c54efd8f8c88a0156a2aaf",
    "figure_id": "2302.08090v1-Figure7-1",
    "image_file": "2302.08090v1-Figure7-1.png",
    "caption": " QTrojan against QLSTM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three curves is the QTrojan?",
    "answer": "The orange curve is the QTrojan.",
    "rationale": "The legend in the bottom left corner of the figure indicates that the orange curve is the QTrojan.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.08090v1",
    "pdf_url": null
  },
  {
    "instance_id": "14a150333bd5440d97d5b0cbb48ba7ec",
    "figure_id": "2202.13196v2-Figure3-1",
    "image_file": "2202.13196v2-Figure3-1.png",
    "caption": " Elapsed time (ms) for the inference of 512 sentence pairs. The result of SimCSEavg and SimCSEcls are overlapped.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the fastest inference time for a sequence length of 100 tokens?",
    "answer": "SimCSE-avg and SimCSE-cls",
    "rationale": "The plot shows that the lines for SimCSE-avg and SimCSE-cls are overlapping and are below the line for CLRCMD for all sequence lengths. This means that SimCSE-avg and SimCSE-cls have a faster inference time than CLRCMD for all sequence lengths, including 100 tokens.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.13196v2",
    "pdf_url": null
  },
  {
    "instance_id": "905f1e7eda574f409e17b017193c4d6a",
    "figure_id": "2306.07117v1-Figure5-1",
    "image_file": "2306.07117v1-Figure5-1.png",
    "caption": " Buyers vs. sellers. Accuracy of Logistic Regression model across different input features, using buyer speech, seller speech, or both. Error bars indicate standard error.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of input feature results in the highest accuracy for the Logistic Regression model?",
    "answer": "Seller + Buyer speech",
    "rationale": "The figure shows that the \"Seller + Buyer\" line has the highest mean accuracy across all fractions of negotiation, indicating that using both buyer and seller speech as input features results in the highest accuracy for the Logistic Regression model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.07117v1",
    "pdf_url": null
  },
  {
    "instance_id": "967caa2a93144574a58f21e91d90380f",
    "figure_id": "2007.16002v1-Figure3-1",
    "image_file": "2007.16002v1-Figure3-1.png",
    "caption": " Two-layer GraphHeat for semi-supervised node classication. (a) select neighboring nodes (solid nodes) for target node a via e−sL. (b-c) the solid line represents edge in graph. Dotted line represents the weights between selected nodes and target node a. Representation of a is updated through weighted average of solid nodes and itself. Node color reflects the update of node representation. (d) label prediction using updated representation in the last layer.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the purpose of the two-layer GraphHeat process?",
    "answer": "To predict the label of a target node in a graph.",
    "rationale": "The figure shows how the GraphHeat process works. In the first layer, the neighboring nodes of the target node are selected. In the second layer, the representation of the target node is updated through a weighted average of the selected nodes and itself. Finally, the label of the target node is predicted using the updated representation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.16002v1",
    "pdf_url": null
  },
  {
    "instance_id": "1af12a8b3fc247a3bd9b3d1c55b2f494",
    "figure_id": "1810.07218v3-Figure4-1",
    "image_file": "1810.07218v3-Figure4-1.png",
    "caption": " Results on tiered-ImageNet with {50, 100, 150, 200} base classes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on average across the different numbers of base classes, LR or MLP?",
    "answer": "MLP",
    "rationale": "The figure shows the accuracy of different models on tiered-ImageNet with different numbers of base classes. The bars for MLP are consistently higher than the bars for LR, indicating that MLP performs better on average.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.07218v3",
    "pdf_url": null
  },
  {
    "instance_id": "604c93ff4fd44b63a5a3e887961705d0",
    "figure_id": "2106.05184v3-Figure9-1",
    "image_file": "2106.05184v3-Figure9-1.png",
    "caption": " CDF of metrics for per-group Random Forest models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric has the highest variance?",
    "answer": "Recall",
    "rationale": "The CDF of Recall has the most spread out curve, indicating that it has the highest variance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05184v3",
    "pdf_url": null
  },
  {
    "instance_id": "fd9ca44279fb42e0a3cb19a2b3aa6eda",
    "figure_id": "1907.11922v2-Figure15-1",
    "image_file": "1907.11922v2-Figure15-1.png",
    "caption": " Visual results of attribute transfer for a specific attribute: Smiling. * means the model is trained with a size of 256 × 256.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best in transferring the attribute of smiling?",
    "answer": "Our model.",
    "rationale": "The figure shows the results of attribute transfer for the attribute of smiling. The \"Our\" column shows the results of our model, and the other columns show the results of other models. The images in the \"Our\" column are the most similar to the target images, which indicates that our model performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.11922v2",
    "pdf_url": null
  },
  {
    "instance_id": "5ff750c4720646a08d0d2a4a2a284143",
    "figure_id": "2208.07638v1-Figure4-1",
    "image_file": "2208.07638v1-Figure4-1.png",
    "caption": " Hyperparameter analysis on NELL995 (Hits@3m). The default setting is: 8 layers, 1024 for hidden size, 𝛼 = 0.7 for label smoothing, and 32 experts.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the optimal number of layers for the model based on the Avg Hit@3 metric?",
    "answer": "8 layers.",
    "rationale": "The figure shows that the Avg Hit@3 metric is highest when the number of layers is 8. This suggests that 8 layers is the optimal number of layers for the model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.07638v1",
    "pdf_url": null
  },
  {
    "instance_id": "9349fa6d58b34d3abe01f4ab36d6e001",
    "figure_id": "2007.03778v2-Figure1-1",
    "image_file": "2007.03778v2-Figure1-1.png",
    "caption": " 3D shape understanding from vision and touch includes: (1) shape sensing with a camera and touch sensor, as well as (2) reconstruction algorithm that fuses vision and touch readings. In this paper, we introduce a dataset that captures object sensing and propose a chart-based fusion model for 3D shape prediction from multi-modal inputs. For touch, we realistically simulate an existing vision-based tactile sensor [41].",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two main components of 3D shape understanding from vision and touch?",
    "answer": "Shape sensing and reconstruction algorithms.",
    "rationale": "The figure shows the two main components of 3D shape understanding: shape sensing with a camera and touch sensor, and reconstruction algorithms that fuse vision and touch readings.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.03778v2",
    "pdf_url": null
  },
  {
    "instance_id": "3330768d5e47466b90f0dad26e5bf685",
    "figure_id": "2004.14923v2-Figure2-1",
    "image_file": "2004.14923v2-Figure2-1.png",
    "caption": " (a) Clustering of TED-53 using the SVCCA-53 representations. At the left, we include the Elbow and Silhouette criteria to define the number of clusters. For the former, it is not clear what is the value to choose, whereas for the later we automatically select the highest peak at ten clusters. (b-d) Elbow method, silhouette analysis and dendrograms for SVCCA-23(US ,LW ) with 30 additionally projected languages, US and LT .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods produced the most consistent results in terms of the number of clusters?",
    "answer": "SVCCA-23(US ,LW )",
    "rationale": "The silhouette analysis plots for all four methods show a peak at 10 clusters. However, the elbow method plots for SVCCA-53, US, and LT are less clear, with multiple possible choices for the number of clusters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14923v2",
    "pdf_url": null
  },
  {
    "instance_id": "d5c4a8c9b7534fa198980b6e602d839e",
    "figure_id": "2006.15090v2-Figure4-1",
    "image_file": "2006.15090v2-Figure4-1.png",
    "caption": " Comparison of the memory consumption for a single gradient evaluation. With D = 5000 our simplified analysis predicts a lower bound in the memory consumption of 400 MB for storing the parameters and the computed gradients; given that at startup time we observe a base memory consumption of almost 200 MB (computing environment + loaded libraries) we can see that our relative gradient implementation comes very close to the theoretical limit. For the naive autodiff implementation, instead, we compute a lower bound of 10.4 GB, which is approximately reflected in the empirical measurements (maximum consumption is almost 13 GB). Note: memory consumption for the autodiff case is reported in GiB, effectively making the scale of the plot one order of magnitude higher then in the relative gradient plot.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How much memory is used by the naive autodiff implementation when the memory consumption is at its highest?",
    "answer": "Approximately 13 GB.",
    "rationale": "The plot on the right shows the memory usage of the naive autodiff implementation over time. The peak of the curve is around 13 GB.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.15090v2",
    "pdf_url": null
  },
  {
    "instance_id": "9dda1f7559ad4951ab9cf1f87b9c07db",
    "figure_id": "2210.01035v1-Figure12-1",
    "image_file": "2210.01035v1-Figure12-1.png",
    "caption": " Comparison with EViT [44] on ADE20K and PASCAL-Context semantic segmentation task based on Segmenter with ViT-L/16. ↑ and ↓ represent higher is better and lower is better respectively.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in terms of mIoU on ADE20K?",
    "answer": "Segmenter + Ours.",
    "rationale": "Figure (a) shows that Segmenter + Ours achieves the highest mIoU (51%) on ADE20K compared to the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.01035v1",
    "pdf_url": null
  },
  {
    "instance_id": "c0a3db37f4fd43f4bf9f2c887de8f3ab",
    "figure_id": "1901.06077v1-Figure4-1",
    "image_file": "1901.06077v1-Figure4-1.png",
    "caption": " AUC vs. different window size wr on Bee-Dance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best across all window sizes?",
    "answer": "KL-CPD.",
    "rationale": "The KL-CPD bars are consistently higher than the other methods across all window sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.06077v1",
    "pdf_url": null
  },
  {
    "instance_id": "dbc09e36f2db4ef58c04d643279469a9",
    "figure_id": "2011.04926v1-Figure5-1",
    "image_file": "2011.04926v1-Figure5-1.png",
    "caption": " Training process of JS-GAN and RS-GAN for two-cluster data. True data are red, fake data are blue. RS-GAN escapes from mode collapse faster than JS-GAN.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GAN is more likely to suffer from mode collapse?",
    "answer": "JS-GAN",
    "rationale": "The figure shows that RS-GAN escapes from mode collapse faster than JS-GAN. This means that JS-GAN is more likely to suffer from mode collapse.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.04926v1",
    "pdf_url": null
  },
  {
    "instance_id": "69e675b0271343d0b7edafd5cb14c021",
    "figure_id": "2012.02780v1-Figure4-1",
    "image_file": "2012.02780v1-Figure4-1.png",
    "caption": " Visual comparisons of different methods for few-shot generation. Top: FFHQ→ Emoji; Bottom: Natural landscape→ Pencil landscape. In each example, Left: 10-shot training examples; Right: 10 generated results by each method (all images are of size 256×256).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method generated the most realistic-looking images?",
    "answer": "Ours.",
    "rationale": "The images generated by our method are the most visually appealing and realistic-looking, especially when compared to the other methods. For example, in the top row, the images generated by our method are more detailed and have more realistic facial features than the images generated by the other methods. In the bottom row, the images generated by our method are more detailed and have more realistic landscapes than the images generated by the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.02780v1",
    "pdf_url": null
  },
  {
    "instance_id": "a4703a4d52434ee397b0f089e43015c3",
    "figure_id": "2106.01540v2-Figure1-1",
    "image_file": "2106.01540v2-Figure1-1.png",
    "caption": " Trade-off between accuracy (y-axis), speed (x-axis) and memory (cir-radius) on LRA.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest accuracy on the LRA task?",
    "answer": "Luna-256",
    "rationale": "The y-axis of the figure represents the average LRA score (w/o retrieval), which is a measure of accuracy. Luna-256 is the highest point on the y-axis, indicating that it achieves the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01540v2",
    "pdf_url": null
  },
  {
    "instance_id": "eab3e6a438b1423b88834c9723f9d609",
    "figure_id": "2302.13848v2-Figure15-1",
    "image_file": "2302.13848v2-Figure15-1.png",
    "caption": " Cross-attention map visualization. We show the average attention across timestep and layers for each word embedding. The attention map corresponding to the learned primary word w0 delineates the subject region.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which word embedding is most likely to correspond to the subject of the image?",
    "answer": "w0",
    "rationale": "The caption states that the attention map corresponding to the learned primary word w0 delineates the subject region. The figure shows that the attention map for w0 is focused on the cat and the dog, which are the subjects of the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.13848v2",
    "pdf_url": null
  },
  {
    "instance_id": "f430a3a60b7442158874635e1a15d473",
    "figure_id": "1905.08977v1-Figure6-1",
    "image_file": "1905.08977v1-Figure6-1.png",
    "caption": " Computational cost of our methods MaxLogHash and MaxLogOPH in comparison with MinHash, HyperLogLog and HyperMinHash.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest for updating the sketch?",
    "answer": "HyperMinHash.",
    "rationale": "The figure shows the update time for different methods as a function of the number of elements inserted into the sketch. HyperMinHash has the lowest update time for all values of k.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.08977v1",
    "pdf_url": null
  },
  {
    "instance_id": "a8c1bab6fe3d41719c46277edac1a065",
    "figure_id": "2308.07795v1-Figure1-1",
    "image_file": "2308.07795v1-Figure1-1.png",
    "caption": " Motivation of the proposed method. In the illustrated race between a turtle and a rabbit, the sleep state is critical in determining the winner of the race. Our method is proposed to identify such critical states.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the critical state in the race between the rabbit and the turtle?",
    "answer": "Sleep.",
    "rationale": "The figure shows that the rabbit loses the race because it sleeps while the turtle continues to walk. This suggests that sleep is a critical state that can determine the outcome of the race.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.07795v1",
    "pdf_url": null
  },
  {
    "instance_id": "1d0979128fd14e14b9fb740a98680901",
    "figure_id": "2206.05564v2-Figure6-1",
    "image_file": "2206.05564v2-Figure6-1.png",
    "caption": " Comparison between L (Upper) andR (Lower) with exponential integrator on CIFAR10.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods appears to converge faster?",
    "answer": "Method R (lower) appears to converge faster.",
    "rationale": "The figure shows that the R method produces images that are closer to the original images after fewer epochs than the L method. For example, the R method produces images that are recognizable after 10 epochs, while the L method produces images that are still very noisy after 30 epochs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.05564v2",
    "pdf_url": null
  },
  {
    "instance_id": "615ef10317ee4218aabaa2064f0488ff",
    "figure_id": "1911.07156v1-Figure5-1",
    "image_file": "1911.07156v1-Figure5-1.png",
    "caption": " Experimental Results",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which comparison method has the highest precision?",
    "answer": "LINE1 + LINE2 + HAN",
    "rationale": "The figure shows the precision, AUC, and recall for different comparison methods. The LINE1 + LINE2 + HAN method has the highest precision.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.07156v1",
    "pdf_url": null
  },
  {
    "instance_id": "50f1c62c730e44c2acb1ccc82519f7ff",
    "figure_id": "2210.03575v2-Figure8-1",
    "image_file": "2210.03575v2-Figure8-1.png",
    "caption": " Examples of compositionality judgments shown to annotators",
    "figure_type": "\"other\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following phrases is an example of a non-compositional phrase?",
    "answer": "\"Ivory tower\"",
    "rationale": "The figure shows that the phrase \"ivory tower\" is an example of a non-compositional phrase because its meaning cannot be inferred from its parts. The meaning of \"ivory tower\" is that someone or something is out of touch with ordinary people, but this meaning is not readily apparent from the words \"ivory\" and \"tower.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.03575v2",
    "pdf_url": null
  },
  {
    "instance_id": "d010208fd85d4de18f9f9be235838fdb",
    "figure_id": "1902.04620v1-Figure1-1",
    "image_file": "1902.04620v1-Figure1-1.png",
    "caption": " Memory-performance tradeoff for language modeling with a Transformer network: final validation perplexity vs. optimizer parameter count. Note that the horizontal scale is logarithmic, so optimizer memory savings are by orders of magnitude.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer has the lowest final validation perplexity?",
    "answer": "AdaGrad",
    "rationale": "The figure shows that AdaGrad has the lowest final validation perplexity of all the optimizers shown. This can be seen by looking at the y-axis of the figure, which shows the final validation perplexity. AdaGrad is the red dot on the figure, and it is the lowest point on the y-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.04620v1",
    "pdf_url": null
  },
  {
    "instance_id": "bd622a02d8ff4929aea8480abb5c8c13",
    "figure_id": "1909.02583v2-Figure3-1",
    "image_file": "1909.02583v2-Figure3-1.png",
    "caption": " Time vs Attack magnitude along action dimension for LAS attacks with B = 4, H = 5 in Lunar Lander environment with PPO RL agent. (a) Variation of attack magnitude along Up-Down and Left-Right action dimensions through different episodes. In all episodes except episode 2, Up-Down action is more heavily attacked than Left-Right. (b) Variation of attack magnitude through time for episode 1 of (a). After 270 steps, the agent is not attacked in the Left-Right dimension, but heavily attacked in Up-Down directions. (c) Actual rendering of Lunar Lander environment for episode 1 of (a) corresponding to (b). Frame 1-5 are strictly increasing time steps showing trajectory of the RL agent controlling the lunar lander.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which action dimension is more heavily attacked in episode 1 of the Lunar Lander LAS attack with B = 4, H = 5?",
    "answer": "Up-Down action dimension",
    "rationale": "The figure shows the variation of attack magnitude along the Up-Down and Left-Right action dimensions through different episodes in (a) and through time for episode 1 in (b). In (a), the bars for Up-Down action are taller than the bars for Left-Right action in episode 1, indicating that Up-Down action is more heavily attacked. This is further confirmed in (b), where the attack magnitude for Up-Down action is higher than that for Left-Right action after 270 steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.02583v2",
    "pdf_url": null
  },
  {
    "instance_id": "e18faa7505424a70b09ee4f5f043fcdb",
    "figure_id": "2106.08185v2-Figure2-1",
    "image_file": "2106.08185v2-Figure2-1.png",
    "caption": " Negative log likelihood values for three different noise models when used alongside the RBF kernel. All of the datasets clearly benefit from the modelling of heterescedastic noise, while three benefit from the additional freedom offered by the shift parameter.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three noise models performed best on the Boston dataset?",
    "answer": "Shifted linear noise",
    "rationale": "The figure shows that the green diamond, which represents shifted linear noise, has the lowest NLPD value for the Boston dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.08185v2",
    "pdf_url": null
  },
  {
    "instance_id": "543da171a1774ed1bbcc5263119e57ec",
    "figure_id": "2308.11796v1-Figure5-1",
    "image_file": "2308.11796v1-Figure5-1.png",
    "caption": " The per class performance of DINO and TIMET is shown for the clustering experiment with K=GT. Cluster-based foreground extraction [74] has been applied to both methods. As it is seen, this paper almost always improves the baseline performance for this evaluation as well. Pascal VOC is used in this experiment.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better for the \"person\" class, DINO or TimeT?",
    "answer": "DINO performs better for the \"person\" class.",
    "rationale": "The figure shows the per-class performance of DINO and TimeT for a clustering experiment. The bars represent the performance of each method for each class. The DINO bar for the \"person\" class is higher than the TimeT bar, indicating that DINO performs better for that class.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.11796v1",
    "pdf_url": null
  },
  {
    "instance_id": "101a376ea1d04c5e83dd01dfee1891a1",
    "figure_id": "2009.02637v1-Figure6-1",
    "image_file": "2009.02637v1-Figure6-1.png",
    "caption": " The left part shows ten community affiliation scores of three selected users in the shopping graph. The right part shows the keywords of their purchased products.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which user is most likely to be interested in purchasing a maxi dress?",
    "answer": "User u1.",
    "rationale": "The figure shows the community affiliation scores of three users and the keywords of their purchased products. The user with the highest community affiliation score for community c1 is u1, and the keywords for u1's purchased products include \"maxi dress\". Therefore, user u1 is most likely to be interested in purchasing a maxi dress.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.02637v1",
    "pdf_url": null
  },
  {
    "instance_id": "ecd9b0a59ecf4e0cbc7e48a4a2e0d7ec",
    "figure_id": "2305.16483v2-Figure2-1",
    "image_file": "2305.16483v2-Figure2-1.png",
    "caption": " Performance on queuing systems",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which scheduling algorithm performs the best in the Phases Criss-Cross network?",
    "answer": "DQN-ASG",
    "rationale": "The figure shows the testing reward for different scheduling algorithms in the Phases Criss-Cross network. The DQN-ASG algorithm has the highest testing reward, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16483v2",
    "pdf_url": null
  },
  {
    "instance_id": "b18ee46e37ae42b69be6ae667125f16a",
    "figure_id": "2202.09422v2-Figure1-1",
    "image_file": "2202.09422v2-Figure1-1.png",
    "caption": " Comparison of our algorithm with Full-Communication, IL, and Random.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms performed the best on the prey tasks?",
    "answer": "Full-Communication",
    "rationale": "The figure shows that the Full-Communication algorithm achieved the highest episodic reward on both prey tasks (prey_n15 and prey_n30). This is evident from the fact that the blue line, which represents the Full-Communication algorithm, is consistently above the other lines in these two tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.09422v2",
    "pdf_url": null
  },
  {
    "instance_id": "f71cb62c0d59426fa6d14e7ed2deb7bb",
    "figure_id": "2110.06149v2-Figure6-1",
    "image_file": "2110.06149v2-Figure6-1.png",
    "caption": " Success rates across the three environments. One-shot planning is competitive with the full method on shorter time horizons.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best on the ProcgenMaze environment?",
    "answer": "PPGS",
    "rationale": "The figure shows the success rate of different algorithms on three different environments. The PPGS algorithm has the highest success rate on the ProcgenMaze environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.06149v2",
    "pdf_url": null
  },
  {
    "instance_id": "2cbc187fe6894df88fc77177702e6571",
    "figure_id": "2202.13290v1-Figure4-1",
    "image_file": "2202.13290v1-Figure4-1.png",
    "caption": " Results for desktop and mobile recordings",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of distortion has the highest MOS score for desktop recordings?",
    "answer": "DT Echo DMOS",
    "rationale": "The table shows the MOS scores for different types of distortion for desktop and mobile recordings. The DT Echo DMOS column has the highest MOS score for desktop recordings, with a score of 4.48.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.13290v1",
    "pdf_url": null
  },
  {
    "instance_id": "25f67583a53e47b7b7daf441d20e2ed2",
    "figure_id": "2208.12666v1-Figure1-1",
    "image_file": "2208.12666v1-Figure1-1.png",
    "caption": " No. of hours of data obtained from Regional radio stations of All India Radio",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which region has the most data available from regional radio stations, according to the figure?",
    "answer": "Hindi",
    "rationale": "The figure shows the number of hours of data obtained from regional radio stations of All India Radio. The largest number is associated with Hindi, indicating that this region has the most data available.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.12666v1",
    "pdf_url": null
  },
  {
    "instance_id": "bd050fed2c20404289715c8c033d7f8e",
    "figure_id": "2007.15353v2-Figure10-1",
    "image_file": "2007.15353v2-Figure10-1.png",
    "caption": " Visualization of retained channel ratio dynamics for each stage in VGG-16.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stage of VGG-16 retains the most channels over the course of training?",
    "answer": "Stage 2",
    "rationale": "The figure shows the epoch-wise channel dynamics for each stage of VGG-16. The y-axis shows the number of layers, and the x-axis shows the number of epochs. The color of each pixel represents the retained channel ratio. The brighter the color, the higher the ratio. As can be seen from the figure, stage 2 has the highest ratio of retained channels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.15353v2",
    "pdf_url": null
  },
  {
    "instance_id": "75ceaeb5a2134d778a289f478e9e32ee",
    "figure_id": "2003.01416v3-Figure1-1",
    "image_file": "2003.01416v3-Figure1-1.png",
    "caption": " Experimental results on the real-world dataset. (a) Instant regret for Thompson Sampling (TS), BayesUCB (B-UCB) and probabilistic greedy (GR) algorithms, applied to Gaussian (prefix N) and Log-Gaussian bandits (prefix LN). (b) Cumulative regret results. (c) Exploration with Thompson Sampling, where the red lines indicate the edges visited by the agent during exploration.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in terms of cumulative regret?",
    "answer": "LN-TS",
    "rationale": "The cumulative regret plot (b) shows that LN-TS has the lowest cumulative regret among all the algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.01416v3",
    "pdf_url": null
  },
  {
    "instance_id": "203d7f3a2b244375aa3f6b628a07f822",
    "figure_id": "2301.09254v1-Figure1-1",
    "image_file": "2301.09254v1-Figure1-1.png",
    "caption": " Comparison of various methods in accuracy vs. #ReLU trade-off plot. SENet outperforms the existing approaches with an accuracy improvement of up to∼4.5% for similar ReLU budget.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on CIFAR-100 dataset?",
    "answer": "SENet",
    "rationale": "The plot shows the test accuracy of different methods on the CIFAR-100 dataset. SENet achieves the highest accuracy for a given number of ReLUs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.09254v1",
    "pdf_url": null
  },
  {
    "instance_id": "fc473a59021a4325865f48429413a410",
    "figure_id": "2009.07465v5-Figure4-1",
    "image_file": "2009.07465v5-Figure4-1.png",
    "caption": " Case study of example questions with supporting paragraphs from HotpotQA dev set.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two models, ABR and IDRQ, performed better on the three questions shown in the image?",
    "answer": "ABR.",
    "rationale": "The image shows three questions and the answers provided by two different models, ABR and IDRQ. For each question, the model's answer is marked with either a green checkmark or a red X, depending on whether it is correct or incorrect. ABR answered all three questions correctly, while IDRQ answered only one question correctly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.07465v5",
    "pdf_url": null
  },
  {
    "instance_id": "209c088afb1542aa9962c18f5face597",
    "figure_id": "2206.02583v3-FigureB.1-1",
    "image_file": "2206.02583v3-FigureB.1-1.png",
    "caption": "Figure B.1: Left: Alternative architecture with hat as the input. Right: Performance comparison in SMAC.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture performed better on the super hard MMM2 task?",
    "answer": "COLA-QMIX (hidden)",
    "rationale": "The figure shows the performance of two different model architectures on the MMM2 task. The red line represents the COLA-QMIX (hidden) model, and the green line represents the COLA-QMIX (obs) model. The figure shows that the COLA-QMIX (hidden) model achieved a higher median test win rate than the COLA-QMIX (obs) model on the MMM2 task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.02583v3",
    "pdf_url": null
  },
  {
    "instance_id": "ea98888ce5264e32bbf13e9e63330c22",
    "figure_id": "2209.14491v3-Figure15-1",
    "image_file": "2209.14491v3-Figure15-1.png",
    "caption": " Extra None-cherry picked examples from EntityDrawBench for different models.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model seems to generate the most realistic images based on the given descriptions?",
    "answer": "DALL-E 2.",
    "rationale": "The images generated by DALL-E 2 seem to be the most photorealistic and accurate representations of the descriptions. For example, the image of the \"Rufous-tailed flycatcher flapping its wings\" is very similar to the reference image, and the image of the \"Sultan Amir Ahmad Bathhouse on the moon\" is a realistic depiction of the bathhouse in a lunar setting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.14491v3",
    "pdf_url": null
  },
  {
    "instance_id": "44cf2bfd45de47bb83045481a41292cd",
    "figure_id": "2006.13408v1-Figure5-1",
    "image_file": "2006.13408v1-Figure5-1.png",
    "caption": " Training curves comparing our online algorithm with and without policy distillation on continuous control environments. The solid curves depict the mean of the experiments and the standard deviations correspond to the standard deviation of the means.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment shows the most improvement when using policy distillation?",
    "answer": "Cartpole",
    "rationale": "The gap between the two curves is largest for Cartpole, indicating that policy distillation leads to the largest performance improvement in this environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.13408v1",
    "pdf_url": null
  },
  {
    "instance_id": "2ce42dacd4424dbb8be490fdfb98826a",
    "figure_id": "1809.06297v2-Figure2-1",
    "image_file": "1809.06297v2-Figure2-1.png",
    "caption": " Test-BLEU score (higher value implies better quality) vs self-BLEU score (lower value implies better diversity). Upper panel is BLEU-3 and lower panel is BLEU-4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the highest Test-BLEU score on the EMNLP WMT dataset for BLEU-3?",
    "answer": "Leak GAN",
    "rationale": "The figure shows the Test-BLEU score on the x-axis and the Self-BLEU score on the y-axis. The Leak GAN data point is the highest on the x-axis for the EMNLP WMT dataset in the BLEU-3 panel.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.06297v2",
    "pdf_url": null
  },
  {
    "instance_id": "4d364173430b4d9f906d5fc4ea5a3985",
    "figure_id": "2110.02180v2-Figure2-1",
    "image_file": "2110.02180v2-Figure2-1.png",
    "caption": " The decision boundaries and test accuracy (in parenthesis) for different training schemes on a toy dataset in binary classification (see Subsection F.2 for details).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training scheme produced the most accurate decision boundary?",
    "answer": "NFM",
    "rationale": "The decision boundary produced by NFM had the highest test accuracy of 90.0%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02180v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd5d261035a44cf089ace59c3df62f3d",
    "figure_id": "2210.01774v1-Figure3-1",
    "image_file": "2210.01774v1-Figure3-1.png",
    "caption": " The accumulated capital of the proposed method and baselines on DJIA,HSI and CSI100.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best on the DJIA dataset?",
    "answer": "MetaTrader",
    "rationale": "The figure shows the accumulated capital of different methods on the DJIA dataset. The MetaTrader method has the highest accumulated capital at the end of the period, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.01774v1",
    "pdf_url": null
  },
  {
    "instance_id": "1cbc97ab377240df9433e9d626624c00",
    "figure_id": "1906.00163v2-Figure1-1",
    "image_file": "1906.00163v2-Figure1-1.png",
    "caption": " Example of a family tree (a), and its representation as a set of input tuples (b). An edge from x to y indicates that x is a parent of y, and is represented symbolically as the tuple parent(x, y). The user wishes to realize the relation samegen(x, y), indicating the fact that x and y occur are from the same generation of the family (c).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Who is Liam's parent?",
    "answer": "Liam has two parents, Noah and Emma.",
    "rationale": "The family tree in (a) shows that Liam has two incoming arrows, one from Noah and one from Emma. This indicates that both Noah and Emma are Liam's parents.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.00163v2",
    "pdf_url": null
  },
  {
    "instance_id": "d463760eb4644854ae74400768c248a3",
    "figure_id": "1908.10383v2-Figure2-1",
    "image_file": "1908.10383v2-Figure2-1.png",
    "caption": " Performance of extractive methods under ROUGE, FAR, and SAR. The results under ROUGE-1/2/L often disagree with each other. UnifiedSum(E) generally performs the best in the facet-aware evaluation.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric seems to be the most consistent across the different summarization methods?",
    "answer": "SAR.",
    "rationale": "The figure shows that the SAR values for the different summarization methods are relatively close together, while the ROUGE and FAR values vary more widely. This suggests that SAR is a more consistent metric than ROUGE and FAR.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.10383v2",
    "pdf_url": null
  },
  {
    "instance_id": "a3c3d23441304b7495dd6fc63e17adae",
    "figure_id": "1905.08622v3-Figure27-1",
    "image_file": "1905.08622v3-Figure27-1.png",
    "caption": " The generated random images of Obj-GAN given text lack diversity.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GAN model generates more diverse images?",
    "answer": "VHE-raster-scan-GAN",
    "rationale": "The figure shows that the generated random images of VHE-raster-scan-GAN are more diverse than the generated random images of Obj-GAN.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.08622v3",
    "pdf_url": null
  },
  {
    "instance_id": "672d6301eb0e40229729f8644035191a",
    "figure_id": "2307.02770v2-Figure29-1",
    "image_file": "2307.02770v2-Figure29-1.png",
    "caption": " Fourth set (289–384) of images among the 500 non-curated censored generation samples with a reward model ensemble and with backward guidance and recurrence. Malign images are labeled with red borders and positioned at the beginning for visual clarity. Qualitatively and subjectively speaking, we observe that censoring makes the malign images less severely “broken” compared to the malign images of the uncensored generation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the images in the figure are malign?",
    "answer": "The malign images are the ones with red borders.",
    "rationale": "The caption states that \"Malign images are labeled with red borders and positioned at the beginning for visual clarity.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.02770v2",
    "pdf_url": null
  },
  {
    "instance_id": "22fa69f4f4aa4f82b39af61a8a01349d",
    "figure_id": "2209.01207v2-Figure2-1",
    "image_file": "2209.01207v2-Figure2-1.png",
    "caption": " Top: A demonstrator of jogging from the CMU MoCap Dataset (CMU 2019). Middle: The co-imitation Humanoid produces a more natural looking jogging motion whereas the pure imitation learner (bottom) learns to run with a poor gait.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three figures shows the most natural-looking jogging motion?",
    "answer": "The middle figure.",
    "rationale": "The caption states that the co-imitation Humanoid (middle figure) produces a more natural-looking jogging motion than the pure imitation learner (bottom figure).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.01207v2",
    "pdf_url": null
  },
  {
    "instance_id": "125aeb45a1d9456387e243932ca76985",
    "figure_id": "2212.01844v1-Figure1-1",
    "image_file": "2212.01844v1-Figure1-1.png",
    "caption": " An example document from the ECPE corpus where ci represents the emotion clause and cj represents the cause clause in pair. The words in red are the keywords about emotion and the words in blue are about cause. The emotion clause c6 and c12 can not be a pair for lack of causal relationship. We translate it from Chinese into English for ease of reading.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which clause is the cause clause for the emotion clause \"The girl couldn't hold back her anger\"?",
    "answer": "\"After thinking of the boy's bad treatment of her\"",
    "rationale": "The figure shows the causal relationship between clauses in the document. The green line connects the clause \"The girl couldn't hold back her anger\" to the clause \"After thinking of the boy's bad treatment of her\", indicating that the latter is the cause of the former.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.01844v1",
    "pdf_url": null
  },
  {
    "instance_id": "e98a1019045448ad93983656279a7c03",
    "figure_id": "2107.11164v1-Figure4-1",
    "image_file": "2107.11164v1-Figure4-1.png",
    "caption": " Effect of context length and latent dimension on translation quality. The BLEU scores (%) are calculated on the validation set of the En⇒De.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which latent dimension size resulted in the highest BLEU score?",
    "answer": "16",
    "rationale": "The plot on the right shows the BLEU score for different latent dimension sizes. The highest point on the plot is at 16, which means that this latent dimension size resulted in the highest BLEU score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.11164v1",
    "pdf_url": null
  },
  {
    "instance_id": "aeb873b360f64436878e4ac3e52c7e9d",
    "figure_id": "2204.11642v1-Figure3-1",
    "image_file": "2204.11642v1-Figure3-1.png",
    "caption": " The joint distributions of legs’ position and the attributes background (left), shape (middle), and color (right). Datapoints are yellow for Peekies and blue for Stretchies. The background is not biased. The shape is biased for legs’ position lower than (0.45) or greater (0.55), but is uniform in the center. The color contains additional predictive information about the target class, as it allow to discriminate between Peeky and Stretchy where the legs’ position overlaps. However, for more extreme arms’ positions the color is uniform and not biased.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which attribute is the most informative for discriminating between Peekies and Stretchies? ",
    "answer": " Color",
    "rationale": " The joint distribution of legs' position and color shows that color is the most informative attribute for discriminating between Peekies and Stretchies. For legs' positions between 0.45 and 0.55, the color is the only attribute that allows us to distinguish between the two classes. In contrast, the background and shape attributes are not as informative, as they are either uniform or biased for only a small range of legs' positions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.11642v1",
    "pdf_url": null
  },
  {
    "instance_id": "9cad731a9c7e496f8088c750148c5520",
    "figure_id": "1805.01954v2-Figure4-1",
    "image_file": "1805.01954v2-Figure4-1.png",
    "caption": " Performance of each technique with respect to the number of post-demonstration interactions. For each domain, ten demonstrated trajectories were considered. BCO(0) is depicted as a horizontal line since all environment interactions happen before the demonstration is provided. Performance values are scaled such that performance of a random policy is zero and the performance of the expert is one. Note that GAIL and FEM have access to demonstration action information whereas BCO does not. *The BCO line is not visible for the CartPole domain because BCO has the same performance as the expert. **FEM is not shown for the Reacher domain because its performance is much worse than the other techniques.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the performance of GAIL compare to that of FEM in the Ant domain?",
    "answer": "GAIL outperforms FEM in the Ant domain.",
    "rationale": "The plot shows that the performance of GAIL is higher than that of FEM for all numbers of interactions in the Ant domain.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.01954v2",
    "pdf_url": null
  },
  {
    "instance_id": "bef2f9c156a243e69d7c7465e2c6aa30",
    "figure_id": "2110.09796v1-Figure4-1",
    "image_file": "2110.09796v1-Figure4-1.png",
    "caption": " Visualization of the value estimation in various AntMaze tasks. Darker colors correspond to the higher value estimation. Each map has several terminals (golden stars) and one of which is reached by the agent (the light red star). The red line is the trajectory of the ant.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which maze had the agent travel the longest distance?",
    "answer": "The Large maze.",
    "rationale": "The figure shows the trajectory of the agent in each maze. The trajectory in the Large maze is clearly the longest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.09796v1",
    "pdf_url": null
  },
  {
    "instance_id": "3eef1274434e4067aea2d502a20c8587",
    "figure_id": "2306.08877v2-Figure1-1",
    "image_file": "2306.08877v2-Figure1-1.png",
    "caption": " Visual bindings of objects and their attributes may fail to match the linguistic bindings between entities and their modifiers. Our approach, SynGen, corrects these errors by matching the cross-attention maps of entities and their modifiers.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three types of errors shown in the figure is the most difficult for SynGen to correct?",
    "answer": "Attribute Neglect.",
    "rationale": "The figure shows that SynGen is able to correct Semantic Leak in Prompt and Semantic Leak out of Prompt errors relatively well. However, it is less successful at correcting Attribute Neglect errors. This is because Attribute Neglect errors involve a mismatch between the visual and linguistic bindings of objects and their attributes, which is a more complex problem to solve.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.08877v2",
    "pdf_url": null
  },
  {
    "instance_id": "20f14ec2d67e43ed81e8b2060b4ab6b2",
    "figure_id": "2312.01457v1-Figure11-1",
    "image_file": "2312.01457v1-Figure11-1.png",
    "caption": " Results for OptDigits dataset",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following algorithms consistently performs the best across all settings in terms of variance?",
    "answer": "SwitchDR",
    "rationale": "In all three subplots, the variance of SwitchDR is consistently the lowest among all algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2312.01457v1",
    "pdf_url": null
  },
  {
    "instance_id": "c11ccfb620f34c4faa59544284a1dfe4",
    "figure_id": "2010.02357v1-Figure3-1",
    "image_file": "2010.02357v1-Figure3-1.png",
    "caption": " Impact of multiple gradient update steps for the pulled-back label, on the synthetic example with 10 clusters. For each point, the best step size η is chosen.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the synthetic example with 10 clusters?",
    "answer": "STE-I",
    "rationale": "The figure shows the validation accuracy and v-measure for different methods with varying numbers of pull-back updates. STE-I has the highest validation accuracy and v-measure for all numbers of pull-back updates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02357v1",
    "pdf_url": null
  },
  {
    "instance_id": "fc50259a7ef44638b2e0bb18726db337",
    "figure_id": "2212.10511v4-Figure10-1",
    "image_file": "2212.10511v4-Figure10-1.png",
    "caption": " The proportion of questions for which various models use retrieval in the Adaptive Retrieval setup on POPQA. When using Adaptive Retrieval, small models must still rely on non-parametric memory for most questions, while larger models have more reliable parametric memories enabling them to use retrieval less often.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model relies the most on non-parametric memory for most questions?",
    "answer": "Adaptive Vanilla/BM25",
    "rationale": "The figure shows that Adaptive Vanilla/BM25 has the highest proportion of questions for which it uses retrieval. This means that it relies the most on non-parametric memory.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10511v4",
    "pdf_url": null
  },
  {
    "instance_id": "f280fd7e3f2747c885484a08483368e0",
    "figure_id": "1811.12470v4-Figure14-1",
    "image_file": "1811.12470v4-Figure14-1.png",
    "caption": " Decision visualizations for global model trained on Fashion MNIST data using 9 benign agents and 1 malicious agent using the baseline attack on benign data.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the visualization methods seems to be most sensitive to the presence of the malicious agent?",
    "answer": "LRP-Epsilon.",
    "rationale": "The LRP-Epsilon visualizations show the most significant differences between the benign and malicious data. For example, in the case of the shirt, the LRP-Epsilon visualization for the benign data shows a clear outline of the shirt, while the visualization for the malicious data shows a more distorted and noisy image. This suggests that the LRP-Epsilon method is more sensitive to the presence of the malicious agent.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.12470v4",
    "pdf_url": null
  },
  {
    "instance_id": "89a488c317784f2ab5afac71e2a7a952",
    "figure_id": "1911.09153v1-Figure3-1",
    "image_file": "1911.09153v1-Figure3-1.png",
    "caption": " Elicitation using MovieLens-100k.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest regret after 10 questions?",
    "answer": "ExhaustiveSearch",
    "rationale": "The figure shows the regret of different algorithms as a function of the number of questions asked. The ExhaustiveSearch algorithm has the lowest regret after 10 questions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.09153v1",
    "pdf_url": null
  },
  {
    "instance_id": "749513ee1ba542b4a6e82759639024d7",
    "figure_id": "2106.02119v1-Figure8-1",
    "image_file": "2106.02119v1-Figure8-1.png",
    "caption": " Spectrum information of algorithmic output X(𝐾) after convergence, experiment of Figure 7 (1000 × 1000 matrix, 𝑟 = 30, ^ = 1010, 𝜌 = 1.5)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is the most accurate?",
    "answer": "MatrixIRLS.",
    "rationale": "The relative error of MatrixIRLS is the lowest among all the algorithms. This can be seen in the plot (b) where the x-axis represents the singular value index and the y-axis represents the relative error. The lower the curve, the lower the error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02119v1",
    "pdf_url": null
  },
  {
    "instance_id": "a77481a4def94391b4f1ae258b225283",
    "figure_id": "1910.08647v2-Figure3-1",
    "image_file": "1910.08647v2-Figure3-1.png",
    "caption": " Security Game G1 in Extensive Form.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the expected value of playing strategy T1 for player A when the probability of D choosing the left branch is 75%?",
    "answer": "-91.995",
    "rationale": "The expected value of playing T1 is calculated as follows: \n* 0.75 * (-72.53) + 0.25 * (-101.86) = -91.995.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.08647v2",
    "pdf_url": null
  },
  {
    "instance_id": "1d150e614e4c43ce8ead3139d56339a1",
    "figure_id": "2104.05433v1-Figure12-1",
    "image_file": "2104.05433v1-Figure12-1.png",
    "caption": " Accuracy of the language models predicting the mean fixation duration (MFD) across various parts of speech for Dutch (left) and English (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language model performs best on predicting the mean fixation duration (MFD) across various parts of speech for English?",
    "answer": "XLM-100",
    "rationale": "The figure shows that XLM-100 has the highest accuracy for all parts of speech in English.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.05433v1",
    "pdf_url": null
  },
  {
    "instance_id": "ddcfad794e3246ff97d3d7f111b409db",
    "figure_id": "2106.04185v1-Figure10-1",
    "image_file": "2106.04185v1-Figure10-1.png",
    "caption": " Left: Self-reenactment performance comparison against state-of-the-art benchmarks of CVPR-W’19 [36], IJCV’19 [37], CVPR’19 [7], BMVC’17 [8], Chen et al. [6] and Wiles et al. [40]. Pre-trained LipNet (for WER) is available only on GRID. Authors of [7] released checkpoint for GRID only. (↑):Higher is better. (↓):Lower is better. Best results are marked in bold. Right: Mean Opinion Scores of user study. The statistical significance of these differences in ratings is confirmed by ANOVA with Tukey post-hoc tests. Please see Appendix- A for details.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the best lip-sync score?",
    "answer": "Real.",
    "rationale": "The table on the right shows the lip-sync scores for each method, and Real has the highest score of 2.95±0.03.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04185v1",
    "pdf_url": null
  },
  {
    "instance_id": "30061e30335d4fa0b390dc0d3c28730e",
    "figure_id": "2101.12463v3-Figure8-1",
    "image_file": "2101.12463v3-Figure8-1.png",
    "caption": " Examples of joint deraining and segmentation. DeepLabv3+ [2] is adopted for segmentation. Zoom in to see the details.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most successful at jointly deraining and segmenting the image?",
    "answer": "Ours.",
    "rationale": "The figure shows the results of several different methods for jointly deraining and segmenting the image. The method labeled \"Ours\" produces the most accurate segmentation, as it is able to correctly identify the man and the cart in the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.12463v3",
    "pdf_url": null
  },
  {
    "instance_id": "fd0cce44df934e2d8b232a62e797d439",
    "figure_id": "1903.00967v2-Figure1-1",
    "image_file": "1903.00967v2-Figure1-1.png",
    "caption": " Average performance on homeless youth social networks (top) and simulated Antelope Valley networks (bottom).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm results in the highest mean DC violation for the birthsex group in the homeless youth social networks?",
    "answer": "The Greedy algorithm.",
    "rationale": "The figure shows the mean DC violation for each algorithm and group. For the birthsex group in the homeless youth social networks, the Greedy algorithm has the highest mean DC violation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.00967v2",
    "pdf_url": null
  },
  {
    "instance_id": "4c646f2b15aa4b34b1ceaec5f1364ca5",
    "figure_id": "2006.06666v3-Figure7-1",
    "image_file": "2006.06666v3-Figure7-1.png",
    "caption": " Bicaptioning vs. Masked Language Modeling: We compare VOC07 mAP of Bicaptioning and Masked LM pretraining tasks. We observe that Masked LM converges slower than Bicaptioning, indicating poor sample efficiency.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, Bicaptioning or Masked Language Modeling, converges faster?",
    "answer": "Bicaptioning.",
    "rationale": "The plot shows that the VOC07 mAP of Bicaptioning increases more rapidly than that of Masked LM, indicating that Bicaptioning converges faster.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06666v3",
    "pdf_url": null
  },
  {
    "instance_id": "c68fc6e34fde44929c3fe0ff566b39df",
    "figure_id": "2106.11485v3-Figure17-1",
    "image_file": "2106.11485v3-Figure17-1.png",
    "caption": " Samples from all different configurations of our model on the Texas housing dataset with setting t′ > t.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the configurations of the model produced the most accurate results?",
    "answer": "Ground Truth.",
    "rationale": "The Ground Truth image is the original image, which is the most accurate representation of the scene. The other images are all outputs of different configurations of the model, and they all have some degree of error compared to the Ground Truth image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.11485v3",
    "pdf_url": null
  },
  {
    "instance_id": "02b124e00cec4c74b58cd1e30322f504",
    "figure_id": "2007.00295v1-Figure2-1",
    "image_file": "2007.00295v1-Figure2-1.png",
    "caption": " Left: cactus plot of runtimes for the 105 instances in the ’or_50’ category solved by BPNN, F2, and ApproxMC3. BPNN-P denotes the time taken to run BPNN in parallel on a GPU divided by the number of instances per batch (batch size=103). Median speedups of BPNN-P over F2 and ApproxMC among the plotted benchmarks are 248 and 3,689 respectively. BPNN-S denotes the time taken to run BPNN sequentially on each instance (using a CPU). Median speedups of BPNN-S over F2 and ApproxMC among the plotted benchmarks are 2.2 and 32, resp. While BPNN solved each instance within 1 second, ApproxMC3 timed out on 12 instances (out of 105) after 5000 seconds, which are not plotted. Right: error in estimated log model count (base e) plotted against the exact model count for ‘or_50’ training and validation benchmarks. BPNN’s validation RMSE was .30 on this category compared with a RMSE of 2.5 for F2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods is the fastest?",
    "answer": "BPNN-P is the fastest method.",
    "rationale": "The left plot shows the runtimes for each method on the OR_50 benchmarks. BPNN-P has the lowest runtime for all benchmarks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.00295v1",
    "pdf_url": null
  },
  {
    "instance_id": "befa66a7ca8b4893a2f807f64f44b279",
    "figure_id": "2305.13797v2-Figure2-1",
    "image_file": "2305.13797v2-Figure2-1.png",
    "caption": " Samples from a mixture of three Gaussians with varying standard deviations. The edges’ strength is proportional to the weights in the affinities Pds (DS) and Pse (SEA) computed with ξ = 5 (for Pds, ξ is the average perplexity such that ∑ i H(Pds i: ) = ∑ i H(Pse i: )). Points’ color represents the perplexity eH(Pi:)−1. Right plot: smallest eigenvalues of the Laplacian for the two affinities.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which affinity matrix, Pds or Pse, results in a more connected graph?",
    "answer": " Pse results in a more connected graph than Pds. ",
    "rationale": " The graphs in the left and middle plots show the connections between data points based on the affinities Pds and Pse, respectively. In the middle plot, there are more connections between data points, indicating that Pse results in a more connected graph. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.13797v2",
    "pdf_url": null
  },
  {
    "instance_id": "97f2cec8e1ac4a00a8f466b83bbcc153",
    "figure_id": "2108.10668v1-Figure3-1",
    "image_file": "2108.10668v1-Figure3-1.png",
    "caption": " Comparison of validation accuracy between MoCo v2 and TKC. The top-1 accuracy is from a kNN classifier.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better on this task, TKC or MoCo v2?",
    "answer": "TKC performs better than MoCo v2 on this task.",
    "rationale": "The plot shows that the top-1 accuracy of TKC is higher than that of MoCo v2 at all epochs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.10668v1",
    "pdf_url": null
  },
  {
    "instance_id": "0df893950c12472f8390d371fd5dcd03",
    "figure_id": "2006.10350v2-Figure1-1",
    "image_file": "2006.10350v2-Figure1-1.png",
    "caption": " Benchmarks of kernel solvers on large scale datasets with millions and billions points (see Section 4). Our approach (red and yellow lines) consistently achieves state of the art accuracy in minutes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which kernel solver consistently achieves state-of-the-art accuracy in minutes?",
    "answer": "Falkon and LogFalkon.",
    "rationale": "The figure shows the error of different kernel solvers on different datasets. The red and yellow lines, which represent Falkon and LogFalkon, respectively, consistently achieve the lowest error in the shortest amount of time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.10350v2",
    "pdf_url": null
  },
  {
    "instance_id": "a2da407768e24349869e28d13ccffb51",
    "figure_id": "2210.14523v2-Figure3-1",
    "image_file": "2210.14523v2-Figure3-1.png",
    "caption": " Semantic optimal transport distance.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which node has the highest degree of connection in the graph?",
    "answer": "Node B",
    "rationale": "Node B has the highest degree of connection because it is connected to all other nodes in the graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.14523v2",
    "pdf_url": null
  },
  {
    "instance_id": "d6cabe30b15d44b7a5adc088af8590a4",
    "figure_id": "2109.03819v1-Figure4-1",
    "image_file": "2109.03819v1-Figure4-1.png",
    "caption": " Visualization of the domain shift mitigation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods performed the best overall?",
    "answer": "SAECON",
    "rationale": "The figure shows that SAECON achieved the highest F1 scores across all four categories.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.03819v1",
    "pdf_url": null
  },
  {
    "instance_id": "b63b7648145a4b4ab368c6429516d590",
    "figure_id": "2105.11683v1-Figure1-1",
    "image_file": "2105.11683v1-Figure1-1.png",
    "caption": " Visual comparison of CSSR-Net with different losses.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three images is the best representation of the original image?",
    "answer": "The HR image.",
    "rationale": "The HR image is the original high-resolution image, and the other two images are reconstructions using different loss functions. The L1 + LPercep and CSD (Ours) images are both blurry compared to the HR image, which is sharp and clear.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.11683v1",
    "pdf_url": null
  },
  {
    "instance_id": "af688fdbb28445b28a3e69faefbe2fb9",
    "figure_id": "2204.11536v1-Figure3-1",
    "image_file": "2204.11536v1-Figure3-1.png",
    "caption": " The accuracy and the training time with FedAvg, HRank of diverse pruning rates, and FedAP.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the highest accuracy on the FedAP dataset?",
    "answer": "CNN.",
    "rationale": "The figure shows that CNN achieves an accuracy of 62.5% on the FedAP dataset, while VGG achieves an accuracy of 64.7%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.11536v1",
    "pdf_url": null
  },
  {
    "instance_id": "c8ebc96d22e24d3f88a42df7248c0387",
    "figure_id": "1911.12949v1-Figure1-1",
    "image_file": "1911.12949v1-Figure1-1.png",
    "caption": " An example of a decomposition tree from incomplete methods (the parameters of some actions are hidden). The initial task ship(pkg1,whA, shopB) is decomposed into a sequence of primitive tasks (the black leaves) according to the original methods. But when plane1 is not in airport A, the sequence is not executable. It becomes executable if arranging plane 1 to airport A before loading the package, which implies that fly(plane1, airpA) should be considered as a subtask of airShip.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the initial task in the decomposition tree?",
    "answer": " The initial task is ship(pkg1, whA, shopB).",
    "rationale": " The initial task is represented by the root node of the tree, which is labeled with \"ship(pkg1, whA, shopB)\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.12949v1",
    "pdf_url": null
  },
  {
    "instance_id": "2af99212e23e4ee8a94dc6771a51c7da",
    "figure_id": "2107.07630v3-Figure5-1",
    "image_file": "2107.07630v3-Figure5-1.png",
    "caption": " Histograms of all numerical and categorical demographic survey responses.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activity are people most experienced in?",
    "answer": "Developing artificial intelligence agents.",
    "rationale": "The histogram for D8: \"I am experienced in developing artificial intelligence agents\" shows the highest frequency of responses in the 6-7 range, indicating that people are most experienced in this activity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.07630v3",
    "pdf_url": null
  },
  {
    "instance_id": "9b30fcac37d4464c93847f0aa14ccf20",
    "figure_id": "2109.11338v2-Figure1-1",
    "image_file": "2109.11338v2-Figure1-1.png",
    "caption": " (a) The signal magnifications Msig for GCNs with and without the orthogonal graph convolutions. (b) The gradient norms of vanilla GCN. (c) The gradient norms of GCN augmented with orthogonal graph convolutions. (d) Test accuracy and node embedding smoothness under different model depths.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture, GCN or Ortho-GCN, has a more stable gradient during training?",
    "answer": "Ortho-GCN",
    "rationale": "Comparing subplots (b) and (c), we can see that the gradient norms of vanilla GCN (b) are much larger and fluctuate more than those of GCN augmented with orthogonal graph convolutions (c). This suggests that Ortho-GCN has a more stable gradient during training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.11338v2",
    "pdf_url": null
  },
  {
    "instance_id": "522188c0c3a84cc99f260ced29c301aa",
    "figure_id": "2103.02766v1-Figure4-1",
    "image_file": "2103.02766v1-Figure4-1.png",
    "caption": " Vertex detection results of Harris3D (left) and ours (right) on the ABC dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is better at detecting vertices?",
    "answer": "Our method is better at detecting vertices.",
    "rationale": "The figure shows that our method detects more vertices than Harris3D.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.02766v1",
    "pdf_url": null
  },
  {
    "instance_id": "74b71e7d3fc448b2b070af9c59d9e09e",
    "figure_id": "1910.12450v1-Figure1-1",
    "image_file": "1910.12450v1-Figure1-1.png",
    "caption": " Small example.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many leaves does the tree in the figure have?",
    "answer": "The tree has 8 leaves.",
    "rationale": "The figure shows a tree with 8 leaves. Each leaf is represented by a square.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.12450v1",
    "pdf_url": null
  },
  {
    "instance_id": "4aa44a4c2cca4c42a6012a0cf5785909",
    "figure_id": "2012.15613v2-Figure5-1",
    "image_file": "2012.15613v2-Figure5-1.png",
    "caption": " Sentence length distributions of monolingual UD corpora tokenized by respective monolingual BERT models and mBERT, compared to the reference tokenizations by human UD treebank annotators.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the shortest average sentence length according to the reference tokenization?",
    "answer": "ZH",
    "rationale": "The figure shows the distribution of sentence lengths for different languages. The peak of the distribution for ZH is the furthest to the left, indicating that it has the shortest average sentence length.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.15613v2",
    "pdf_url": null
  },
  {
    "instance_id": "7648bd17783b4479859ee03c2d485143",
    "figure_id": "2003.03711v3-Figure3-1",
    "image_file": "2003.03711v3-Figure3-1.png",
    "caption": " Reconstruction result on 5 of the 2,894 untruncated and unoccluded ‘chairs’ in Pix3D. GT indicates the ground-truth.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods produced the most accurate reconstructions of the chairs?",
    "answer": "Mem3D",
    "rationale": "The Mem3D reconstructions are the most similar to the ground truth (GT) images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.03711v3",
    "pdf_url": null
  },
  {
    "instance_id": "db444a0659984a54921fafb2161ef86d",
    "figure_id": "2306.11264v1-Figure6-1",
    "image_file": "2306.11264v1-Figure6-1.png",
    "caption": " (a) The curves of homophily ratios for latent structures during the learning process. (b) The variance of neighborhood distribution of nodes with the same label in original graphs and learnt structure.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which graph has the highest variance in neighborhood distribution of nodes with the same label in the original graph?",
    "answer": "Johns Hopkins55",
    "rationale": "The bar chart in (b) shows the variance of neighborhood distribution of nodes with the same label in the original graph and the learnt structure. The red bar for Johns Hopkins55 is the highest, indicating that it has the highest variance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.11264v1",
    "pdf_url": null
  },
  {
    "instance_id": "cf378edbe03a4d4a8c513d210a0117f9",
    "figure_id": "2010.12785v1-Figure5-1",
    "image_file": "2010.12785v1-Figure5-1.png",
    "caption": " Testing accuracy vs. training epochs for the AdderNet [5] and pruned ShiftAddNets on ResNet-20 with CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture achieves the highest testing accuracy on the CIFAR-10 dataset?",
    "answer": "AdderNet",
    "rationale": "The figure shows that the AdderNet (green line) achieves the highest testing accuracy of around 85% after 160 epochs of training. The pruned ShiftAddNets with different pruning ratios (red, yellow, blue, and cyan lines) achieve lower testing accuracies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12785v1",
    "pdf_url": null
  },
  {
    "instance_id": "e4ac672ccbca406bb3514ac44a07074f",
    "figure_id": "2305.04619v3-Figure7-1",
    "image_file": "2305.04619v3-Figure7-1.png",
    "caption": " Learned semantic relatedness reflects representation consistency among items connected in transition graph.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the color red represent in the figure?",
    "answer": "The color red represents the target item.",
    "rationale": "The target item is highlighted in red in each of the panels. This is the item whose neighbors are being shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.04619v3",
    "pdf_url": null
  },
  {
    "instance_id": "c24b4b905b4041268dd4f1077401056e",
    "figure_id": "2306.10563v1-Figure5-1",
    "image_file": "2306.10563v1-Figure5-1.png",
    "caption": " Left panel: t-SNE visualization of clustered viseme and phoneme centers (ellipses highlight the undesirably gathered centers). Right panel: confusion matrix of phoneme matching and viseme-phoneme mapping. In (g)-(i), the vertical axis indicates phoneme center IDs and the horizontal axis indicates real phonemes predicted by pre-trained model (Phy, 2022), while in (j)-(l) the horizontal axis indicates viseme center IDs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which clustering method produces the most distinct viseme and phoneme centers?",
    "answer": "Online Clustering + AMIE",
    "rationale": "The t-SNE visualizations in the left panel of the figure show the results of three different clustering methods. In (a) and (b), the viseme and phoneme centers are clustered together, which is undesirable. In (c), the viseme and phoneme centers are more distinct, indicating that Online Clustering + AMIE is the best method for clustering viseme and phoneme centers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.10563v1",
    "pdf_url": null
  },
  {
    "instance_id": "b511db38c277472f9d3a6e831abd08e3",
    "figure_id": "2002.12001v2-Figure1-1",
    "image_file": "2002.12001v2-Figure1-1.png",
    "caption": " Example of a MIF-DCOPs.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the domain of the variable x3?",
    "answer": "The domain of the variable x3 is [-10, 10].",
    "rationale": "The domain of a variable is the set of all possible values that it can take. In the figure, the domain of x3 is shown next to the variable as D3 = [-10, 10]. This means that x3 can take any value between -10 and 10, inclusive.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.12001v2",
    "pdf_url": null
  },
  {
    "instance_id": "72ccc28c4791444383d22fa9c1c17d54",
    "figure_id": "2103.09330v3-Figure1-1",
    "image_file": "2103.09330v3-Figure1-1.png",
    "caption": " A sentence example with the annotations for the four IE tasks. Blue words corresponds to entity mentions while red words are event triggers. Also, orange edges represent relations while green edges indicate argument roles.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the person and the vehicle?",
    "answer": "The person is driving the vehicle.",
    "rationale": "The figure shows that the person is connected to the vehicle by the \"ART\" edge, which indicates that the person is the agent of the event \"driving\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.09330v3",
    "pdf_url": null
  },
  {
    "instance_id": "56dc44987c2b4fb8b0ecc2021046d347",
    "figure_id": "1911.09298v2-Figure7-1",
    "image_file": "1911.09298v2-Figure7-1.png",
    "caption": " Results on SCUT-FBP. The target attribute is attractiveness score (1 to 5). Values from Attr0 to Attr4 correspond to score of 1.375, 2.125, 2.875, 3.625 and 4.5, respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attribute score corresponds to the highest level of attractiveness?",
    "answer": "Attr 4",
    "rationale": "The caption states that the target attribute is attractiveness score (1 to 5). Values from Attr0 to Attr4 correspond to scores of 1.375, 2.125, 2.875, 3.625, and 4.5, respectively. Since 4.5 is the highest score, Attr 4 corresponds to the highest level of attractiveness.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.09298v2",
    "pdf_url": null
  },
  {
    "instance_id": "e745701e11ac4741ab953ea6fe1f51b3",
    "figure_id": "2207.04491v2-Figure8-1",
    "image_file": "2207.04491v2-Figure8-1.png",
    "caption": " Statistical analysis on Inverse-Text compared with Total-Text. (a) The number of images with different cared text instances counts. (b) The number of cared instances with different text length. (c) Frequency distribution of different characters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common text length in the Inverse-Text dataset?",
    "answer": "11 characters.",
    "rationale": "Figure (b) shows the distribution of text lengths in the Inverse-Text and Total-Text datasets. The peak of the Inverse-Text distribution is at 11 characters, indicating that this is the most common text length in the Inverse-Text dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.04491v2",
    "pdf_url": null
  },
  {
    "instance_id": "ea67945beaff466ea3fa312a2a94ce75",
    "figure_id": "2202.07261v4-Figure4-1",
    "image_file": "2202.07261v4-Figure4-1.png",
    "caption": " Visualization of the generated adversarial examples in both the data and spectral domains. Specifically, we compare our GSDA with GeoA via evaluation metrics of the perturbation budget Dc in the data domain and the perturbed energy E∆ in the spectral domain.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most significant changes in the data domain?",
    "answer": "Our GSDA method.",
    "rationale": "The figure shows that our GSDA method has a smaller Dc value than GeoA for all four targeted attacks. Dc represents the perturbation budget in the data domain, so a smaller Dc value indicates that fewer changes were made to the original data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.07261v4",
    "pdf_url": null
  },
  {
    "instance_id": "360aeaf4161742ada6f84193f34f8f68",
    "figure_id": "2210.12566v2-Figure17-1",
    "image_file": "2210.12566v2-Figure17-1.png",
    "caption": " Comparison of the distributional versions of DecQN, DQN, and IQN. Decoupling of the value representation in conjunction with bang-bang action representations plays a key role in scaling these approaches to high-dimensional continuous control tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best on the Dog Walk task?",
    "answer": "DecQN + C51 (3 Bin)",
    "rationale": "The figure shows the performance of three algorithms on six different tasks. The DecQN + C51 (3 Bin) algorithm consistently outperforms the other two algorithms on all tasks, including the Dog Walk task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12566v2",
    "pdf_url": null
  },
  {
    "instance_id": "692a57deadd64fbda203d76b3c63f4f1",
    "figure_id": "2208.04022v2-Figure3-1",
    "image_file": "2208.04022v2-Figure3-1.png",
    "caption": " Computational cost and memory efficiency for all compared models. The x-axes are on logarithmic scales for all three plots. The y-axis for Fig.3b is on a logarithmic scale.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is the most memory efficient for long user sequences?",
    "answer": "SASRec",
    "rationale": "Figure 3c shows that SASRec uses the least amount of GPU memory for all user sequence lengths, and its memory usage stays relatively constant as the user sequence length increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.04022v2",
    "pdf_url": null
  },
  {
    "instance_id": "d76168cd222d42628d7ba107b7dac021",
    "figure_id": "2106.03163v5-Figure5-1",
    "image_file": "2106.03163v5-Figure5-1.png",
    "caption": " Finding the upper bound of the mean with D+ = [0, 1]",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which bound has the highest expected value for a sample size of 10 when beta is (1,5) and the distribution is uniform?",
    "answer": "The Anderson bound.",
    "rationale": "In Figure (b), the expected values of the bounds are plotted against the sample size for different values of beta and different distributions. For beta = (1,5) and a uniform distribution, the Anderson bound is the highest curve at a sample size of 10.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03163v5",
    "pdf_url": null
  },
  {
    "instance_id": "2770c2c50c474f78a7988755ca432c6c",
    "figure_id": "2103.10379v1-Figure1-1",
    "image_file": "2103.10379v1-Figure1-1.png",
    "caption": " A timeline of events extracted from the ICEWS14 knowledge graph. The events are chronologically sorted from top to bottom, demonstrating interactions between, president Obama, NATO, Afghanistan and Russia.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What event occurred on 2014-01-22?",
    "answer": "President Obama hosted a visit.",
    "rationale": "The figure shows that on 2014-01-22, President Obama hosted a visit. This is indicated by the arrow pointing from President Obama to the date 2014-01-22.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.10379v1",
    "pdf_url": null
  },
  {
    "instance_id": "a2399e6394a34650878654703dc59080",
    "figure_id": "2210.08459v2-Figure5-1",
    "image_file": "2210.08459v2-Figure5-1.png",
    "caption": " Correlation of upvote number - aspect rating (Human) and the correlation of predicted preference scores and predicted aspect rating (Model (Ours)). The correlation values are all statistical significant. (i.e. p ≤ 0.01)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which aspect category does the model have the highest correlation with human ratings?",
    "answer": "funny",
    "rationale": "The figure shows the correlation between human ratings and model predictions for each aspect category. The bar for the \"funny\" category is the highest for the model, indicating that the model has the highest correlation with human ratings in this category.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.08459v2",
    "pdf_url": null
  },
  {
    "instance_id": "48851ca63ba241f09d29a9eb259df187",
    "figure_id": "2104.00308v2-Figure5-1",
    "image_file": "2104.00308v2-Figure5-1.png",
    "caption": " The per-group results on VG dataset (SGGen task).",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method performs the best on the Head category? ",
    "answer": " GPSNet",
    "rationale": " The figure shows the mRecall@1000 (%) for different methods on the VG dataset (SGGen task). The Head category is the second group of bars from the left. The GPSNet method has the highest bar in this group, indicating that it performs the best on the Head category. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.00308v2",
    "pdf_url": null
  },
  {
    "instance_id": "2aa5d775c8ea42f1b17006b9de6cc7ec",
    "figure_id": "2002.07214v1-Figure1-1",
    "image_file": "2002.07214v1-Figure1-1.png",
    "caption": " An illustration of med-E-UCB scheme, where the blue blocks denote the pure exploration rounds and the white blocks denote the UCB rounds.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which round is the first pure exploration round?",
    "answer": "The first round.",
    "rationale": "The blue blocks denote the pure exploration rounds. The first block in the figure is blue, indicating that the first round is a pure exploration round.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.07214v1",
    "pdf_url": null
  },
  {
    "instance_id": "1cdc90cc6638446eb54efc07a9bea239",
    "figure_id": "2101.10050v3-Figure4-1",
    "image_file": "2101.10050v3-Figure4-1.png",
    "caption": " Classification accuracy results for both node and graph classification tasks (Validation ROC-AUC for OGBG-MOLHIV). Lower case letters denote a node classification task, while capital letters a graph classification task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization algorithm achieved the highest accuracy for the Cora dataset?",
    "answer": "Standard",
    "rationale": "The figure shows that the Standard optimization algorithm achieved the highest accuracy for the Cora dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.10050v3",
    "pdf_url": null
  },
  {
    "instance_id": "7bb4ffe6dfad42648c1c402dd3424e28",
    "figure_id": "1903.10427v1-Figure5-1",
    "image_file": "1903.10427v1-Figure5-1.png",
    "caption": " Semantic segmentation visualization for validation set images. The incorporation of SAND features improves the overall level of detail and consistency of the segmented regions.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most accurate segmentation results?",
    "answer": "32D-GL-FT",
    "rationale": "The figure shows the results of four different semantic segmentation methods on a validation set of images. The ground truth is shown in (a), and the results of the three other methods are shown in (b), (c), and (d). The 32D-GL-FT method produces the most accurate segmentation results, as its results are most similar to the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.10427v1",
    "pdf_url": null
  },
  {
    "instance_id": "ddba4a9f303b44e2a70f9bbce3366fbf",
    "figure_id": "1703.04908v2-Figure1-1",
    "image_file": "1703.04908v2-Figure1-1.png",
    "caption": " An example of environments we consider.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between agent 1 and landmark v?",
    "answer": "Agent 1 communicates with landmark v.",
    "rationale": "The dashed line between agent 1 and landmark v indicates communication.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1703.04908v2",
    "pdf_url": null
  },
  {
    "instance_id": "c4caa23c61294068b8854c53516979a8",
    "figure_id": "2104.06697v1-Figure5-1",
    "image_file": "2104.06697v1-Figure5-1.png",
    "caption": " Qualitative comparisons of long-term video generation results across models. Although all models succeed in generating plausible frames in a short-term (t = 1 ∼ 40), only our approach can generate persistent and convincing futures even in the end (t = 251 ∼ 500). Click the image to play the video in a browser. More results are available at https://1konny.github.io/HVP/.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models generates the most realistic long-term video sequences?",
    "answer": "Ours.",
    "rationale": "The figure shows that all three models generate plausible frames in the short term (t = 1 ~ 40). However, in the long term (t = 251 ~ 500), only our approach can generate persistent and convincing futures. The other two models, SVG-extend and Bayes-WD-SL, produce blurry and unrealistic frames.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.06697v1",
    "pdf_url": null
  },
  {
    "instance_id": "132fda8214574746a603e53805ec34c7",
    "figure_id": "2012.11727v1-Figure6-1",
    "image_file": "2012.11727v1-Figure6-1.png",
    "caption": " A-distances comparison for four tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest A-distance for the Syn-Real adaptation task?",
    "answer": "CDLM",
    "rationale": "The bar corresponding to CDLM in the Syn-Real task is the shortest among the three bars.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.11727v1",
    "pdf_url": null
  },
  {
    "instance_id": "47071db3e98a4d5eaf18cd2da54a5951",
    "figure_id": "2010.07835v3-Figure4-1",
    "image_file": "2010.07835v3-Figure4-1.png",
    "caption": " Classification performance on MIT-R. From left to right: visualization of ExMatch, results after the initialization step, results after contrastive self-training, and wrong-label correction during self-training.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which aspect of the restaurant is the model most accurate at predicting?",
    "answer": "Dish.",
    "rationale": "The confusion matrix on the far right shows that the model is most accurate at predicting the dish, with a score of 0.91.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.07835v3",
    "pdf_url": null
  },
  {
    "instance_id": "a866901643ce4318a5f94465f8b7760c",
    "figure_id": "2210.01427v4-Figure5-1",
    "image_file": "2210.01427v4-Figure5-1.png",
    "caption": " Visual comparison with challenging examples on image super-resolution (×4).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which super-resolution model performed the best on the Urban100 dataset?",
    "answer": "ART",
    "rationale": "The figure shows the PSNR values for each model on the Urban100 dataset. ART has the highest PSNR value, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.01427v4",
    "pdf_url": null
  },
  {
    "instance_id": "4654bc1283a64c538436ba797b910fa8",
    "figure_id": "1809.04497v3-Figure6-1",
    "image_file": "1809.04497v3-Figure6-1.png",
    "caption": " Traversals across latent dimensions for β-VAE, FactorVAE, and CHyVAE on 3DFaces dataset annotated with the factor of variation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is able to disentangle the latent factors of variation the most effectively?",
    "answer": "CHyVAE",
    "rationale": "The figure shows that CHyVAE is able to disentangle the latent factors of variation more effectively than the other two models. This is because the traversals across the latent dimensions for CHyVAE show a clear separation of the different factors of variation (azimuth, elevation, and lighting).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.04497v3",
    "pdf_url": null
  },
  {
    "instance_id": "cb82552fb1284a80b13ac856ebe923c0",
    "figure_id": "2105.03363v3-Figure9-1",
    "image_file": "2105.03363v3-Figure9-1.png",
    "caption": " Performance comparison on the competition environments in Multi-Agent Particle Environment. Each bar shows the [0, 1] normalized agent score (agent reward minus adversary reward).",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed best on the keep-away environment?",
    "answer": "AORPO.",
    "rationale": "The bar for AORPO vs. AORPO in the keep-away environment is the highest among all the bars in that environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.03363v3",
    "pdf_url": null
  },
  {
    "instance_id": "db2547ed0d08417fab02e0cce1baf15e",
    "figure_id": "2006.04924v1-Figure3-1",
    "image_file": "2006.04924v1-Figure3-1.png",
    "caption": " t-SNE [32] visualization of logits vs. feature representation of randomly selected classes from ImageNet validation set. Logits are computed from VGG16 [42] last layer while features are extracted from ”Block3-Conv3” of the same model. Our intuition is based on the observation that features space is shared among input samples rather than the logit space. Attacking such shared representation space removes task dependency constraint during adversary generation optimization and produces generalizable adversarial examples.",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two representations, logits or features, is more likely to be shared among different input samples?",
    "answer": "Features.",
    "rationale": "The t-SNE visualization of the features shows that the different classes are more clustered together than in the logits space. This suggests that the features are more likely to be shared among different input samples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04924v1",
    "pdf_url": null
  },
  {
    "instance_id": "8eb90b6c1c5648ac8533b2b0add4c8e8",
    "figure_id": "2208.07227v2-Figure19-1",
    "image_file": "2208.07227v2-Figure19-1.png",
    "caption": " Qualitative results of our method for object manipulation on DM-SR dataset. The dark red boxes highlight the differences.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method appears to perform the best for object manipulation based on the figure?",
    "answer": "Ours.",
    "rationale": "The figure shows the results of different methods for object manipulation on the DM-SR dataset. The last column shows the ground truth, which is the desired outcome. The method that produces results closest to the ground truth is Ours.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.07227v2",
    "pdf_url": null
  },
  {
    "instance_id": "21ce699185054a819dc02ec15d94d486",
    "figure_id": "2112.15311v1-Figure7-1",
    "image_file": "2112.15311v1-Figure7-1.png",
    "caption": " Results from the experiment described in §E.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in the robot pushing experiment?",
    "answer": "EI-FN",
    "rationale": "The figure shows the best objective value achieved by each algorithm as a function of the number of evaluations. EI-FN consistently achieves the highest objective value, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.15311v1",
    "pdf_url": null
  },
  {
    "instance_id": "36bbb82cf6264b4e8633e156d246cc25",
    "figure_id": "1805.08166v4-Figure15-1",
    "image_file": "1805.08166v4-Figure15-1.png",
    "caption": " Impact of diversity aware exploration on all conv2d operators in ResNet-18.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which configuration of λ leads to the highest TFLOPS for the C7 operator?",
    "answer": "λ = 1.",
    "rationale": "The figure shows the TFLOPS for the C7 operator for different values of λ. The blue line, which represents λ = 1, is the highest for the C7 operator.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.08166v4",
    "pdf_url": null
  },
  {
    "instance_id": "4e185dfc5f0747bf86033bd7af5b8057",
    "figure_id": "2305.17699v1-Figure5-1",
    "image_file": "2305.17699v1-Figure5-1.png",
    "caption": " Visualization of different methods.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " How many clusters are there in the figure? ",
    "answer": " There are 10 clusters in the figure.",
    "rationale": " The figure shows 10 different colors, each representing a different cluster. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.17699v1",
    "pdf_url": null
  },
  {
    "instance_id": "af2e6b6b04874d4e8304e59b96ffdbfa",
    "figure_id": "2202.09277v2-Figure8-1",
    "image_file": "2202.09277v2-Figure8-1.png",
    "caption": " Qualitative responses from the AVSD-QA dataset for various types of questions.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the man doing in the image?",
    "answer": "The man is cleaning the kitchen.",
    "rationale": "The man is standing in the kitchen and he is holding a bottle of water, a cloth, and a small broom. He is also standing next to the stove, which suggests that he is cleaning the stove.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.09277v2",
    "pdf_url": null
  },
  {
    "instance_id": "cea0484bf1064cdaa52f97a4c97b5754",
    "figure_id": "2203.11048v1-Figure10-1",
    "image_file": "2203.11048v1-Figure10-1.png",
    "caption": " Screenshot of annotation interface for sentence-level role, as well as summary sentence selection.",
    "figure_type": "\"other\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sentence(s) best summarizes the factors that contributed to the decline of the Populist Party?",
    "answer": "Sentence 1 and 2.",
    "rationale": "The image shows a screenshot of an annotation interface for sentence-level role, as well as summary sentence selection. The question asks the student to identify the sentence(s) that best summarize the factors that contributed to the decline of the Populist Party. Sentences 1 and 2 both address this question by stating that the party never recovered from its failures in 1896 and that its national alliance with the Democrats ultimately proved detrimental to its success.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.11048v1",
    "pdf_url": null
  },
  {
    "instance_id": "8075b4971a28450b8a61e943074f6a9a",
    "figure_id": "2209.13802v2-Figure6-1",
    "image_file": "2209.13802v2-Figure6-1.png",
    "caption": " Visualization results of token pruning for samples with different recognition difficulties.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stage of token pruning results in the most accurate reconstruction of the original image?",
    "answer": "Stage 3.",
    "rationale": "The figure shows the results of token pruning for samples with different recognition difficulties. The original image is shown in the leftmost column, and the results of token pruning at stages 1, 2, and 3 are shown in the remaining columns. The reconstructed image at stage 3 is the most similar to the original image, indicating that this stage results in the most accurate reconstruction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.13802v2",
    "pdf_url": null
  },
  {
    "instance_id": "fc799604546b4f9b82bea41d4214de35",
    "figure_id": "2004.09228v1-Figure3-1",
    "image_file": "2004.09228v1-Figure3-1.png",
    "caption": " Gradient Analysis for MCL-τ and MMCL. It is clear that, MMCL does not suffer from the vanishing gradient issue.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods in the figure is more susceptible to the vanishing gradient problem?",
    "answer": "MCL-τ",
    "rationale": "The figure shows that the magnitude of the gradient for MCL-τ decreases to zero as the classification score approaches 1, while the magnitude of the gradient for MMCL remains relatively constant. This indicates that MCL-τ is more susceptible to the vanishing gradient problem than MMCL.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.09228v1",
    "pdf_url": null
  },
  {
    "instance_id": "71688a7a12574f4d874de3a94fba0e1a",
    "figure_id": "2212.03467v2-Figure3-1",
    "image_file": "2212.03467v2-Figure3-1.png",
    "caption": " An instance with two client locations A,B and two possible facility locations Ok, Op with distances between any two adjacent locations labeled.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the distance between client location A and facility location Ok?",
    "answer": "1",
    "rationale": "The figure shows that the distance between client location A and facility location Ok is labeled as 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.03467v2",
    "pdf_url": null
  },
  {
    "instance_id": "1c91195cfbae4bfeb163aec163ee40d6",
    "figure_id": "2109.04732v1-Figure10-1",
    "image_file": "2109.04732v1-Figure10-1.png",
    "caption": " Test-retest reliability of target words, The word embeddings are trained with GloVe on r/AskHistorians.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which scoring rule has the highest test-retest reliability for the OCC16 target word list?",
    "answer": "DB/WA",
    "rationale": "The box plot for DB/WA is the highest for the OCC16 target word list.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.04732v1",
    "pdf_url": null
  },
  {
    "instance_id": "4d070ca71c02459cb6825d6533112b0b",
    "figure_id": "1908.05867v2-Figure3-1",
    "image_file": "1908.05867v2-Figure3-1.png",
    "caption": " Illustration of structures with relationship matrix U. The hollow circle and solid black circle indicate ‘0’ and ‘1’ respectively. A matrix of ones(a), identity matrix(b) and block diagonal matrix(d) imply regular convolution, depthwise convolution and group convolution (GConv) respectively. (c) and (e) show Dynamic Grouping Convolution (DGConv) under two non-adjacent group strategies respectively, one with a group number of 4 and the other with 2. (f) is a random group strategy, while it cannot been achieved under our constraint. (g) illustrates the construct process of DGConv when g = [0, 0, 1] and g = [0, 1, 0]. The binary relationship matrix U disables weights of ω via elementwise product operation.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of convolution is represented by the matrix in (b)?",
    "answer": "Depthwise convolution.",
    "rationale": "The identity matrix in (b) has ones on the diagonal and zeros elsewhere. This means that each output channel is only connected to one input channel, which is the definition of depthwise convolution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.05867v2",
    "pdf_url": null
  },
  {
    "instance_id": "bb6815ff9b4a42b18eee0aa231a2711b",
    "figure_id": "2005.06136v1-Figure6-1",
    "image_file": "2005.06136v1-Figure6-1.png",
    "caption": " Raw images (top) and trajectories (bottom) recovered by different methods on TUM-RGBD dataset",
    "figure_type": "photograph(s) and plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most accurate for recovering trajectories on the TUM-RGBD dataset?",
    "answer": "Ground Truth",
    "rationale": "The Ground Truth trajectories are the most accurate because they are the actual trajectories of the camera. The other methods are all estimates of the true trajectory, and they all have some degree of error. This can be seen in the bottom row of the figure, where the Ground Truth trajectories are shown in blue, and the other methods are shown in different colors. The Ground Truth trajectories are always the closest to the true path of the camera.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.06136v1",
    "pdf_url": null
  },
  {
    "instance_id": "82c8cdba4b6d4e28801ea947cc580822",
    "figure_id": "1810.11953v4-Figure45-1",
    "image_file": "1810.11953v4-Figure45-1.png",
    "caption": " CIFAR-10 only-zero shift (fixed) plus medium image shift (variable), univariate twosample tests + Bonferroni aggregation.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest p-value for all levels of perturbation?",
    "answer": "NoRed.",
    "rationale": "The plots in the first row show the p-values for different methods, and the NoRed line is consistently the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.11953v4",
    "pdf_url": null
  },
  {
    "instance_id": "b646f1c0c97f416a9692a9aede819146",
    "figure_id": "1810.07217v2-Figure15-1",
    "image_file": "1810.07217v2-Figure15-1.png",
    "caption": " Synthesized mel-spectrograms demonstrating independent control of speaking style and prosody. The same input text is used for all samples: “He waited a little, in the vain hope that she would relent: she turned away from him.” In the top row, F0 is controlled by setting different values for dimension eight. F0 tracks show that the F0 range increases from left to right, while other attributes such as speed and rhythm do not change. In the second row, the duration of pause before the phrase “she turned away from him.” (red boxes) is varied. The three spectrograms are very similar, except for the width of the red boxes, indicating that only the pause duration changed. In the bottom row, the “roughness” of the voice is varied. The same region of spectrograms is zoomed-in for clarity, where the spectrograms became less blurry and the harmonics becomes better defined from left to right. Audio samples can be found at https://google.github.io/tacotron/ publications/gmvae_controllable_tts#singlespk_audiobook.control.",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which dimension controls the roughness of the voice? ",
    "answer": " Dimension 14. ",
    "rationale": " The caption states that the roughness of the voice is varied in the bottom row of the figure, and the bottom row is labeled \"Dimension 14: roughness.\" \n\n**Figure type:** Plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.07217v2",
    "pdf_url": null
  },
  {
    "instance_id": "45eebcfa78fc40b0961d76064f992d7d",
    "figure_id": "2211.05955v2-Figure3-1",
    "image_file": "2211.05955v2-Figure3-1.png",
    "caption": " Distributions of most argument roles for each language.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which language has the highest percentage of arguments related to the entity role? ",
    "answer": " Spanish",
    "rationale": " The figure shows that Spanish has the highest bar segment for the entity role.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.05955v2",
    "pdf_url": null
  },
  {
    "instance_id": "08318d07595d43969a11f980391a43b4",
    "figure_id": "2106.13423v5-Figure7-1",
    "image_file": "2106.13423v5-Figure7-1.png",
    "caption": " Distributions of performance gains of all clients using original node features versus one-hot degree features on the oneDS (top) and multiDS (bottom) settings.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best on the multiDS: MIX dataset when using the original node features?",
    "answer": "GCFL+",
    "rationale": "The figure shows the distribution of performance gains for each method on each dataset. The GCFL+ method has the highest median performance gain on the multiDS: MIX dataset when using the original node features.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.13423v5",
    "pdf_url": null
  },
  {
    "instance_id": "2271fb9c0921462b8556e6273e67ab11",
    "figure_id": "2004.12770v3-Figure1-1",
    "image_file": "2004.12770v3-Figure1-1.png",
    "caption": " Examples of questions in the CLEVR [16] dataset that show a significant variation in the number of reasoning steps that are needed to answer them correctly.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many cubes are there in the image?",
    "answer": "5",
    "rationale": "The image shows 5 cubes: one red, one blue, one yellow, one cyan, and one purple.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.12770v3",
    "pdf_url": null
  },
  {
    "instance_id": "b7227741365349a18bb68bb601b4a1f7",
    "figure_id": "2011.10875v2-Figure9-1",
    "image_file": "2011.10875v2-Figure9-1.png",
    "caption": " Qualitative results of nine trackers in six typical difficult challenges: WineGlass-7 with rotation, Bulb-5 with background clutter, GlassSlab-15 with aspect ratio change, JuggleBubble-1 with partial occlusion, ShotGlass-10 with motion blur and TransparentAnimal-11 with scale variation. The proposed TransATOM robustly locates target objects under various challenges owing to transparency feature.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tracker performs best in the WineGlass-7 sequence with rotation?",
    "answer": "DiMP",
    "rationale": "The figure shows the qualitative results of nine trackers in six different challenging sequences. The WineGlass-7 sequence is shown in the top left panel of the figure. The DiMP tracker is the only tracker that is able to track the wine glass accurately throughout the entire sequence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.10875v2",
    "pdf_url": null
  },
  {
    "instance_id": "c2bdffdd18c54bd18dfc6c615a523625",
    "figure_id": "1810.03292v3-Figure25-1",
    "image_file": "1810.03292v3-Figure25-1.png",
    "caption": " Comparison between explanations for a true model and one trained on random labels. MLP on FMNIST.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which explanation method has the highest Spearman rank correlation for the model trained on true labels?",
    "answer": "Integrated Gradients",
    "rationale": "The bar plot on the bottom left shows the Spearman rank correlation for each explanation method. The bar for Integrated Gradients is the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.03292v3",
    "pdf_url": null
  },
  {
    "instance_id": "08fcacd745a04c5abe9ab067390d1b5f",
    "figure_id": "1806.04522v1-Figure2-1",
    "image_file": "1806.04522v1-Figure2-1.png",
    "caption": " Learning curves on test error (top) and negative test LL (bottom).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization method performs the best in terms of generalization error and negative log-likelihood (LL)?",
    "answer": "SGHMC.",
    "rationale": "The figure shows the learning curves for different optimization methods, including SGHMC, NNSGHMC, SGLD, and PSGDL. The learning curves show that SGHMC achieves the lowest test error and negative LL. This suggests that SGHMC performs the best in terms of generalization error and negative LL.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.04522v1",
    "pdf_url": null
  },
  {
    "instance_id": "095b570143434388b11e6508783e9ee8",
    "figure_id": "2303.12384v3-Figure7-1",
    "image_file": "2303.12384v3-Figure7-1.png",
    "caption": " Visualization of registration errors. Point clouds colored yellow and blue indicate transformed source points by the ground truth (GT) and our estimated pose. Registration errors are visualized by a red vector pointing from estimated points to the GT.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three images shows the largest registration error?",
    "answer": "Image (c) shows the largest registration error.",
    "rationale": "The red vectors in image (c) are longer than the red vectors in images (a) and (b), indicating a larger registration error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.12384v3",
    "pdf_url": null
  },
  {
    "instance_id": "3cf02174a7294f308fae674dbec5c60a",
    "figure_id": "2105.14937v2-Figure3-1",
    "image_file": "2105.14937v2-Figure3-1.png",
    "caption": " Safe motion planning for cartpole (a)-(b) and 6-DoF rocket powered landing (c)-(d).",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method, ALTRO or Safe PDP, results in lower constraint violation for the rocket landing task? ",
    "answer": " Safe PDP ",
    "rationale": " Subplots (c) and (d) show the constraint violation during optimization for the rocket landing task. The constraint violation for Safe PDP is lower than that of ALTRO. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14937v2",
    "pdf_url": null
  },
  {
    "instance_id": "c040d9193b37406994fe23a92da9fc9f",
    "figure_id": "2004.03720v2-Figure2-1",
    "image_file": "2004.03720v2-Figure2-1.png",
    "caption": " English subword vocabulary and corpus profiles. The unigram LM method produces longer tokens on average (a) and uses its vocabulary space more effectively (b), with more tokens of moderate frequency.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces tokens with a higher average frequency?",
    "answer": "The BPE method.",
    "rationale": "In subfigure (b), the BPE curve is above the Unigram LM curve, which means that BPE has more tokens with a higher frequency.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.03720v2",
    "pdf_url": null
  },
  {
    "instance_id": "70366b069b254cdc8c4e645cbce6101a",
    "figure_id": "2210.16732v1-Figure4-1",
    "image_file": "2210.16732v1-Figure4-1.png",
    "caption": " Change in Pearson correlation when error types are omitted. Higher value indicates a greater influence of the error type on overall correlation result.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which error type has the largest influence on the overall correlation result for OpenIE?",
    "answer": "OutE",
    "rationale": "The figure shows the change in Pearson correlation when different error types are omitted. The bar for OutE is the highest for OpenIE, indicating that omitting this error type has the largest impact on the correlation result.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.16732v1",
    "pdf_url": null
  },
  {
    "instance_id": "88887d9d34a04c94a76a3ef44169abdf",
    "figure_id": "2004.05794v1-Figure4-1",
    "image_file": "2004.05794v1-Figure4-1.png",
    "caption": " Visual comparisons on the GoPro dataset. From left to right, we show two examples with the blurred image, results of MPN [50], BHA [31] and our approach, as well as groundtruth sharp image, respectively. Zoom in for better view.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the sharpest image?",
    "answer": "The ground truth (GT) image is the sharpest.",
    "rationale": "The figure shows that the GT image has the most detail and is the least blurry. The other methods, MPN, BHA, and Ours, all produce images that are less sharp than the GT image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.05794v1",
    "pdf_url": null
  },
  {
    "instance_id": "fadf91ad0fcf4b569796f8e5150cb525",
    "figure_id": "2110.07692v1-Figure2-1",
    "image_file": "2110.07692v1-Figure2-1.png",
    "caption": " Left: Detected ACOs from EPIC videos. White boxes: active object detections. Red/blue boxes: sampled (object, ACO) tuple (Equation 2). Last column shows failure cases—sponge not held (top), liquid:washing false positive (bottom). Right: ACO compatibility scores in THOR. Red: active object, blue: top corresponding ACOs. Edge thickness indicates compatibility (Equation 3). Our approach (bottom right) prioritizes object relationships for “Pan” that are relevant for activity (e.g., StoveBurner, StoveKnob) over semantically similar or spatially co-occurring objects (e.g., Pot, Shelf in top-right, bottom-left respectively).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the different types of ACO compatibility scores shown in the figure?",
    "answer": "Uniform, WordEmbed, ScenePriors, and Ours.",
    "rationale": "The right side of the figure shows four different types of ACO compatibility scores. Each type of score is represented by a different graph, with the active object in the center and the top corresponding ACOs connected to it by edges. The thickness of the edges indicates the compatibility score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.07692v1",
    "pdf_url": null
  },
  {
    "instance_id": "294e0d8d4a5646d4b63de46f441b1899",
    "figure_id": "2110.06465v2-Figure2-1",
    "image_file": "2110.06465v2-Figure2-1.png",
    "caption": " The errors of different modes in different methods.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the smallest error for the T1 C + R mode?",
    "answer": "NICEGAN.",
    "rationale": "The error maps for each method are shown in the figure. The error map for NICEGAN is mostly blue, indicating a smaller error than the other methods, which have more red and yellow in their error maps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.06465v2",
    "pdf_url": null
  },
  {
    "instance_id": "9b61c934e14a4f068c88b461704fdeb1",
    "figure_id": "2210.15837v1-Figure1-1",
    "image_file": "2210.15837v1-Figure1-1.png",
    "caption": " Empirical distribution (in validation) of batch expense for RAP, under different risk aversion levels, and RNP when B=1/2 B̄",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which risk aversion level is most likely to result in a batch expense exceeding the total budget?",
    "answer": "α=0.5",
    "rationale": "The vertical line at the right side of the plot represents the total budget. The only cumulative probability curve that goes beyond this line is the one for α=0.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.15837v1",
    "pdf_url": null
  },
  {
    "instance_id": "1e567f9575e941a98ba2ce590b7a7d74",
    "figure_id": "2309.16859v1-Figure20-1",
    "image_file": "2309.16859v1-Figure20-1.png",
    "caption": " Effect of optimising only the model parameters θtarget (top row) and optimising both the model parameters and the latent code wtarget (bottom row, Ours). The visual results are very similar. Tbl. 4 lists quantitative metrics.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which row of the figure shows the results of optimizing both the model parameters and the latent code?",
    "answer": "The bottom row.",
    "rationale": "The caption states that the bottom row shows the results of optimizing both the model parameters and the latent code.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.16859v1",
    "pdf_url": null
  },
  {
    "instance_id": "c76ff31e5928491485e0a6fc71d53c55",
    "figure_id": "2206.02511v1-Figure6-1",
    "image_file": "2206.02511v1-Figure6-1.png",
    "caption": " Case study on universality and safeness.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in terms of safeness?",
    "answer": "Our + GP.",
    "rationale": "The safeness plot shows that Our + GP has the lowest average NCE, which means it is the safest algorithm.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.02511v1",
    "pdf_url": null
  },
  {
    "instance_id": "133d38d4be784c2b9adec951e3d00501",
    "figure_id": "1902.07151v2-Figure10-1",
    "image_file": "1902.07151v2-Figure10-1.png",
    "caption": " Win rate matrix for the Tournament between teams: from top to bottom, ordered by Elo, ascending: ff + evo; ff + evo + rwd shp; lstm q + evo + rwd shp; lstm q + evo + rwd shp + channels; lstm + evo + rwd shp + channels. ELo derived from the tournament is given in the table.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which team has the highest Elo rating?",
    "answer": "lstm + evo + rwd_shp + channels",
    "rationale": "The table on the right side of the figure shows the Elo rating for each team. The team with the highest Elo rating is lstm + evo + rwd_shp + channels, with an Elo rating of 1071.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.07151v2",
    "pdf_url": null
  },
  {
    "instance_id": "8fd44c7634d74d09bd363b347907c852",
    "figure_id": "2201.06578v2-Figure12-1",
    "image_file": "2201.06578v2-Figure12-1.png",
    "caption": " Randomly-generated images using the proposed method trained on the AnimalFace dataset. Each row represents a different class. The FID score is 16.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different classes of animals are shown in the image?",
    "answer": "14",
    "rationale": "The caption states that each row represents a different class. There are 14 rows in the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.06578v2",
    "pdf_url": null
  },
  {
    "instance_id": "d2a6f2ed242944c8b4e75b5df391c182",
    "figure_id": "2006.06873v2-Figure4-1",
    "image_file": "2006.06873v2-Figure4-1.png",
    "caption": " Glicko-2 ranking. The models have different numbers of attention heads (A), layers (L), and pitch averaged to three values per input symbol instead of one (3Pitch).",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest Glicko-2 ranking?",
    "answer": "FastPitch 1A 6L.",
    "rationale": "The figure shows the Glicko-2 ranking for different FastPitch models. The y-axis shows the Glicko-2 rating, and the x-axis shows the different models. The model with the highest Glicko-2 ranking is FastPitch 1A 6L, which has a rating of about 1580.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06873v2",
    "pdf_url": null
  },
  {
    "instance_id": "7b20db91b8094e9ca047d78fc5f272f1",
    "figure_id": "2302.05675v1-Figure5-1",
    "image_file": "2302.05675v1-Figure5-1.png",
    "caption": " Prediction accuracy by varying the data hospital’s feature number. 100 1500 3000 5000 0.70",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best when the number of shared samples between hospitals is 5000?",
    "answer": "VFedTrans(our)",
    "rationale": "The figure shows that the VFedTrans(our) method has the highest accuracy when the number of shared samples between hospitals is 5000. This is evident from the red line being the highest at the point where the x-axis is at 5000.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.05675v1",
    "pdf_url": null
  },
  {
    "instance_id": "12b8457434a34b77a690154f7fcd2560",
    "figure_id": "2106.02097v3-Figure21-1",
    "image_file": "2106.02097v3-Figure21-1.png",
    "caption": " OOD performance under a gradient of difficulty in Turn-and-Forward tasks with color distractions. All error bars are obtained from 20 independent runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms performs best in the most difficult OOD setting?",
    "answer": "CP(8)+",
    "rationale": "The figure shows the success rate of four algorithms in five different OOD settings, with increasing difficulty from left to right. In the most difficult setting (d), CP(8)+ has the highest success rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02097v3",
    "pdf_url": null
  },
  {
    "instance_id": "797f114e11384a6c8cda10bc5c9bc101",
    "figure_id": "2207.08675v2-FigureA.2-1",
    "image_file": "2207.08675v2-FigureA.2-1.png",
    "caption": "Figure A.2: 1D convection: Error on test set during training. We train a NN with the PDE residual loss function (“soft constraint” baseline) and the same NN architecture with our PDE-CL (“hard constraint”). During training, we track error on the test set, which we plot on a log-log scale. (a) PDE residual loss on the test set, during training. We observe that the NN starts by fitting the initial and boundary condition regression loss during training, which explains why the PDE residual loss seems to go up initially. (b) Relative error on the test set, during training. Both measures show that our hard-constrained model starts at a much lower error on the test set at the very start of training. The grey, dashed line shows that the hard-constrained model achieves the same relative error as the soft-constrained model in over 100x fewer iterations, and ultimately achieves lower relative error. Wall-time comparison figures are given in Appendix B.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which constraint type achieves lower relative error on the test set during training?",
    "answer": "The hard constraint.",
    "rationale": "The figure shows the relative error on the test set for both hard and soft constraints. The hard constraint (blue line) starts at a lower error and decreases faster than the soft constraint (orange line). The hard constraint also achieves a lower final relative error than the soft constraint.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.08675v2",
    "pdf_url": null
  },
  {
    "instance_id": "725e35d8e1b241148dc2c51e730ef91f",
    "figure_id": "2210.12563v1-Figure8-1",
    "image_file": "2210.12563v1-Figure8-1.png",
    "caption": " See Table 1 for a description of this Figure.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language pair has the highest BLEURT score for the reference translation?",
    "answer": "en-cs",
    "rationale": "The BLEURT score for the reference translation is indicated by the red X in each plot. The highest BLEURT score for the reference translation is in the en-cs plot, which is approximately 0.2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12563v1",
    "pdf_url": null
  },
  {
    "instance_id": "73d975e416fa4eed9c806085b0bdbee6",
    "figure_id": "2006.04176v2-Figure3-1",
    "image_file": "2006.04176v2-Figure3-1.png",
    "caption": " Agent’s performance during on-policy training in the object sorting task. A: Comparison of different action selection strategies for the agent driven by the full Eq. (8). B-C: Comparison of agents driven by different functionals, limited to state estimations of a single step into the future. In C, the violin plots represent behavior driven by P (at) (the planner) and the gray box plots driven by the habitual network Qφa (at). D-F: Reconstruction loss and total correlation during learning for 4 different functionals. In A, B and D-F, the shaded areas represent the standard deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which action selection strategy resulted in the highest reward per round?",
    "answer": "MCTS",
    "rationale": "In panel A, the MCTS line is consistently above the other lines, indicating that it achieved the highest reward per round.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04176v2",
    "pdf_url": null
  },
  {
    "instance_id": "2a196ea58f31464a8788321020eed2ef",
    "figure_id": "1810.12894v1-Figure4-1",
    "image_file": "1810.12894v1-Figure4-1.png",
    "caption": " Different ways of combining intrinsic and extrinsic rewards. Combining non-episodic stream of intrinsic rewards with the episodic stream of extrinsic rewards outperforms combining episodic versions of both steams in terms of number of explored rooms, but performs similarly in terms of mean return. Single value estimate of the combined stream of episodic returns performs a little better than the dual value estimate. The differences are more pronounced with RNN policies. CNN runs are more stable than the RNN counterparts.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which policy type, RNN or CNN, resulted in more stable performance in terms of mean episodic return and mean number of rooms found?",
    "answer": "CNN policies resulted in more stable performance.",
    "rationale": "The shaded areas in the plots represent the standard deviation of the results. For both the mean episodic return and the mean number of rooms found, the shaded areas are narrower for the CNN policies than for the RNN policies. This indicates that the CNN policies produced more consistent results across different runs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.12894v1",
    "pdf_url": null
  },
  {
    "instance_id": "e2a4c8e86e9546fdb2114716ef2e7597",
    "figure_id": "1909.06349v2-Figure6-1",
    "image_file": "1909.06349v2-Figure6-1.png",
    "caption": " For each application dataset (Section 4.1) we report all relative, slice-level metrics compared to VANILLA for each model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed best on the CyDet dataset?",
    "answer": "SBL (Ours)",
    "rationale": "The figure shows that the SBL (Ours) model had the highest relative improvement over the Vanilla Model F1 score for all slices of the CyDet dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.06349v2",
    "pdf_url": null
  },
  {
    "instance_id": "f5f62737187b405696d9c31e8e042a87",
    "figure_id": "2306.14421v1-Figure7-1",
    "image_file": "2306.14421v1-Figure7-1.png",
    "caption": " The learned multi-head attention weights.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which roads are most likely to be included in the top-5 historical trips?",
    "answer": "Roads 13 and 14.",
    "rationale": "The figure shows the learned multi-head attention weights for the roads. The higher the weight, the more likely the road is to be included in the top-5 historical trips. Roads 13 and 14 have the highest weights, so they are most likely to be included in the top-5 historical trips.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.14421v1",
    "pdf_url": null
  },
  {
    "instance_id": "eb43aa9c7b074234a200bc5f454d4778",
    "figure_id": "2003.10780v1-Figure3-1",
    "image_file": "2003.10780v1-Figure3-1.png",
    "caption": " Mean conditional weights {εi} within each class vs. training epochs on CIFAR-LT-10 (left: IF = 100; right: IF = 10).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class has the most consistent mean instance weights across epochs?",
    "answer": "Class 9.",
    "rationale": "The plot on the right shows that the mean instance weights for Class 9 are relatively stable across epochs, while the weights for the other classes fluctuate more.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.10780v1",
    "pdf_url": null
  },
  {
    "instance_id": "b331448ec0e54dc5b319c47b14cab8f2",
    "figure_id": "2210.07370v1-Figure2-1",
    "image_file": "2210.07370v1-Figure2-1.png",
    "caption": " Visualization of the hierarchies contained within the S2ORC portion of M2D2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two most common categories of papers in the S2ORC portion of M2D2?",
    "answer": "math and astro-ph.",
    "rationale": "The figure shows a pie chart of the different categories of papers in the S2ORC portion of M2D2. The two largest slices of the pie chart are labeled \"math\" and \"astro-ph\", indicating that these are the two most common categories of papers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.07370v1",
    "pdf_url": null
  },
  {
    "instance_id": "d8ab557d1b53439ab4a780ba5c3fe787",
    "figure_id": "2211.00863v3-Figure5-1",
    "image_file": "2211.00863v3-Figure5-1.png",
    "caption": " Effective dimension of the penultimate layer of the value network (top) and performance (bottom) over the training on the Halfcheetah-medium-expert-v2 task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models, CQL, CQL (with encoder), or CQL (pretrained by BPR), performs the best on the Halfcheetah-medium-expert-v2 task?",
    "answer": "CQL (pretrained by BPR)",
    "rationale": "The bottom plot shows the cumulative returns for each model over the course of training. The CQL (pretrained by BPR) model has the highest cumulative return, indicating that it performs the best on the task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.00863v3",
    "pdf_url": null
  },
  {
    "instance_id": "5412c31b15054fd48c1f1df51db5a1b4",
    "figure_id": "1906.02290v1-Figure5-1",
    "image_file": "1906.02290v1-Figure5-1.png",
    "caption": " The results iteration-by-iteration. The compared methods minimizing an energy function iteratively: the proposed Prog-X (red), Multi-X [3] (blue) and PEARL [5] (orange). (a) The number of stored instances are shown (vertical axis) as a function of the iteration number (horizontal). The values in the 1st iteration are 2 200 for PEARL and 550 for Multi-X. (b) The ratio (vertical) of the number of ground truth models covered by an instance and the number of all instances stored are reported. (c) The processing times (vertical; in ms) are shown for each iteration (horizontal). The times in the 1st iteration are 6 904 ms for PEARL and 1 949 ms for Multi-X. In total, PEARL required 9 880 ms, MultiX 2 065 ms and Prog-X 1 534 ms. (d) The energy divided by the energy in the 1st iteration (horizontal) is is shown.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, PEARL, Multi-X, or Prog-X, has the lowest energy at the final iteration?",
    "answer": "Prog-X",
    "rationale": "Figure (d) shows the relative energy of each method as a function of iteration. The relative energy of Prog-X is the lowest at the final iteration.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02290v1",
    "pdf_url": null
  },
  {
    "instance_id": "4ce1a6b7012f4b8386ffc4bedf6e9c27",
    "figure_id": "1908.06698v1-Figure6-1",
    "image_file": "1908.06698v1-Figure6-1.png",
    "caption": " Learning curves compared with baselines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms consistently performs the best in terms of episode organic traffic increment?",
    "answer": "Manual",
    "rationale": "The manual algorithm is represented by the blue line in the plot. It can be seen that the blue line is consistently above all other lines, indicating that the manual algorithm has the highest episode organic traffic increment throughout the entire training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.06698v1",
    "pdf_url": null
  },
  {
    "instance_id": "93332a19721e45efaab7257bd2445948",
    "figure_id": "2106.05933v2-Figure120-1",
    "image_file": "2106.05933v2-Figure120-1.png",
    "caption": " Sparsity over layers for wav2vec-base finetuned for Tatar tt at 80% sparsity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer of the wav2vec2-base model has the highest sparsity when fine-tuned for Tatar tt at 80% sparsity?",
    "answer": "Layer 11 has the highest sparsity at 89.914%.",
    "rationale": "The bar chart shows the sparsity percentage for each layer of the model. The x-axis shows the layer number, and the y-axis shows the sparsity percentage. The height of each bar represents the sparsity percentage for the corresponding layer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05933v2",
    "pdf_url": null
  },
  {
    "instance_id": "f5437ca40fae4319ba28c6669ef70a2b",
    "figure_id": "2207.01413v1-Figure6-1",
    "image_file": "2207.01413v1-Figure6-1.png",
    "caption": " We synthesize a frame from Barn dataset at four different timestamps (rows), using four latent codes (columns). The latent codes express the weather",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which latent code is associated with a rainy day?",
    "answer": "Latent code 4",
    "rationale": "The bottom right photograph in the figure shows a scene with dark clouds and rain. This corresponds to latent code 4, which is labeled as \"rainy day\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.01413v1",
    "pdf_url": null
  },
  {
    "instance_id": "fc5f64a5b78b4499b0b0605161c87a22",
    "figure_id": "2208.10364v3-Figure8-1",
    "image_file": "2208.10364v3-Figure8-1.png",
    "caption": " The average firing rate of SpikeNet at different time intervals.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which time interval does SpikeNet have the highest average firing rate for all three datasets?",
    "answer": "(0.6T, T)",
    "rationale": "The figure shows that the firing rate for all three datasets is highest in the (0.6T, T) time interval.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.10364v3",
    "pdf_url": null
  },
  {
    "instance_id": "abd8dab3b198468a89e637eca03dec73",
    "figure_id": "2301.10900v2-Figure1-1",
    "image_file": "2301.10900v2-Figure1-1.png",
    "caption": " Graph visualization of sequences from two easily confused classes (“point to something” and “take a selfie”). The graphs are learned by CTR-GCN (Chen et al., 2021b). We take the tip of the hand that does the action as the anchor. The size of the red circles and the width of the blue lines both denote the strengths of connections between joints. For simplicity, only representative frames are visualized. (a) Three sequences from class “point to something” are correctly classified, where the graphs contain connections to the body joints. (b) Three sequences from class “take a selfie” are correctly classified, where the graphs highly emphasize the connections to the hands, while the connections to the body are suppressed. (c) A sequence from class “point to something” is misclassified as “take a selfie”, whose graph resembles the graphs in (b), but is dissimilar from graphs in (a). Hence, we realize that the class-ambiguous graph representations would make negative impacts on recognition performance.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following statements is true about the graph visualization of sequences from the \"point to something\" class?",
    "answer": "The graphs contain connections to the body joints.",
    "rationale": "In Figure (a), we can see that the graphs for the \"point to something\" class have strong connections between the hand and the body joints. This is in contrast to the graphs for the \"take a selfie\" class, which have weaker connections to the body joints and stronger connections to the hands.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.10900v2",
    "pdf_url": null
  },
  {
    "instance_id": "6c5952ab172a4601a0470c6d3b8b37eb",
    "figure_id": "2005.01172v1-Figure9-1",
    "image_file": "2005.01172v1-Figure9-1.png",
    "caption": " Similarity heatmaps of layers in various models under neuron-level and representation-level similarity measures, using the English Web Treebank corpus.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which similarity measure results in the most similar representations across all models?",
    "answer": "The mixedsim similarity measure.",
    "rationale": "The mixedsim heatmap in Figure (e) shows the highest level of similarity across all models, with most of the squares being a dark red color. This indicates that the representations produced by the different models are more similar to each other when using the mixedsim measure than when using the other measures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.01172v1",
    "pdf_url": null
  },
  {
    "instance_id": "5c10cdd3f1dc4218b1eb8832d3f33628",
    "figure_id": "2006.09662v1-Figure6-1",
    "image_file": "2006.09662v1-Figure6-1.png",
    "caption": " T-SNE embedding of parameters of specialized networks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many clusters are there in the T-SNE embedding?",
    "answer": "9",
    "rationale": "The T-SNE embedding shows 9 distinct clusters of points, each representing a different specialized network. The clusters are labeled with numbers 1-9.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.09662v1",
    "pdf_url": null
  },
  {
    "instance_id": "11ea7c31417942558621fe9007306fb7",
    "figure_id": "2211.13345v3-Figure13-1",
    "image_file": "2211.13345v3-Figure13-1.png",
    "caption": " Cumulative benefit obtained as a function of cumulative effort cost (without budget limitation) on v6.3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is the most efficient in terms of cumulative benefit obtained per unit of effort cost?",
    "answer": "DISCLOSE-v6.3.",
    "rationale": "The figure shows the cumulative benefit obtained as a function of the cumulative effort cost for three different algorithms. The slope of the curve for DISCLOSE-v6.3 is the steepest, indicating that it achieves the highest cumulative benefit for a given effort cost.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.13345v3",
    "pdf_url": null
  },
  {
    "instance_id": "19fa34b60f4c4ba7a619fcd87ab07dee",
    "figure_id": "2107.04867v1-Figure4-1",
    "image_file": "2107.04867v1-Figure4-1.png",
    "caption": " Keypoint transfer results for five categories: airplane, bathtub, chair, motorcycle, mug, and vessel. Each row contains two shape each with ground-truth keypoints and its pairwise transfer result.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is able to transfer keypoints from Shape A to Shape B most accurately?",
    "answer": "Ours",
    "rationale": "The figure shows that our method is able to transfer keypoints from Shape A to Shape B most accurately, as the transferred keypoints are closest to the ground-truth keypoints.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.04867v1",
    "pdf_url": null
  },
  {
    "instance_id": "5952407912274c72916d37b618315687",
    "figure_id": "2106.05137v2-Figure6-1",
    "image_file": "2106.05137v2-Figure6-1.png",
    "caption": " Comparison of signaling strategies in a navigation application, with a uniform congestion level at each step. All other settings are the same as Figure 3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which signaling strategy is the most robust to changes in the number of steps, _m_?",
    "answer": "OptSig-MYOP",
    "rationale": "The figure shows the performance of different signaling strategies as a function of the number of steps, _m_. OptSig-MYOP is the only strategy that maintains a high level of performance for all values of _m_.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05137v2",
    "pdf_url": null
  },
  {
    "instance_id": "9cae97ddb5a74f3582f3eeef7ccb61b3",
    "figure_id": "2302.03018v1-Figure4-1",
    "image_file": "2302.03018v1-Figure4-1.png",
    "caption": " Left: Quantitative fractional anisotropy map comparisons (major differences highlighted with arrows). Right: DDM2 demonstrates statistically significant SNR/CNR improvements (twosided t-test, < 1e−4 p-values) over all competing methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method resulted in the highest relative SNR and CNR values?",
    "answer": "Ours.",
    "rationale": "The boxplot on the right shows that the \"Ours\" method has the highest median and upper quartile values for both relative SNR and CNR.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.03018v1",
    "pdf_url": null
  },
  {
    "instance_id": "62521c28c6664d969e339f2d598a4566",
    "figure_id": "2205.03811v2-Figure3-1",
    "image_file": "2205.03811v2-Figure3-1.png",
    "caption": " Evaluation of different loss functions (teacher model is GIN-5-128).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function achieves the highest test accuracy for the PROTEINS dataset?",
    "answer": "L-MAE",
    "rationale": "The figure shows the test accuracy for different loss functions and student models on the PROTEINS and COLLAB datasets. For the PROTEINS dataset, the L-MAE loss function achieves the highest test accuracy of around 70%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.03811v2",
    "pdf_url": null
  },
  {
    "instance_id": "c8f367486ab7442b9d44aa3ec58bacd8",
    "figure_id": "2303.11700v1-Figure2-1",
    "image_file": "2303.11700v1-Figure2-1.png",
    "caption": " Overview toDynamically Expandable GraphConvolution. Take 2-layer graph convolution as an example. Operations 1,2,3 correspond to the methods introduced in Sec. 4.2. And operations 4,5,6,7 illustrate the methods mentioned in Sec. 4.3.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which operations in the figure correspond to the methods introduced in Section 4.2?",
    "answer": "Operations 1, 2, and 3.",
    "rationale": "The caption states that \"Operations 1, 2, 3 correspond to the methods introduced in Sec. 4.2.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.11700v1",
    "pdf_url": null
  },
  {
    "instance_id": "11a9a9010eb84d7e90abe9d3d8384702",
    "figure_id": "1904.09569v1-Figure2-1",
    "image_file": "1904.09569v1-Figure2-1.png",
    "caption": " Visual comparisons for salient object detection with different combinations of our proposed GGM and FAMs. (a) Source image; (b) Ground truth; (c) Results of FPN baseline; (d) Results of FPN + FAMs; (e) Results of FPN + PPM; (f) Results of FPN + GGM; (g) Results of FPN + GGM + FAMs.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which combination of methods produced the most accurate results for salient object detection?",
    "answer": "FPN + GGM + FAMs",
    "rationale": "The results of FPN + GGM + FAMs (g) are the closest to the ground truth (b).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.09569v1",
    "pdf_url": null
  },
  {
    "instance_id": "512ad3b93451412789b2f02181cbe0ee",
    "figure_id": "2209.02650v2-Figure3-1",
    "image_file": "2209.02650v2-Figure3-1.png",
    "caption": " Comparison of S-SYMLTL and CEGLTL in terms of the inference time for three LTL ground truth formulas.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is faster for inferring the ground truth formula ψ2?",
    "answer": "CEGLTL",
    "rationale": "The figure shows that the green line (CEGLTL) is below the red line (S-SYMLTL) for all values of n. This means that CEGLTL takes less time to infer the ground truth formula ψ2 than S-SYMLTL.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.02650v2",
    "pdf_url": null
  },
  {
    "instance_id": "81765266766642eabfcd65d2147e22a6",
    "figure_id": "1911.06600v1-Figure5-1",
    "image_file": "1911.06600v1-Figure5-1.png",
    "caption": " Qualitative performance of PSG [4] and PCDNet-UpResGraphX on some real life images taken from Pix3D. The predictions from PSG have high variance compared to ours, which present clear and solid shapes.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, PSG or PCDNet-UpResGraphX, produces point clouds with higher variance?",
    "answer": "PSG",
    "rationale": "The caption states that \"The predictions from PSG have high variance compared to ours, which present clear and solid shapes.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.06600v1",
    "pdf_url": null
  },
  {
    "instance_id": "15c0d94cf1ac4941afecde8efcd581bf",
    "figure_id": "2305.15212v1-Figure2-1",
    "image_file": "2305.15212v1-Figure2-1.png",
    "caption": " Visualization of the learned weights of the prefix token for SuperGLUE task COPA on BERT-large and NER task CoNLL04 on BERT-base, with darker colors indicating higher weights.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task shows a more even distribution of weights across the model layers?",
    "answer": "COPA.",
    "rationale": "The figure shows the learned weights of the prefix token for two different tasks: COPA and CoNLL04. In COPA, the weights are more evenly distributed across the model layers, while in CoNLL04, the weights are concentrated in the earlier layers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.15212v1",
    "pdf_url": null
  },
  {
    "instance_id": "a7a90062c6964549ba753ba0f37d4d32",
    "figure_id": "1806.03085v2-Figure5-1",
    "image_file": "1806.03085v2-Figure5-1.png",
    "caption": " Nonlinear regression example: performance comparison between SVNfull, SVNCG, and SVNbd after 20 iterations",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, SVNfull, SVNCG, or SVNbd, performs the best after 20 iterations?",
    "answer": "SVNfull-H",
    "rationale": "The figure shows the performance of the three methods after 20 iterations. SVNfull-H has the closest fit to the target, which is indicated by the green dots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.03085v2",
    "pdf_url": null
  },
  {
    "instance_id": "2e0184ac08134d8f9e09664fa8c94ea9",
    "figure_id": "2002.09629v1-Figure2-1",
    "image_file": "2002.09629v1-Figure2-1.png",
    "caption": " Vendors’ management of security bulletins compared to Android.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which vendor had the highest fraction of CVEs that were not mentioned in their security bulletins?",
    "answer": "LG",
    "rationale": "The figure shows that LG had the highest fraction of CVEs that were not mentioned in their security bulletins. This is shown by the red bars in the figure, which represent the fraction of CVEs that were not mentioned.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.09629v1",
    "pdf_url": null
  },
  {
    "instance_id": "9c16469f278341658411f2757805738c",
    "figure_id": "1906.04201v1-Figure6-1",
    "image_file": "1906.04201v1-Figure6-1.png",
    "caption": " Qualitative results on virtual scans from SUNCG. Note that our method handles complex CAD arrangements better than Scan2CAD.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method handles complex CAD arrangements better, Scan2CAD or the method proposed in the paper?",
    "answer": "The method proposed in the paper.",
    "rationale": "The figure shows that the proposed method is able to more accurately reconstruct the furniture and layout of the room, while Scan2CAD produces a more simplistic and less accurate reconstruction. This is particularly evident in the more complex scenes, such as the bedroom and dining room.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04201v1",
    "pdf_url": null
  },
  {
    "instance_id": "208bd591cf1d41c7a80fd8ed7976542a",
    "figure_id": "1907.08467v2-Figure7-1",
    "image_file": "1907.08467v2-Figure7-1.png",
    "caption": " FPS generated by different emulation engines on System I in Table 2 for different Atari games, as a function of the number of environments, and different load conditions (the main A2C [14] loop is run here, with N-step bootstrapping, N = 5.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which emulation engine is the most efficient for all three games?",
    "answer": "OpenAI.",
    "rationale": "The OpenAI lines are the highest in all three plots, indicating that it generates the most FPS for all three games.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.08467v2",
    "pdf_url": null
  },
  {
    "instance_id": "b7fa16227414414b8102bb5fa9335829",
    "figure_id": "2108.13264v4-FigureA.28-1",
    "image_file": "2108.13264v4-FigureA.28-1.png",
    "caption": "Figure A.28: Average Probability of Improvement on ALE. Each subplot shows the probability of improvement of a given algorithm compared to all other algorithms. The interval estimates are based on stratified bootstrap with independent sampling with 2000 bootstrap re-samples",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest probability of improvement over the other algorithms?",
    "answer": "DreamerV2",
    "rationale": "The figure shows the probability of improvement of each algorithm compared to all other algorithms. The DreamerV2 algorithm has the highest probability of improvement over all other algorithms, as can be seen in the last subplot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.13264v4",
    "pdf_url": null
  },
  {
    "instance_id": "e15a53b597c54359adb057416bdeb701",
    "figure_id": "2303.18110v1-Figure1-1",
    "image_file": "2303.18110v1-Figure1-1.png",
    "caption": " WER of selected systems on conversations from the development set of EdAcc where both speakers has the same English variety.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which English variety has the highest WER for Whisper?",
    "answer": "Nigerian English",
    "rationale": "The figure shows the WER of different systems on different English varieties. The WER for Whisper is shown in red, and the highest WER for Whisper is for Nigerian English.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.18110v1",
    "pdf_url": null
  },
  {
    "instance_id": "384ed0dfe3bf41828d105c93e027a16c",
    "figure_id": "2211.12668v1-Figure4-1",
    "image_file": "2211.12668v1-Figure4-1.png",
    "caption": " Case study sampled from FinQA.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the maximum exposure to loss from vies in 2017?",
    "answer": "$412",
    "rationale": "The table shows the maximum exposure to loss from vies in 2017 is $412.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.12668v1",
    "pdf_url": null
  },
  {
    "instance_id": "bde28387953d4c24bd1084c1278ada22",
    "figure_id": "1811.10597v2-Figure12-1",
    "image_file": "1811.10597v2-Figure12-1.png",
    "caption": " At left, visualizations of the highest-activating image patches (from a sample of 1000) for three units. (a) the lowest-FID unit that is manually flagged as showing artifacts (b) the highest-FID unit that is not manually flagged (c) the highest-FID unit overall, which is also manually flagged. At right, the precision-recall curve for unit FID as a predictor of the manually flagged artifact units. A FID threshold selecting the top 20 FID units will identify 10 (of 20) of the manually flagged units.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which unit has the highest FID score and is also manually flagged as showing artifacts?",
    "answer": "Unit 231.",
    "rationale": "The caption states that unit 231 is the highest-FID unit overall and is also manually flagged as showing artifacts. This is also shown in the figure, where unit 231 has the highest FID score of 46.29 and is marked with a yellow outline.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.10597v2",
    "pdf_url": null
  },
  {
    "instance_id": "10ce20effdb94f489a8538d23972d554",
    "figure_id": "2203.11197v2-Figure6-1",
    "image_file": "2203.11197v2-Figure6-1.png",
    "caption": " “Best advice” is OffsetAdvice. Y-axis includes advice from both grounding and improvement across all four Point Maze test envs. RL results stretch off the plot, indicating we were unable to run RL for long enough to converge to the success rates of the other methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest total supervision cost?",
    "answer": "Best Advice",
    "rationale": "The bar for \"Best Advice\" is the shortest, indicating that it has the lowest total supervision cost.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.11197v2",
    "pdf_url": null
  },
  {
    "instance_id": "1976aeb04e4d431d87f740474195adf2",
    "figure_id": "2103.00958v1-Figure4-1",
    "image_file": "2103.00958v1-Figure4-1.png",
    "caption": " Results for solving nonconvex VFL models (Problem 14), where the number of epoches (points) denotes how many passes over the dataset the algorithm makes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four algorithms converges to the lowest sub-optimality value on the D1 dataset?",
    "answer": "VFB²-SAGA(ours)",
    "rationale": "The plot in Figure (a) shows that VFB²-SAGA(ours) has the lowest sub-optimality value of all four algorithms at the end of the run.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.00958v1",
    "pdf_url": null
  },
  {
    "instance_id": "1986f6c93e214dd3b81c47079ab6aa6f",
    "figure_id": "2006.03204v2-Figure6-1",
    "image_file": "2006.03204v2-Figure6-1.png",
    "caption": " Average saliency maps of selected MS-COCO categories cropped, aligned and averaged for all detections. Upper row features mean saliency maps as well as mean images for ‘snowboard’ averaged across all detections (left) and across three scales of detection areas ([0:30:70:100] percentiles) (right). Aspect ratios are preserved for all the average saliency maps.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the objects in the figure is most likely to be detected at multiple scales?",
    "answer": "The snowboard.",
    "rationale": "The figure shows the average saliency maps of selected MS-COCO categories cropped, aligned and averaged for all detections. The upper row features mean saliency maps as well as mean images for ‘snowboard’ averaged across all detections (left) and across three scales of detection areas ([0:30:70:100] percentiles) (right). This suggests that the snowboard is more likely to be detected at multiple scales than the other objects in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.03204v2",
    "pdf_url": null
  },
  {
    "instance_id": "186d7b02ad9d45128f8272302be74bd2",
    "figure_id": "2307.08873v3-Figure7-1",
    "image_file": "2307.08873v3-Figure7-1.png",
    "caption": " Expected return, return variance and Gini deviation of different methods in Maze when goal reward is 20. Curves are averaged over 10 seeds with shaded regions indicating standard errors.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the highest expected return in the Maze environment when the goal reward was 20?",
    "answer": "MVO",
    "rationale": "The figure shows the expected return of different methods in the Maze environment when the goal reward is 20. The MVO method has the highest expected return, as shown by the blue line in the leftmost plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.08873v3",
    "pdf_url": null
  },
  {
    "instance_id": "76118a8d485c4719a2f44e1878b92bbd",
    "figure_id": "1906.02329v1-Figure3-1",
    "image_file": "1906.02329v1-Figure3-1.png",
    "caption": " Comparison on sample complexity among the multi-task learning models for (a) document ranking and (b) query suggestion tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best for document ranking and query suggestion tasks?",
    "answer": "MNSRF",
    "rationale": "The figure shows that MNSRF achieves the highest MAP and F1 scores for both tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02329v1",
    "pdf_url": null
  },
  {
    "instance_id": "0684a475aeae43ed889363dfa46c2d77",
    "figure_id": "2110.03680v2-Figure8-1",
    "image_file": "2110.03680v2-Figure8-1.png",
    "caption": " Comparisons for burst denoising on gray-scale [39] and color datasets [55]. Our BIPNet produces more sharper and clean results than other competing approaches. Many more examples are provided in the supplementary material.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the sharpest and cleanest results for burst denoising?",
    "answer": "BIPNet.",
    "rationale": "The figure shows the results of three different burst denoising methods on four different images. BIPNet produces the sharpest and cleanest results, as evidenced by the lack of noise and the preservation of detail in the images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.03680v2",
    "pdf_url": null
  },
  {
    "instance_id": "fb30d38c57bf41f1ad7e70a0a041aadd",
    "figure_id": "2004.06100v2-Figure6-1",
    "image_file": "2004.06100v2-Figure6-1.png",
    "caption": " We finetune BERT Base on one category of Amazon reviews and then evaluate it on other categories. Models predict the review’s star rating with 5-way classification. We use five clothing categories: Clothes (C), Women’s Clothing (WC), Men’s Clothing (MC), Baby Clothing (BC), and Shoes (S); and two entertainment categories: Music (MS), Movies (MV). BERT is robust for closely related categories such as men’s, women’s, and baby clothing. However, BERT struggles when there is an extreme distribution shift such as Baby Clothing to Music (dark blue region). Note this shift is closer to a domain adaptation setting.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which category of Amazon reviews did BERT struggle with the most when it was finetuned on Baby Clothing reviews?",
    "answer": "Music",
    "rationale": "The figure shows the accuracy of BERT on different test datasets when it is finetuned on different training datasets. The darker the blue color, the higher the accuracy. The figure shows that when BERT is finetuned on Baby Clothing reviews, it achieves the lowest accuracy on Music reviews.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.06100v2",
    "pdf_url": null
  },
  {
    "instance_id": "7f4bee7ff5b94cab8853f332659cb946",
    "figure_id": "2210.11689v1-Figure16-1",
    "image_file": "2210.11689v1-Figure16-1.png",
    "caption": " The LM accuracy on the guo & zai paradigm.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language model performed the best on the guo & zai paradigm for the multi-lingual task?",
    "answer": "gpt3-davinci",
    "rationale": "The figure shows the accuracy of different language models on the guo & zai paradigm. The gpt3-davinci model has the highest accuracy of 98.18% for the multi-lingual task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.11689v1",
    "pdf_url": null
  },
  {
    "instance_id": "4e10f321c4e341b68ba18c94829d82be",
    "figure_id": "1912.12485v2-Figure9-1",
    "image_file": "1912.12485v2-Figure9-1.png",
    "caption": " Generation of our method on a mixture of 25 Gaussians dataset and swissroll datatset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which figure shows a mixture of 25 Gaussians dataset?",
    "answer": "Figure (a)",
    "rationale": "Figure (a) shows a set of points that are clustered together in 25 distinct groups. This is characteristic of a mixture of Gaussians dataset, where each cluster represents a different Gaussian distribution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.12485v2",
    "pdf_url": null
  },
  {
    "instance_id": "0e245dd031bc446c9fccd160e63c8018",
    "figure_id": "2305.05706v1-Figure8-1",
    "image_file": "2305.05706v1-Figure8-1.png",
    "caption": " Different Pre-train Methods. Evaluation success rate of different methods in Faucet and Bucket tasks. The shaded area indicates the standard deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pre-training method performs the best on the Faucet task?",
    "answer": "Segmentation on PMM.",
    "rationale": "The figure shows that the success rate for Segmentation on PMM is the highest for the Faucet task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.05706v1",
    "pdf_url": null
  },
  {
    "instance_id": "536237bed08f42d7ba71f12eecbd2e40",
    "figure_id": "2106.05963v3-Figure5-1",
    "image_file": "2106.05963v3-Figure5-1.png",
    "caption": " Feature visualizations for AlexNet-based encoder trained on some of the proposed datasets, for the 3rd and 5th (last) convolutional layer, using the procedure in [57].",
    "figure_type": "Other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset is most likely to produce features that are similar to those of the \"Places\" dataset?",
    "answer": "The \"Imagenet100\" dataset.",
    "rationale": "The features visualizations for the \"Places\" and \"Imagenet100\" datasets are both located in the bottom right corner of the figure. They are also the only two datasets that show features that resemble real-world objects, such as buildings and faces.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05963v3",
    "pdf_url": null
  },
  {
    "instance_id": "5344d4a809764dfbbbd83bd4a8d0945d",
    "figure_id": "1902.03045v1-Figure5-1",
    "image_file": "1902.03045v1-Figure5-1.png",
    "caption": " Parameter sensitivity and convergence analysis for SURE. (a) Sensitivity analysis of λ on Lost; (b) Sensitivity analysis of β on MSRCv2; (c) Convergence analysis on Lost; (d) Convergence analysis on MSRCv2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tradeoff parameter has a greater impact on the classification accuracy of the SURE model?",
    "answer": "λ",
    "rationale": "From figures (a) and (b), we can see that the classification accuracy changes more drastically with changes in λ than with changes in β.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.03045v1",
    "pdf_url": null
  },
  {
    "instance_id": "bede09a80b914c4283cd5831902dfe53",
    "figure_id": "2209.01404v1-Figure5-1",
    "image_file": "2209.01404v1-Figure5-1.png",
    "caption": " Performance of replacing binary convolutional blocks with binary MLP blocks. We start from full convolutional ReActNet-A.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which combination of activation and weight precision results in the highest Top 1 accuracy on ImageNet?",
    "answer": "W/A 32/1",
    "rationale": "The plot on the left shows the Top 1 accuracy for different combinations of activation and weight precision. The W/A 32/1 combination has the highest accuracy at 91.0%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.01404v1",
    "pdf_url": null
  },
  {
    "instance_id": "3ea13a9562ee446ba11db64c807515c2",
    "figure_id": "2001.06427v2-Figure7-1",
    "image_file": "2001.06427v2-Figure7-1.png",
    "caption": " Average user evaluation scores based on image quality and target part similarity. The part similarity is based on users’ scores on the edited part structure similarity between the edited image and the target image.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best in terms of image quality and target part similarity?",
    "answer": "Our Model",
    "rationale": "The figure shows that Our Model has the highest bars for both image quality and target part similarity across all three shapes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.06427v2",
    "pdf_url": null
  },
  {
    "instance_id": "288a2f903d9540bba828897d052c4978",
    "figure_id": "2307.13371v1-Figure6-1",
    "image_file": "2307.13371v1-Figure6-1.png",
    "caption": " Simulation results on each task are shown here. The error bar demonstrates the standard error. The x-axis denotes the number of iterations, and the y-axis denotes the filtering ratio. The ratio for the 10 initial randomly picked warm-up datasets are clipped. (G), (R), and (R+G) means the global model only, the ROI model only, and the ROI model combined with the global model correspondingly.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in terms of remaining ratio for both GB1 and Rosetta datasets?",
    "answer": "BALLET-ICI (R+G)",
    "rationale": "The lines for BALLET-ICI (R+G) are consistently lower than the other lines for both datasets, indicating that it has the lowest remaining ratio (i.e., the highest filtering ratio) across all iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.13371v1",
    "pdf_url": null
  },
  {
    "instance_id": "fc80c30e411d4800beeb567e1c221f74",
    "figure_id": "2310.05296v1-Figure3-1",
    "image_file": "2310.05296v1-Figure3-1.png",
    "caption": " Comparison of four GNN baselines, three graph structure learning baselines and STAGNN on four common node classification datasets. The missing result of Deezer is due to out-of-memory.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four GNN baselines performed best on the Cora dataset?",
    "answer": "GCN.",
    "rationale": "The bar for GCN on the Cora plot is the tallest among the four GNN baselines, indicating that it achieved the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.05296v1",
    "pdf_url": null
  },
  {
    "instance_id": "6fe44cec77d547c4937523d309e2e925",
    "figure_id": "2110.13953v1-Figure10-1",
    "image_file": "2110.13953v1-Figure10-1.png",
    "caption": " Histogram of accuracies of unique combinations from the miniImageNet dataset of 1-shot support examples evaluated by Algorithm 1 for 3 iterations,",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieved the highest test accuracy?",
    "answer": "MetaOptNet-SVM",
    "rationale": "The histogram for MetaOptNet-SVM shows the highest count of test accuracies in the higher range (above 60%).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.13953v1",
    "pdf_url": null
  },
  {
    "instance_id": "54f3a976d41e49c0b88a35c4d91e26b0",
    "figure_id": "1805.07819v4-Figure2-1",
    "image_file": "1805.07819v4-Figure2-1.png",
    "caption": " Multilayer perceptron based ensemble",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many layers are in the multilayer perceptron shown in the image?",
    "answer": "Two",
    "rationale": "The figure shows a schematic of a multilayer perceptron. The perceptron has two layers, an input layer and an output layer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.07819v4",
    "pdf_url": null
  },
  {
    "instance_id": "8d0b4c53f6ce4a6bbef4e996abf1e40d",
    "figure_id": "1811.02644v2-Figure6-1",
    "image_file": "1811.02644v2-Figure6-1.png",
    "caption": " Comparison of local prediction performance using DeepDPM in different regions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which functional zone does DeepDPM perform the worst?",
    "answer": "Industry",
    "rationale": "The bar chart in Figure (c) shows the RMSE of DeepDPM in different functional zones. The industry zone has the highest RMSE, indicating that DeepDPM performs the worst in this zone.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.02644v2",
    "pdf_url": null
  },
  {
    "instance_id": "175861b8f5144fa2a7d2bb0acadc4ec5",
    "figure_id": "2012.08976v1-Figure6-1",
    "image_file": "2012.08976v1-Figure6-1.png",
    "caption": " Qualitative comparisons with other methods including EDN (2019), FSV2V (2019), LWGAN (2019b), SGWGAN (2018), ClothFlow (2019), albated variants without FTC loss, LC-DConv. Yellow, blue and red circles point out blurry surfaces, over-stretched clothes patterns, and black chinks (caused by misplacements), respectively. Please zoom in for a better view.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method appears to produce the most realistic images?",
    "answer": "Ours",
    "rationale": "The figure shows the results of several different methods for generating images of people dancing. The \"Ours\" column shows the results of the method proposed in the paper, and these images appear to be the most realistic, with the least amount of blur, over-stretching, or black chinks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.08976v1",
    "pdf_url": null
  },
  {
    "instance_id": "f18d9c1a777f459cbb48cc0dae86ab99",
    "figure_id": "1906.04356v2-Figure4-1",
    "image_file": "1906.04356v2-Figure4-1.png",
    "caption": " Number of pulls versus error probability for various datasets and distance metrics. (a) Netflix 20k, cosine [8]. (b) RNA-Seq 100k, `1 [4] (c) MNIST, `2 [11]",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset and distance metric combination results in the highest error probability for a given number of pulls?",
    "answer": "The MNIST dataset with the `2 distance metric.",
    "rationale": "In all three subplots, the MNIST dataset with the `2 distance metric (shown in red) has the highest error probability for any given number of pulls. This means that this combination is the least effective at accurately identifying similar items.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04356v2",
    "pdf_url": null
  },
  {
    "instance_id": "8e714257857449adb696ebfc41926ba3",
    "figure_id": "2303.11296v1-Figure3-1",
    "image_file": "2303.11296v1-Figure3-1.png",
    "caption": " Anonymization results on CelebA-HQ [26] in comparison to DeepPrivacy (DP) [17] and CIAGAN [28].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the anonymization methods produces the most realistic-looking images?",
    "answer": "Ours (0.0)",
    "rationale": "The images produced by Ours (0.0) are the most similar to the original images, and they do not have any obvious artifacts or distortions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.11296v1",
    "pdf_url": null
  },
  {
    "instance_id": "34566b73cd0745618553d44a0fe4b0e1",
    "figure_id": "2304.01577v3-Figure4-1",
    "image_file": "2304.01577v3-Figure4-1.png",
    "caption": " Average # of Characters in Each Key Value Pair",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which key-value pair has the highest average number of characters in the form data?",
    "answer": "com_nm",
    "rationale": "The bar for com_nm in Figure (a) is the longest, indicating that it has the highest average number of characters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.01577v3",
    "pdf_url": null
  },
  {
    "instance_id": "d40251c55d8b4483b0be7f98fb76da5d",
    "figure_id": "1809.02156v2-Figure5-1",
    "image_file": "1809.02156v2-Figure5-1.png",
    "caption": " Examples of how TopDown (TD) sentences change when we enforce that objects cannot be hallucinated: SPICE (S), Meteor (M), CIDEr (C), see Section 3.4.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image is an example of a TopDown (TD) sentence where objects cannot be hallucinated?",
    "answer": "The image on the right.",
    "rationale": "The image on the right shows a black and white animal laying on the ground. The TD sentence for this image is \"A black and white animal laying on the ground.\" This sentence does not mention any objects that are not present in the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.02156v2",
    "pdf_url": null
  },
  {
    "instance_id": "400bb764545c4c6bb9cc34753b4c8fcf",
    "figure_id": "2010.09546v2-Figure2-1",
    "image_file": "2010.09546v2-Figure2-1.png",
    "caption": " Performance curves of AMPO and other model-based and model-free baselines on six continuous control benchmarking environments. We average the results over five random seeds, where solid curves depict the mean of five trials and shaded areas indicate the standard deviation. The dashed reference lines are the asymptotic performance of Soft Actor-Critic (SAC).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the six environments is the most difficult for all algorithms to learn?",
    "answer": "HalfCheetah",
    "rationale": "The HalfCheetah environment has the lowest average return for all algorithms, and the curves for all algorithms are still increasing at the end of the training period, indicating that they have not yet converged.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.09546v2",
    "pdf_url": null
  },
  {
    "instance_id": "1a49d12a96554abe9295e89e8e5a3cd4",
    "figure_id": "2212.03697v1-Figure2-1",
    "image_file": "2212.03697v1-Figure2-1.png",
    "caption": " Distribution of average AUCs, calculated on the held-out test set, relative to the AUC of Global on synthetic datasets. Left: setting 1, Right: setting 2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in Setting 2?",
    "answer": "Our Method",
    "rationale": "The boxplot for \"Our Method\" in Setting 2 is the highest, indicating that it has the highest median AUC.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.03697v1",
    "pdf_url": null
  },
  {
    "instance_id": "d7510ef4caee4cfc99b4b5cadf75c1fe",
    "figure_id": "2204.07705v3-Figure10-1",
    "image_file": "2204.07705v3-Figure10-1.png",
    "caption": " Distribution of SUP-NATINST tasks in terms of their (a) task types (b) languages (c) domains. y-axes are in log scale.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task type is the most common in the SUP-NATINST dataset?",
    "answer": "Question Answering",
    "rationale": "The figure shows the distribution of SUP-NATINST tasks in terms of their task types. The x-axis shows the different task types, and the y-axis shows the number of tasks for each task type. The bar for Question Answering is the highest, indicating that it is the most common task type in the dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.07705v3",
    "pdf_url": null
  },
  {
    "instance_id": "cbdf93183d364e5fac6ba11876588e20",
    "figure_id": "2004.14096v1-Figure3-1",
    "image_file": "2004.14096v1-Figure3-1.png",
    "caption": " Pearson correlation between UD/SUD probing accuracy and supervised UAS, per layer.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model and layer has the highest correlation between UD and SUD probing accuracy?",
    "answer": "Model 1 and layer 06.",
    "rationale": "The figure shows the Pearson correlation between UD and SUD probing accuracy for each model and layer. The correlation is highest for Model 1 and layer 06, as indicated by the darkest red color in the heatmap.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14096v1",
    "pdf_url": null
  },
  {
    "instance_id": "181bbfa9f7164a189e620ff28a206b9e",
    "figure_id": "1901.02039v1-Figure6-1",
    "image_file": "1901.02039v1-Figure6-1.png",
    "caption": " Visualization of semantic segmentation results on test set. Our results are generated on a level-5 spherical mesh and mapped to the equirectangular grid for visualization. Model underperforms in complex environments, and fails to predict ceiling lights due to incomplete RGB inputs.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods is the most accurate at predicting the location of the ceiling lights?",
    "answer": "Ground Truth",
    "rationale": "The figure shows the results of three different semantic segmentation methods on a test set of images. The \"Ground Truth\" column shows the actual locations of the objects in the images. The \"FCN8s,\" \"U-Net,\" and \"Ours\" columns show the results of the three methods. The \"Ours\" column is the most accurate at predicting the location of the ceiling lights. This can be seen by comparing the \"Ours\" column to the \"Ground Truth\" column. The other methods do not predict the location of the ceiling lights as accurately.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.02039v1",
    "pdf_url": null
  },
  {
    "instance_id": "248f671024474fdf8ce7983dd0db63a1",
    "figure_id": "1802.06472v4-Figure2-1",
    "image_file": "1802.06472v4-Figure2-1.png",
    "caption": " Doubly-Stochastic Matrices. Fig.2(a): Clipped Long-term Constraint Violation. Fig.2(b): Long-term Constraint Violation. Fig.2(c): Cumulative Regret of the Loss function",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest cumulative regret after 20,000 iterations?",
    "answer": "Our-Strong",
    "rationale": "The cumulative regret is shown in Figure 2(c). The Our-Strong line is the lowest of all the lines at the end of the graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1802.06472v4",
    "pdf_url": null
  },
  {
    "instance_id": "e0865cfa91534553bc522c12e78c45b1",
    "figure_id": "2008.03633v2-Figure2-1",
    "image_file": "2008.03633v2-Figure2-1.png",
    "caption": " Effects of exponential disparity discretization.",
    "figure_type": "plot and photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which discretization method produces the most accurate depth estimates at greater depths?",
    "answer": "The depth discretization method.",
    "rationale": "The plot on the left shows that the depth discretization method (blue line) produces depth estimates that are closer to the ground truth (gray line) at greater depths than the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.03633v2",
    "pdf_url": null
  },
  {
    "instance_id": "e02cac60636e4081be6221bdd59aba50",
    "figure_id": "2110.02095v1-Figure37-1",
    "image_file": "2110.02095v1-Figure37-1.png",
    "caption": " The effect of increasing head learning rate in performance of upstream (JFT) versus performance of downstream (ImageNet1k and Caltech101).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows a higher decrease in accuracy when increasing the head learning rate?",
    "answer": "ImageNet1k",
    "rationale": "The slope of the line for ImageNet1k is steeper than the slope of the line for Caltech101, indicating that the accuracy decreases more quickly for ImageNet1k as the head learning rate increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02095v1",
    "pdf_url": null
  },
  {
    "instance_id": "dad4ccb5036f4398ba6677c436061041",
    "figure_id": "2210.03466v2-Figure25-1",
    "image_file": "2210.03466v2-Figure25-1.png",
    "caption": " Left: Test errors for different models and datasets. Right: For each dataset, we plot ground truth and predictions for NODEP, ODE2VAE and our model (top to bottom). Each sub-plot shows the ground truth as the first row, and the prediction as the second row. We plot test prediction with the median test error (for each model and dataset we select the value of N which gives the best predictions).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Pendulum dataset?",
    "answer": "Our model.",
    "rationale": "The plot on the left shows the test error for each model on the Pendulum dataset. Our model has the lowest test error, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.03466v2",
    "pdf_url": null
  },
  {
    "instance_id": "d6410c59f3454403b782bc9e7c318c6b",
    "figure_id": "2203.10358v3-Figure3-1",
    "image_file": "2203.10358v3-Figure3-1.png",
    "caption": " Facial Landmark Semantic Group. Image source: [55]",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which semantic group of facial landmarks is represented by the green dots?",
    "answer": "The green dots represent the eyebrows.",
    "rationale": "The figure shows a face with different colored dots representing different semantic groups of facial landmarks. The green dots are located on the eyebrows.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.10358v3",
    "pdf_url": null
  },
  {
    "instance_id": "45be523e8cf0416a900fd7b9e53f5ea0",
    "figure_id": "2211.15144v2-Figure2-1",
    "image_file": "2211.15144v2-Figure2-1.png",
    "caption": " Offline multi-task performance on 40 games with sub-optimal data. Left. Scaled QL significantly outperforms the previous state-of-the-art method, DT, attaining about a 2.5x performance improvement in normalized IQM score. To contextualize the absolute numbers, we include online multi-task Impala DQN (Espeholt et al., 2018) trained on 5x as much data. Right. Performance profiles (Agarwal et al., 2021) showing the distribution of normalized scores across all 40 training games (higher is better). Scaled QL stochastically dominates other offline RL algorithms and achieves superhuman performance in 40% of the games. “Behavior policy” corresponds to the score of the dataset trajectories. Online MT DQN (5X), taken directly from Lee et al. (2022), corresponds to running multi-task online RL for 5x more data with IMPALA (details in Appendix B.5).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best on the sub-optimal data?",
    "answer": "Scaled QL.",
    "rationale": "The left plot shows that Scaled QL has the highest normalized IQM score, which is a measure of performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.15144v2",
    "pdf_url": null
  },
  {
    "instance_id": "7425c0155e4f47b38bfca56aac3009e7",
    "figure_id": "2012.13255v1-Figure5-1",
    "image_file": "2012.13255v1-Figure5-1.png",
    "caption": " We plot the intrinsic dimension and the respective relative generalization gap across a set of varied tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest relative generalization gap for all values of d90?",
    "answer": "SST-2",
    "rationale": "The line for SST-2 is consistently above the lines for all other datasets, indicating that it has the highest relative generalization gap for all values of d90.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.13255v1",
    "pdf_url": null
  },
  {
    "instance_id": "8a13790e1b0a4a638e4c21232397a278",
    "figure_id": "1910.13092v1-Figure3-1",
    "image_file": "1910.13092v1-Figure3-1.png",
    "caption": " Best found values of various synthetic benchmark test functions using different algorithms. Plotting mean and standard error over 30 repetitions. (Best seen in color)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best on the Egg-holder test function?",
    "answer": "EI-Vanilla",
    "rationale": "The plot shows that the EI-Vanilla algorithm achieved the highest best found values on the Egg-holder test function.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.13092v1",
    "pdf_url": null
  },
  {
    "instance_id": "f51ddad783f04ba59fdab3cbd28b2f46",
    "figure_id": "2107.11027v1-Figure4-1",
    "image_file": "2107.11027v1-Figure4-1.png",
    "caption": " Qualitative comparison of WaveFill with the state-of-the-art: WaveFill generates more realistic inpainting with much less artifacts (over the dataset CelebA-HQ[17] with central square masks). ∗ means that the model is trained with official implementation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most realistic inpainting results?",
    "answer": "WaveFill.",
    "rationale": "The figure shows the results of inpainting using different methods, and WaveFill produced the most realistic results with the least artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.11027v1",
    "pdf_url": null
  },
  {
    "instance_id": "32a511aa5b404142a71a625e4c7884e4",
    "figure_id": "2008.09105v1-Figure3-1",
    "image_file": "2008.09105v1-Figure3-1.png",
    "caption": " Visualization on TGIF-QA dataset. The boxes in transparent green are K detected objects. The boxes in red are the query object. The boxes in blue are the objects with high values regarding to query object in the adjacent matrix.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many objects are detected in the image?",
    "answer": "5",
    "rationale": "The caption states that the boxes in transparent green are the K detected objects. Counting the boxes in the image reveals 5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.09105v1",
    "pdf_url": null
  },
  {
    "instance_id": "41b56f89dd5642f9883f843a78546e37",
    "figure_id": "2002.06817v1-Figure4-1",
    "image_file": "2002.06817v1-Figure4-1.png",
    "caption": " Visualization of the embeddings (projected into 2D by t-SNE) generated by the models trained on the Origin training set for the testing samples (5s segment; under the album split). Upper: the result of CRNN (i.e., the model shown in Figure 1 but without the melody branch); lower: the result of CRNNM (i.e., the model shown in Figure 1).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is better at separating the different classes of music?",
    "answer": "CRNNM.",
    "rationale": "The t-SNE plot of CRNNM shows that the different classes of music are more clearly separated than in the t-SNE plot of CRNN. This suggests that CRNNM is better at learning features that distinguish between different classes of music.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.06817v1",
    "pdf_url": null
  },
  {
    "instance_id": "08a84107be1c48af90533f7201b60ac4",
    "figure_id": "2303.00957v1-Figure4-1",
    "image_file": "2303.00957v1-Figure4-1.png",
    "caption": " Averaged human evaluation results on 4 AntMaze tasks. Numbers denote the statistics of the evaluators’ responses over 40 trials. PT received higher ratings compared to both MR and NMR.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which baseline method did the evaluators prefer more often, MR or NMR?",
    "answer": "MR",
    "rationale": "The figure shows that 23.9% of evaluators preferred MR over PT, while only 16.3% preferred NMR over PT. This indicates that, on average, evaluators preferred MR over NMR.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.00957v1",
    "pdf_url": null
  },
  {
    "instance_id": "3cb747d2d209482fa7cf027f001d685e",
    "figure_id": "1911.09514v2-Figure2-1",
    "image_file": "1911.09514v2-Figure2-1.png",
    "caption": " Evaluating catastrophic forgetting by measuring performance retention. Classification accuracy of the initial task is monitored along with the progression of tasks. Results are displayed for five datasets. CLAW is the least forgetful algorithm since performance levels achieved on the initial task do not degrade as much as in the other methods after facing new tasks. The legend and λ values for EWC are the same as in Figure 1. Best viewed in colour.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best on the Split MNIST dataset?",
    "answer": "CLAW",
    "rationale": "The figure shows the performance of different algorithms on different datasets. On the Split MNIST dataset, CLAW has the highest performance, as indicated by the blue line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.09514v2",
    "pdf_url": null
  },
  {
    "instance_id": "516f673ffa4d48218bcdb4dfaababc2c",
    "figure_id": "2305.11383v2-Figure4-1",
    "image_file": "2305.11383v2-Figure4-1.png",
    "caption": " Results for the Random Guessing baseline which randomly guesses an answer from the output space (labels). The left figure shows the format correctness, which calculates the accuracy of model predictions lied in the label space for classification (CLS) tasks. The right figure shows the average exact-match score of CLS tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on classification tasks according to the format correctness accuracy?",
    "answer": "TK-Instruct(T5 w/ IT)",
    "rationale": "The left plot shows the format correctness accuracy for different models on classification tasks. The green line, which represents TK-Instruct(T5 w/ IT), is the highest among all the lines, indicating that it has the highest format correctness accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.11383v2",
    "pdf_url": null
  },
  {
    "instance_id": "9d989d2a28924585a8321af17078ee78",
    "figure_id": "1811.01146v3-Figure5-1",
    "image_file": "1811.01146v3-Figure5-1.png",
    "caption": " Accuracies per task for EMNIST (A) and SVHN (B). Generated images for SVHN and EMNIST (C). CloGAN preserves performance for early tasks throughout training. MeRGAN has degenerated performance and image generation for early tasks. In EMNIST, degradation is a darkening in the first tasks (a,...,f) in contrast to the last task (v,w,x).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method shows the best performance on early tasks in the SVHN dataset?",
    "answer": "CloGAN (0.5%)",
    "rationale": "The figure shows the accuracy of each method on each task for the SVHN dataset. CloGAN (0.5%) has the highest accuracy on all tasks, and its performance does not degrade over time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.01146v3",
    "pdf_url": null
  },
  {
    "instance_id": "5f1547d971464bd097b0faa5f318816f",
    "figure_id": "1811.06521v1-Figure5-1",
    "image_file": "1811.06521v1-Figure5-1.png",
    "caption": " Performance at each game as a function of human (or synthetic) effort, adding labeling time (at 750 labels/hour) and demonstration time (at 15 fps).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which game shows the most improvement in performance with the addition of demos and preferences?",
    "answer": "Breakout",
    "rationale": "The figure shows the performance of the RL agent on different Atari games as a function of human effort. The x-axis shows the number of human hours, and the y-axis shows the mean episode return. The different lines represent different conditions, such as no demos, demos + preferences, etc. We can see that for Breakout, the performance increases significantly with the addition of demos and preferences, compared to the other conditions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.06521v1",
    "pdf_url": null
  },
  {
    "instance_id": "5ab232486bed4e8b8f3f80517c7d7b23",
    "figure_id": "2305.19753v2-Figure43-1",
    "image_file": "2305.19753v2-Figure43-1.png",
    "caption": " Substituting layer experiment. VGG-19 trained on the sequence of two tasks on splitCIFAR10. First task 7 class, second task 3 classes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task is more difficult for the bottom-up and top-down models?",
    "answer": "Task 2 is more difficult for both models.",
    "rationale": "The accuracy for both models is lower for Task 2 than for Task 1. This can be seen in the figure, where the lines for Task 2 are lower than the lines for Task 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19753v2",
    "pdf_url": null
  },
  {
    "instance_id": "75cb057f8b9945d3832cd12f461ecca5",
    "figure_id": "2007.03778v2-Figure9-1",
    "image_file": "2007.03778v2-Figure9-1.png",
    "caption": " Local Chamfer distance at expanding distances around each touch site.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which modality performs better in the occluded case, vision or vision + touch?",
    "answer": "Vision + touch.",
    "rationale": "In the occluded case, the orange line (\"Vision (Occluded) + Touch\") shows lower local Chamfer distances compared to the blue line (\"Vision (Occluded)\"). This indicates that the addition of touch information improves performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.03778v2",
    "pdf_url": null
  },
  {
    "instance_id": "ac276385e6704f39b8c36e0c1947eef2",
    "figure_id": "2004.12000v2-Figure3-1",
    "image_file": "2004.12000v2-Figure3-1.png",
    "caption": " Evaluation of reenactment systems in terms of their ability to represent the driver pose and to preserve reference identity (arrows point towards improvement). See text for details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which reenactment system has the lowest pose reconstruction error and the highest identity error?",
    "answer": "X2Face",
    "rationale": "The figure shows the pose reconstruction error on the y-axis and the identity error on the x-axis. The X2Face data point is the highest on the y-axis and the furthest to the right on the x-axis, indicating it has the lowest pose reconstruction error and the highest identity error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.12000v2",
    "pdf_url": null
  },
  {
    "instance_id": "991059bd75c14683bd4df9158a650128",
    "figure_id": "2306.11203v1-Figure3-1",
    "image_file": "2306.11203v1-Figure3-1.png",
    "caption": " AVOIDDS dataset overview.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following weather conditions is most likely to occur in the afternoon?",
    "answer": "Scattered clouds.",
    "rationale": "The image shows that the afternoon sky is typically filled with scattered clouds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.11203v1",
    "pdf_url": null
  },
  {
    "instance_id": "199f2ccd5f1c4eacb65af350fd6e49d5",
    "figure_id": "1904.06535v1-Figure5-1",
    "image_file": "1904.06535v1-Figure5-1.png",
    "caption": " The visualization of detection results. (a) (b) are sampled from ICDAR2017-RCTW, (c) (d) are from SCUT-CTW1500, (e) (f) are from Total-Text, (g) (h) are from ICDAR2015, and (i) (j) are from ICDAR2017-MLT. The yellow polygons are ground truth annotations. The localization quadrangles in blue and in green represent the detection results of DR and IRM respectively. The contours in red are the detection results of SEM.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better in detecting curved text?",
    "answer": "SEM",
    "rationale": "The figure shows that the contours in red, which represent the detection results of SEM, are closer to the ground truth annotations for curved text than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.06535v1",
    "pdf_url": null
  },
  {
    "instance_id": "9f50841ce33b4471a48089085989eb0b",
    "figure_id": "1906.08495v2-Figure1-1",
    "image_file": "1906.08495v2-Figure1-1.png",
    "caption": " Framework overview. Each possible triplet is associated with a binary indicator (circles), indicating whether it is true (3) or not (7). The observed (yellow circles) and hidden (grey circles) indicators are connected by a set of logic rules, with each rule having a weight (red number). For the center triplet, the KGE model predicts its indicator through embeddings, while the logic rules consider the Markov blanket of the triplet (all connected triplets). If any indicator in the Markov blanket is hidden, we simply fill it with the prediction from the KGE model. In the E-step, we use the logic rules to predict the center indicator, and treat it as extra training data for the KGE model. In the M-step, we annotate all hidden indicators with the KGE model, and then update the weights of rules.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Why is the indicator for \"(Alan Turing, Born in, London)\" green?",
    "answer": "Because it is a true statement.",
    "rationale": "The green checkmark next to the indicator for \"(Alan Turing, Born in, London)\" indicates that this statement is true.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.08495v2",
    "pdf_url": null
  },
  {
    "instance_id": "d66d5a803f2340918702205bd365f924",
    "figure_id": "2212.00261v1-Figure16-1",
    "image_file": "2212.00261v1-Figure16-1.png",
    "caption": " Agreement score approximation for meta-optimization. Left: The agreement score at every iteration of the inner-loop optimization of two networks for random-labelled, human-labelled and discovered tasks. Standard deviation for each group is over 5 tasks and 3 seeds (accounts for initialization and data-order). We can see that the differentiation between high-AS (human-labelled and discovered) tasks and randomlabelled occurs early in training and the AS after only 50 steps can be reliably used for task discovery. Right: The dependence between the proxy-AS (50 steps/soft labels/(negative)cross-entropy agreement/SGD) and the original Agreement Score (100 epochs/hard labels/0-1 agreement/Adam). We see that the two correlate well with Spearman’s rank correlation of 0.92 making it a good optimization proxy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of task is most likely to be discovered by the meta-optimization process?",
    "answer": "Human-labelled tasks.",
    "rationale": "The left plot shows that the agreement score for human-labelled tasks is consistently higher than that for random-labelled tasks. This suggests that the meta-optimization process is more likely to discover tasks that are similar to human-labelled tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.00261v1",
    "pdf_url": null
  },
  {
    "instance_id": "d9dfb33d525b4f4e925ddef95b10e122",
    "figure_id": "1903.00709v5-Figure8-1",
    "image_file": "1903.00709v5-Figure8-1.png",
    "caption": " A few results from the ShapeNet fine-grained segmentation challenge. Besides segmentation, PartNet can also recover the relations (adjacency and symmetry) between the segmented parts. We visualize the recovered symmetry relations with colored arrows (Reflective: Red; Translational: Blue; Rotational: Green).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of symmetry is shown between the two wings of the airplane?",
    "answer": "Reflective symmetry.",
    "rationale": "The red arrows in the figure indicate reflective symmetry, which is a type of symmetry where one half of the object is a mirror image of the other half.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.00709v5",
    "pdf_url": null
  },
  {
    "instance_id": "773f1af4460640388ea2c63605eac97c",
    "figure_id": "1808.08493v1-Figure2-1",
    "image_file": "1808.08493v1-Figure2-1.png",
    "caption": " Pairwise cosine distance for all language pairs in the IWSLT-15 and IWSLT-17 datasets. Darker colors represent more similar languages.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which language is most similar to English in the IWSLT-15 dataset?",
    "answer": "Czech (cs)",
    "rationale": "The figure shows a heatmap of pairwise cosine distances between languages in the IWSLT-15 and IWSLT-17 datasets. The darker the color, the more similar the languages are. In the IWSLT-15 dataset, the cell corresponding to English (en) and Czech (cs) has the darkest color, indicating that they are the most similar languages.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.08493v1",
    "pdf_url": null
  },
  {
    "instance_id": "e4a6e33f13074fc8a693a3f515d525b8",
    "figure_id": "2305.19000v1-Figure4-1",
    "image_file": "2305.19000v1-Figure4-1.png",
    "caption": " Empirical evaluation of a stability criterion. We plot a condition number (Eq. (2)), gradient magnitude similarity [54], and minimal cosine distance during training on the CITYSCAPES three-task benchmark. This benchmark suffers from high dominance since instance segmentation loss is of much larger scale than the others. The most intuitive way to define the dominance is the maximum ratio of task gradients magnitudes. The condition number coincides with this definition in a nearly orthogonal case, as in this benchmark Fig. 4c. However, gradient magnitude similarity measure Fig. 4b proposed in [54] does not reveal much correlation with a condition number(and, accordingly, with a maximal gradients magnitude ratio) Fig. 4a, so we assume it does not represent dominance issues comprehensively. From the empirical point of view Table 1, the value of a target metric is more correlated with the condition number, than with the gradient magnitude similarity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric is most correlated with the condition number?",
    "answer": "The target metric.",
    "rationale": "The caption states that \"the value of a target metric is more correlated with the condition number, than with the gradient magnitude similarity.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19000v1",
    "pdf_url": null
  },
  {
    "instance_id": "e315f5340db0401884fd1e8d16f47799",
    "figure_id": "1912.10205v1-Figure6-1",
    "image_file": "1912.10205v1-Figure6-1.png",
    "caption": " CER improvements of DAN on different text lengths and corresponding misalignments. ‘Bah’ and ‘Luong’ denote Bahdanau’s attention and Luong’s attention, respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the best CER improvement for text lengths of 40-50 characters?",
    "answer": "CER improvement.",
    "rationale": "The figure shows that the CER improvement line is higher than the other two lines for text lengths of 40-50 characters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.10205v1",
    "pdf_url": null
  },
  {
    "instance_id": "abbc0d7eaec749899305e32c1cf68578",
    "figure_id": "1901.00686v1-Figure4-1",
    "image_file": "1901.00686v1-Figure4-1.png",
    "caption": " Examples of images generated from the given caption from the MS-COCO data set. A) shows the original images and the respective image captions, B) shows images generated by our StackGAN+OP (with the corresponding bounding boxes for visualization), and C) shows images generated by the original StackGAN (Zhang et al., 2017)3",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following images is most likely to have been generated by StackGAN+OP?",
    "answer": "B",
    "rationale": "The figure shows that StackGAN+OP is able to generate images that are more realistic and detailed than the original StackGAN. The images in B are more realistic and detailed than the images in C, which suggests that they were generated by StackGAN+OP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.00686v1",
    "pdf_url": null
  },
  {
    "instance_id": "920cff22c03e484ab9e773f527bc3511",
    "figure_id": "2006.02684v2-Figure7-1",
    "image_file": "2006.02684v2-Figure7-1.png",
    "caption": " Variance comparison between theoretical bound and empirical variance. (a) Different link sampling probabilities p. (b) Different filter orders K. (c) Different number of filters F (d) Different number of layers L.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following factors has the greatest effect on the variance of the output of the neural network?",
    "answer": "The number of layers L.",
    "rationale": "The figure shows that the variance of the output of the neural network increases with the number of layers L. This is because each layer adds additional noise to the signal, and the noise accumulates as the signal passes through more layers. The other factors, such as the link sampling probability p, the filter order K, and the number of filters F, have a smaller effect on the variance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.02684v2",
    "pdf_url": null
  },
  {
    "instance_id": "b10fea90ac4443478f05ee7652555195",
    "figure_id": "2103.10246v2-Figure1-1",
    "image_file": "2103.10246v2-Figure1-1.png",
    "caption": " Representative price distribution for three platforms and one advertiser",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which platform has the highest proportion of impressions at prices between 0.005 and 0.01?",
    "answer": "Platform 3.",
    "rationale": "The histogram for platform 3 shows a peak in the bin between 0.005 and 0.01, which indicates that a large proportion of impressions occurred at these prices.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.10246v2",
    "pdf_url": null
  },
  {
    "instance_id": "b8486fa020da4735a3a5676bb7f03da5",
    "figure_id": "2211.15918v1-Figure7-1",
    "image_file": "2211.15918v1-Figure7-1.png",
    "caption": " ROC curve of MAS+SD, MSD, MU high, MU mid, MU low and MFE on ResNet50 trained on Market1501.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of True Positive Rate and False Positive Rate?",
    "answer": "The AS+SD model performs the best.",
    "rationale": "The ROC curve shows the trade-off between True Positive Rate (TPR) and False Positive Rate (FPR) for different models. The closer the curve is to the top-left corner, the better the model performs. In this case, the AS+SD model has the highest TPR for any given FPR, indicating that it is the best performing model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.15918v1",
    "pdf_url": null
  },
  {
    "instance_id": "f9821cef8ebb47fb879d9a110134c0ed",
    "figure_id": "2206.04436v2-Figure1-1",
    "image_file": "2206.04436v2-Figure1-1.png",
    "caption": " Cumulative reward curves for VPG, TRPO, PPO, PG-CMDP and our CPPO. The x-axes indicate the number of steps interacting with the environment, and the y-axes indicate the performance of the agent, including average rewards with standard deviations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieved the highest performance on the HalfCheetah task?",
    "answer": "VPG",
    "rationale": "The figure shows that the VPG curve is the highest among all the algorithms on the HalfCheetah task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.04436v2",
    "pdf_url": null
  },
  {
    "instance_id": "43ae914fe82447989457396ee8402011",
    "figure_id": "1911.03070v4-Figure5-1",
    "image_file": "1911.03070v4-Figure5-1.png",
    "caption": " T-SNE visualization of embeddings before (left) and after (right) CLIME updates. From one Sinhalese user study, we inspect two keywords, “ill” and “plague”, and their five closest neighbors in English (blue) and Sinhalese (green). The Sinhalese words are labeled with English translations. Shape denotes the type of feedback: “+” for positive neighbors and “x” for negative neighbors.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the words \"ill\" and \"sick\"?",
    "answer": "The words \"ill\" and \"sick\" are synonyms.",
    "rationale": "The figure shows that the words \"ill\" and \"sick\" are both located in the same cluster of words, which indicates that they are semantically similar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.03070v4",
    "pdf_url": null
  },
  {
    "instance_id": "0915bf05b0664421aba2b0c5c4087d34",
    "figure_id": "1811.10800v4-Figure9-1",
    "image_file": "1811.10800v4-Figure9-1.png",
    "caption": " Comparison of MC-Dropout SSD to SSD-300. SSD-300 is shown to be spatially over-confident leading to low scores despite tighter boxes.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is more spatially over-confident?",
    "answer": "SSD-300",
    "rationale": "The figure shows that the SSD-300 model has tighter boxes around the objects than the MC-Dropout SSD model. This indicates that the SSD-300 model is more confident in its predictions of the locations of the objects. However, the caption states that the SSD-300 model is spatially over-confident, which means that it is too confident in its predictions of the locations of the objects. This can be seen in the figure by the fact that the SSD-300 model has lower scores than the MC-Dropout SSD model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.10800v4",
    "pdf_url": null
  },
  {
    "instance_id": "88038b3a7fbf4f2a810ebce4ff631816",
    "figure_id": "2011.09766v1-Figure6-1",
    "image_file": "2011.09766v1-Figure6-1.png",
    "caption": " Speed (FPS) versus accuracy (mIoU) on iSAID val set. The radius of circles represents the number of parameters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest accuracy and speed?",
    "answer": "Our method has the highest accuracy and speed.",
    "rationale": "The figure shows a scatter plot of accuracy (mIoU) vs. speed (FPS) for different methods. Our method is represented by the red point in the upper right corner of the plot, which indicates that it has the highest accuracy and speed.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.09766v1",
    "pdf_url": null
  },
  {
    "instance_id": "adb15fc9f3bf4a0ea6cbf0e87a85ac85",
    "figure_id": "2009.13977v1-Figure3-1",
    "image_file": "2009.13977v1-Figure3-1.png",
    "caption": " Comparisons of the running times for FastH against previous algorithms. The sequential algorithm from [17] crashed when d > 448. (a) Running times of different algorithms for d × d matrices. (b) Running times of FastH relative to previous algorithms, i.e., the mean time of a previous algorithm divided by the mean time of FastH.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is the fastest for large matrices?",
    "answer": "FastH",
    "rationale": "The figure shows the running time of different algorithms for different sizes of matrices. The running time of FastH is the lowest for all sizes of matrices, and the relative improvement of FastH over other algorithms increases as the size of the matrix increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.13977v1",
    "pdf_url": null
  },
  {
    "instance_id": "20f855fc98b24159ad82481b39e276cb",
    "figure_id": "1908.07274v1-Figure9-1",
    "image_file": "1908.07274v1-Figure9-1.png",
    "caption": " Refinement quality versus patch of numbers for different approaches. (a) S-measure vs. patch of numbers. (b) MAE versus patch of numbers. Results are measured on the outputs of LRN.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two metrics, S-measure or MAE, shows a stronger dependence on the number of patches?",
    "answer": "MAE",
    "rationale": "The figure shows that the MAE values for both methods decrease significantly as the number of patches increases, while the S-measure values remain relatively stable. This suggests that MAE is more sensitive to the number of patches used for refinement.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.07274v1",
    "pdf_url": null
  },
  {
    "instance_id": "f3dcd003486049d5a24da5193d905dde",
    "figure_id": "2302.09483v1-Figure4-1",
    "image_file": "2302.09483v1-Figure4-1.png",
    "caption": " On CIFAR10, pretraining on the public data significantly improves accuracy compared to posttraining on the public data for both ID public data (left) and OOD public data (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method of training is more effective for ID public data: pre-training or post-training?",
    "answer": "Pre-training is more effective for ID public data.",
    "rationale": "The figure shows that the accuracy of pre-training is higher than that of post-training for all three values of σ2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.09483v1",
    "pdf_url": null
  },
  {
    "instance_id": "d3b45170eb134e1a92c2ce432609fab3",
    "figure_id": "2010.12527v4-Figure6-1",
    "image_file": "2010.12527v4-Figure6-1.png",
    "caption": " An example of IRRR answering a question from HotpotQA by generating natural language queries to retrieve paragraphs, then rerank them to compose reasoning paths and read them to predict the answer. Here, IRRR recovers from an initial retrieval/reranking mistake by retrieving more paragraphs, before arriving at the gold supporting facts and the correct answer.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the name of the book that the character Gollum is from?",
    "answer": "The Lord of the Rings",
    "rationale": "The figure shows how IRRR answers a question by generating natural language queries to retrieve paragraphs, then rerank them to compose reasoning paths and read them to predict the answer. In this example, IRRR is asked \"The Ingerophrynus gollum is named after a character in a book that sold how many copies?\". IRRR first retrieves a paragraph about the genus Ingerophrynus, which is not relevant to the question. Then, it retrieves a paragraph about the species Ingerophrynus gollum, which states that it is named after the character Gollum from The Lord of the Rings. Finally, it retrieves a paragraph about The Lord of the Rings, which states that it is one of the best-selling novels ever written, with 150 million copies sold.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12527v4",
    "pdf_url": null
  },
  {
    "instance_id": "7996b49c3ea64909a53897541e33f264",
    "figure_id": "2304.00359v1-Figure16-1",
    "image_file": "2304.00359v1-Figure16-1.png",
    "caption": " Qualitative comparison with SOTA methods on real-world images.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is able to best reconstruct the 3D pose of the person in the image?",
    "answer": "Ours",
    "rationale": "The figure shows the results of different methods for reconstructing the 3D pose of a person from an image. The method labeled \"Ours\" produces the most accurate and detailed reconstruction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.00359v1",
    "pdf_url": null
  },
  {
    "instance_id": "49999463f9db4e69a316b05363e694bf",
    "figure_id": "1910.09284v1-Figure4-1",
    "image_file": "1910.09284v1-Figure4-1.png",
    "caption": " Effect of online training data, N = 10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of accuracy when trained with a small amount of data?",
    "answer": "CovNet Init",
    "rationale": "The figure shows the accuracy of different models as a function of the amount of training data. The CovNet Init model has the highest accuracy for all amounts of training data, including small amounts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.09284v1",
    "pdf_url": null
  },
  {
    "instance_id": "66a66cb1955249528c75ed957699e5e3",
    "figure_id": "2102.01813v1-Figure1-1",
    "image_file": "2102.01813v1-Figure1-1.png",
    "caption": " The architecture of the CNN with attention used as a classifier in this work.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the input to the network?",
    "answer": "The input to the network is a log Mel spectrogram.",
    "rationale": "The figure shows that the log Mel spectrogram is at the bottom of the network, which means that it is the input.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.01813v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee2cc2b6f6db444c99dd8a03d1d3d082",
    "figure_id": "1911.09514v2-Figure1-1",
    "image_file": "1911.09514v2-Figure1-1.png",
    "caption": " Average test classification accuracy vs. the number of observed tasks in 6 experiments. CLAW achieves significantly higher classification results than the competing continual learning frameworks. Statistical significance values are presented in Section E in the Appendix. The value of λ for EWC is 10,000 in (c), and 100 in the other experiments. Best viewed in colour.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which continual learning framework achieves the highest classification accuracy on the Split notMNIST dataset?",
    "answer": "CLAW.",
    "rationale": "The plot shows that CLAW (blue line) has the highest classification accuracy on the Split notMNIST dataset (sub-figure c).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.09514v2",
    "pdf_url": null
  },
  {
    "instance_id": "b34a3e27eec14aa1b508bb8c18b24ed7",
    "figure_id": "2310.16832v2-Figure4-1",
    "image_file": "2310.16832v2-Figure4-1.png",
    "caption": " Qualitative Results on frontal and non-frontal scenes. Zoomed-in comparison between NeRF [23], MobileR2L [4] and our LightSpeed approach.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is able to capture the scene most accurately?",
    "answer": "LightSpeed",
    "rationale": "The LightSpeed method is able to capture the scene most accurately because it is able to produce images that are very similar to the ground truth images. This can be seen in the zoomed-in comparison between the NeRF, MobileR2L, and LightSpeed methods. The LightSpeed method is able to capture the details of the scene more accurately than the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.16832v2",
    "pdf_url": null
  },
  {
    "instance_id": "fc6c77e59ce440d390a720750d3831f2",
    "figure_id": "2210.12048v3-Figure9-1",
    "image_file": "2210.12048v3-Figure9-1.png",
    "caption": " Node clusterings based on curvature features differ radically from clusterings based on other local features. We show the normalized mutual information (upper triangle) and the adjusted rand score (lower triangle) of node clusterings based on different method/feature combinations, computed on the citation hypergraph of PRB from the aps-cv collection, with curvatures computed using α = 0.1, µWE, and AGGA.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of feature produces node clusterings that are most similar to the clusterings produced by other local features?",
    "answer": "Curvature features.",
    "rationale": "The figure shows the normalized mutual information (NMI) and the adjusted rand score (ARS) of node clusterings based on different method/feature combinations. The NMI and ARS are measures of similarity between two clusterings. A higher NMI or ARS indicates that the two clusterings are more similar. The figure shows that the NMI and ARS values for clusterings based on curvature features are generally higher than the NMI and ARS values for clusterings based on other local features. This indicates that the clusterings produced by curvature features are more similar to the clusterings produced by other local features than the clusterings produced by other local features are to each other.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12048v3",
    "pdf_url": null
  },
  {
    "instance_id": "10c7ee21380648a0b121b6f43a6eefa6",
    "figure_id": "2304.03483v2-Figure7-1",
    "image_file": "2304.03483v2-Figure7-1.png",
    "caption": " Reconstruction PSNR vs. t for the (a) time-varying walnut, and (b) compressed material for P=256. The red shading for TD-DIP indicates, for each t, the interval between the best and worst PSNR with the early stopping oracle explained in Section VI-D1 in three runs with different random initial conditions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which reconstruction method performed the best for the walnut data?",
    "answer": "RED-PSM",
    "rationale": "The PSNR for RED-PSM is consistently higher than the other methods for the walnut data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.03483v2",
    "pdf_url": null
  },
  {
    "instance_id": "712c0355ac374a738f31c9d7223fdf2a",
    "figure_id": "1902.10068v2-Figure2-1",
    "image_file": "1902.10068v2-Figure2-1.png",
    "caption": " Results per class for the models trained on all gaze datasets combined.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models performed best for the PERSON class?",
    "answer": "The baseline model.",
    "rationale": "The baseline model achieved the highest accuracy for the PERSON class, as shown by the tallest bar in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.10068v2",
    "pdf_url": null
  },
  {
    "instance_id": "c818b89b40b74027962e717e13109179",
    "figure_id": "2003.12327v1-Figure6-1",
    "image_file": "2003.12327v1-Figure6-1.png",
    "caption": " Analysis of the diversity of the stochastic sequences in estimating Ĝ. We report the histogram of δ(ĜM) and δ̃(ĜM).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, 2CA_g or 2CA_s, produces a wider range of values for δ(ĜM)?",
    "answer": "2CA_g",
    "rationale": "The histogram for 2CA_g in Figure (a) shows a wider range of values for δ(ĜM) than the histogram for 2CA_s.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.12327v1",
    "pdf_url": null
  },
  {
    "instance_id": "62f8febe5d6d4bf9a6be9fdf902d341c",
    "figure_id": "2208.04303v2-Figure9-1",
    "image_file": "2208.04303v2-Figure9-1.png",
    "caption": " BD-rate savings versus RGB PSNR for flow and residual bitstreams and in-total for a SSF baseline. Lower is better.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three datasets, UVG, HEVC-B, or MCL-JCV, shows the highest BD-rate savings for the Flow-AE bitstream?",
    "answer": "HEVC-B",
    "rationale": "The figure shows that the Flow-AE line for the HEVC-B dataset is consistently lower than the Flow-AE lines for the other two datasets. This indicates that the Flow-AE bitstream achieves a higher BD-rate saving for the HEVC-B dataset than for the other two datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.04303v2",
    "pdf_url": null
  },
  {
    "instance_id": "1249af3a1fdd4598b98836efda013ab7",
    "figure_id": "1908.10770v1-Figure2-1",
    "image_file": "1908.10770v1-Figure2-1.png",
    "caption": " SLU performance of different methods with varying number of seed samples randomly selected from the oracle DSTC3 training set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best with the largest number of seed samples?",
    "answer": "HD oracle.",
    "rationale": "The figure shows the F-score of different methods as a function of the number of seed samples. The HD oracle method has the highest F-score for all numbers of seed samples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.10770v1",
    "pdf_url": null
  },
  {
    "instance_id": "325d68b846064f58a6f32eb9123f3761",
    "figure_id": "2105.13913v7-Figure6-1",
    "image_file": "2105.13913v7-Figure6-1.png",
    "caption": " Logistic Regression: Convergence of ℎ(x𝑡 ) and 𝑔(x𝑡 ) vs. 𝑡 and wall-clock time for the a4a LIBSVM dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges the fastest in terms of wall-clock time?",
    "answer": "GSC-FW",
    "rationale": "The figure shows the convergence of different algorithms in terms of iterations and wall-clock time. We can see that GSC-FW converges the fastest in terms of wall-clock time, as its curve reaches the lowest point first.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.13913v7",
    "pdf_url": null
  },
  {
    "instance_id": "f2e86d9ad2ae4e5a8f43e4117bfd05b0",
    "figure_id": "1906.01770v3-Figure3-1",
    "image_file": "1906.01770v3-Figure3-1.png",
    "caption": " Lifelong learning experiments with a changing set of actions in the maze domain. The learning curves correspond to the running mean of the best performing setting for each of the algorithms. The shaded regions correspond to standard error obtained using 10 trials. Vertical dotted bars indicate when the set of actions was changed.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in the maze domain?",
    "answer": "LAICA(2)",
    "rationale": "The plot shows that LAICA(2) achieved the highest total expected return among all the algorithms tested.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.01770v3",
    "pdf_url": null
  },
  {
    "instance_id": "f441296cccdd4bee92e9970ce829c345",
    "figure_id": "2003.05334v2-Figure5-1",
    "image_file": "2003.05334v2-Figure5-1.png",
    "caption": " Comparison of RL algorithms and their online meta-learning enhancement. Box plots of the Max Average Return over 5 trials of all time steps.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which RL algorithm has the highest median performance across all environments?",
    "answer": "SAC-MC",
    "rationale": "The figure shows the distribution of the Max Average Return for each RL algorithm in each environment. The median is represented by the orange line in each box plot. SAC-MC has the highest median across all environments.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.05334v2",
    "pdf_url": null
  },
  {
    "instance_id": "66c2331d0db24a278d5ec3b7069c59cd",
    "figure_id": "2203.10445v1-Figure1-1",
    "image_file": "2203.10445v1-Figure1-1.png",
    "caption": " The estimated Q values of SAL, AL and DQN are evaluated. The depicted return is averaged over 10 test episodes every 5000 steps. The mean and 95% confidence interval are shown across 5 independent runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, SAL, AL, or DQN, appears to have the highest estimated Q value on the Asterix game?",
    "answer": "DQN",
    "rationale": "The plot for the Asterix game shows that the blue line, which represents DQN, is consistently above the green and red lines, which represent AL and SAL, respectively. This indicates that DQN has the highest estimated Q value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.10445v1",
    "pdf_url": null
  },
  {
    "instance_id": "81a31addb1e14429877900408ee3a1f0",
    "figure_id": "2202.09931v2-Figure8-1",
    "image_file": "2202.09931v2-Figure8-1.png",
    "caption": " Example plots of so max pro les obtained from ResNet-50 training on ImageNet, displaying a variety of interesting behaviors.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image has the highest ImageNet accuracy?",
    "answer": "Image 3018, labeled \"LADYBUG\"",
    "rationale": "The x-axis of the plot represents the ImageNet accuracy. The highest point on the x-axis for Image 3018 is around 0.65, which is higher than the highest points on the x-axis for the other two images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.09931v2",
    "pdf_url": null
  },
  {
    "instance_id": "250fb81e00db4253a31c7c8005ee836f",
    "figure_id": "2010.02983v2-Figure5-1",
    "image_file": "2010.02983v2-Figure5-1.png",
    "caption": " Comparison of plug and play methods for unsupervised style transfer on the Yelp sentiment transfer task’s test set. Up and right is better",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods compared in the figure performs the best on the Yelp sentiment transfer task's test set?",
    "answer": "Shen et al. (2019)",
    "rationale": "The figure shows that the method proposed by Shen et al. (2019) achieves the highest accuracy for all values of self-BLEU.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02983v2",
    "pdf_url": null
  },
  {
    "instance_id": "66e09ef638974a2fae17a0716fd7c4c4",
    "figure_id": "1809.09528v2-Figure3-1",
    "image_file": "1809.09528v2-Figure3-1.png",
    "caption": " The distribution of questions in clusters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common number of questions in a cluster?",
    "answer": "1.",
    "rationale": "The figure shows that the most common number of questions in a cluster is 1, with over 4,000 clusters having only one question.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.09528v2",
    "pdf_url": null
  },
  {
    "instance_id": "9a84e2b8bf8c44019e3c4b042af1c952",
    "figure_id": "2306.00658v3-Figure2-1",
    "image_file": "2306.00658v3-Figure2-1.png",
    "caption": " Visualization of selected testing shapes, among which the dragon, bucket, and nail meshes are highly anisotropic, and the heptoroid mesh is with high genus.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the objects in the image is the most anisotropic?",
    "answer": "The nail.",
    "rationale": "The caption states that the dragon, bucket, and nail meshes are highly anisotropic. Of these three objects, the nail is the most elongated and has the least uniform shape, making it the most anisotropic.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.00658v3",
    "pdf_url": null
  },
  {
    "instance_id": "ef4247f65ed349dabad52460402a0278",
    "figure_id": "1906.04634v1-Figure2-1",
    "image_file": "1906.04634v1-Figure2-1.png",
    "caption": " The SIFCN architecture with VGG16 backbone. The network consists of three modules: (a) feature extraction layers, (b) feature fusion layers, and (c) output layers.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which module of the SIFCN architecture is responsible for combining features from different layers?",
    "answer": " Feature fusion layers",
    "rationale": " The figure shows that the feature fusion layers (b) take features from different layers of the feature extraction module (a) and combine them. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04634v1",
    "pdf_url": null
  },
  {
    "instance_id": "90ad8d9665244fb5b5fb145f1e89d259",
    "figure_id": "1905.10989v4-Figure6-1",
    "image_file": "1905.10989v4-Figure6-1.png",
    "caption": " Coverage for word guessing game.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest coverage for the word guessing game when using both P and O?",
    "answer": "Quasimodo",
    "rationale": "The figure shows the coverage for each method in two scenarios: when using both P and O and when using only O. In the first scenario, Quasimodo has the highest bar, indicating the highest coverage.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10989v4",
    "pdf_url": null
  },
  {
    "instance_id": "12039ed396a942f0b32d8646f83e6898",
    "figure_id": "2009.14737v2-Figure4-1",
    "image_file": "2009.14737v2-Figure4-1.png",
    "caption": " Correlation between ACC(ω̄∗θ) and ACC(ω∗), where ω∗ denotes the optimal network parameters for a fixed augmentation policy θ and ω̄∗θ denotes the network parameters obtained by a proxy task via finetuing the checkpoint. Four proxy tasks, PAF , PNF , PIT and PAV are investigated.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which proxy task is most correlated with the optimal network parameters?",
    "answer": "PAF",
    "rationale": "The figure shows the correlation between the accuracy of the proxy tasks and the accuracy of the optimal network parameters. The correlation coefficient for PAF is 0.85, which is the highest among all the proxy tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.14737v2",
    "pdf_url": null
  },
  {
    "instance_id": "feadba92ef2d45338a8739fcfaf58e2c",
    "figure_id": "2010.05563v1-Figure4-1",
    "image_file": "2010.05563v1-Figure4-1.png",
    "caption": " The histgram of absolute bias between the property of graphs and subgraphs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods (Ours, Att07, and Att05) has the lowest bias for the DRD2 dataset?",
    "answer": "Ours",
    "rationale": "The figure shows the distribution of absolute bias for each method on the four datasets. The x-axis represents the bias, and the y-axis represents the number of data points. For the DRD2 dataset, the \"Ours\" method has the most data points concentrated near zero bias, indicating it has the lowest bias overall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.05563v1",
    "pdf_url": null
  },
  {
    "instance_id": "c4cfadf122c949dcb683f28e81da54b3",
    "figure_id": "2210.12599v2-Figure4-1",
    "image_file": "2210.12599v2-Figure4-1.png",
    "caption": " The max IoU trial versus the current iteration is displayed for all three baseline Bayesian optimizations. Each model improves greatly in the beginning but then converges at around or before the 20th iteration, with minor or no improvements in performance afterwards.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest IoU score and at which trial number?",
    "answer": "TransUNet, trial 23.",
    "rationale": "The green line in the plot represents the TransUNet model, and it reaches the highest point on the y-axis (IoU score) at around trial 23.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12599v2",
    "pdf_url": null
  },
  {
    "instance_id": "e3901d51b9cb44d582ec2862b511e784",
    "figure_id": "1804.06459v2-Figure4-1",
    "image_file": "1804.06459v2-Figure4-1.png",
    "caption": " The x-axis is time steps during learning. The y-axis is the average reward over the last 100 training episodes. The deep blue curves are for the baseline PPO architecture. The light blue curves are for the PPO-bonus baseline. The red curves are for our LIRPG based augmented architecture. The green curves are for our LIRPG architecture in which the policy module was trained with only intrinsic rewards. The dark curves are the average of 10 runs with different random seeds. The shaded area shows the standard errors of 10 runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which architecture performed the best on the Ant environment?",
    "answer": "PPO + bonus",
    "rationale": "The figure shows the average reward over the last 100 training episodes for different architectures on different environments. The y-axis shows the average reward, and the x-axis shows the time steps during learning. The light blue curve, which represents the PPO + bonus architecture, is the highest for the Ant environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.06459v2",
    "pdf_url": null
  },
  {
    "instance_id": "44042715a567438aa509ce301d2b7c8c",
    "figure_id": "2203.00887v3-Figure1-1",
    "image_file": "2203.00887v3-Figure1-1.png",
    "caption": " Results on the German Credit Risk dataset with age < 25 as the protected group. For Fair ϵ-greedy we use ϵ = 0.3 (see Figures 5 and 7 for other values of ϵ). In the plots, the means of the DP and the random walk algorithms are almost coinciding.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest representation in the top 100 ranks for the German Credit Risk dataset with age < 25 as the protected group?",
    "answer": "GDL21",
    "rationale": "The figure shows the representation of different algorithms in the top 100 ranks for the German Credit Risk dataset with age < 25 as the protected group. The GDL21 algorithm has the highest representation in the top 100 ranks, as its line is the highest in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.00887v3",
    "pdf_url": null
  },
  {
    "instance_id": "0d10eb2f61fb4bf2abb9257261b73cf2",
    "figure_id": "2003.08152v1-Figure2-1",
    "image_file": "2003.08152v1-Figure2-1.png",
    "caption": " The framework of our proposed method. It contains three sub-networks: text swapping network, background completion and fusion network.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sub-network is responsible for generating the background of the image?",
    "answer": "The background completion and fusion network.",
    "rationale": "The figure shows that the background completion and fusion network takes the output of the text swapping network and the self-attention network as input and produces the final image with the new background.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.08152v1",
    "pdf_url": null
  },
  {
    "instance_id": "896edfffed534a20a848f0cb98329cf5",
    "figure_id": "2106.05933v2-Figure62-1",
    "image_file": "2106.05933v2-Figure62-1.png",
    "caption": " Sparsity over layers for wav2vec-base finetuned for Italian it at 40% sparsity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the highest sparsity?",
    "answer": "Layer 11.",
    "rationale": "The figure shows the sparsity of each layer, and layer 11 has the highest value (55.112%).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05933v2",
    "pdf_url": null
  },
  {
    "instance_id": "510d234932f24b04a6fea58bbc3ac9f3",
    "figure_id": "2106.12535v2-Figure3-1",
    "image_file": "2106.12535v2-Figure3-1.png",
    "caption": " Decision surface for Bayesian Naive Bayes, the PAC-Bayesian methods First Order, Second Order, C-Bound and Binomial (with N “ 100) and our method on the two-moons dataset (where each half-circle is a class and inputs lie in X “ r´2, 2s2) with 16 (top) and 128 (bottom) decision stumps as base classifiers (axis-aligned and evenly distributed over the input space). Predicted labels are plotted with different colors, and training points are marked in black. When available, the value of the generalization bound is marked in the right-hand-side-top corner.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the most accurate decision surface for the two-moons dataset?",
    "answer": "Our method.",
    "rationale": "The decision surface for our method is the closest to the true decision surface, which is the line that separates the two half-circles. The other methods have decision surfaces that are either too smooth or too jagged, which results in misclassified points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.12535v2",
    "pdf_url": null
  },
  {
    "instance_id": "c6ec0c1c573f454ea2f96ad638a55cc7",
    "figure_id": "2010.01051v4-FigureB.1-1",
    "image_file": "2010.01051v4-FigureB.1-1.png",
    "caption": "Figure. B.1. Comparison between standard bootstrapping, amortized bootstrapping [31], and NeuBoots in Classification.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest error rate when the number of models in the ensemble is 25?",
    "answer": "NeuBoots Bagging",
    "rationale": "The figure shows that the blue line, which represents NeuBoots Bagging, has the lowest error rate when the number of models in the ensemble is 25.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.01051v4",
    "pdf_url": null
  },
  {
    "instance_id": "972e940d44404bc9b874708782bccd71",
    "figure_id": "2212.06384v3-Figure9-1",
    "image_file": "2212.06384v3-Figure9-1.png",
    "caption": " Samples generated by PV3D, see our project page for video results.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the different types of data that PV3D can generate?",
    "answer": "PV3D can generate images, videos, and 3D models.",
    "rationale": "The figure shows examples of images and 3D models generated by PV3D. The project page also mentions video results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.06384v3",
    "pdf_url": null
  },
  {
    "instance_id": "fcff52a6e5124665a1c2b95240eb98e8",
    "figure_id": "2101.05036v3-Figure5-1",
    "image_file": "2101.05036v3-Figure5-1.png",
    "caption": " Histogram of differential entropy for false positives bounding box predictive distributions produced by probabilistic detectors with FasterRCNN and RetinaNet as a backend. Results for DETR exhibit similar trends (Figure F.4).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of object detection model and loss function combination produces the most false positives?",
    "answer": "FasterRCNN with ES loss function.",
    "rationale": "The figure shows the distribution of differential entropy for false positive bounding boxes produced by different object detection models and loss functions. The ES loss function produces the most false positives for both FasterRCNN and RetinaNet. This is evident from the higher density of the distribution for the ES loss function compared to the other loss functions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.05036v3",
    "pdf_url": null
  },
  {
    "instance_id": "cd287990a6b6456189cc60eee35a67c9",
    "figure_id": "2210.06077v1-Figure9-1",
    "image_file": "2210.06077v1-Figure9-1.png",
    "caption": " Proportion of correctly predicted instances for which each approach yields the highest certification across σ for MNIST. Red represents the proportion for which the boundary treatment produces the largest certification, with Green, Orange, and Blue representing the same for Double transitivity, Single transitivity, or Cohen et al.. While our approaches subsume Cohen, if no other technique is able to improve upon the base certification, we assign the largest certification as having been calculated by Cohen et al..",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method consistently produces the largest certification across all values of σ?",
    "answer": "The boundary treatment method.",
    "rationale": "The red bars in the figure represent the proportion of correctly predicted instances for which the boundary treatment method yields the highest certification. The red bars are consistently taller than the other bars across all values of σ, indicating that the boundary treatment method is the most effective method for producing the largest certification.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06077v1",
    "pdf_url": null
  },
  {
    "instance_id": "4009120b14534041ab79b3b263a0bf40",
    "figure_id": "2110.07232v2-Figure1-1",
    "image_file": "2110.07232v2-Figure1-1.png",
    "caption": " Figures (a) to (c) show simple regret (median of 10 runs) of different algorithms on synthetic functions with DNF feedbacks. Figures (d) to (f) show the cross-validation accuracy (median of 5 runs) achieved on the hyperparameter tuning of classifiers on datasets with DNF feedbacks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the lowest simple regret on the Branin function?",
    "answer": "MF-GP-UCB",
    "rationale": "The plot in Figure (b) shows that the MF-GP-UCB algorithm has the lowest simple regret on the Branin function.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.07232v2",
    "pdf_url": null
  },
  {
    "instance_id": "3678b0ed6652420195da83c2091329ae",
    "figure_id": "2003.12929v1-Figure14-1",
    "image_file": "2003.12929v1-Figure14-1.png",
    "caption": " Comparsion of superpixel segmentation results on HR-VS. Note we do not enforce the superpixel connectivity here.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following images shows the most accurate superpixel segmentation?",
    "answer": "\"Ours_joint\"",
    "rationale": "The \"Ours_joint\" images show the most accurate superpixel segmentation because they have the most consistent and well-defined boundaries between the different objects in the scene. The \"Left image\" and \"Ours_fixed\" images have less accurate segmentation, with some objects being incorrectly segmented or having poorly defined boundaries.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.12929v1",
    "pdf_url": null
  },
  {
    "instance_id": "cc9a67fcbd1540389fb245873a727efc",
    "figure_id": "2105.09295v2-Figure4-1",
    "image_file": "2105.09295v2-Figure4-1.png",
    "caption": " Effect of committee size K on sample complexity and representation loss for CMDP (known p) and RL-CMDP (unknown p), on the two different Bayesian networks (1) and (2) fitted on the Census Income dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more sample efficient, CMDP or RL-CMDP?",
    "answer": "CMDP is more sample efficient than RL-CMDP.",
    "rationale": "The figure shows that the average sample size required by CMDP is much smaller than that required by RL-CMDP for both Bayesian networks (1) and (2).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.09295v2",
    "pdf_url": null
  },
  {
    "instance_id": "e2a3bf074525477ca1e9cf29e725726a",
    "figure_id": "2004.01588v1-Figure7-1",
    "image_file": "2004.01588v1-Figure7-1.png",
    "caption": " Samples of NYU [28] depth images with 2D overlay of the estimated 3D hand pose. Our method produces more accurate results compared to WHSP-Net [17] and DeepHPS [14] methods.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods (DeepHPS, WHSP-Net, or Ours) produces the most accurate results for estimating 3D hand pose?",
    "answer": "Ours",
    "rationale": "The caption states that \"Our method produces more accurate results compared to WHSP-Net [17] and DeepHPS [14] methods.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.01588v1",
    "pdf_url": null
  },
  {
    "instance_id": "3b0c3ecb2f2e42b39f78f5e701f84f3c",
    "figure_id": "1905.09932v1-Figure6-1",
    "image_file": "1905.09932v1-Figure6-1.png",
    "caption": " Quality of precipitation prediction with optical flow and the convolutional neural network. In our preliminary experiments, the simpler optical flow approach provided slightly better predictions than the neural network.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed best in predicting precipitation?",
    "answer": "Optical flow.",
    "rationale": "The figure shows the F1 score, a measure of prediction quality, for three different methods: neural network, optical flow, and persistence. The optical flow method has the highest F1 score throughout the entire time period shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.09932v1",
    "pdf_url": null
  },
  {
    "instance_id": "ae74e309feb946b78ca3ea388590d8aa",
    "figure_id": "2006.15657v2-Figure6-1",
    "image_file": "2006.15657v2-Figure6-1.png",
    "caption": " Decoding the Trajectories: We run our trained subject-verb-object decoder on different segments of Oops! videos. Row 1 shows clips of intentional action, and the trained decoder predicts the latent goal. Row 2 shows unintentional action, and the trained decoder now predicts failures instead. The final row also shows unintentional videos, but we run our auto-correction algorithm before predicting SVOs. The trained decoder returns to predicting goals, suggesting the auto-correct procedure shifts the failed trajectories towards successful ones.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which row of the figure shows examples of unintentional actions that have been auto-corrected?",
    "answer": "Row 3.",
    "rationale": "The caption states that \"The final row also shows unintentional videos, but we run our auto-correction algorithm before predicting SVOs.\" This suggests that the videos in Row 3 have been auto-corrected.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.15657v2",
    "pdf_url": null
  },
  {
    "instance_id": "1a9e85a9626644eab809df8bd4d094d8",
    "figure_id": "2102.05109v1-Figure2-1",
    "image_file": "2102.05109v1-Figure2-1.png",
    "caption": " Subjective tests: (a) In pairwise tests, ours is typically preferred over MelGAN for single speaker and cross-speaker synthesis. (b) MOS tests show denoising methods are improved by CDPAM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the highest MOS score?",
    "answer": "Clean achieved the highest MOS score of 4.33.",
    "rationale": "The figure shows the MOS scores for different methods. The Clean method has the highest bar, indicating it has the highest MOS score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.05109v1",
    "pdf_url": null
  },
  {
    "instance_id": "8955dfca473a44a89150832e7adca477",
    "figure_id": "2106.15147v2-Figure6-1",
    "image_file": "2106.15147v2-Figure6-1.png",
    "caption": " Left: Win matrix comparing different corruption strategies when z-score feature normalization is used in the fully labeled, noiseless setting. Right: The same matrix but when min-max feature scaling is used. We see that SCARF is better than alternative corruption strategies for different types of feature scaling.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which corruption strategy performs best when z-score feature normalization is used?",
    "answer": "SCARF",
    "rationale": "The left plot in the figure shows the win matrix comparing different corruption strategies when z-score feature normalization is used. The win ratio is the proportion of times a particular corruption strategy resulted in a better model than another corruption strategy. The SCARF corruption strategy has the highest win ratio for most of the other corruption strategies, indicating that it performs best when z-score feature normalization is used.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15147v2",
    "pdf_url": null
  },
  {
    "instance_id": "5a956923b329453795b7377aad64723a",
    "figure_id": "2007.05515v3-Figure7-1",
    "image_file": "2007.05515v3-Figure7-1.png",
    "caption": " Full AViD hierarchy",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following activities is not a type of driving?",
    "answer": "snowplow",
    "rationale": "The figure shows a hierarchy of activities, with \"driving\" as a sub-category of \"crossing river\". Snowplowing is not listed as a sub-category of driving.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.05515v3",
    "pdf_url": null
  },
  {
    "instance_id": "0bc98c8f428f467b9f9088dcd546f32b",
    "figure_id": "2102.01072v2-Figure2-1",
    "image_file": "2102.01072v2-Figure2-1.png",
    "caption": " Left: Android DEX file structure, composed of three major components—(1) header, (2) ids, and (3) data. Right: binary image representation of the DEX file.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three major components of an Android DEX file?",
    "answer": "Header, ids, and data.",
    "rationale": "The figure shows the structure of an Android DEX file, which is composed of three major components: header, ids, and data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.01072v2",
    "pdf_url": null
  },
  {
    "instance_id": "cdce392c0ba34a39af8e33123f5c9fc1",
    "figure_id": "2305.05560v3-Figure1-1",
    "image_file": "2305.05560v3-Figure1-1.png",
    "caption": " A taxonomy of solution sets in multi-objective decision making.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which solution set is the most general? ",
    "answer": " The CH solution set.",
    "rationale": " The figure shows a hierarchy of solution sets in multi-objective decision making. The CH solution set is at the top of the hierarchy, which means it is the most general solution set. The other solution sets are subsets of the CH solution set. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.05560v3",
    "pdf_url": null
  },
  {
    "instance_id": "70c0e9ac53404dc790a554877354045f",
    "figure_id": "2006.11834v3-Figure2-1",
    "image_file": "2006.11834v3-Figure2-1.png",
    "caption": " BLEU scores over iterations on the ChineseEnglish validation set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function leads to the highest BLEU score?",
    "answer": "Lclean + Ladv",
    "rationale": "The figure shows the BLEU score for different loss functions over iterations. The line for Lclean + Ladv is consistently higher than the other lines, indicating that this loss function leads to the highest BLEU score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.11834v3",
    "pdf_url": null
  },
  {
    "instance_id": "c3699ce9e9b34c49af16732196c86117",
    "figure_id": "1711.03539v2-Figure3-1",
    "image_file": "1711.03539v2-Figure3-1.png",
    "caption": " Rewards and regret over the Yahoo! dataset with K = 5",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest regret at the end of the experiment?",
    "answer": "PHT-UCB",
    "rationale": "The regret plot in (b) shows that PHT-UCB has the lowest regret at the end of the experiment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.03539v2",
    "pdf_url": null
  },
  {
    "instance_id": "5377dd2230dc465780409ff62cb47b7d",
    "figure_id": "2303.12688v1-Figure8-1",
    "image_file": "2303.12688v1-Figure8-1.png",
    "caption": " We compare our method to additional image based editing methods, Null-inversion [30] and Promptto-Prompt [16]. When such methods are applied naively in a per-frame manner, they yield inconsistent appearance across frames.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most consistent appearance across frames?",
    "answer": "Ours.",
    "rationale": "The figure shows three different methods for editing images: Null-inversion, Prompt-to-Prompt, and ours. Each method is applied to three different frames of a video of a leopard running. The results show that our method produces the most consistent appearance across frames, while the other two methods produce inconsistent results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.12688v1",
    "pdf_url": null
  },
  {
    "instance_id": "c1ba9fcb4a5e4c5682252879cfa2940c",
    "figure_id": "2104.06392v3-Figure2-1",
    "image_file": "2104.06392v3-Figure2-1.png",
    "caption": " ShapeMOD consists of two alternating phases: proposing new candidate macros (top) and refactoring programs to use some of the proposed macros (bottom).",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following actions is not part of the refactoring phase? \n\nA. Analyze the dataset.\nB. Propose new macros.\nC. Evaluate macros.\nD. Add macros to the library.",
    "answer": "B. Propose new macros.",
    "rationale": "The figure shows that the ShapeMOD workflow consists of two phases: proposal and refactoring. The proposal phase is responsible for analyzing the dataset and proposing new macros, while the refactoring phase is responsible for using the library of macros to refactor programs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.06392v3",
    "pdf_url": null
  },
  {
    "instance_id": "b33a2cde56164292ad82bc8bd67ec084",
    "figure_id": "2206.01995v5-Figure6-1",
    "image_file": "2206.01995v5-Figure6-1.png",
    "caption": " Regrets of Algorithms Run on G3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest regret in this experiment?",
    "answer": "BLM-LR",
    "rationale": "The figure shows the cumulative regret of different algorithms over time. The regret of an algorithm is the difference between the reward it receives and the reward that the best possible algorithm would receive. The BLM-LR algorithm has the highest cumulative regret at the end of the experiment, as its line is the highest on the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.01995v5",
    "pdf_url": null
  },
  {
    "instance_id": "12769de9c086417db32bccfd7f985e95",
    "figure_id": "1810.08851v1-Figure9-1",
    "image_file": "1810.08851v1-Figure9-1.png",
    "caption": " Kendall results on IQA dataset. Color area represents 95% confidence intervals over 100 times iterations. y-axis is rescaled using Fisher transformation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of Kendall rank correlation coefficient?",
    "answer": "Hybrid-MST",
    "rationale": "The figure shows the Kendall rank correlation coefficient for three methods: Crowd-BT, Hodge-active, and Hybrid-MST. The Hybrid-MST method has the highest Kendall rank correlation coefficient for all the standard trials, indicating that it performs best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.08851v1",
    "pdf_url": null
  },
  {
    "instance_id": "e603c62785964e0fb14862154f97a393",
    "figure_id": "1907.11308v1-Figure9-1",
    "image_file": "1907.11308v1-Figure9-1.png",
    "caption": " Distribution of #objects for each room type.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which room type has the highest percentage of rooms with less than 20 objects?",
    "answer": "Bedroom",
    "rationale": "The figure shows that the bedroom has the highest percentage of rooms with less than 20 objects (35%).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.11308v1",
    "pdf_url": null
  },
  {
    "instance_id": "c4c476b96347485bb1826aed418def64",
    "figure_id": "1906.08042v1-Figure4-1",
    "image_file": "1906.08042v1-Figure4-1.png",
    "caption": " Low-resource performance (software genre).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which machine learning model performs best when the number of labeled training examples is low?",
    "answer": "Deep Active.",
    "rationale": "The figure shows the F1 score of different machine learning models as a function of the number of labeled training examples. The Deep Active model has the highest F1 score when the number of labeled training examples is low.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.08042v1",
    "pdf_url": null
  },
  {
    "instance_id": "c1a0be3be23144e9ac0f57c79dbb014f",
    "figure_id": "1903.05926v4-Figure6-1",
    "image_file": "1903.05926v4-Figure6-1.png",
    "caption": " Detailed results in the GridWorld.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best in terms of the number of steps taken to reach the goal?",
    "answer": "Q-learning.",
    "rationale": "The figure shows that Q-learning reaches the goal in the fewest number of steps, as its curve is the lowest of all the curves.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.05926v4",
    "pdf_url": null
  },
  {
    "instance_id": "021fa20dce654bf69b2b19dcd05fcc22",
    "figure_id": "1905.01941v2-Figure10-1",
    "image_file": "1905.01941v2-Figure10-1.png",
    "caption": " Performance of FAZE for normalizing the 3 × Fg-dimensional gaze code along the 3 or Fg dimensions, respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which normalization method performs better when the number of calibration samples is low?",
    "answer": "Normalization along 3 values.",
    "rationale": "The plot shows that the mean test error for normalization along 3 values is lower than the mean test error for normalization along Fg values when the number of calibration samples is low.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.01941v2",
    "pdf_url": null
  },
  {
    "instance_id": "37efeed1e457452988daffffb0a53025",
    "figure_id": "1902.10677v2-Figure2-1",
    "image_file": "1902.10677v2-Figure2-1.png",
    "caption": " Example probabilistic database. Tuples are now of the form 〈t : p〉 where p is the probability of the tuple t being present.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the probability that a paper was co-authored by Einstein and Erdős?",
    "answer": "0.8",
    "rationale": "The table shows that the tuple 〈Einstein, Erdős : 0.8〉 is present, which means that the probability of a paper being co-authored by Einstein and Erdős is 0.8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.10677v2",
    "pdf_url": null
  },
  {
    "instance_id": "dd42066a969b456cacbdb9e13f1e8294",
    "figure_id": "1905.10714v1-Figure7-1",
    "image_file": "1905.10714v1-Figure7-1.png",
    "caption": " The classification accuracy on testing dataset as a function of µ.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest accuracy when μ is 0.8?",
    "answer": "ADAM",
    "rationale": "The figure shows that the ADAM algorithm has the highest accuracy when μ is 0.8. This can be seen by looking at the line for the ADAM algorithm, which is the highest line at the point where μ is 0.8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10714v1",
    "pdf_url": null
  },
  {
    "instance_id": "583e22fd172b4920b13db11ca51cf1c1",
    "figure_id": "1910.10840v1-Figure6-1",
    "image_file": "1910.10840v1-Figure6-1.png",
    "caption": " Feature standard deviation in the Breakout environment (v0)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest feature standard deviation at rollout 2.5e6?",
    "answer": "ICM, double attention",
    "rationale": "The plot shows the feature standard deviation for different algorithms as a function of rollout. The ICM, double attention line is the highest at rollout 2.5e6.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.10840v1",
    "pdf_url": null
  },
  {
    "instance_id": "accd111e21d5475dbf8dfcf269ab130a",
    "figure_id": "2306.09869v3-Figure16-1",
    "image_file": "2306.09869v3-Figure16-1.png",
    "caption": " Further results for real image editing: horse to zebra.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most realistic zebra image?",
    "answer": "Our method.",
    "rationale": "The images in the \"Ours\" column look the most realistic and are visually indistinguishable from real zebra images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.09869v3",
    "pdf_url": null
  },
  {
    "instance_id": "d2abf2e0e5db45c9853a5dcfc04a2af4",
    "figure_id": "2106.03596v1-Figure8-1",
    "image_file": "2106.03596v1-Figure8-1.png",
    "caption": " Results of the synthetic experiments for the multiclass spam filtering. The plot shows the best results of algorithms with parameters suggested by theory, or tuned with all parameters set to 1, except for T . The rows are the different values for K and the columns are the different values for d. The whiskers represent the minimum and maximum error rates of the ten repetitions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best when the number of classes is 6 and the number of features is 120?",
    "answer": "GapLog",
    "rationale": "The plot shows the error rate for each algorithm at different values of K (number of classes) and d (number of features). The lowest error rate for K = 6 and d = 120 is achieved by GapLog.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03596v1",
    "pdf_url": null
  },
  {
    "instance_id": "cd6807a0360044dbb49dce2bfb195ed2",
    "figure_id": "2110.04383v1-Figure7-1",
    "image_file": "2110.04383v1-Figure7-1.png",
    "caption": " Histogram of the number of conformers per enantiomer in the R/S classification dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which number of conformers is the most common?",
    "answer": "3 conformers per enantiomer.",
    "rationale": "The x-axis of the histogram shows the number of conformers, and the y-axis shows the number of stereoisomers. The bar at 3 conformers is the tallest, indicating that this is the most common number of conformers per enantiomer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.04383v1",
    "pdf_url": null
  },
  {
    "instance_id": "f76c010ce3194d1c93a5306e440d84ae",
    "figure_id": "1909.13003v4-Figure8-1",
    "image_file": "1909.13003v4-Figure8-1.png",
    "caption": " Training curves of DualSMC and baseline methods for the modified Reacher environment (averaged over 5 seeds)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in terms of episodic reward?",
    "answer": "DualSMC (adv)",
    "rationale": "The plot shows the episodic reward for different methods over training episodes. DualSMC (adv) consistently has the highest episodic reward, indicating it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.13003v4",
    "pdf_url": null
  },
  {
    "instance_id": "89443a2461604018aa211f7e2793630d",
    "figure_id": "1904.03941v1-Figure2-1",
    "image_file": "1904.03941v1-Figure2-1.png",
    "caption": " Representation of the predefined transformations (a) and the resulting degrees of freedom for the three camera 3D registrations, with two point correspondences between point clouds one & two and two & three (b). (c) shows the remaining degrees of freedom for four point clouds and two 3D point correspondences between one & two, two & three, and three & four.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the remaining degrees of freedom for the three-camera 3D registration with two point correspondences between point clouds one and two, and two and three?",
    "answer": "The remaining degrees of freedom are α and β.",
    "rationale": "The figure shows that the predefined transformations fix the position and orientation of the first point cloud. The two point correspondences between point clouds one and two, and two and three, fix the relative position and orientation of the second and third point clouds with respect to the first point cloud. However, there are still two degrees of freedom remaining, which are represented by the angles α and β. These angles represent the rotation of the second and third point clouds around the axis connecting the two corresponding points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03941v1",
    "pdf_url": null
  },
  {
    "instance_id": "6a8e7719ad4b4d99851021fe851a5636",
    "figure_id": "2104.02297v1-Figure11-1",
    "image_file": "2104.02297v1-Figure11-1.png",
    "caption": " Digit flipping results: Deep SHAPNET explanations perform well against post-hoc methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of log-odds between 4 and 1, 8 and 3, and 8 and 6?",
    "answer": "SHAPNET",
    "rationale": "The box plots for SHAPNET consistently have the highest median values and the smallest interquartile ranges across all three comparisons, indicating that it performs the best in terms of log-odds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.02297v1",
    "pdf_url": null
  },
  {
    "instance_id": "0e467d6a6e374451bf97dbb133ec18be",
    "figure_id": "2212.05707v1-Figure3-1",
    "image_file": "2212.05707v1-Figure3-1.png",
    "caption": " Recall@10 of COVID-HK and COVID-MLC on the frequency of location appearances. DGDI outperforms baselines on all groups.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best for all frequency groups in both COVID-HK and COVID-MLC datasets?",
    "answer": "DGDI",
    "rationale": "The figure shows the Recall@10 for different methods on different frequency groups for the COVID-HK and COVID-MLC datasets. The DGDI method has the highest Recall@10 for all frequency groups in both datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.05707v1",
    "pdf_url": null
  },
  {
    "instance_id": "9a4fd3aaa4864375bf5553bcac6dc16c",
    "figure_id": "2207.04497v2-Figure6-1",
    "image_file": "2207.04497v2-Figure6-1.png",
    "caption": " Distribution (%) of weight mask values via masking different positions.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which position resulted in the largest percentage of weight mask values between 70% and 80%?",
    "answer": "Position 0.1",
    "rationale": "The figure shows the distribution of weight mask values for different positions. The color of each cell indicates the percentage of weight mask values that fall within a certain range. The cell corresponding to position 0.1 and the range 70%-80% is the darkest red, which indicates the highest percentage.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.04497v2",
    "pdf_url": null
  },
  {
    "instance_id": "449350b51e964c859d161543b3eed937",
    "figure_id": "1909.03341v2-Figure4-1",
    "image_file": "1909.03341v2-Figure4-1.png",
    "caption": " X-En validation BLEU for models without contextualization, with local contextualization (depth-wise convolution) and with long-range contextualization (Bi-GRU). The y axis starts from 28.2 to focus on the gain portions and facilitate comparison across different vocabularies.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of contextualization results in the highest BLEU score for the BPE 32K vocabulary?",
    "answer": "Bi-GRU contextualization.",
    "rationale": "The figure shows that the Bi-GRU ctx bars are consistently taller than the other two bars for each vocabulary size, indicating that Bi-GRU contextualization leads to the highest BLEU scores.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.03341v2",
    "pdf_url": null
  },
  {
    "instance_id": "0c66a027300040ebb8818f92a93e3a57",
    "figure_id": "1805.09746v2-Figure4-1",
    "image_file": "1805.09746v2-Figure4-1.png",
    "caption": " Attention weights visualization per genre.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which genre is most likely to be found in the first half of the book?",
    "answer": "Detective Mystery",
    "rationale": "The attention weights for Detective Mystery are highest in the first half of the book, as indicated by the dark shading in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.09746v2",
    "pdf_url": null
  },
  {
    "instance_id": "04e05078e5cd4d11a6cb1b8a4c2fc0bd",
    "figure_id": "2102.12624v1-Figure3-1",
    "image_file": "2102.12624v1-Figure3-1.png",
    "caption": " Spotting F1 scores across different N and agents for k=4",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which agent performs best for k=4?",
    "answer": "Siamese.",
    "rationale": "The figure shows that the Siamese agent has the highest F1 score for all values of N.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.12624v1",
    "pdf_url": null
  },
  {
    "instance_id": "43b3161a953e4d44aea74876c73b069d",
    "figure_id": "2203.12990v1-Figure3-1",
    "image_file": "2203.12990v1-Figure3-1.png",
    "caption": " Fact checking performance of models trained only on claims (i.e. no evidence). Training on our generated claims result in performance closer to random (indicating fewer data artifacts) than training on the original SciFact claims.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of precision, recall, and F1 score?",
    "answer": "CG-BART + KBIN",
    "rationale": "The figure shows that the CG-BART + KBIN model has the highest bars for all three metrics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.12990v1",
    "pdf_url": null
  },
  {
    "instance_id": "ea6c9758eedc41fd8702a08304a65925",
    "figure_id": "2211.15936v1-Figure1-1",
    "image_file": "2211.15936v1-Figure1-1.png",
    "caption": " Structure of a randomized policy network.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the inputs to the neural network in the figure?",
    "answer": "Observation and randomness.",
    "rationale": "The arrows point from \"observation\" and \"randomness\" to the \"neural network,\" indicating that these are the inputs to the network.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.15936v1",
    "pdf_url": null
  },
  {
    "instance_id": "63c0c9b9b3a6432eb33c8591f5809653",
    "figure_id": "1911.01591v1-Figure2-1",
    "image_file": "1911.01591v1-Figure2-1.png",
    "caption": " (a) Normalized Mutual Information vs Storage Complexity of different methods for COIL dataset. (b) Computation Time vs Storage Complexity of different methods for COIL dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the best clustering quality for the COIL dataset?",
    "answer": "t-SNE",
    "rationale": "The figure shows that t-SNE has the highest normalized mutual information (NMI) for all storage costs. NMI is a measure of clustering quality, and higher values indicate better clustering.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.01591v1",
    "pdf_url": null
  },
  {
    "instance_id": "ca862d72955e46e39696e9e82b84db9e",
    "figure_id": "1910.09615v1-Figure2-1",
    "image_file": "1910.09615v1-Figure2-1.png",
    "caption": " Average performance of TRPO, PPO, PDO, CPO and IPO under Point Gather, Point Circle and HalfCheetahSafe with discounted cumulative constraints. The x-axis is the number of trajectories. The dashed lines are constrained limits for different tasks which is 0.1 for Point Gather, 5 for Point Circle and 50 for HalfCheetah-Safe.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of reward for the Point Gather task?",
    "answer": "TRPO",
    "rationale": "The plot in (a) shows the average reward for each algorithm on the Point Gather task. TRPO has the highest average reward.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.09615v1",
    "pdf_url": null
  },
  {
    "instance_id": "418e18b8d2e0480e96bc933b5528d713",
    "figure_id": "2310.17124v1-Figure5-1",
    "image_file": "2310.17124v1-Figure5-1.png",
    "caption": " Performance on complex signal structure under t(3) distribution. The boxplots are depicted based on 30 runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest prediction accuracy for signal A?",
    "answer": "Lasso",
    "rationale": "The boxplot for Lasso in the prediction accuracy plot (b) is the lowest for signal A.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.17124v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee5ff0ff619f4759ac8aec6d57529279",
    "figure_id": "2310.20178v2-Figure7-1",
    "image_file": "2310.20178v2-Figure7-1.png",
    "caption": " Performance comparisons on Deepmind Control Suite. Aggregate statistics and performance profiles with 95% bootstrap confidence interval are provided [2], which are calculated across 120 seeds (15 seeds × 8 tasks).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the Deepmind Control Suite tasks according to the median score?",
    "answer": "SCRATCH",
    "rationale": "The median score for SCRATCH is the highest among all methods, as shown in the \"MEDIAN\" plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.20178v2",
    "pdf_url": null
  },
  {
    "instance_id": "d39209938fe6490dbe4efb6098782e89",
    "figure_id": "1806.04606v2-Figure3-1",
    "image_file": "1806.04606v2-Figure3-1.png",
    "caption": " Robustness test of ResNet-110 solutions found by ONE, DML, and vanilla training algorithms on CIFAR100. Each curve corresponds to a specific perturbation direction v.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training algorithm is the most robust to perturbations on the training data?",
    "answer": "ONE",
    "rationale": "The figure shows that the ONE training algorithm has the lowest train error for all perturbation magnitudes. This indicates that the ONE training algorithm is the most robust to perturbations on the training data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.04606v2",
    "pdf_url": null
  },
  {
    "instance_id": "ba0260f8fb5a44c08faa02040855c4fd",
    "figure_id": "2210.09396v1-Figure19-1",
    "image_file": "2210.09396v1-Figure19-1.png",
    "caption": " Average marginal effects of listening contexts in setting-tagged playlists on listener affective responses, controlling for songs and user demographic variables; standard errors are shown.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which setting is associated with the highest level of arousal?",
    "answer": "Night",
    "rationale": "The figure shows the average marginal effects of listening contexts in setting-tagged playlists on listener affective responses. The x-axis shows the average marginal effect, and the y-axis shows the different settings. The setting with the highest average marginal effect on arousal is night.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.09396v1",
    "pdf_url": null
  },
  {
    "instance_id": "b8076fa86f8c447c920aebebe97b2af2",
    "figure_id": "2302.00878v3-Figure11-1",
    "image_file": "2302.00878v3-Figure11-1.png",
    "caption": " Comparisons on high-dimensional synthetic regression data. Metrics are aggregated over 10 synthetic datasets. Solid points are averages and error bars are standard errors. The dashed horizontal line in the middle indicates the true sparsity level.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of F1 of nonzeros?",
    "answer": "Deep neural network",
    "rationale": "The deep neural network line is the highest in the F1 of nonzeros plot, indicating that it has the highest F1 score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.00878v3",
    "pdf_url": null
  },
  {
    "instance_id": "b72c93b946d54237863a494d23c6526b",
    "figure_id": "2110.02180v2-Figure6-1",
    "image_file": "2110.02180v2-Figure6-1.png",
    "caption": " Pre-actived ResNet-18 evaluated on CIFAR-10c.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which data augmentation technique performed the best on the CIFAR-10c dataset with speckle noise at severity level 5?",
    "answer": "NFM (*)",
    "rationale": "The figure shows the test accuracy of different data augmentation techniques on the CIFAR-10c dataset with different types of noise and severity levels. The NFM (*) technique has the highest test accuracy for speckle noise at severity level 5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02180v2",
    "pdf_url": null
  },
  {
    "instance_id": "11cfc948fac5421d9381188c44cb37d1",
    "figure_id": "2306.01801v1-Figure2-1",
    "image_file": "2306.01801v1-Figure2-1.png",
    "caption": " Negative log likelihoods of truncated top-𝑘dependent CDMs where the context set is only the top-𝑘 chosen alternatives. Here 𝑘 = 0 is equivalent to the linear MNL, and 𝑘 = 𝑚, the total number of offered programs, is equivalent to the standard CDM.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " How does the negative log likelihood (NLL) of the truncated top-𝑘 dependent CDM change as the value of k increases?",
    "answer": " The NLL decreases as the value of k increases.",
    "rationale": " The figure shows that the NLL curves for both the train and test sets decrease as k increases. This means that the model fits the data better as more alternatives are included in the context set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01801v1",
    "pdf_url": null
  },
  {
    "instance_id": "40c8ca04142b4644a79b8b137db4950f",
    "figure_id": "2210.08169v1-Figure5-1",
    "image_file": "2210.08169v1-Figure5-1.png",
    "caption": " Result of diagnosis case study. The horizontal coordinate is the knowledge concept, the left vertical coordinate is the student’s mastery level of the knowledge concept, and the right vertical coordinate is the difficulty of the exercise on the corresponding knowledge concept. The suffixes in the figure legends indicate the diagnosis results of different models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which knowledge concept was mastered the least by the student?",
    "answer": "Knowledge concept D.",
    "rationale": "The figure shows the student's mastery level for each knowledge concept. The bar for knowledge concept D is the lowest, indicating that the student mastered this concept the least.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.08169v1",
    "pdf_url": null
  },
  {
    "instance_id": "ab7d4aba25794c90b4d74867f8efbc6a",
    "figure_id": "2111.05498v2-Figure13-1",
    "image_file": "2111.05498v2-Figure13-1.png",
    "caption": " Showing the largest summation term of the circle intersection and its exponential approximation in comparison to the full circle intersection for a large range of d values when n = 64. Blue is the largest sum, represented on left hand side of Eq. 16. Orange is a log linear regression fit to this line using Eq. 10 to show how this first summation term, like the full circle intersection is approximately exponential. Green is the Normal approximation in Eq. 18 to the largest sum. It slightly upper bounds the real largest sum value. Red is the Taylor approximation in Eq. 20 that becomes less accurate as dv on the x-axis increases. Purple is the full circle intersection equation to show how well the largest sum approximates it as a lower bound. It is a tight bound for smaller Hamming d radii when there are fewer other terms to sum. The y-axis gives the log fraction of the vector space p occupied by the largest sum (this is independent of n). The x-axis is up to values of 0.1n where the Taylor approximation holds well.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approximation to the largest sum of the circle intersection is the most accurate for small values of dv?",
    "answer": "The Taylor approximation.",
    "rationale": "The Taylor approximation (red line) is closest to the actual largest sum (blue line) for small values of dv. This can be seen in the figure, where the red line closely follows the blue line for small values of dv.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.05498v2",
    "pdf_url": null
  },
  {
    "instance_id": "1823a85ecdfa41f3aae89cec471edf3c",
    "figure_id": "2310.15166v1-Figure9-1",
    "image_file": "2310.15166v1-Figure9-1.png",
    "caption": " e-SNLI-VE qualitative examples. Leftmost: As the connection to daredevil is not obvious in BLIP and OFA’s captions, although Cola-Zero is misled, Cola-FT correctly answers maybe. Left: Similar to the left example, Cola-FT answer correctly as no obvious connections are seen from the captions to this question. Right: Similar to the left example, the fact of catch catfish is not reasonable from the captions, Cola-FT picks the correct answer maybe. Rightmost: As girl gets hit is not obvious in BLIP and OFA’s captions and answers, Cola-Zero and Cola-FT both follow BLIP to choose the correct answer no. The correct choices are underlined. Cola-Zero answers are given in zero-shot settings.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the e-SNLI-VE dataset?",
    "answer": "Cola-FT",
    "rationale": "The figure shows that Cola-FT correctly answered all four questions, while the other models made at least one mistake.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.15166v1",
    "pdf_url": null
  },
  {
    "instance_id": "106f172e084d45fb9a6bf18287391e21",
    "figure_id": "2207.01117v1-Figure5-1",
    "image_file": "2207.01117v1-Figure5-1.png",
    "caption": " Visualization of task relation learned by SRDML on real-world dataset. Zoom in for detail.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows a stronger correlation between attributes, CelebA or COCO?",
    "answer": "CelebA",
    "rationale": "The heatmap for CelebA has more intense colors (both blue and red) than the heatmap for COCO, indicating that the correlations between attributes are stronger in CelebA.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.01117v1",
    "pdf_url": null
  },
  {
    "instance_id": "f5b3ab18e6254045b28aa6ef2a0f4ec6",
    "figure_id": "2012.01988v6-Figure1-1",
    "image_file": "2012.01988v6-Figure1-1.png",
    "caption": " Committee-based models achieve a higher accuracy than single models on ImageNet while using fewer FLOPs. For example, although Inception-v4 (‘Incep-v4’) outperforms all single ResNet models, a ResNet cascade can still outperform Incep-v4 with fewer FLOPs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest accuracy and uses the least FLOPs?",
    "answer": "ResNet-101 Cascade.",
    "rationale": "The plot shows that the ResNet-101 Cascade model has the highest accuracy and uses fewer FLOPs than other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.01988v6",
    "pdf_url": null
  },
  {
    "instance_id": "46a18da6210b4fb9a13bd540477f6c9b",
    "figure_id": "2305.04445v2-Figure4-1",
    "image_file": "2305.04445v2-Figure4-1.png",
    "caption": " Experimental results for atomic interventions (log scale)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which intervention strategy consistently has the lowest cost across all types and node sizes?",
    "answer": "Our k=1 strategy",
    "rationale": "The figure shows that the \"Ours (k=1)\" line is consistently below the other lines for all types and node sizes. This means that our k=1 strategy has the lowest cost for all types and node sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.04445v2",
    "pdf_url": null
  },
  {
    "instance_id": "fd16ab796e774855907ef2500890903e",
    "figure_id": "2306.07930v1-Figure21-1",
    "image_file": "2306.07930v1-Figure21-1.png",
    "caption": " Performance of Gamine under cost functions 𝑐 ∈ {𝑐𝐵1, 𝑐𝐵2, 𝑐𝑅1, 𝑐𝑅2}, run on YT-100k with 𝑑 = 5, 𝛼 = 0.05, and 𝜒 = U.Gamine is strongest under the binary cost function 𝑐𝐵1, weakest under the real-valued cost function 𝑐𝑅2, and roughly equally strong under the binary 𝑐𝐵2 and the real-valued 𝑐𝑅1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which cost function results in the best performance for Gamine?",
    "answer": "cB1",
    "rationale": "The figure shows that Gamine performs best under the binary cost function cB1. This can be seen in both subfigures (a) and (b), where the curve for cB1 is consistently higher than the curves for the other cost functions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.07930v1",
    "pdf_url": null
  },
  {
    "instance_id": "70133b586da24d6389f045852677ba3d",
    "figure_id": "2001.02773v2-Figure3-1",
    "image_file": "2001.02773v2-Figure3-1.png",
    "caption": " Comparison of rate of convergence between BVI, L-BVI, and C2F-BVI with 20% evidence, on RGM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods converges the fastest?",
    "answer": "BVI.",
    "rationale": "The figure shows the free energy of the three methods over time. BVI reaches the lowest free energy first, indicating that it converges the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.02773v2",
    "pdf_url": null
  },
  {
    "instance_id": "c1b3903dc9ba4477a34ceb592b696300",
    "figure_id": "2104.08894v1-Figure9-1",
    "image_file": "2104.08894v1-Figure9-1.png",
    "caption": " Validation of MLE estimates on synthetic daisy data with d̄ = 10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of k produces the most accurate estimate of the intrinsic dimension?",
    "answer": "k = 10",
    "rationale": "The figure shows that the estimated dimensionality converges to the true dimensionality (d̄ = 10) as the number of samples increases, and this convergence is most evident for k = 10.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.08894v1",
    "pdf_url": null
  },
  {
    "instance_id": "57e53bc44367428eb71883f6307f74c3",
    "figure_id": "2102.05245v1-Figure3-1",
    "image_file": "2102.05245v1-Figure3-1.png",
    "caption": " Overview of the DNN architecture computing the 32 gains ĝb and 32 strengths r̂b from the 100-dimensional input feature vector f . The number of units on each layer is indicated above the layer type.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many layers does the DNN have?",
    "answer": "6",
    "rationale": "The figure shows the DNN architecture with 6 layers. The first layer is a fully connected layer with 128 units, the second and third layers are convolutional layers with 512 units each, the fourth, fifth and sixth layers are GRU layers with 512, 128 and 32 units respectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.05245v1",
    "pdf_url": null
  },
  {
    "instance_id": "5a45f7b3e71b4d22a3b0dc58fa1577a2",
    "figure_id": "2111.03602v1-Figure7-1",
    "image_file": "2111.03602v1-Figure7-1.png",
    "caption": " LCE Framework applied to single-fidelity algorithms on NAS-Bench-111, NAS-Bench-311, NAS-Bench-NLP11, and NAS-Bench-201.",
    "figure_type": "** plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which algorithm achieves the lowest regret on NAS-Bench-111 CIFAR10? ",
    "answer": " BANANAS-WPM.",
    "rationale": " The figure shows the regret of different algorithms on NAS-Bench-111 CIFAR10. The regret of BANANAS-WPM is lower than the regret of all other algorithms at all times. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.03602v1",
    "pdf_url": null
  },
  {
    "instance_id": "f77c99fe156b4394a437d0a74e376d70",
    "figure_id": "1905.07512v3-Figure5-1",
    "image_file": "1905.07512v3-Figure5-1.png",
    "caption": " Qualitative comparison of Point-Nav policies on MP3D validation. An exemplar validation episode (fixed start and end location) and the predicted trajectories from baselines and SplitNet.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the highest success rate?",
    "answer": "SplitNet BC",
    "rationale": "The success rate is shown in the title of each subplot. SplitNet BC has the highest success rate of 0.938.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.07512v3",
    "pdf_url": null
  },
  {
    "instance_id": "37535ad03fd94920b0b8ef61d45df9cb",
    "figure_id": "2210.05499v2-Figure4-1",
    "image_file": "2210.05499v2-Figure4-1.png",
    "caption": " Answer F1 of the proposed CGSN model and two ablated models on the dev set of Qasper. In the left subfigure, we evaluate each model by the context document length in each data instance. On the right side, the dev set is partitioned via the number of tokens between the first and the last gold evidence paragraphs in the context document of each QA pair, called “Maximum Evidence Distance”.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best for documents with a length of 4k-8k tokens?",
    "answer": "The full model.",
    "rationale": "The figure shows that the full model has the highest Answer F1 score for documents with a length of 4k-8k tokens.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.05499v2",
    "pdf_url": null
  },
  {
    "instance_id": "c09c57336a084941bcea466336ea1ebc",
    "figure_id": "2211.15029v2-Figure3-1",
    "image_file": "2211.15029v2-Figure3-1.png",
    "caption": " BLEU scores on the LM1B test set. Left is better, lower is better. For GPT and Transformer decoder, we control quality-variation with sampling temperature. D3PM and DiffusionBERT are controlled by truncation sampling hyperparameter K.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the best BLEU score?",
    "answer": "Transformer Decoder",
    "rationale": "The figure shows the BLEU scores for different models. The Transformer Decoder model has the lowest BLEU score, which means it has the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.15029v2",
    "pdf_url": null
  },
  {
    "instance_id": "c123efa5cfcc4232bf6089ada2bcedad",
    "figure_id": "2203.08887v1-Figure10-1",
    "image_file": "2203.08887v1-Figure10-1.png",
    "caption": " Distribution of (a) NB301 predicted and (b) actual test error of archs sampled. Random: random archs without constraints; Skip: archs with residual links and otherwise randomly sampled; Prim: random archs using {s3, s5, skip} only. PrimSkip: archs satisfying both Skip and Prim.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampling method produced architectures with the lowest predicted test error?",
    "answer": "PrimSkip",
    "rationale": "The boxplot in (a) shows the distribution of predicted test error for each sampling method. The PrimSkip method has the lowest median and the smallest range of predicted test error, indicating that it produced architectures with the lowest predicted test error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.08887v1",
    "pdf_url": null
  },
  {
    "instance_id": "10497881f58041d19b2d3321c56e0d87",
    "figure_id": "2005.07099v1-Figure4-1",
    "image_file": "2005.07099v1-Figure4-1.png",
    "caption": " Experimental results of the antagonist attack.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment shows the greatest decrease in average return as the number of attack steps increases?",
    "answer": "Hopper",
    "rationale": "The Hopper environment shows a sharp decrease in average return from around 3000 to around 1000 as the number of attack steps increases from 1 to 5. This is the largest decrease observed in any of the environments shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.07099v1",
    "pdf_url": null
  },
  {
    "instance_id": "67484e3724984139b03df6f3af572796",
    "figure_id": "2106.05470v3-Figure1-1",
    "image_file": "2106.05470v3-Figure1-1.png",
    "caption": " (a)(b): Performance of 5 SSL tasks ranked best (1) to worst (5) by color on node clustering and classification, showing disparate performance across datasets and tasks. (c): Clustering performance heatmap on Citeseer when combining 2 SSL tasks, PAIRSIM and PAIRDIS, with different weights. (d) AUTOSSL’s search trajectory for task weights, achieving near-ideal performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which self-supervised learning (SSL) task performed the best on the Citeseer dataset for node clustering?",
    "answer": "PairSim",
    "rationale": "Figure (a) shows the performance of different SSL tasks on node clustering for different datasets. The color of each cell indicates the ranking of the task, with darker colors representing better performance. For the Citeseer dataset, the PairSim task has the darkest color, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05470v3",
    "pdf_url": null
  },
  {
    "instance_id": "5af8c70a7796407e821d02a45ef31aeb",
    "figure_id": "2104.06629v1-Figure1-1",
    "image_file": "2104.06629v1-Figure1-1.png",
    "caption": " Summary of existing back-propagation structures. Grads, Smooth grads, Integrated Grads, LRP, DTD, and PatternAttribution adopt the forward structure. Deconvnet adopts the backward structure, while GBP adopts the forward and backward structure.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which back-propagation structure uses both forward and backward structures?",
    "answer": "GBP.",
    "rationale": "The figure shows that GBP has both forward and backward ReLUs, indicating that it uses both structures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.06629v1",
    "pdf_url": null
  },
  {
    "instance_id": "08ac21fdc211404da44a6719b9522a78",
    "figure_id": "2111.06464v1-Figure26-1",
    "image_file": "2111.06464v1-Figure26-1.png",
    "caption": " Scramble with tile 16. Top panel: average value of metrics for various noise levels. The shaded area corresponds to bootstrapped 95%-confidence intervals for this estimator. Bottom panel: kernel density estimators for metrics and noise levels across seeds. Here topo stands for topographic similarity, conf for conflict count, cont for context independence, pos for positional disentanglement and acc for accuracy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric shows the highest variation across seeds at a noise level of 0.12?",
    "answer": "cont (context independence)",
    "rationale": "The bottom panel of the figure shows the kernel density estimators for each metric at different noise levels. At a noise level of 0.12, the cont metric has the widest distribution, indicating the highest variation across seeds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.06464v1",
    "pdf_url": null
  },
  {
    "instance_id": "6f0a7e978aa74011bd51716fdf753670",
    "figure_id": "2210.04222v2-Figure3-1",
    "image_file": "2210.04222v2-Figure3-1.png",
    "caption": " The SINR performances of CorInfoMax (ours), LD-InfoMax, PMF, ICA-InfoMax, NSM, and BSM, averaged over 100 realizations, (y-axis) with respect to the correlation factor ρ (x-axis). SINR vs. ρ curves for (a) nonnegative antisparse (B`∞,+), (b) antisparse (B`∞ ) source domains.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which source separation algorithm performs best for nonnegative antisparse sources when the correlation factor is 0.8?",
    "answer": "CorInfoMax",
    "rationale": "The figure shows the SINR performance of different source separation algorithms for different values of the correlation factor. For nonnegative antisparse sources, CorInfoMax has the highest SINR when the correlation factor is 0.8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.04222v2",
    "pdf_url": null
  },
  {
    "instance_id": "547f85ec62124803af39e1ff257c2323",
    "figure_id": "1906.01334v2-Figure3-1",
    "image_file": "1906.01334v2-Figure3-1.png",
    "caption": " Number of output template repetitions for the 20 most frequent templates (+STYLE has the fewest repetitions, i.e. it is the most varied).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods tested (+adj, +sent, +style) produced the most varied output?",
    "answer": "+style",
    "rationale": "The figure shows the number of output template repetitions for the 20 most frequent templates. The +style method has the fewest repetitions, indicating that it produced the most varied output.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.01334v2",
    "pdf_url": null
  },
  {
    "instance_id": "e6cd8ac097454ae390ee709b4b331497",
    "figure_id": "2111.01007v1-Figure5-1",
    "image_file": "2111.01007v1-Figure5-1.png",
    "caption": " Training progress on LSUN church at 2562 pixels. Shown are samples for a fixed noise vector z over k images. From top to bottom: FastGAN, StyleGAN2-ADA, Projected GAN.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models is the most successful at generating realistic images of churches?",
    "answer": "Projected GAN.",
    "rationale": "The images generated by Projected GAN are the most realistic and have the least amount of artifacts. The images generated by FastGAN and StyleGAN2-ADA are less realistic and have more artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.01007v1",
    "pdf_url": null
  },
  {
    "instance_id": "4c9246f65e0b47f08e229c95544900ca",
    "figure_id": "2206.05499v1-Figure5-1",
    "image_file": "2206.05499v1-Figure5-1.png",
    "caption": " Weight distributions of 2 graphs sampled from COLLAB.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the highest degree of connectivity?",
    "answer": "Layer 3.",
    "rationale": "The figure shows that Layer 3 has the most connections between nodes, as evidenced by the greater number of edges in the graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.05499v1",
    "pdf_url": null
  },
  {
    "instance_id": "7586cb0ad4af45c3b9506a6102660c71",
    "figure_id": "2109.12258v1-Figure3-1",
    "image_file": "2109.12258v1-Figure3-1.png",
    "caption": " Performance Change, WeeBit Data Size",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better when the data size is small?",
    "answer": "BERT-GB-T1",
    "rationale": "The plot shows that BERT-GB-T1 has a higher performance than both BERT and GB-T1 when the data size is small.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.12258v1",
    "pdf_url": null
  },
  {
    "instance_id": "4b545255fc124820aa4b8726f1baeb3d",
    "figure_id": "1907.04662v2-Figure9-1",
    "image_file": "1907.04662v2-Figure9-1.png",
    "caption": " Comparison of the algorithms’ state entropy in the Knight Quest environment with parameters ξ = 0.01, ζ = 1, N = 2500 (40 runs, 95% c.i.).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest state entropy?",
    "answer": "IDE^3 AL",
    "rationale": "The red line in the figure represents the state entropy of IDE^3 AL, which is consistently higher than the other two algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.04662v2",
    "pdf_url": null
  },
  {
    "instance_id": "3761bc34bf434259b083adb99e220a27",
    "figure_id": "2103.17258v3-Figure6-1",
    "image_file": "2103.17258v3-Figure6-1.png",
    "caption": " The learning curves of Table 3; ablation of Tanh transformation. A line that stopped in the middle means that its training has stopped at that step due to numerical error. We test SAC without tanh squashing, AWR with tanh, and MPO with tanh.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm consistently achieves the highest cumulative reward across all environments?",
    "answer": "MPO with tanh.",
    "rationale": "The figure shows the cumulative reward for different algorithms in different environments. The lines for MPO with tanh are consistently higher than the lines for other algorithms in all environments.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.17258v3",
    "pdf_url": null
  },
  {
    "instance_id": "14d7cfa47ac34113bb4a8539f840ed8c",
    "figure_id": "1908.06917v1-Figure3-1",
    "image_file": "1908.06917v1-Figure3-1.png",
    "caption": " Directed relation example (dbp:doctoralStudents and dbp:doctoralAdvisor hierarchy) that requires modeling directionality of the relation. LC-QuAD question #3267: “Name the scientist whose supervisor also supervised Mary Ainsworth?” (correct answer: Abraham Maslow) can be easily confusedwith a question: “Name the scientist who supervised also the supervisor of Mary Ainsworth?” (correct answer: Lewis Terman). LC-QuAD benchmark is not suitable for evaluating directionality interpretations, since only 35 questions (3.5%) of the LC-QuAD test split use relations of this type, which explains high performance results ofQAmp that treats all relation as undirected.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, who was the doctoral advisor of Abraham Maslow?",
    "answer": "Harry Harlow",
    "rationale": "The figure shows a directed relationship between Harry Harlow and Abraham Maslow, with an arrow pointing from Harlow to Maslow. This indicates that Harlow was Maslow's doctoral advisor.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.06917v1",
    "pdf_url": null
  },
  {
    "instance_id": "41fe5abf2e674e62af7726e210be86c8",
    "figure_id": "1904.06062v1-Figure6-1",
    "image_file": "1904.06062v1-Figure6-1.png",
    "caption": " Sensitivity results on the accuracy of Ci.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in terms of accuracy for both Ci and Cu?",
    "answer": "SD",
    "rationale": "The SD model has the highest accuracy for both Ci and Cu, as shown by the blue line being the highest on the graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.06062v1",
    "pdf_url": null
  },
  {
    "instance_id": "fb90e1a681154c1a8ba04845fd607beb",
    "figure_id": "2207.10276v4-Figure5-1",
    "image_file": "2207.10276v4-Figure5-1.png",
    "caption": " Comparison of clean sample selection on CIFAR-10 and CIFAR-100 dataset with 90% symmetric label noise. The threshold of DivideMix and the forget rate of JoCoR have been re-tuned to get the best results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest precision on CIFAR-100 with 90% symmetric label noise?",
    "answer": "ProMix.",
    "rationale": "The bar for ProMix is the highest for the precision metric in Figure (b).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.10276v4",
    "pdf_url": null
  },
  {
    "instance_id": "ad81678a79d94faea81e489badd8d401",
    "figure_id": "2110.15688v1-Figure1-1",
    "image_file": "2110.15688v1-Figure1-1.png",
    "caption": " Policy simplex for a three-armed bandit. The shaded area is the optimistic set Ptφ. The blue dot is the TS policy and the red dot is the VBOS policy.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which policy is more optimistic about the expected reward of arm 1, TS or VBOS?",
    "answer": "VBOS",
    "rationale": "The figure shows the optimistic set for each policy, which is the set of all possible expected reward vectors that are consistent with the observed data. The VBOS optimistic set is larger than the TS optimistic set, which means that VBOS is more optimistic about the expected reward of arm 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.15688v1",
    "pdf_url": null
  },
  {
    "instance_id": "939dad20cd514f84a32b9db059f2340b",
    "figure_id": "2210.17409v2-Figure2-1",
    "image_file": "2210.17409v2-Figure2-1.png",
    "caption": " The top-1 accuracy difference between “off-theshelf” pre-trained models on 4 down-stream tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pre-trained model performed the best on the CUB dataset?",
    "answer": "Swin-T In1k",
    "rationale": "The figure shows the top-1 accuracy difference between different pre-trained models on 4 down-stream tasks. The Swin-T In1k model has the highest accuracy difference on the CUB dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.17409v2",
    "pdf_url": null
  },
  {
    "instance_id": "33ac44f6a7b446eaafc8dec411a552de",
    "figure_id": "2104.06521v3-Figure3-1",
    "image_file": "2104.06521v3-Figure3-1.png",
    "caption": " Training curves (n-score vs.training progress) of the 8 comparison methods in one plot. Each curve is a mean of a method’s n-score curves over all the 14 tasks, where the method is run with 3 random seeds on each task. See Figure 8 and Figure 9 (Appendix K) for the complete set of individual training curves.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the highest normalized score at the end of training?",
    "answer": "TAAAC (1td)",
    "rationale": "The black line representing TAAAC (1td) is the highest among all the curves at the end of the training progress (x-axis = 1.0).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.06521v3",
    "pdf_url": null
  },
  {
    "instance_id": "fc11561e2e2c451ca61eaf139f314fa1",
    "figure_id": "2004.14990v5-Figure11-1",
    "image_file": "2004.14990v5-Figure11-1.png",
    "caption": " Learning curves of PPO and RAD in the modified Coinrun. The solid line and shaded regions represent the mean and standard deviation, respectively, across three runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which augmentation technique performed the best in terms of learning efficiency?",
    "answer": "RAD (flip)",
    "rationale": "The learning curve for RAD (flip) shows the highest score after 200 million timesteps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14990v5",
    "pdf_url": null
  },
  {
    "instance_id": "3c8e0354b7a843eeba71a6daa5f69b74",
    "figure_id": "2005.08455v1-Figure4-1",
    "image_file": "2005.08455v1-Figure4-1.png",
    "caption": " Training curves of the proposed soft-balance sampling. Soft-balance with λ = 0.7 achieves the best performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of lambda produces the highest mAP?",
    "answer": "Lambda = 0.7",
    "rationale": "The figure shows that the training curve for lambda = 0.7 is the highest, indicating that this value of lambda produces the highest mAP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.08455v1",
    "pdf_url": null
  },
  {
    "instance_id": "f426a2cc6a67440db4969a5300accd1d",
    "figure_id": "2202.02643v1-Figure3-1",
    "image_file": "2202.02643v1-Figure3-1.png",
    "caption": " Uncertainty estimation (ECE). The experiments are conducted with various models on CIFAR-10. Lower ECE values represent better uncertainty estimation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture and sparsity level achieve the lowest ECE?",
    "answer": "ResNet-110 with a sparsity level of 0.9.",
    "rationale": "The plot shows that the ECE decreases as the sparsity level increases, and that ResNet-110 has the lowest ECE among all the models tested.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.02643v1",
    "pdf_url": null
  },
  {
    "instance_id": "4df779f87be34c2da51cdf84de67dd00",
    "figure_id": "1905.13288v3-Figure3-1",
    "image_file": "1905.13288v3-Figure3-1.png",
    "caption": " Example qualitative results.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is better at removing noise from images, c-Glow or DnCNN?",
    "answer": "DnCNN is better at removing noise from images.",
    "rationale": "The figure shows that DnCNN produces images that are closer to the ground truth than c-Glow. For example, in the image of the airplane, DnCNN is able to remove the noise from the wings and fuselage, while c-Glow leaves some noise behind.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.13288v3",
    "pdf_url": null
  },
  {
    "instance_id": "aab8b80b6ffc4a6a839809e222030e47",
    "figure_id": "2111.06316v1-Figure2-1",
    "image_file": "2111.06316v1-Figure2-1.png",
    "caption": " Spectrograms for an utterance pronounced by a male speaker in Voice Bank (no.232) contaminated with Cafe background provided by DEMAND at SNR level 0 dB and its clean reference and enhanced versions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods was most effective at removing noise from the utterance?",
    "answer": "DAT",
    "rationale": "The DAT enhanced spectrogram most closely resembles the clean spectrogram, with the least amount of noise visible.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.06316v1",
    "pdf_url": null
  },
  {
    "instance_id": "154334897f6a44078a93c8b2395c3a3d",
    "figure_id": "2005.01119v1-Figure3-1",
    "image_file": "2005.01119v1-Figure3-1.png",
    "caption": " Increase of information ∆I in three scenarios: syntactic split, topic split and random split.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of split resulted in the largest increase in information?",
    "answer": "Syntactic split.",
    "rationale": "The figure shows that the blue squares, which represent the syntactic split, are generally higher than the other two types of splits.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.01119v1",
    "pdf_url": null
  },
  {
    "instance_id": "0eb2ea43bba04c76a74a9976b9183564",
    "figure_id": "2002.04632v2-Figure5-1",
    "image_file": "2002.04632v2-Figure5-1.png",
    "caption": " The objective function value as a function of the accumulated number of simulator calls for (a) Nonlinear Submanifold Three Hump problem, ψ ∈ R40, (b) Neural Network Weights Optimization problem , ψ ∈ R91.",
    "figure_type": "\"other\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the passage and caption, which problem is more challenging for L-GSO to optimize, the Nonlinear Submanifold Three Hump problem or the Neural Network Weights Optimization problem?",
    "answer": "The Neural Network Weights Optimization problem.",
    "rationale": "The caption states that the Neural Network Weights Optimization problem has a higher dimensionality (ψ ∈ R91) compared to the Nonlinear Submanifold Three Hump problem (ψ ∈ R40). Higher dimensionality generally increases the difficulty of optimization problems.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.04632v2",
    "pdf_url": null
  },
  {
    "instance_id": "47b05aae769940aaa1bd8e25acf01323",
    "figure_id": "2105.14039v3-Figure11-1",
    "image_file": "2105.14039v3-Figure11-1.png",
    "caption": " Comparison of HCAM (labeled as HTM) with different chunk sizes to TrXL across the different ballet levels. The performance of the HCAM model is robust to varying chunk size, indicating that HCAM does not need a task-relevant segmentation to perform well. The results reported in the main text use chunk size 32; panels a, c, and f correspond to main text Fig. 3. These comparisons were run before a minor bug was fixed in HCAM memory writing. Results are from three seeds in each condition.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the 2-dances, short delays task?",
    "answer": "TrXL",
    "rationale": "The figure shows the accuracy of different models on the 2-dances, short delays task. The TrXL model has the highest accuracy, as its line is consistently above the other lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14039v3",
    "pdf_url": null
  },
  {
    "instance_id": "b946a4c90b1f4eeb9de55a2106ec6300",
    "figure_id": "2002.11005v1-Figure1-1",
    "image_file": "2002.11005v1-Figure1-1.png",
    "caption": " The upper bound on the error given by (3) as a function of time, evaluated for k = 1, 2, 3, 4, 5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest error at the end of the experiment?",
    "answer": "Adaptive fastest-k",
    "rationale": "The figure shows the error as a function of time for different methods. The adaptive fastest-k method has the lowest error at the end of the experiment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11005v1",
    "pdf_url": null
  },
  {
    "instance_id": "63b08662094640d49c65deea266570f4",
    "figure_id": "2101.06061v1-Figure9-1",
    "image_file": "2101.06061v1-Figure9-1.png",
    "caption": " Cylindrical \"spike\" of height h and radius ρ inside the ball B(x, r).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the cylinder as ρ approaches 0?",
    "answer": "The cylinder becomes infinitely thin.",
    "rationale": "The figure shows a cylinder with radius ρ inside a ball. The caption states that ρ approaches 0, which means that the cylinder's radius becomes smaller and smaller. As the radius approaches 0, the cylinder becomes infinitely thin.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.06061v1",
    "pdf_url": null
  },
  {
    "instance_id": "b24756152b0c47f9a7340d0c92632a3e",
    "figure_id": "2103.02458v2-Figure17-1",
    "image_file": "2103.02458v2-Figure17-1.png",
    "caption": " Robustness to weight initialization. Inception scores on CIFAR-10 with different methods and initial weight sets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most robust to weight initialization?",
    "answer": "Ours",
    "rationale": "The figure shows that the \"Ours\" method has the highest Inception score for all five initial weight sets, which indicates that it is the most robust to weight initialization.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.02458v2",
    "pdf_url": null
  },
  {
    "instance_id": "884027aaa02f44a1a12afd967b5b3dca",
    "figure_id": "2106.09297v1-Figure1-1",
    "image_file": "2106.09297v1-Figure1-1.png",
    "caption": " Overview of the product search system in Taobao. The head of each circle denotes different phase. The bottom is the scale of the corresponding candidate set.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which phase of the Taobao product search system is the candidate set reduced from tens of thousands to thousands?",
    "answer": "Rank",
    "rationale": "The figure shows that the Rank phase is the stage where the candidate set is reduced from tens of thousands to thousands.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.09297v1",
    "pdf_url": null
  },
  {
    "instance_id": "80f53733c77f43d0bf8d03e72eb63770",
    "figure_id": "2110.08896v2-Figure1-1",
    "image_file": "2110.08896v2-Figure1-1.png",
    "caption": " Learning curves of DuelingDQN, DuelingDQN-RAA and our approach DuelingDQNStable AA on SpaceInvaders, Enduro, Breakout, and CrazyClimber games over 3 seeds. Shaded region corresponds to the standard deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three algorithms performs the best on the Crazy Climber game?",
    "answer": "DuelingDQN_StableAA(ours)",
    "rationale": "The figure shows that the DuelingDQN_StableAA(ours) algorithm achieves the highest average return on the Crazy Climber game. This can be seen by looking at the red line, which is consistently higher than the other two lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.08896v2",
    "pdf_url": null
  },
  {
    "instance_id": "f45b04024d8c4ba783a52c63840d1be5",
    "figure_id": "2012.01988v6-Figure5-1",
    "image_file": "2012.01988v6-Figure5-1.png",
    "caption": " Cascades of EfficientNet, ResNet, MobileNetV2 or ViT models on ImageNet. Compared with single models, cascades can obtain a higher accuracy with similar cost (red squares) or achieve a significant speedup while being equally accurate (green squares; e.g., 5.4x speedup for B7). The benefit of cascades generalizes to all four architecture families and all computation regimes. Numerical results are also available in Table 13&14 in appendix.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture family achieves the highest accuracy with the smallest computation cost?",
    "answer": "EfficientNet",
    "rationale": "The figure shows that EfficientNet models achieve the highest accuracy for a given computation cost compared to ResNet, MobileNetV2, and ViT. This is evident from the red squares in the figure, which represent models with similar accuracy but different computation costs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.01988v6",
    "pdf_url": null
  },
  {
    "instance_id": "7da84a5b2fb24dd988dfa0eaf9983fea",
    "figure_id": "2012.07654v3-Figure3-1",
    "image_file": "2012.07654v3-Figure3-1.png",
    "caption": " MRR of PrefXMRtree (c-8 in Table 2) and baselines for different prefix lengths on AOL search logs test set where next query is seen at train time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the AOL search logs test set when the next query is seen at train time?",
    "answer": "PREFIX XMR TREE",
    "rationale": "The figure shows the Mean Reciprocal Rank (MRR) of four different models on the AOL search logs test set. The PREFIX XMR TREE model has the highest MRR for all prefix lengths, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.07654v3",
    "pdf_url": null
  },
  {
    "instance_id": "e303c8f3d3af48d5a2628fbcec1b879d",
    "figure_id": "2211.16494v5-Figure8-1",
    "image_file": "2211.16494v5-Figure8-1.png",
    "caption": " Comparison of GNN accuracies following sparsification of input edges — WIS, the edge sparsification algorithm brought forth by our theory (Algorithm 1), markedly outperforms alternative methods. This figure supplements Figure 3 from Section 5.2 by including experiments with: (i) a depth L = 3 GIN over the Cora, DBLP, and OGBN-ArXiv datasets; (ii) a depth L = 10 ResGCN over the Cora, DBLP, and OGBN-ArXiv datasets; and (iii) a depth L = 3 GCN over the Chameleon, Squirrel, and Amazon Computers datasets. Markers and error bars report means and standard deviations, respectively, taken over ten runs per configuration for GCN and GIN, and over five runs per configuration for ResGCN (we use fewer runs due to the larger size of ResGCN). For further details see caption of Figure 3 as well as Appendix G.2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sparsification method performs best on the Cora dataset with a depth 3 GIN?",
    "answer": "1-WIS (ours)",
    "rationale": "The figure shows that the 1-WIS (ours) method achieves the highest test accuracy for all percentages of removed edges on the Cora dataset with a depth 3 GIN.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.16494v5",
    "pdf_url": null
  },
  {
    "instance_id": "1dc3696965824d459e7d0e1e12facba3",
    "figure_id": "2211.13524v1-Figure8-1",
    "image_file": "2211.13524v1-Figure8-1.png",
    "caption": " Results on real-world degradation. We can see that GLEAN tends to replicate the degradation that GT suffers, while PDN is not affected and tends to generate clear results. Note PDN only uses 16× bicubic(antialias) to synthesize LRs for training, without any simulated degradation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three images is the most realistic?",
    "answer": "PDN(Ours)",
    "rationale": "The caption states that GLEAN tends to replicate the degradation that GT suffers, while PDN is not affected and tends to generate clear results. This can be seen in the images, where PDN(Ours) has the sharpest and most realistic appearance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.13524v1",
    "pdf_url": null
  },
  {
    "instance_id": "79a1c9cca0b24bb6ab8e110d5f6e7559",
    "figure_id": "2201.06025v2-Figure9-1",
    "image_file": "2201.06025v2-Figure9-1.png",
    "caption": " For the sensitive words used in keyword matching, we analyzed their occurrences in the test set of COLDATASET. The y-axis denotes the number of texts containing the keywords.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which keyword is most frequently used in the test set of COLDATASET?",
    "answer": "\"死于\" (Death)",
    "rationale": "The bar for \"死于\" is the highest in the figure, indicating that it appears in the most texts in the test set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.06025v2",
    "pdf_url": null
  },
  {
    "instance_id": "73883e76967545dba1f0c26f7ee10b4f",
    "figure_id": "1811.12752v1-Figure2-1",
    "image_file": "1811.12752v1-Figure2-1.png",
    "caption": " Power of different tests with increase number of verticesn, and for rank parameter r = 2, 4. The dotted line under null hypothesis corresponds to the significance level of 5%.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of power for r = 4 and n = 1000?",
    "answer": "Asymp-TW",
    "rationale": "The figure shows the power of different tests with increasing number of vertices n, and for rank parameter r = 2, 4. The Asymp-TW method has the highest power for r = 4 and n = 1000.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.12752v1",
    "pdf_url": null
  },
  {
    "instance_id": "1438cf8ad23f4f1f8959135fe5cbe697",
    "figure_id": "2211.10549v1-Figure2-1",
    "image_file": "2211.10549v1-Figure2-1.png",
    "caption": " A conceptual view of feature ordering using maximum spanning tree. Left: A toy tabular dataset containing four instances and seven features; Middle: Correlation between features (Lighter color denotes stronger correlation); Right: maximum spanning tree constructed using feature correlation.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two features are most correlated?",
    "answer": "Features f1 and f3.",
    "rationale": "The correlation matrix shows the correlation between each pair of features. The lighter the color, the stronger the correlation. The cell corresponding to features f1 and f3 is the lightest, indicating that these two features are most correlated.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.10549v1",
    "pdf_url": null
  },
  {
    "instance_id": "56472836bc864920a8c0a212cf396c9a",
    "figure_id": "1904.06145v2-Figure3-1",
    "image_file": "1904.06145v2-Figure3-1.png",
    "caption": " (a) The competing KL divergence terms that the encoder assigns for x (training samples) and x̂ (generated samples) in PIONEER show two pathological properties. Relatively early on, the encoder overpowers the decoder leading the terms to diverge from each other, and the dynamic range of each grows, leading to an upper bound on the learning capacity. (b) The individual contribution of our changes, shown for KL divergence terms for early stages of CELEBA-HQ training. FID (blue dots) is shown for 128×128 stage onwards (with stage fade-in starting at 16.52M samples). PixelNorm (PN) or Equalized Learning Rate (EQLR) alone or in combination (top) will lead to poor FID even with KL margin (bottom left). On the other hand, merely adding Spetral Normalization (SN) without KL margin will lead to divergence of the terms and training collapse (bottom middle). Adding SN with the margin produces the optimal outcome (bottom right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which normalization technique leads to the best performance in terms of FID?",
    "answer": "Spectral Normalization (SN) with KL margin.",
    "rationale": "The bottom right plot in Figure (b) shows that the combination of SN and KL margin results in the lowest FID values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.06145v2",
    "pdf_url": null
  },
  {
    "instance_id": "32a296c70e3a42c6b59386e8a5f3a332",
    "figure_id": "2211.15114v1-Figure8-1",
    "image_file": "2211.15114v1-Figure8-1.png",
    "caption": " Recall@1000 for varying embedding size for link prediction.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the largest improvement in recall with increasing embedding size?",
    "answer": "Citeseer.",
    "rationale": "The recall values for Citeseer increase more steeply than the other datasets as the embedding size increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.15114v1",
    "pdf_url": null
  },
  {
    "instance_id": "f53ba60d49ad4caebd359303db8a4a0b",
    "figure_id": "2104.05700v1-Figure1-1",
    "image_file": "2104.05700v1-Figure1-1.png",
    "caption": " SNMT vs UNMT MACROF1 on the most frequent 500 types. UNMT outperforms SNMT on frequent types that are weighed heavily by BLEU however, SNMT is generally better than UNMT on rare types; hence, SNMT has a higher MACROF1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does SNMT compare to UNMT on frequent types?",
    "answer": "UNMT outperforms SNMT on frequent types.",
    "rationale": "The figure shows the MacroF1 percentiles for SNMT and UNMT on the most frequent 500 types. UNMT has a higher MacroF1 percentile than SNMT on all of the types shown, which indicates that UNMT performs better on frequent types.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.05700v1",
    "pdf_url": null
  },
  {
    "instance_id": "85bb57378399452db8ebd6b9f1376426",
    "figure_id": "2303.09119v2-Figure3-1",
    "image_file": "2303.09119v2-Figure3-1.png",
    "caption": " Visualization Results of Our DiffGesture on Two Datasets. Three cases are picked up, where (i) and (ii) are TED Expressive cases, and (iii) is a TED Gesture case. We highlight dull cases generated by comparison methods with rectangles, indicating the mode collapse phenomenon of baselines.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method generated the most accurate gestures for the TED Expressive cases?",
    "answer": "DiffGesture",
    "rationale": "The figure shows the ground truth gestures and the gestures generated by different methods. For the TED Expressive cases, DiffGesture's generated gestures are closest to the ground truth gestures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.09119v2",
    "pdf_url": null
  },
  {
    "instance_id": "d4ad9e5277594cf79cc1d1b57782b470",
    "figure_id": "2211.14842v1-Figure7-1",
    "image_file": "2211.14842v1-Figure7-1.png",
    "caption": " More samples of the image caption task based on CUB-200 dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which bird has a mohawk-shaped feather on its head?",
    "answer": "The medium sized bird with grey feathers.",
    "rationale": "The passage describes the bird as having \"a mohawk-shaped distinctive feature on the head, of black feathers protruding from the head.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.14842v1",
    "pdf_url": null
  },
  {
    "instance_id": "02d7153e24634420bfc298b7c3d383e7",
    "figure_id": "2211.00918v1-Figure6-1",
    "image_file": "2211.00918v1-Figure6-1.png",
    "caption": " Comparison with WACNN [57], which is the baseline method that does not perform adaptive optimization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves higher PSNR for all image types?",
    "answer": "Ours.",
    "rationale": "The plots show that the orange line (\"Ours\") is consistently above the black line (\"WACNN [57]\") for all four image types. This indicates that our method achieves higher PSNR than WACNN [57] for all image types.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.00918v1",
    "pdf_url": null
  },
  {
    "instance_id": "c90f9efc27194a65bf8f08726633ef8e",
    "figure_id": "2305.05221v2-Figure3-1",
    "image_file": "2305.05221v2-Figure3-1.png",
    "caption": " The regret (showing the gap to the optimal value) per communication round of BARA.",
    "figure_type": "** plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which dataset shows the fastest convergence to the optimal value?",
    "answer": " FMNIST.",
    "rationale": " The plot for FMNIST shows that the regret decreases most rapidly and reaches a lower value than the other datasets. This indicates that BARA converges to the optimal value faster for FMNIST.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.05221v2",
    "pdf_url": null
  },
  {
    "instance_id": "f6b2ae4f94e24659a2371b69901eebad",
    "figure_id": "2011.02008v1-Figure1-1",
    "image_file": "2011.02008v1-Figure1-1.png",
    "caption": " SDR of separated singing voice (left) and accompaniment (right) with mixture phase versus clean phase.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does the SDR of the separated singing voice and accompaniment generally increase or decrease when using the clean phase instead of the mixture phase?",
    "answer": "Increase.",
    "rationale": "The plots show that the SDR values for both the singing voice and accompaniment are generally higher for the clean phase (solid circles) than for the mixture phase (x's).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.02008v1",
    "pdf_url": null
  },
  {
    "instance_id": "db85d855c7204152b82957e9a18fe37e",
    "figure_id": "1805.07674v3-Figure14-1",
    "image_file": "1805.07674v3-Figure14-1.png",
    "caption": " An 8-vertices complete graph can be decomposed into 7 perfect matchings",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many perfect matchings are there in a complete graph with 8 vertices?",
    "answer": "7",
    "rationale": "The figure shows a complete graph with 8 vertices, and the edges are colored to represent 7 perfect matchings.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.07674v3",
    "pdf_url": null
  },
  {
    "instance_id": "8d4c24bd5e064f12843f8df6d855b81f",
    "figure_id": "2204.00862v2-Figure2-1",
    "image_file": "2204.00862v2-Figure2-1.png",
    "caption": " Pearson correlation on the generated results from four generation models in the task of topiccontrolled text generation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which generation model performed the best in terms of consistency?",
    "answer": "CTRL",
    "rationale": "The figure shows that CTRL has the highest Pearson correlation coefficient for consistency.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.00862v2",
    "pdf_url": null
  },
  {
    "instance_id": "63ba960233c34e49a14e1e9ef7e8055a",
    "figure_id": "2211.01169v1-Figure1-1",
    "image_file": "2211.01169v1-Figure1-1.png",
    "caption": " MIMO system decomposition into parallel MISO systems",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many MISO systems are formed by the decomposition of the MIMO system?",
    "answer": "Three.",
    "rationale": "The figure shows three circles, each representing a MISO system. Each MISO system consists of a base station and several mobile users.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.01169v1",
    "pdf_url": null
  },
  {
    "instance_id": "cb565995abe049409db69058b23c4a05",
    "figure_id": "1902.05872v3-Figure11-1",
    "image_file": "1902.05872v3-Figure11-1.png",
    "caption": " Detection result for flow-based method showing correctly detected car u-turn.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the car doing in the image?",
    "answer": "The car is making a U-turn.",
    "rationale": "The car is positioned perpendicular to the flow of traffic, and its front wheels are turned towards the opposite direction of travel. This indicates that the car is in the process of making a U-turn.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.05872v3",
    "pdf_url": null
  },
  {
    "instance_id": "7b8eeb2df6fe4332b42b30837aec258d",
    "figure_id": "1812.01285v1-Figure3-1",
    "image_file": "1812.01285v1-Figure3-1.png",
    "caption": " Examples of mask estimation results on the PCD dataset. From top to bottom, the figures in the columns b and d shows the result of “Under samp.”, “Transfer”, and “VLAE w/ sim. (ours)”, respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method appears to be the most accurate in estimating the change mask?",
    "answer": "VLAE w/ sim. (ours)",
    "rationale": "The figures in column (d) show the estimated change mask for each method. The VLAE w/ sim. (ours) method appears to be the most accurate, as its estimated change mask closely matches the ground truth change mask shown in column (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.01285v1",
    "pdf_url": null
  },
  {
    "instance_id": "1c081bf550a94a0785a0b8231a32642b",
    "figure_id": "2107.00833v1-Figure18-1",
    "image_file": "2107.00833v1-Figure18-1.png",
    "caption": " Side by side comparison of baseline and maximum discovery across user gender (left 3 panels) and availability across artist gender (rightmost panel). Reachability evaluated on ML-1M and LastFM with LibFM model, K = 10, different action spaces, and β = 2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which action space resulted in the highest percentage of items with items above random p?",
    "answer": "The \"Next k\" action space.",
    "rationale": "The leftmost panel of the figure shows the results for the \"Next k\" action space. The violin plot for the \"max\" condition is higher than the violin plot for the \"baseline\" condition, indicating that the \"max\" condition resulted in a higher percentage of items with items above random p.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.00833v1",
    "pdf_url": null
  },
  {
    "instance_id": "addf419e6aeb43b0ac95cf552737379e",
    "figure_id": "2306.02330v1-Figure6-1",
    "image_file": "2306.02330v1-Figure6-1.png",
    "caption": " Test results in terms of Recall@20 and NDCG@20 w.r.t training epochs on Yelp dataset. The stars represent points of convergence. Compared with baselines, the faster convergence rate of our GFormer can be observed.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model converges the fastest?",
    "answer": "GFormer",
    "rationale": "The stars in the figure represent the points of convergence for each model. GFormer reaches its point of convergence before the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.02330v1",
    "pdf_url": null
  },
  {
    "instance_id": "221edfe1b68d4a308fdbab38fb540f93",
    "figure_id": "2307.02759v1-Figure6-1",
    "image_file": "2307.02759v1-Figure6-1.png",
    "caption": " Hyperparameter Study of KGRec.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Alibaba-Fashion dataset?",
    "answer": "Last-FM",
    "rationale": "The figure shows the change in recall for three different models on the Alibaba-Fashion dataset. Last-FM has the highest recall for all values of masking size, CL keep ratio, and CL temperature.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.02759v1",
    "pdf_url": null
  },
  {
    "instance_id": "018f2504482b4ec49a4f9c0d9de367d8",
    "figure_id": "1901.10848v1-Figure1-1",
    "image_file": "1901.10848v1-Figure1-1.png",
    "caption": " An illustration explaining that there are (posv(c)−1 i−1 ) · (m−posv(c) `−i ) possible sets S that satisfy c ∈ S, |S| = ` and c is ranked at position i in S by v. Each field indicates a candidate and the number below a field indicates the position. The yellow field indicates c, a red field indicates a candidate in S that was ranked after c by v and a green field indicates a candidate in S that was ranked before c by v. A white field indicates a candidate that was not ranked by v. In this example m = 11, i = 3, posv(c) = 6, and ` = 4.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many candidates are ranked after c by v in this example?",
    "answer": "1",
    "rationale": "The red field indicates a candidate in S that was ranked after c by v. There is only one red field in the image, so there is only one candidate ranked after c by v.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.10848v1",
    "pdf_url": null
  },
  {
    "instance_id": "c4b9bd4056744cba8070797c76bed191",
    "figure_id": "2207.01708v1-Figure4-1",
    "image_file": "2207.01708v1-Figure4-1.png",
    "caption": " The seen compositional actions correspond to VsNs (the upper left part), and unseen actions include VsNu, VuNs and VuNu (the rest). We visualize the scope of close / open / macro-open world settings.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four types of compositional actions are seen?",
    "answer": "VsNs.",
    "rationale": "The seen compositional actions correspond to VsNs (the upper left part), and unseen actions include VsNu, VuNs and VuNu (the rest). We visualize the scope of close / open / macro-open world settings.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.01708v1",
    "pdf_url": null
  },
  {
    "instance_id": "402cdf760b564aa28552c3b4d954fd49",
    "figure_id": "2212.01567v1-Figure9-1",
    "image_file": "2212.01567v1-Figure9-1.png",
    "caption": " Additional qualitative results against classical methods and other state-of-the-art photo-realistic stylization methods.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most realistic results?",
    "answer": "Our method.",
    "rationale": "The results produced by our method are visually more similar to the original images than the results produced by the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.01567v1",
    "pdf_url": null
  },
  {
    "instance_id": "1ab708ed7c2543819914d6afe99581b3",
    "figure_id": "2109.05433v1-Figure2-1",
    "image_file": "2109.05433v1-Figure2-1.png",
    "caption": " Gender bias analysis with different top-K results.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the lowest gender bias on the MS-COCO 1K Test Set?",
    "answer": "CLIP-clip.",
    "rationale": "The figure shows the gender bias of different models on three different test sets. The CLIP-clip model has the lowest gender bias on the MS-COCO 1K Test Set, as its line is the lowest on the plot for that test set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.05433v1",
    "pdf_url": null
  },
  {
    "instance_id": "ada37eab4d274c00b1efe20c41c759bd",
    "figure_id": "2009.01974v4-Figure7-1",
    "image_file": "2009.01974v4-Figure7-1.png",
    "caption": " FEDAVG with ConvNet on Dirichletnon-i.i.d CIFAR-10 with or without learning rate decay at latter rounds of communication. We experimented with different values of α in Dir(α).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the test accuracy of FEDAVG with ConvNet on Dirichlet non-i.i.d CIFAR-10 change with increasing α when global learning rate decay is used?",
    "answer": "The test accuracy increases with increasing α.",
    "rationale": "The plot shows that the test accuracy for FEDAVG with ConvNet on Dirichlet non-i.i.d CIFAR-10 with global learning rate decay (blue line) increases as α increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.01974v4",
    "pdf_url": null
  },
  {
    "instance_id": "bcec0613347e4a0ba0464658be941cc3",
    "figure_id": "2208.06124v1-Figure13-1",
    "image_file": "2208.06124v1-Figure13-1.png",
    "caption": " Additional experiments for P2 with varying values of K",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest loss for K = 5?",
    "answer": "UGC",
    "rationale": "The loss for UGC is the lowest for K = 5, as seen in the top left plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.06124v1",
    "pdf_url": null
  },
  {
    "instance_id": "6ca22500c2b24673ac5407c60bd289c1",
    "figure_id": "2004.07320v3-Figure6-1",
    "image_file": "2004.07320v3-Figure6-1.png",
    "caption": " Effect of Quantization on Model Structures. Results are shown on the validation set of Wikitext-103. (a) Quantizing Attention, FFN, and Embeddings in different order. (b) More Extreme compression of different structures.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three structures (Attention, Embeddings, FFN) is most sensitive to quantization?",
    "answer": "Attention",
    "rationale": "The left figure shows that when Attention is quantized first (attn emb ff), the valid perplexity increases the most compared to other orders of quantization. This suggests that Attention is more sensitive to quantization than the other two structures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.07320v3",
    "pdf_url": null
  },
  {
    "instance_id": "eea8bd80371d4546b50ee06b04b6ad63",
    "figure_id": "2003.05878v2-Figure8-1",
    "image_file": "2003.05878v2-Figure8-1.png",
    "caption": " Comparing the learning convergence to random options in the 4Rooms domain. The learning convergence depicting the average number of steps to goal for each learning episode. The solid line represents the mean value and the light colors represent the standard deviation. The start state is the bottom left corner, and the goal state is at the top right corner. The inset presents the same plot for 400 episodes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method learns the fastest in the 4Rooms domain?",
    "answer": "Diffusion options.",
    "rationale": "The plot shows the average number of steps to goal for each learning episode. The diffusion options method has the lowest average number of steps to goal, which means it learns the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.05878v2",
    "pdf_url": null
  },
  {
    "instance_id": "29869122537c419e82dbd2ace2183745",
    "figure_id": "1903.05789v2-Figure1-1",
    "image_file": "1903.05789v2-Figure1-1.png",
    "caption": " Validation of Theorem 4. (a) The red line shows the evolution of log γ, converging close to 0 during training as expected. The two blue curves compare the associated pixelwise reconstruction errors with γ fixed at 1 and with a learnable γ respectively. (b) The FID score obtained using reconstructed images from various VAE models (reconstructed image FID is another way of evaluating reconstruction quality; it is distinct from measuring generated sample quality via FID scores). In general, the VAE with learnable γ produces the best reconstructions as expected.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the VAE models has the best reconstruction quality according to the FID score?",
    "answer": "VAE with learnable γ",
    "rationale": "Figure (b) shows the reconstruction FID score for different VAE models on different datasets. The VAE with learnable γ has the lowest FID score for all datasets, indicating that it has the best reconstruction quality.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.05789v2",
    "pdf_url": null
  },
  {
    "instance_id": "d689bb62791a45358054350d46b57e7d",
    "figure_id": "1908.05054v2-Figure2-1",
    "image_file": "1908.05054v2-Figure2-1.png",
    "caption": " Dual Encoder architecture with late fusion. The model extracts a single visual feature vector from the entire image. Bounding boxes are ignored.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two encoders used in this architecture?",
    "answer": "ResNet-152 and BERT.",
    "rationale": "The figure shows two encoders, one for the image and one for the text. The image encoder is ResNet-152, and the text encoder is BERT.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.05054v2",
    "pdf_url": null
  },
  {
    "instance_id": "c73ed87853624819893473159b9dead0",
    "figure_id": "2210.02914v2-Figure4-1",
    "image_file": "2210.02914v2-Figure4-1.png",
    "caption": " The ratio of training samples of different learning strategies in each epoch.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which learning strategy utilizes the highest percentage of training samples in the first epoch?",
    "answer": "FT",
    "rationale": "The blue line representing FT is at 100% for the first epoch, which is higher than the other two lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.02914v2",
    "pdf_url": null
  },
  {
    "instance_id": "03381f536fad4175915c972a1a729d52",
    "figure_id": "2010.09132v3-Figure4-1",
    "image_file": "2010.09132v3-Figure4-1.png",
    "caption": " PESQ and STOI gains obtained by the SASEGAN-ls, 3 ≤ l ≤ 11 over the SEGAN baseline.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does SASEGAN-ls generally outperform SEGAN in terms of PESQ and STOI?",
    "answer": "Yes.",
    "rationale": "The figure shows that the PESQ and STOI values for SASEGAN-ls are generally higher than the baseline values for SEGAN.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.09132v3",
    "pdf_url": null
  },
  {
    "instance_id": "f53b6c7c3a294973af5ca67f2d8b9ad3",
    "figure_id": "2010.05134v2-Figure3-1",
    "image_file": "2010.05134v2-Figure3-1.png",
    "caption": " Demonstrations for the table lift task. Figure (a) shows the training trajectories in (x, y, z) of left and right grippers for 2500 demonstrations. One sample trajectory is shown in color to highlight the trajectory for each primitive, while the rest are grey. The black dot is the starting location. Figure (b) shows the task executed in our simulator. Each image represents the last step of each primitive. Videos of the demonstrations are provided in the supplementary material.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the six primitives involves the robot moving the table sideways?",
    "answer": "Move Sideways",
    "rationale": "Figure (b) shows the six primitives, and the second primitive is labeled \"Move Sideways.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.05134v2",
    "pdf_url": null
  },
  {
    "instance_id": "b12601baa2c0476abcbda9e1e6cd93df",
    "figure_id": "2208.12210v7-Figure10-1",
    "image_file": "2208.12210v7-Figure10-1.png",
    "caption": " RCD-learned model of MovieLens+ (Maier et al. 2013).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between users and movies?",
    "answer": "Users rate movies.",
    "rationale": "The figure shows that there is a directed edge from the USER entity to the RATES entity. This means that users can rate movies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.12210v7",
    "pdf_url": null
  },
  {
    "instance_id": "9e72f4e7cfea4cdcbd0e8a168bad2d3b",
    "figure_id": "2006.03829v3-Figure6-1",
    "image_file": "2006.03829v3-Figure6-1.png",
    "caption": " Speed of convergence in Retinopathy classifcation. Our models also converge faster in this task",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model converges the fastest in the Retinopathy classification task?",
    "answer": "The baseline 2D model.",
    "rationale": "The baseline 2D model reaches the highest validation accuracy first, and its curve is consistently above the other models' curves.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.03829v3",
    "pdf_url": null
  },
  {
    "instance_id": "451b83ca983943968ccf1acbaa5db50f",
    "figure_id": "2010.10151v1-Figure5-1",
    "image_file": "2010.10151v1-Figure5-1.png",
    "caption": " Figure 5a: Decision boundaries for class B of f+ when R1∩R2 = ∅. Figure 5b: Decision boundaries for class B of f+ when R1 ∩ R2 = R1. Figure 5c: Decision boundaries for class B of f+ when R1 ∩R2 6∈ {∅, R1}.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which figure shows the decision boundaries for class B of f+ when R1 ∩ R2 = R1?",
    "answer": "Figure 5b",
    "rationale": "The caption states that Figure 5b shows the decision boundaries for class B of f+ when R1 ∩ R2 = R1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.10151v1",
    "pdf_url": null
  },
  {
    "instance_id": "3a19c2bbd93c460ca9bfab0aa723c2d2",
    "figure_id": "2111.03874v1-Figure1-1",
    "image_file": "2111.03874v1-Figure1-1.png",
    "caption": " Joint density plots of accuracy vs. confidence to measure the calibration of classifiers on CIFAR-100-LT-100 during training. A well-calibrated classifier’s density will lay around the red dot line y = x, indicating prediction score reflects the actual likelihood of accuracy. mixup manages to regularize classifier on balanced datasets. However, both mixup and its extensions tend to be overconfident in LT scenarios. Our UniMix reconstructs a more balanced dataset and Bayias-compensated CE erases prior bias to ensure better calibration. Without loss of accuracy, either of proposed methods trains the same classifier more calibrated and their combination achieves the best. How to measure calibration and more visualization results are available in Appendix D.2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most overconfident?",
    "answer": "mixup and its extensions",
    "rationale": "The figure shows that mixup and its extensions tend to be overconfident in LT scenarios. This is because the density of their predictions is located above the red dotted line, indicating that the predicted confidence is higher than the actual accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.03874v1",
    "pdf_url": null
  },
  {
    "instance_id": "70d62bb9949846d59d7a8b66b8b1d15f",
    "figure_id": "2203.04176v3-Figure11-1",
    "image_file": "2203.04176v3-Figure11-1.png",
    "caption": " Evaluation of further variational objectives for the two moons (top) and the SLCP (bottom) task. Left: Variations of the forward KL (with and without self-normalized weights). Middle: Variations of the IW-ELBO (with and without STL). Right: Variations of the α-divergence (with and without STL as well as for different values of α.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which variational objective consistently performs best across both tasks and all variations?",
    "answer": "L_fKL (fVB)",
    "rationale": "The figure shows that L_fKL (fVB) consistently achieves the lowest C2ST values across both tasks and all variations of the variational objectives. This suggests that it is the most effective objective for these tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.04176v3",
    "pdf_url": null
  },
  {
    "instance_id": "600f87883b2f4199a1448474c47b97f5",
    "figure_id": "2005.14137v1-Figure4-1",
    "image_file": "2005.14137v1-Figure4-1.png",
    "caption": " The source and target images for online API experiments. All images are resized to 3 × 224 × 224. Image 4a is the target-image for both APIs. Image 4b is the source-image for attacking Face++ ‘compare’ API, and 4c the source-image for Azure ‘detect’ API.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image is used as the target image for both APIs?",
    "answer": "Image 4a",
    "rationale": "The caption states that \"Image 4a is the target-image for both APIs.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.14137v1",
    "pdf_url": null
  },
  {
    "instance_id": "b35ba7c9042741f2b255fd46feb63c1b",
    "figure_id": "1904.03746v6-Figure2-1",
    "image_file": "1904.03746v6-Figure2-1.png",
    "caption": " Perplexity of the different models grouped by sentence length on PTB.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on sentences of length 11-20?",
    "answer": "RNNLM",
    "rationale": "The figure shows that the RNNLM model has the lowest perplexity for sentences of length 11-20.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03746v6",
    "pdf_url": null
  },
  {
    "instance_id": "855a65f74f614b77a527f7eae75ed26a",
    "figure_id": "2110.08484v2-Figure1-1",
    "image_file": "2110.08484v2-Figure1-1.png",
    "caption": " Examples of VQA and Captioning tasks. In our setup, we convert the tasks into generative tasks in which models need to generate target text given input text and an image.",
    "figure_type": "photograph",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the breed of the dog in the image?",
    "answer": "The dog in the image is a dachshund.",
    "rationale": "The dog has the characteristic long, low body and short legs of a dachshund.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.08484v2",
    "pdf_url": null
  },
  {
    "instance_id": "9350aae66d9f4a0aacf27dab7b0167d7",
    "figure_id": "2112.07787v1-Figure8-1",
    "image_file": "2112.07787v1-Figure8-1.png",
    "caption": " SDE-APD of detectors with different output representations (T=0s).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which detector has the highest SDE-APD when T=0s?",
    "answer": "StarPoly",
    "rationale": "The figure shows the SDE-APD of different detectors when T=0s. The StarPoly detector has the highest bar in the figure, which indicates that it has the highest SDE-APD.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.07787v1",
    "pdf_url": null
  },
  {
    "instance_id": "9535efda9b694b79892d5c7523c34b12",
    "figure_id": "2305.18362v2-Figure5-1",
    "image_file": "2305.18362v2-Figure5-1.png",
    "caption": " Demonstration of feature selection results in the supervised setting (CelebA).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature is most correlated with the feature \"wearing a hat\"?",
    "answer": "\"Straight hair\"",
    "rationale": "The heatmap shows the correlation between different features. The color of the square at the intersection of two features indicates the strength of the correlation. The square at the intersection of \"wearing a hat\" and \"straight hair\" is dark purple, which indicates a strong positive correlation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.18362v2",
    "pdf_url": null
  },
  {
    "instance_id": "7721f0393dfe4d6fa1f0795cd9d0879a",
    "figure_id": "2009.05697v2-Figure6-1",
    "image_file": "2009.05697v2-Figure6-1.png",
    "caption": " The accuracy (mAP) and speed (FPS) comparison of YOLObile under different compression rate and different approaches.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest accuracy on the MS COCO dataset?",
    "answer": "Ours-14x (CPU&GPU)",
    "rationale": "The figure shows that Ours-14x (CPU&GPU) has the highest mAP (mean Average Precision) value, which is a measure of accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.05697v2",
    "pdf_url": null
  },
  {
    "instance_id": "f91f6590e0594341935717ab9cf16a5a",
    "figure_id": "2303.09650v2-Figure4-1",
    "image_file": "2303.09650v2-Figure4-1.png",
    "caption": " Trainability comparison of the IHT and ISS-P. The layerwise gradient L2-norm and variance in the pruning stage (1×105 iterations) and the first 1×105 iterations of the fine-tuning stage are plotted. We choose two representative layers, i.e., a fully connected layer (top) and a convolution (bottom) from the SwinIR.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, IHT or ISS-P, results in more stable gradients during the pruning stage?",
    "answer": "ISS-P.",
    "rationale": "The figure shows the gradient L2-norm and variance for both IHT and ISS-P during the pruning stage (first 1x10^5 iterations). For both the fully connected layer and the convolutional layer, the ISS-P curves are smoother and have lower variance than the IHT curves. This indicates that ISS-P produces more stable gradients during the pruning stage.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.09650v2",
    "pdf_url": null
  },
  {
    "instance_id": "ed8801a272784f649ee5d1614c9f64e5",
    "figure_id": "2210.13942v1-Figure7-1",
    "image_file": "2210.13942v1-Figure7-1.png",
    "caption": " Learning curves in terms of the win rate of EnDi and baselines in multi-agent RTFM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stage has the most consistent win rate for all methods?",
    "answer": "Stage 2",
    "rationale": "In stage 2, the win rate for all methods is relatively stable, with little variation over time. This can be seen in the relatively flat lines in the plot for stage 2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.13942v1",
    "pdf_url": null
  },
  {
    "instance_id": "48c8a3705e4c4a51b9d509ea3af9b4ba",
    "figure_id": "1906.02037v1-Figure7-1",
    "image_file": "1906.02037v1-Figure7-1.png",
    "caption": " NDCG@50 v.s. the amount of training data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Amazon dataset when the ratio of training data is 50%?",
    "answer": "Fac2T",
    "rationale": "The figure shows the NDCG@50 for different models on the Amazon and Yelp datasets. When the ratio of training data is 50%, the Fac2T model has the highest NDCG@50 on the Amazon dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02037v1",
    "pdf_url": null
  },
  {
    "instance_id": "8276250e66df44cf8d32d3d89bf93560",
    "figure_id": "2203.07519v2-Figure4-1",
    "image_file": "2203.07519v2-Figure4-1.png",
    "caption": " Results on varying training sizes. We test methods with different training sizes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on PIQA with the largest training size?",
    "answer": "CMCL + PSA + ANS.",
    "rationale": "The plot shows that the red line, which represents the CMCL + PSA + ANS model, achieves the highest accuracy on PIQA when the training size is 2000.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.07519v2",
    "pdf_url": null
  },
  {
    "instance_id": "d9488e9ea3594a87bb71c2524d3cf790",
    "figure_id": "2112.03257v1-Figure14-1",
    "image_file": "2112.03257v1-Figure14-1.png",
    "caption": " Sensitivity to σ. σ = 0.001 is a good default across all of these state-based environments. Results are averaged over 5 seeds, using a Fourier dimension of 1024, and the shaded region denotes 1 standard error.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of σ seems to be the best choice for all environments?",
    "answer": "σ = 0.001",
    "rationale": "The plot shows the average return for different values of σ. The average return is highest for σ = 0.001 in all environments.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.03257v1",
    "pdf_url": null
  },
  {
    "instance_id": "ea7db10a64484ac8bb7b9fd2ffb9997e",
    "figure_id": "2204.10532v1-Figure13-1",
    "image_file": "2204.10532v1-Figure13-1.png",
    "caption": " Performance metrics on black-box datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods has the best performance on black-box datasets?",
    "answer": "Our method (skell).",
    "rationale": "The figure shows that our method (skell) has the highest mean R2 and the lowest formula complexity and inference time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.10532v1",
    "pdf_url": null
  },
  {
    "instance_id": "e0d79156f2a74b04be499fb52faa4465",
    "figure_id": "2303.01384v1-Figure4-1",
    "image_file": "2303.01384v1-Figure4-1.png",
    "caption": " Spearman rank correlation between different disentanglement metrics and downstream accuracy of the abstract visual reasoning task (Van Steenkiste et al., 2019) after 6,000 training steps for FactorVAE and β-TCVAE. The same metrics as in Figure 3 are evaluated. Correlation for unsupervised metrics are more sensitive to model architecture than for supervised metrics. PIPE shows higher correlation with downstream performance than UDR and clearly outperforms the Rec baseline.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which disentanglement metric has the highest correlation with downstream accuracy for FactorVAE on the Shapes3D dataset?",
    "answer": "DCI",
    "rationale": "The figure shows the Spearman rank correlation between different disentanglement metrics and downstream accuracy for FactorVAE and β-TCVAE on the Shapes3D and AbstractDSprites datasets. The DCI metric has the highest correlation with downstream accuracy for FactorVAE on the Shapes3D dataset, with a correlation of 76.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.01384v1",
    "pdf_url": null
  },
  {
    "instance_id": "5682bde3e8be415497449ebc14f7cc08",
    "figure_id": "1905.13725v4-Figure3-1",
    "image_file": "1905.13725v4-Figure3-1.png",
    "caption": " Distribution shift on CIFAR-10",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on CIFAR-10 when the number of unsupervised samples is 32k?",
    "answer": "UAT++ with  um",
    "rationale": "The figure shows that the accuracy of UAT++ with  um is higher than the accuracy of the other two methods when the number of unsupervised samples is 32k.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.13725v4",
    "pdf_url": null
  },
  {
    "instance_id": "7938b864706f4fafbd87d1747a6d8d1b",
    "figure_id": "2111.00643v2-Figure6-1",
    "image_file": "2111.00643v2-Figure6-1.png",
    "caption": " DiscoNet qualitatively outperforms the state-of-the-art methods. Green and red boxes denote ground-truth and detection, respectively. (a) Output of when2com. (b) Output of V2VNet. (c) Output of DiscoNet. (d)-(f) Matrix-valued edge weights. (Ego agent: 1; neighbour agents: 2 and 3.)",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, When2com, V2VNet, or DiscoNet, performs the best in detecting other agents?",
    "answer": "DiscoNet",
    "rationale": "The figure shows the outputs of the three methods, with green boxes denoting ground-truth and red boxes denoting detection. DiscoNet correctly detects all of the other agents, while When2com and V2VNet miss some of them.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.00643v2",
    "pdf_url": null
  },
  {
    "instance_id": "fa2c460bf50d4ecd976141574ed0d4e7",
    "figure_id": "2305.13758v1-Figure3-1",
    "image_file": "2305.13758v1-Figure3-1.png",
    "caption": " The number of overlapping note onsets between piano and violin audios in the two datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of note pair has the highest number of overlapping onsets in the PFVN-synth dataset?",
    "answer": "The original pair.",
    "rationale": "The bar for the original pair in the PFVN-synth dataset is the highest, indicating that it has the most overlapping onsets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.13758v1",
    "pdf_url": null
  },
  {
    "instance_id": "92e20210377941e0a9ca42279edd35be",
    "figure_id": "2106.06295v2-Figure5-1",
    "image_file": "2106.06295v2-Figure5-1.png",
    "caption": " Rel. improvements in test scores obtained by the feedforward baseline compared to LSTM after 50 M env. steps.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which game environment showed the least improvement when using the LSTM model compared to the feedforward baseline?",
    "answer": "Alien",
    "rationale": "The bar chart shows the relative improvements in test scores obtained by the feedforward baseline compared to LSTM after 50 M env. steps. The game environment with the lowest bar is Alien, indicating the least improvement.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.06295v2",
    "pdf_url": null
  },
  {
    "instance_id": "56e6edcc4d2540c09362b3e515e06897",
    "figure_id": "2108.13073v1-Figure3-1",
    "image_file": "2108.13073v1-Figure3-1.png",
    "caption": " Comparing convergence of the overall best randomly initialized (blue) and pre-trained (green) models on ReVerb45K and FB15K237. Pre-trained models converge in fewer training steps despite a smaller learning rate.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model converges faster, the randomly initialized model or the pre-trained model?",
    "answer": "The pre-trained model converges faster.",
    "rationale": "The figure shows that the pre-trained model (green line) reaches a higher MRR in fewer training steps than the randomly initialized model (blue line) on both datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.13073v1",
    "pdf_url": null
  },
  {
    "instance_id": "8b9008530cbf425f972aa44b09c0f20d",
    "figure_id": "2108.05884v1-Figure9-1",
    "image_file": "2108.05884v1-Figure9-1.png",
    "caption": " KL divergence of the generated dataset from test dataset, which compares the object count distribution (number of instances of a particular object category per scene) for each object category",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which object category has the largest difference in object count distribution between the generated and test datasets?",
    "answer": " \"Window\"",
    "rationale": " The figure shows the KL divergence for each object category, which measures the difference in distribution between the generated and test datasets. The object category with the highest KL divergence value is \"window\", indicating the largest difference in object count distribution.\n\n**Figure type:** Plot",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.05884v1",
    "pdf_url": null
  },
  {
    "instance_id": "5715af8752bc40e9aa1fba6e11389038",
    "figure_id": "2205.07295v1-Figure3-1",
    "image_file": "2205.07295v1-Figure3-1.png",
    "caption": " The number of field values selecting 5 or 20 bins.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which bin selection method (5 bins or 20 bins) resulted in a greater number of field values for groups 0, 1, and 2?",
    "answer": "Selecting 5 bins.",
    "rationale": "The plot shows that the blue line (representing 5 bins) is higher than the pink line (representing 20 bins) for groups 0, 1, and 2. This indicates that more field values were selected when using 5 bins compared to 20 bins for these groups.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.07295v1",
    "pdf_url": null
  },
  {
    "instance_id": "148bb966b3614c18b0e991b163ce7945",
    "figure_id": "2307.08657v2-Figure18-1",
    "image_file": "2307.08657v2-Figure18-1.png",
    "caption": " Figure equivalent to Figure 3 for Kodak-C dataset",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of noise appears to be most effectively suppressed by SH NIC, ELIC, JPEG2000, and JPEG across all bitrates?",
    "answer": "Snow noise.",
    "rationale": "The plots in the top row of the figure show that all four methods achieve the highest PSNR values for snow noise, compared to glass blur and shot noise.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.08657v2",
    "pdf_url": null
  },
  {
    "instance_id": "1d9ec1958c7447f9991f5e069aa63ae2",
    "figure_id": "2308.10014v1-Figure3-1",
    "image_file": "2308.10014v1-Figure3-1.png",
    "caption": " Scatter plot comparison of the sample covariances of the posterior. The X-axis and Yaxis represent the estimates from the ground truth MCMC runs and the corresponding SIVI variants respectively. The red lines are the regression lines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which SIVI variant has the lowest RMSE?",
    "answer": "UIVI",
    "rationale": "The RMSE values are shown in the top right corner of each plot. The UIVI plot has the lowest RMSE value of 0.0313.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.10014v1",
    "pdf_url": null
  },
  {
    "instance_id": "f5ce2685f8f74360ad9bef2a659b7edf",
    "figure_id": "2210.15221v1-Figure5-1",
    "image_file": "2210.15221v1-Figure5-1.png",
    "caption": " The per sample time to generate adversarial samples (in second) and average query number to victim models of TextFooler, T3 and TASA, using all kinds of victim models on SQuAD 1.1 dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model took the least amount of time per sample to generate adversarial samples?",
    "answer": "TASA",
    "rationale": "The figure shows the per sample time for each model. TASA has the lowest bar in the top plot, which indicates it took the least amount of time per sample.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.15221v1",
    "pdf_url": null
  },
  {
    "instance_id": "91318072f9fe49948b6868bb09b5cee1",
    "figure_id": "2101.03501v1-Figure9-1",
    "image_file": "2101.03501v1-Figure9-1.png",
    "caption": " Finite sample identifiability of the causal direction via entropic causality, where p(x) ∼ Dir(1) (uniform on the simplex). (a) Probability of correctly discovering the causal direction X → Y as a function of n and number of samples N , using the conditional entropies as the test. (b) Probability of correctly discovering the causal direction X → Y as a function of n and number of samples N , using the greedy MEC algorithm to test the direction. (c) Samples N required to reach 98% correct detection as a function of n, derived from the plots in Figure 9a and Figure 9b.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, conditional entropy or greedy MEC, requires fewer samples to reach 98% correct detection for a fixed value of n?",
    "answer": "Greedy MEC.",
    "rationale": "In Figure 9c, the blue line representing Greedy MEC is consistently below the red line representing Conditional Entropy, indicating that Greedy MEC requires fewer samples to reach 98% correct detection for any given value of n.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.03501v1",
    "pdf_url": null
  },
  {
    "instance_id": "84cefce7001440e1af165a89c9afc100",
    "figure_id": "2211.04656v2-Figure12-1",
    "image_file": "2211.04656v2-Figure12-1.png",
    "caption": " Several actors from the MEVID dataset and their annotated tracks.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many actors are shown in the image?",
    "answer": "There are 10 actors shown in the image.",
    "rationale": "The figure shows 10 different actors, each with their own annotated track.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.04656v2",
    "pdf_url": null
  },
  {
    "instance_id": "99cb577c82244a48a01e1ad9b8e81c21",
    "figure_id": "2212.07242v3-Figure5-1",
    "image_file": "2212.07242v3-Figure5-1.png",
    "caption": " Perceptual study. Each bar shows the percentage of sequences for which participants preferred our method over the baseline, i.e., SNUG [43], SSCH [45] or ARCSIM [37]. Our method comfortably outperforms learned approaches while being on par with a genuine physical simulator. We do not compare to ARCSIM on the new sequences due to a large number of self-collisions in them, that ARCSIM failed to resolve.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method was preferred by participants for new sequences?",
    "answer": "Ours.",
    "rationale": "The bar chart shows that for new sequences, the percentage of participants who preferred our method over the baseline was higher than for any other method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.07242v3",
    "pdf_url": null
  },
  {
    "instance_id": "821a86fdccee4045bf97cf46621f44e9",
    "figure_id": "1812.08442v1-Figure1-1",
    "image_file": "1812.08442v1-Figure1-1.png",
    "caption": " Given the same image (1st column), imitating different visual effects (2nd column) can yield distinct interpretations of figure-ground segmentation (3rd column), which are derived by our method via referencing the following visual effects (from top to bottom): black background, color selectivo, and defocus/Bokeh. The learned VERs are shown in the last column, respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which visual effect results in the most accurate figure-ground segmentation for the cat image?",
    "answer": "Black background",
    "rationale": "The figure shows that the black background visual effect results in the most accurate figure-ground segmentation for the cat image, as the segmented image clearly shows the cat as the figure and the background as the ground.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.08442v1",
    "pdf_url": null
  },
  {
    "instance_id": "7bdcb42e389e4f9aa56909db556715ea",
    "figure_id": "2305.10196v1-Figure1-1",
    "image_file": "2305.10196v1-Figure1-1.png",
    "caption": " An overview of pro-drop languages by considering their typological patterns and language families. Example of ZP phenomenon in other languages (i.e. Korean, Hungarian and Hindi). Words in brackets are pronouns that are invisible in source language (implicit and explicit). The underlined words are corresponding antecedents. “EN” represents the human translation in English, which is a non-pro-drop language. “OT” is output translated by SOTA NMT systems with inappropriate translations.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which language family has the most pro-drop languages?",
    "answer": "Indo-European.",
    "rationale": "The figure shows that the Indo-European language family has the most branches, including Romance languages like Spanish, Italian, and Portuguese.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.10196v1",
    "pdf_url": null
  },
  {
    "instance_id": "eea0f38c537c40ad971b307f16bfb61c",
    "figure_id": "2005.06870v1-Figure10-1",
    "image_file": "2005.06870v1-Figure10-1.png",
    "caption": " WideResNet-28-8 trained by dynamic sparse training with α = 10−5",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which layer has the highest remaining ratio at the end of training? ",
    "answer": " FC 1 ",
    "rationale": " The figure shows the remaining ratio of each layer as a function of the number of epochs. The FC 1 layer has the highest remaining ratio at the end of training, which means that it has the most parameters remaining after pruning. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.06870v1",
    "pdf_url": null
  },
  {
    "instance_id": "b77aac9d74a542df9ba45f905724fa06",
    "figure_id": "2110.13502v1-Figure3-1",
    "image_file": "2110.13502v1-Figure3-1.png",
    "caption": " Left: Computation time. Algorithms are fit on data generated from model (1) with a super-Gaussian density. For different values of the number of samples, we plot the Amari distance and the fitting time. Thick lines link median values across seeds. Right: Robustness w.r.t intra-subject variability in MEG. (top) `2 distance between shared components corresponding to the same stimuli in different trials. (bottom) Fitting time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is the fastest and most robust to intra-subject variability?",
    "answer": "ShICA-ML",
    "rationale": "The figure shows that ShICA-ML has the lowest fitting time and the lowest l2 distance between shared components corresponding to the same stimuli in different trials.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.13502v1",
    "pdf_url": null
  },
  {
    "instance_id": "f5c725e2912342c18abc9b2c3c2f222e",
    "figure_id": "2003.08282v2-Figure11-1",
    "image_file": "2003.08282v2-Figure11-1.png",
    "caption": " Additional qualitative results from DVS Optical Flow and IROS18. (First Row) A single APS frame from each dataset. The remaining rows show the APS frame overlayed with denoised DVS events from each algorithm. The APS images for columns \"Fast Drone\" and \"3 Objects\" have been contrast enhanced but remain dark due to limited signal. Limited APS signal does not impact DVS event generation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best at denoising DVS events in the \"Boxes\" dataset?",
    "answer": "EDmCNN.",
    "rationale": "The figure shows the APS frame overlayed with denoised DVS events from each algorithm. The EDmCNN algorithm produces the most accurate and complete denoised events, as can be seen in the bottom row of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.08282v2",
    "pdf_url": null
  },
  {
    "instance_id": "57e412cc001246ff9f1d9f0ce6d150ea",
    "figure_id": "2202.00734v1-Figure4-1",
    "image_file": "2202.00734v1-Figure4-1.png",
    "caption": " Estimated global consistency differentiate over distributions of Adult dataset. Each of the test examples was randomly assigned to one of two populations based on its label. Example with label “≤50K” (resp. “>50K”) chances of being assigned to the first population are 0.75 (resp. 0.25) and chances to be assigned to the second are 0.25 (resp. 0.75). The displayed results are the mean of 5 executions with std bars.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest estimated consistency in the population with mostly individuals earning less than or equal to 50K?",
    "answer": "Anchors",
    "rationale": "The bar for Anchors in the \"Mostly <=50K\" population is the tallest, indicating the highest estimated consistency.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.00734v1",
    "pdf_url": null
  },
  {
    "instance_id": "a54639a77146431b9a391f33e6d419cc",
    "figure_id": "1903.10645v3-Figure3-1",
    "image_file": "1903.10645v3-Figure3-1.png",
    "caption": " This figure shows our predictive Dice score (x axis) vs real Dice score (y axis). For each row, the segmentation algorithm is tested on the left most dataset. The four figures in each row show how the segmentation results are evaluated by 4 different methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which uncertainty measure seems to be most effective for predicting the real Dice score?",
    "answer": "Entropy",
    "rationale": "In the first row of the figure, the scatter plot for the Entropy method shows that the points are closest to the orange line, which represents a perfect correlation between the predicted and real Dice scores.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.10645v3",
    "pdf_url": null
  },
  {
    "instance_id": "9a0a02baa79b48428af109e8de1d332d",
    "figure_id": "2303.05512v1-Figure3-1",
    "image_file": "2303.05512v1-Figure3-1.png",
    "caption": " The photorealistic dataset used to evaluate system identification and geometry estimation. The dataset includes a variety of continuum materials, including Newtonian fluids (Droplet, Letter), non-Newtonian fluids (Cream, Toothpaste), granular media (Trophy), deformable solids (Torus, Bird), and plasticine (Cat, Playdoh). All objects freely fall under the influence of gravity, undergoing collisions. Objects are rendered under complex environmental lighting conditions for photorealism.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the objects in the figure are most likely to exhibit non-Newtonian behavior?",
    "answer": "Cream and Toothpaste.",
    "rationale": "The caption states that the dataset includes Newtonian fluids (Droplet, Letter), non-Newtonian fluids (Cream, Toothpaste), granular media (Trophy), deformable solids (Torus, Bird), and plasticine (Cat, Playdoh). Non-Newtonian fluids are those whose viscosity changes with the applied shear stress.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.05512v1",
    "pdf_url": null
  },
  {
    "instance_id": "47777f5daffa468eb4859b0b020d354d",
    "figure_id": "1909.05106v2-Figure4-1",
    "image_file": "1909.05106v2-Figure4-1.png",
    "caption": " BRL for batch queueing. (a) Considered two-server queueing network. (b) Expected returns over the number of learning episodes, each consisting of twenty state transitions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which policy learned the fastest?",
    "answer": "The policy trained with PG (Policy Gradient)",
    "rationale": "In the plot (b) in the figure, we can see that the blue line (representing the policy trained with PG) rises the fastest, reaching a higher expected return in fewer episodes than the other two policies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.05106v2",
    "pdf_url": null
  },
  {
    "instance_id": "417dd130e657423e919a05578b939282",
    "figure_id": "2206.00484v2-Figure17-1",
    "image_file": "2206.00484v2-Figure17-1.png",
    "caption": " DEP-MPO outperforms MPO in sparse and dense reward point-reaching for arm26 with all virtual action spaces. Top: Learning performance for dense rewards. DEP-MPO strongly outperforms MPO, no significant movement learning could be detected for MPO with 600 actions. Bottom: Success rates for sparse reward reaching. While HER seems to increase the performance of the MPO baseline in most cases, the success rate only increases marginally for 600 actions, even after almost 1.5× 107 steps. DEP-MPO solves all tasks with or without the addition of HER.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best in the sparse reward point-reaching task with 600 actions?",
    "answer": "DEP-MPO",
    "rationale": "The figure shows the success rates for the sparse reward point-reaching task with different numbers of actions. DEP-MPO has the highest success rate for all numbers of actions, including 600 actions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.00484v2",
    "pdf_url": null
  },
  {
    "instance_id": "777cedb74746439296250c38913e8cd1",
    "figure_id": "1906.00675v2-Figure7-1",
    "image_file": "1906.00675v2-Figure7-1.png",
    "caption": " (a) Locations of the auxiliary classifiers added to the ResNet-1202 backbone network evaluated on the CIFAR-100 dataset. The grey thick arrows indicate the layer locations where auxiliary classifiers are added. We denote these three auxiliary classifiers as C2, C3 and C4, respectively. (b) Structure of the auxiliary classifiers. All the convolutional layers in this structure have the same kernel size (= 3×3) and the same stride (= 1), but have different number of filters (yielding different number of output channels). The numbers of convolutional layers and the corresponding filters for every auxiliary classifier can be found in Table 7.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which auxiliary classifier has the most convolutional layers?",
    "answer": "C4",
    "rationale": "The figure shows that C4 has four convolutional layers, while C3 has three and C2 has two.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.00675v2",
    "pdf_url": null
  },
  {
    "instance_id": "0b55840acb5c4924bef157da9cd82116",
    "figure_id": "1812.06677v3-Figure11-1",
    "image_file": "1812.06677v3-Figure11-1.png",
    "caption": " Qualitative comparison with point cloud alignment methods using feature descriptors. (a) 3DMatch [42]; (b) FPFH [27]; (c) ours.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most complete and accurate point cloud alignment?",
    "answer": "Ours.",
    "rationale": "The figure shows the results of three different point cloud alignment methods. (a) 3DMatch produces a very incomplete and inaccurate alignment, with many points missing and some points misaligned. (b) FPFH produces a more complete and accurate alignment, but there are still some missing points and some misaligned points. (c) Our method produces the most complete and accurate alignment, with all points correctly aligned.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.06677v3",
    "pdf_url": null
  },
  {
    "instance_id": "b51f4cbc7e5341669c0d69be63ad1fa5",
    "figure_id": "1908.01998v4-Figure9-1",
    "image_file": "1908.01998v4-Figure9-1.png",
    "caption": " Our application results on the penguin dataset [64]. Given 5 penguin images as support, our approach can detect all penguins in the wild in the given query image.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many penguins are detected in the query image?",
    "answer": "22 penguins",
    "rationale": "The query image shows a group of penguins on a rocky shore, with green bounding boxes drawn around each penguin. Counting the number of bounding boxes gives us the number of penguins detected.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.01998v4",
    "pdf_url": null
  },
  {
    "instance_id": "4439349966df4212b557032ba59e6237",
    "figure_id": "2112.14531v2-Figure10-1",
    "image_file": "2112.14531v2-Figure10-1.png",
    "caption": " Comparisons of test accuracy and the MAD value on the Cora dataset. Predefined aggregation operations are the GraphSAGE and GAT. “L4” represents the 4 layer baseline (𝑁 = 4 in our method), and so on. Darker colors mean larger values.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best performance in terms of both test accuracy and MAD value on the Cora dataset?",
    "answer": "F^2SAGE",
    "rationale": "The figure shows that F^2SAGE has the highest test accuracy and the lowest MAD value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.14531v2",
    "pdf_url": null
  },
  {
    "instance_id": "b862226db22f45a5816cd3807ddfe7cf",
    "figure_id": "2101.08929v2-Figure4-1",
    "image_file": "2101.08929v2-Figure4-1.png",
    "caption": " Distance Matrix",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the minimum distance between q2 and p2*?",
    "answer": "r2",
    "rationale": "The distance between q2 and p2* is given by the element in the second row and second column of the distance matrix. This element is r2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.08929v2",
    "pdf_url": null
  },
  {
    "instance_id": "bdf11ee34eb34e58a8908d1ddbc6c56d",
    "figure_id": "1908.06442v2-Figure1-1",
    "image_file": "1908.06442v2-Figure1-1.png",
    "caption": " Annotations overview for 3D human recovery. We study five kinds of annotations that are typically used in training deep networks for 3D human recovery. The number of ‘$’ indicates the annotation cost of obtaining the corresponding annotations. A higher number of ‘$’ suggests a higher cost.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of annotation is the most expensive to obtain?",
    "answer": "In-the-wild 3D",
    "rationale": "The figure shows that the \"In-the-wild 3D\" annotation has the highest number of dollar signs, which indicates that it is the most expensive to obtain.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.06442v2",
    "pdf_url": null
  },
  {
    "instance_id": "9429d968731d4db0af2198ce095cb12e",
    "figure_id": "2104.07705v2-Figure1-1",
    "image_file": "2104.07705v2-Figure1-1.png",
    "caption": " Distribution of the validation-set loss after 24 hours of training across different hyperparameters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which hyperparameter has the largest effect on the validation-set loss?",
    "answer": "The learning rate.",
    "rationale": "The box plots for the learning rate show the widest range of values, indicating that the learning rate has the largest effect on the validation-set loss.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.07705v2",
    "pdf_url": null
  },
  {
    "instance_id": "f58f6d7914d84b07b498fc3eb3411666",
    "figure_id": "2303.12217v1-Figure3-1",
    "image_file": "2303.12217v1-Figure3-1.png",
    "caption": " Denoising baseline comparisons. We compare to AmbientGAN, DIP, and TV-RML with weight λ and report the average PSNR across all 95 reconstructions. We show both early stopping and full training results using DIP. Our results exhibit higher PSNR than all other baselines while maintaining distinct features of the ground-truth images.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the best results in terms of PSNR?",
    "answer": "Our method.",
    "rationale": "The average PSNR values are shown above each set of images. Our method has the highest average PSNR of 27.04, while the other methods have lower PSNR values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.12217v1",
    "pdf_url": null
  },
  {
    "instance_id": "1dd6a39f5d08470ca17783cea7fed489",
    "figure_id": "2312.01457v1-Figure17-1",
    "image_file": "2312.01457v1-Figure17-1.png",
    "caption": " Results for CIFAR-100 dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest variance when varying the number of training data (m)?",
    "answer": "SwitchDR",
    "rationale": "The figure shows the variance of different methods as a function of the number of training data. The variance of SwitchDR is consistently lower than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2312.01457v1",
    "pdf_url": null
  },
  {
    "instance_id": "c0180390ad9544208ee9692cceadbb52",
    "figure_id": "2002.12687v6-Figure9-1",
    "image_file": "2002.12687v6-Figure9-1.png",
    "caption": " PCK results under various distance thresholds (0-0.1) for compared algorithms.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the compared algorithms achieved the highest PCK score at a distance threshold of 0.05?",
    "answer": "PointNet++",
    "rationale": "The figure shows the PCK results for various distance thresholds. At a distance threshold of 0.05, the line for PointNet++ is the highest, indicating that it achieved the highest PCK score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.12687v6",
    "pdf_url": null
  },
  {
    "instance_id": "6733b572ab064de5b02cfc361c870a20",
    "figure_id": "1906.07832v3-Figure3-1",
    "image_file": "1906.07832v3-Figure3-1.png",
    "caption": " The squared error for the Korobov space (d ∈ {2, 3}, s ∈ {1, 2}): (Top) the results for the dimension d = 2, (Left) the results for the regularity s = 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the smallest squared error for all values of N?",
    "answer": "DPPKQ",
    "rationale": "The DPPKQ method is represented by the blue line in all of the plots. This line is always below the other lines, indicating that it has the smallest squared error for all values of N.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.07832v3",
    "pdf_url": null
  },
  {
    "instance_id": "a6557c7ad1b340b78463b9fc5a5f1a11",
    "figure_id": "1902.09852v2-Figure4-1",
    "image_file": "1902.09852v2-Figure4-1.png",
    "caption": " Comparison of our baseline method and ASIS on instance segmentation. Different colors represent different instances.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better in terms of instance segmentation?",
    "answer": "ASIS",
    "rationale": "The figure shows that ASIS is able to more accurately segment the different instances in the scene. For example, in the first row, the baseline method incorrectly segments the chair and table as one instance, while ASIS correctly segments them as two separate instances.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.09852v2",
    "pdf_url": null
  },
  {
    "instance_id": "0c2d8dfe327f419f926461f451a5468b",
    "figure_id": "1911.11489v1-Figure4-1",
    "image_file": "1911.11489v1-Figure4-1.png",
    "caption": " Comparison results on beam search and top-k sampling. Specifically, if the length of the longest common substring between response and query is larger than 4, then the response is regarded as a “copy” of query. If a response contains the word/phrase loop over 3 times, it is regarded as a response with repetition.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which decoding strategy achieves the lowest copy rate?",
    "answer": "Top-k sampling.",
    "rationale": "The left plot in the figure shows the copy rate for each decoding strategy. The copy rate is the percentage of responses that are copies of the query. Top-k sampling has the lowest copy rate for all models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.11489v1",
    "pdf_url": null
  },
  {
    "instance_id": "7cb968a9ac55405586fe84a0d451afd6",
    "figure_id": "2308.04826v2-Figure5-1",
    "image_file": "2308.04826v2-Figure5-1.png",
    "caption": " The qualitative results of our WaveNeRF and the comparison with PixelNeRF [37], MVSNet [3], and GeoNeRF [13]. We show the scenes from LLFF dataset [24] (horn), DTU dataset [12] (scan40), and NeRF synthetic dataset [25] (chair). Our WaveNeRF model can preserve more details than the previous generalizable NeRF.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods shown in the figure preserves the most detail in the scene?",
    "answer": "WaveNeRF.",
    "rationale": "The figure shows the results of four different methods for reconstructing a scene from a set of images. The WaveNeRF method produces the most detailed reconstruction, as can be seen in the images of the horn, the scan40, and the chair.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.04826v2",
    "pdf_url": null
  },
  {
    "instance_id": "d970e57a028346a5bef88cca9d37a226",
    "figure_id": "2007.07804v3-Figure4-1",
    "image_file": "2007.07804v3-Figure4-1.png",
    "caption": " Average wins for player 1 and player 2 in two gridworld environments. 10 runs, c.i. 98%",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm leads to the most consistent performance across both players in the first environment?",
    "answer": "NOHD",
    "rationale": "In the first environment, the NOHD algorithm shows the least amount of variance between the two players, as evidenced by the narrow range of the shaded areas around the lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.07804v3",
    "pdf_url": null
  },
  {
    "instance_id": "25c1657da503400ea69bf1d6b403cb66",
    "figure_id": "2101.05834v1-Figure10-1",
    "image_file": "2101.05834v1-Figure10-1.png",
    "caption": " Sample initial conditions for the advection-diffusion type dynamics.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " How many different initial conditions are shown in the figure?",
    "answer": " Three",
    "rationale": " The figure shows three different curves, each representing a different initial condition for the advection-diffusion type dynamics. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.05834v1",
    "pdf_url": null
  },
  {
    "instance_id": "24a50ffee1ed453facceb506487a57f5",
    "figure_id": "2305.12260v2-Figure6-1",
    "image_file": "2305.12260v2-Figure6-1.png",
    "caption": " The matchings of SG and SC structures.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest SG coincidence rate on scene graphs between input vision and target caption?",
    "answer": "The SSR model.",
    "rationale": "The figure shows the SG coincidence rate for different models. The SSR model has the highest bar in the graph, which indicates that it has the highest SG coincidence rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.12260v2",
    "pdf_url": null
  },
  {
    "instance_id": "b6326b19397f45e9b2a13062ceacca47",
    "figure_id": "1711.05767v1-Figure5-1",
    "image_file": "1711.05767v1-Figure5-1.png",
    "caption": " Synthetic topology of 20 links",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many nodes are there in the network?",
    "answer": "8",
    "rationale": "The network is represented by a graph, where the nodes are represented by circles and the links are represented by arrows. By counting the circles in the figure, we can see that there are 8 nodes in the network.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.05767v1",
    "pdf_url": null
  },
  {
    "instance_id": "0c6db0bf9e4749e0a824a0477b7f1022",
    "figure_id": "1812.05040v2-Figure1-1",
    "image_file": "1812.05040v2-Figure1-1.png",
    "caption": " Our work aims to learn a semantic segmentation model from synthetic data, which can also be applied to real-world data. The domain adaptation is reinforced by geometric information in the synthetic data, which can be obtained easily from virtual environment.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three types of data used to train the semantic segmentation model?",
    "answer": "Synthetic images, semantic labels, and depth maps.",
    "rationale": "The figure shows that the source domain contains synthetic images, semantic labels, and depth maps, which are used to train the semantic segmentation model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.05040v2",
    "pdf_url": null
  },
  {
    "instance_id": "c05e6fefa6d04a148ece4793a5c185cd",
    "figure_id": "2303.17569v2-Figure21-1",
    "image_file": "2303.17569v2-Figure21-1.png",
    "caption": " Complete comparisons with all methods and the reference image on the BAID test dataset. Our CLIP-LIT produces most natural appearance than the compared methods.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most natural-looking image?",
    "answer": "CLIP-LIT",
    "rationale": "The figure shows the results of different methods for low-light image enhancement. The CLIP-LIT method produced the most natural-looking image, which is most similar to the reference image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.17569v2",
    "pdf_url": null
  },
  {
    "instance_id": "f69ce4b2db9d479ca15ddd0168fdcdc0",
    "figure_id": "2106.00672v1-Figure61-1",
    "image_file": "2106.00672v1-Figure61-1.png",
    "caption": " Analysis of choice gradient penalty λ (C47): 95th percentile of performance scores conditioned on choice (top) and distribution of choices in top 5% of configurations (bottom).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment has the largest performance gap between the top 5% and the 95th percentile?",
    "answer": "Door expert.",
    "rationale": "The top row of the figure shows the 95th percentile of performance scores for each environment, while the bottom row shows the distribution of choices in the top 5% of configurations. The difference between the two is largest for the Door expert environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.00672v1",
    "pdf_url": null
  },
  {
    "instance_id": "2daa0873ec4c49eba947468737459de5",
    "figure_id": "2106.15482v2-Figure5-1",
    "image_file": "2106.15482v2-Figure5-1.png",
    "caption": " Test error vs. an estimated upper bound over 10 clients on CIFAR-10 with varying degrees of a training set data size using the Bayes classifier. Each dot represents a combination of client and data size. In parenthesis - the average difference between the empirical and the test error.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which percentage of data resulted in the smallest average difference between the empirical and the test error?",
    "answer": "100%",
    "rationale": "The average difference between the empirical and the test error is shown in parenthesis next to each data size percentage in the legend. The smallest value is 0.12, which corresponds to 100% of the data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15482v2",
    "pdf_url": null
  },
  {
    "instance_id": "2637aeec09494ebebcb19ed3d3f693ea",
    "figure_id": "1908.10546v1-Figure7-1",
    "image_file": "1908.10546v1-Figure7-1.png",
    "caption": " Letter accuracy vs. iteration in the Ours+face ROI setting, showing an example ROI zooming ratio sequence found by beam search (shown, red curve). Blue stars: accuracy with other zooming ratios considered.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which zooming ratio resulted in the highest letter accuracy?",
    "answer": "The optimal ratio, shown by the red curve.",
    "rationale": "The figure shows that the letter accuracy increased with each iteration of the optimal ratio, and reached a peak of about 48% at iteration 4. The other zooming ratios considered, shown by the blue stars, did not achieve as high of an accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.10546v1",
    "pdf_url": null
  },
  {
    "instance_id": "8fc62a37aa6846b2b0311c7e147c4f93",
    "figure_id": "2001.02773v2-Figure2-1",
    "image_file": "2001.02773v2-Figure2-1.png",
    "caption": " Comparison of the negative log probability of the approximate MAP assignment versus running time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most efficient for Paper Popularity?",
    "answer": "C2F BVI",
    "rationale": "The figure shows that C2F BVI has the lowest negative log probability for Paper Popularity, which means it is the most efficient method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.02773v2",
    "pdf_url": null
  },
  {
    "instance_id": "4d71405036a54a659c75edee0a9dab2e",
    "figure_id": "2211.14091v2-Figure4-1",
    "image_file": "2211.14091v2-Figure4-1.png",
    "caption": " Statistics of the language parsing results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which color is mentioned most frequently in the parsed language data?",
    "answer": "White",
    "rationale": "The bar chart for \"Color\" shows that the bar for \"white\" is the tallest, indicating that it is the most frequent color mentioned.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.14091v2",
    "pdf_url": null
  },
  {
    "instance_id": "aab99186db6045278787cee79caeabc0",
    "figure_id": "2110.10461v3-Figure14-1",
    "image_file": "2110.10461v3-Figure14-1.png",
    "caption": " Single-pass runtime distributions for our larger-scale experiments. Worst-case complexities are dominated by second derivative computations, but remain competitive with conventional multipass HPO techniques, which scale our Random results by the number of configurations sampled.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method has the most consistent runtime across all three datasets?",
    "answer": " Ours WD+LR+M",
    "rationale": " The figure shows the distribution of runtimes for each method on each dataset. The method with the most consistent runtime is the one with the narrowest distribution, which is Ours WD+LR+M. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.10461v3",
    "pdf_url": null
  },
  {
    "instance_id": "9ef2806253d541e49369bf5a6d89e1ff",
    "figure_id": "1905.01744v1-Figure6-1",
    "image_file": "1905.01744v1-Figure6-1.png",
    "caption": " Image samples from our benchmark grouped by their domain categories (sunny, night, cloudy and rainy). In each group, left are original images and right are images with corresponding bounding box annotations.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which domain category has the most cars?",
    "answer": "It is not possible to tell from the image.",
    "rationale": "The image shows a few samples from each domain category, but it does not show the total number of cars in each category.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.01744v1",
    "pdf_url": null
  },
  {
    "instance_id": "4d84b1cf32134057bbceaa78adad6b9d",
    "figure_id": "2205.15759v2-Figure8-1",
    "image_file": "2205.15759v2-Figure8-1.png",
    "caption": " The ad-exposure distribution. The height of each bar on \"𝑘 :\" indicates the ad proportion within slots 𝑘 to 𝑘 +3. There is no ad at the first four slots due to the top ad slot constraint. The dotted lines indicate average ad positions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest ad proportion in slots 13 to 16?",
    "answer": "β-WPO",
    "rationale": "The figure shows the ad proportion for three algorithms: β-WPO, β-GEA, and HCA2E. The height of each bar on \"𝑘 :\" indicates the ad proportion within slots 𝑘 to 𝑘 +3. The bar for β-WPO in slots 13 to 16 is the tallest, indicating that it has the highest ad proportion in those slots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.15759v2",
    "pdf_url": null
  },
  {
    "instance_id": "821db11ce3a04188894f52046a883a5c",
    "figure_id": "2010.14438v1-Figure4-1",
    "image_file": "2010.14438v1-Figure4-1.png",
    "caption": " Feature efficiency. Our model performs better even when the feature-space is compact.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best overall?",
    "answer": "CAL",
    "rationale": "The CAL model has the highest mAP, nDCG, and mREL values across all feature sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.14438v1",
    "pdf_url": null
  },
  {
    "instance_id": "9ab195829c47437ea69d6e4300484d97",
    "figure_id": "1805.10408v2-Figure5-1",
    "image_file": "1805.10408v2-Figure5-1.png",
    "caption": " Plot of the singular values of the linear operators associated with the convolutional layers of the pretrained \"ResNet V2\" from the TensorFlow website.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the plot, which convolutional layer has the largest singular value?",
    "answer": "conv2d_3",
    "rationale": "The plot shows the singular values of the linear operators associated with the convolutional layers of the pretrained \"ResNet V2\". The singular values are plotted against the singular value rank, and the convolutional layer with the largest singular value is the one with the highest curve on the plot. In this case, it is conv2d_3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.10408v2",
    "pdf_url": null
  },
  {
    "instance_id": "06331748ee1247d69cad3c94fdda6173",
    "figure_id": "1811.01715v1-Figure3-1",
    "image_file": "1811.01715v1-Figure3-1.png",
    "caption": " Regret and Compensation of modified ε-greedy.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which epsilon value results in the highest regret compensation?",
    "answer": "Epsilon-19-C.",
    "rationale": "The figure shows the regret compensation for different epsilon values. The line for Epsilon-19-C is the highest on the plot, indicating that it has the highest regret compensation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.01715v1",
    "pdf_url": null
  },
  {
    "instance_id": "a4b380726efc4bf8807add2fe496a046",
    "figure_id": "2009.08695v1-Figure2-1",
    "image_file": "2009.08695v1-Figure2-1.png",
    "caption": " Super-resolution results on Set5 dataset with different scale factors.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best for the butterfly image with a scale factor of ×3?",
    "answer": "SLB",
    "rationale": "The figure shows the super-resolution results for different methods and scale factors. The PSNR values are listed below each image. For the butterfly image with a scale factor of ×3, SLB has the highest PSNR value of 33.01, indicating it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.08695v1",
    "pdf_url": null
  },
  {
    "instance_id": "ae2b35daed4a415bb74425c5f5176e61",
    "figure_id": "1904.11681v3-Figure4-1",
    "image_file": "1904.11681v3-Figure4-1.png",
    "caption": " Compact problem-dependent geometric covering (CPGC) intervals. In the figure, each interval is denoted by [ ].",
    "figure_type": "Table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the intervals in the figure has the longest duration?",
    "answer": "The interval for $s_1$.",
    "rationale": "The interval for $s_1$ starts at $t=1$ and ends at $t=15$, which is longer than any other interval in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.11681v3",
    "pdf_url": null
  },
  {
    "instance_id": "929fe7427a7245a68dc3793fa3490e3f",
    "figure_id": "1908.01314v4-Figure6-1",
    "image_file": "1908.01314v4-Figure6-1.png",
    "caption": " Overall search cost vs. the number of target platforms.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods is the most efficient in terms of GPU cost?",
    "answer": "MoGA is the most efficient in terms of GPU cost.",
    "rationale": "The figure shows that MoGA has the lowest GPU cost for all numbers of target platforms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.01314v4",
    "pdf_url": null
  },
  {
    "instance_id": "ce9e9df1451a4bb6a5456dac16e18096",
    "figure_id": "2210.14648v3-Figure3-1",
    "image_file": "2210.14648v3-Figure3-1.png",
    "caption": " Masking ratio ablations: linear evaluation results (%).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which masking ratio leads to the highest linear evaluation score for Music?",
    "answer": "0.8",
    "rationale": "The figure shows the linear evaluation scores for different masking ratios. The highest score for Music is achieved when the masking ratio is 0.8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.14648v3",
    "pdf_url": null
  },
  {
    "instance_id": "b72ce6dc74ec4c89b5bac170cd5e340f",
    "figure_id": "2302.10174v1-Figure6-1",
    "image_file": "2302.10174v1-Figure6-1.png",
    "caption": " t-SNE visualization of real (red) and fake (blue) images using the feature space of different image encoders. CLIP:ViT’s feature space best separates the real features from fake.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the image encoders shown in the figure best separates the real features from the fake features?",
    "answer": "CLIP:ViT-L/14",
    "rationale": "The t-SNE visualization shows that the CLIP:ViT-L/14 encoder produces the most distinct clusters of real and fake features, with the least overlap between the two clusters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.10174v1",
    "pdf_url": null
  },
  {
    "instance_id": "fa0e245ffe5b411cacc10ad4d5a92312",
    "figure_id": "1907.09728v1-Figure9-1",
    "image_file": "1907.09728v1-Figure9-1.png",
    "caption": " Example question of the user experiment. The uppercase words represent simplified prototype sequences.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the sentiment of the target sentence?",
    "answer": "Positive.",
    "rationale": "The target sentence contains positive words such as \"outstanding,\" \"great,\" and \"fantastic,\" and it also recommends a specific dish.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.09728v1",
    "pdf_url": null
  },
  {
    "instance_id": "ec200791508d4ced821ba7ef432342f9",
    "figure_id": "2012.15562v3-Figure1-1",
    "image_file": "2012.15562v3-Figure1-1.png",
    "caption": " Example tokens of unseen scripts.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the scripts shown in the table has the most visually complex characters?",
    "answer": "Tibetan",
    "rationale": "The Tibetan characters have the most intricate and detailed shapes compared to the other two scripts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.15562v3",
    "pdf_url": null
  },
  {
    "instance_id": "5b18d42522d74dfd8f52c8c0fec6e504",
    "figure_id": "2303.00749v1-Figure16-1",
    "image_file": "2303.00749v1-Figure16-1.png",
    "caption": " We render more 180 degree panorama views for visual comparisons. Scenes are from the Waymo datasets.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, Urban NeRF, Mip-NeRF 360, or S-NeRF, produces the most realistic-looking RGB image?",
    "answer": "Our S-NeRF RGB.",
    "rationale": "The RGB images produced by Urban NeRF and Mip-NeRF 360 are blurry and have artifacts, while the RGB image produced by Our S-NeRF is sharp and clear.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.00749v1",
    "pdf_url": null
  },
  {
    "instance_id": "36006cc481be4c8e9a7e07c73aa93c00",
    "figure_id": "2105.01047v1-Figure16-1",
    "image_file": "2105.01047v1-Figure16-1.png",
    "caption": " Effective and Optimal Steps. Results for the remaining seven simulation categories not shown in the paper due to space restrictions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which simulation category has the highest fraction of efficient steps for the \"Ours-Touch\" condition?",
    "answer": "Eyeglasses",
    "rationale": "The figure shows the fraction of efficient steps for each simulation category and condition. The \"Ours-Touch\" condition is the first bar in each group. The Eyeglasses category has the highest bar for the \"Ours-Touch\" condition.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.01047v1",
    "pdf_url": null
  },
  {
    "instance_id": "9548a4e477ea4c3b96c66f6eefe74bcd",
    "figure_id": "2006.02963v2-Figure8-1",
    "image_file": "2006.02963v2-Figure8-1.png",
    "caption": " RarePlanes dataset locations. The dataset features 112 real (blue points) and 15 synthetic locations (red points). Atlanta, Miami, and Salt Lake City feature both real and synthetic data.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which continent has the most real locations in the RarePlanes dataset?",
    "answer": "North America.",
    "rationale": "The figure shows that North America has the most blue points, which represent real locations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.02963v2",
    "pdf_url": null
  },
  {
    "instance_id": "625ddff24b3f4f1d817232d48bb01799",
    "figure_id": "2210.03516v4-Figure13-1",
    "image_file": "2210.03516v4-Figure13-1.png",
    "caption": " Relative change in fitness, referred to as fitness gain, on the adaptation tasks. Comparing the algorithms’ performances on modified environments to their own performances on the nominal environments offers another perspective on the resilience capabilities of the methods under study.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is the most robust to changes in actuator coefficient?",
    "answer": "PGA-AURORA",
    "rationale": "The figure shows that PGA-AURORA has the smallest decrease in fitness gain when the actuator coefficient is increased. This indicates that it is the most robust to changes in actuator coefficient.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.03516v4",
    "pdf_url": null
  },
  {
    "instance_id": "a5a99a91ee334e6588f5ed2dfd45daa1",
    "figure_id": "2205.13320v2-Figure9-1",
    "image_file": "2205.13320v2-Figure9-1.png",
    "caption": " Best normalized function value of ROSENBROCK ROTATED with std, averaged over 100 runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which search strategy performed the best in terms of finding the best normalized function value?",
    "answer": "The Eagle Strategy.",
    "rationale": "The Eagle Strategy consistently reaches the highest normalized function value across all trials, as shown by the blue line in the corresponding plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.13320v2",
    "pdf_url": null
  },
  {
    "instance_id": "4b42939bc8f1444c868ba387f880a911",
    "figure_id": "2112.09169v2-Figure7-1",
    "image_file": "2112.09169v2-Figure7-1.png",
    "caption": " The returns and intervention rate of sensor pilot assisted by the copilot trained with different parameters for different methods on Lunar Lander with continuous action space.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which parameter setting leads to the highest returns for the Penalty Adapting method?",
    "answer": "A penalty of 15 and an intervention rate of 0.8.",
    "rationale": "The plot for the Penalty Adapting method shows that the returns are highest when the penalty is 15 and the intervention rate is 0.8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.09169v2",
    "pdf_url": null
  },
  {
    "instance_id": "479e43d9646340b79748dda59aff5ee3",
    "figure_id": "2008.07303v7-Figure5-1",
    "image_file": "2008.07303v7-Figure5-1.png",
    "caption": " For the new highway data set , this is roughly the recorded highway section (only the lower lane, incl. exit/entry). Note that the recorded section is in fact slightly more to the right than the picture indicates.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of road is shown in the image?",
    "answer": "The image shows a highway.",
    "rationale": "The figure shows a large, multi-lane road with a high volume of traffic. This is consistent with the characteristics of a highway.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.07303v7",
    "pdf_url": null
  },
  {
    "instance_id": "309ff503e5f44466877330e78bae5124",
    "figure_id": "2207.01570v1-Figure6-1",
    "image_file": "2207.01570v1-Figure6-1.png",
    "caption": " Achieved returns (mean of 10 episodes) of policies created by fully trained generators as a function of the given return command. A perfect generator would produce policies that lie on the diagonal identity line (if the environment permits such returns). For each environment, results of five independent runs are shown.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment seems to be the most difficult for the policy to achieve the desired return?",
    "answer": "MountainCarContinuous-v0.",
    "rationale": "In all of the plots, the orange lines represent the achieved returns, while the blue line represents the desired return. The closer the orange lines are to the blue line, the better the policy is at achieving the desired return. In the plot for MountainCarContinuous-v0, the orange lines are furthest from the blue line, indicating that the policy is not as good at achieving the desired return in this environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.01570v1",
    "pdf_url": null
  },
  {
    "instance_id": "3cd6bad55acb427185463f5587ab35ab",
    "figure_id": "1906.04158v1-Figure3-1",
    "image_file": "1906.04158v1-Figure3-1.png",
    "caption": " An ablation study by comparing the prediction performances after removing each channel of the social signal input from the trained networks of “Face+Body”in Table 1. The performance drops after removing each part, compared to the original performances, are shown by colors and circle sizes. The left figure is the result by using the target person’s own signals, and the right figure is by using the other seller’s signals. The colorbar on the right shows the frame drops in percentage from the original performances.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which social signal has the largest impact on the prediction performance of the network when using the target person's own signals?",
    "answer": "Body pose.",
    "rationale": "The left figure shows the performance drops after removing each channel of the social signal input from the trained networks of “Face+Body”. The body pose channel has the largest circle size, indicating the largest performance drop.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04158v1",
    "pdf_url": null
  },
  {
    "instance_id": "5d9b3dc561154287b497e2ebb4f0801f",
    "figure_id": "1905.01941v2-Figure9-1",
    "image_file": "1905.01941v2-Figure9-1.png",
    "caption": " Performance of FAZE for different dimensions Fg of the 3× Fg-dimensional latent gaze code.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows a more consistent decrease in mean test error as the number of calibration samples increases?",
    "answer": "GazeCapture.",
    "rationale": "In Figure (a), the mean test error for GazeCapture decreases fairly consistently as the number of calibration samples increases, for all values of Fg. In Figure (b), the mean test error for MPIIGaze is more erratic, with some fluctuations as the number of calibration samples increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.01941v2",
    "pdf_url": null
  },
  {
    "instance_id": "7776a4a599ba4109a34e8c8562539511",
    "figure_id": "2009.09801v1-Figure13-1",
    "image_file": "2009.09801v1-Figure13-1.png",
    "caption": " The Count CQ q, which is the conjunction of q edge 1 , qcol1 , qcol0 (left part) and q edge 2 , qcol2 (right part).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many edges are there in the Count CQ q?",
    "answer": "There are two edges in the Count CQ q.",
    "rationale": "The figure shows two edges, one in the left part and one in the right part.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.09801v1",
    "pdf_url": null
  },
  {
    "instance_id": "2636074036584426b18a4ffefa345822",
    "figure_id": "2206.03377v1-Figure4-1",
    "image_file": "2206.03377v1-Figure4-1.png",
    "caption": " Case study.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many shares of the company did Nanfang Tongzheng pledge to ORG3?",
    "answer": "Nanfang Tongzheng pledged SHARE2 shares to ORG3.",
    "rationale": "This information can be found in the \"Event Records for the Equity Pledge (EP) Event Type\" tables. Each table shows the pledger, pledged shares, pledgee, total holding shares, total holding ratio, total pledged shares, start date, and release date for each equity pledge event. In the tables, it shows that Nanfang Tongzheng pledged SHARE2 shares to ORG3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.03377v1",
    "pdf_url": null
  },
  {
    "instance_id": "af8d0a68645642e8a4b53d9120ff78c2",
    "figure_id": "2106.15499v6-Figure7-1",
    "image_file": "2106.15499v6-Figure7-1.png",
    "caption": " Qualitative examples for mitigating vanishing gradient. Along with the original image, we visualized the gradient when training with SupCon (Left) and SelfCon-M loss (Right). Note that all the gradients are from the same model checkpoint of ResNet-18.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function appears to have less vanishing gradient?",
    "answer": "SelfCon-M",
    "rationale": "The figure shows the gradient of the image for the SupCon loss (left) and the SelfCon-M loss (right). The gradient of the SelfCon-M loss is brighter and more vibrant, indicating that it has less vanishing gradient.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15499v6",
    "pdf_url": null
  },
  {
    "instance_id": "76ddbc8d14134ba29d4a613e102e79e7",
    "figure_id": "2309.05569v1-Figure9-1",
    "image_file": "2309.05569v1-Figure9-1.png",
    "caption": " Ablation on the quantity of reference images. More reference images (> 10) help possibly due to more diversity and less noise. ITI-GEN is robust in the low data regime (Section 3.3).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric is more sensitive to the number of reference images?",
    "answer": "KL divergence.",
    "rationale": "The KL divergence curve (blue) decreases more sharply than the FID curve (orange) as the number of reference images increases. This indicates that the KL divergence is more sensitive to the number of reference images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.05569v1",
    "pdf_url": null
  },
  {
    "instance_id": "71a5f4be5fb44a879c75698ed3d2935c",
    "figure_id": "2011.05429v1-Figure36-1",
    "image_file": "2011.05429v1-Figure36-1.png",
    "caption": " An example Mouse-Cat Image.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the cat and the mouse in the image?",
    "answer": "The cat is a predator and the mouse is its prey.",
    "rationale": "The image shows a cat looking at a mouse. The cat is much larger than the mouse, and it has sharp teeth and claws. This suggests that the cat is likely to attack and eat the mouse.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.05429v1",
    "pdf_url": null
  },
  {
    "instance_id": "74dfea3f350545d0b001ac720257b5b4",
    "figure_id": "2205.13320v2-Figure16-1",
    "image_file": "2205.13320v2-Figure16-1.png",
    "caption": " (Lower is better) Aggregated comparisons of normalized regret and mean ranks across all search spaces on the continuous search spaces of HPO-B-v3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which hyperparameter optimization algorithm performs best in terms of normalized regret on the continuous search spaces of HPO-B-v3?",
    "answer": "OptFormer (EI)",
    "rationale": "The figure shows the average rank of each hyperparameter optimization algorithm across all search spaces on the continuous search spaces of HPO-B-v3. OptFormer (EI) has the lowest average rank, which indicates that it performs best in terms of normalized regret.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.13320v2",
    "pdf_url": null
  },
  {
    "instance_id": "a21b303f791643149fb8b01f66f1e097",
    "figure_id": "2008.09777v4-Figure21-1",
    "image_file": "2008.09777v4-Figure21-1.png",
    "caption": " (Left) Distribution of validation error in dependence of the number of parameter-free operations in the normal cell on the Surr-NAS-Bench-DARTS dataset. (Middle and Right) Predictions of the GIN and XGB surrogate model. The collected groundtruth data is shown as scatter plot. Violin plots are cut off at the respective observed minimum and maximum value.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest validation error for architectures with 4 parameter-free operations?",
    "answer": "Surr-NAS-Bench-DARTS.",
    "rationale": "The validation error for each model is shown in the violin plots. The Surr-NAS-Bench-DARTS model has the highest validation error for architectures with 4 parameter-free operations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.09777v4",
    "pdf_url": null
  },
  {
    "instance_id": "8958544b4bb04c2f8861a6694bc1a2a7",
    "figure_id": "2010.12536v1-Figure6-1",
    "image_file": "2010.12536v1-Figure6-1.png",
    "caption": " Adoption rate comparison. We compare all methods for adoption rates between 0% (NT) and 60%. Gist: All methods are able to improve over NT, even at low adoption rates. At 30% and 45%, ST-PCT performs the best by a relatively wide margin while DS-PCT outperforms it at 60%.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best at an adoption rate of 30%?",
    "answer": "ST-PCT",
    "rationale": "The figure shows the performance of four methods at different adoption rates. The performance is measured by the R metric, which is shown on the y-axis. The higher the R metric, the better the performance. At an adoption rate of 30%, the ST-PCT method has the highest R metric.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12536v1",
    "pdf_url": null
  },
  {
    "instance_id": "566abb1b5cf04a65b568198e22d09318",
    "figure_id": "1812.03443v3-Figure5-1",
    "image_file": "1812.03443v3-Figure5-1.png",
    "caption": " Comparison of operator runtime on two devices. Runtime is in micro-second (us). Orange bar denotes the runtime on iPhone X and blue bar denotes the runtime on Samsung S8. The upper three operators are faster on iPhone X, therefore they are automatically adopted in FBNet-iPhoneX. The lower three operators are faster on Samsung S8, and they are also automatically adopted in FBNet-S8.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which operator has the largest difference in runtime between the two devices?",
    "answer": "hw14_c336_k5_s1",
    "rationale": "The figure shows the runtime of each operator on both devices. The operator with the largest difference in runtime is hw14_c336_k5_s1, which has a runtime of approximately 1000 us on iPhone X and approximately 500 us on Samsung S8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.03443v3",
    "pdf_url": null
  },
  {
    "instance_id": "827a587064cc4013856e1e37bfde8ab5",
    "figure_id": "2106.05390v3-Figure8-1",
    "image_file": "2106.05390v3-Figure8-1.png",
    "caption": " Number of tasks that use a given critical dimension for each convolutional layer for a model trained on 20-split CIFAR100. Critical dimensions are defined as those that when turned off affect the correct classification of samples over a threshold. As we move closer to the output, layers increase the amount of critical modules, as well as intertask dependency on dimensions. However, no dimensions are universally useful. This suggests that while knowledge is reusable, there is still task-specific knowledge being stored in the KB. We use a threshold of N=5 samples which is a 1% deviation in accuracy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which block has the most critical modules?",
    "answer": "Block 3",
    "rationale": "The figure shows that Block 3 has the highest bars, which indicates that it has the most critical modules.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05390v3",
    "pdf_url": null
  },
  {
    "instance_id": "e0d7ee38b9e54a20926ea6325aa4e0a8",
    "figure_id": "2105.00187v1-Figure1-1",
    "image_file": "2105.00187v1-Figure1-1.png",
    "caption": " Illustration of the DeepFake-in-the-Wild (DFW). The deepfakes shown in (a) are of the celebrities created by various unknown generation methods. In (b) famous movie characters are replaced with some other celebrity and (c) contains deepfakes of famous political figures.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sub-figure shows a deepfake of a celebrity created by an unknown method?",
    "answer": "Sub-figure (a)",
    "rationale": "Sub-figure (a) shows three sets of images. Each set contains two images, one real and one deepfake. The caption states that the deepfakes in (a) were created by unknown methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.00187v1",
    "pdf_url": null
  },
  {
    "instance_id": "acf5599a726e4469b3a9d87995dcd961",
    "figure_id": "2103.00436v1-Figure5-1",
    "image_file": "2103.00436v1-Figure5-1.png",
    "caption": " Cumulative CTR of online experiments. We report the relative improvements compared with the Uniform algorithm. There are more than 150,000 impressions per algorithm per day, covering more than 290,000 creatives.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in terms of relative CTR gain?",
    "answer": "FM-EG",
    "rationale": "The figure shows the relative CTR gain for different algorithms over time. The FM-EG line is the highest at the end of the experiment, indicating that it had the highest relative CTR gain.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.00436v1",
    "pdf_url": null
  },
  {
    "instance_id": "520d17d4fab240a9ad4de9945f1416ea",
    "figure_id": "2010.13927v1-Figure3-1",
    "image_file": "2010.13927v1-Figure3-1.png",
    "caption": " NMAE vs rank initialization for fraction of observed entries of 0.5 (left) and 0.25 (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the five methods performed best overall?",
    "answer": "softImpute-ALS",
    "rationale": "The NMAE value for softImpute-ALS is lower than that of the other methods across the entire range of d values for both the 0.5 and 0.25 fraction of observed entries.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.13927v1",
    "pdf_url": null
  },
  {
    "instance_id": "3b064fefc2d947939b4559536e2e6596",
    "figure_id": "1812.10382v1-Figure1-1",
    "image_file": "1812.10382v1-Figure1-1.png",
    "caption": " Temporal evolution of English language and the banded structure",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the anchor difference change over time?",
    "answer": "The anchor difference increases over time.",
    "rationale": "The figure shows the anchor difference between two words in the English language over time. The x-axis shows the time in decades (a) and years (b), and the y-axis shows the anchor difference. The color of the cells indicates the magnitude of the anchor difference, with darker colors indicating a larger difference. The figure shows that the anchor difference increases over time, as the cells become darker as time progresses.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.10382v1",
    "pdf_url": null
  },
  {
    "instance_id": "436de8cd9c794ab6979aecde4ea8698b",
    "figure_id": "2205.02392v1-Figure8-1",
    "image_file": "2205.02392v1-Figure8-1.png",
    "caption": " Defense human evaluation results. Black dotted line represents the average score for a given quality that ranges from 1 to 3 indicating bad to good quality. Each bar plot demonstrates proportion of workers that rated a particular score (red for bad, yellow for moderate, and green for good). Toxicity ratings are binary.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four categories (Coherency, Fluency, Relevance, Toxicity) has the most agreement among the workers?",
    "answer": "Toxicity",
    "rationale": "The figure shows that for Toxicity, almost all workers agree that the responses are not toxic. This is indicated by the tall green bars and very small red bars in the Toxicity plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.02392v1",
    "pdf_url": null
  },
  {
    "instance_id": "49991e59e1cd49de9afa49b7effb964c",
    "figure_id": "2110.14032v1-Figure1-1",
    "image_file": "2110.14032v1-Figure1-1.png",
    "caption": " Accuracy vs sparsity ratio on ImageNet using ResNet-50 dense model. Our proposed MEST framework: MEST+EM (Elastic Mutation) and MEST+EM&S (with Soft Memory Bound) are compared with the SOTA sparse training algorithms i.e., GraSP [10], SNIP [9], RigL [11], SNFS [12], DSR [13], SET [14], and DeepR [15].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sparsity training algorithm achieved the highest accuracy on ImageNet with a sparsity ratio of 90%?",
    "answer": "MEST+EM&S",
    "rationale": "The figure shows the accuracy of different sparsity training algorithms on ImageNet with varying sparsity ratios. The MEST+EM&S line is the highest at a sparsity ratio of 90%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14032v1",
    "pdf_url": null
  },
  {
    "instance_id": "7602c5f3287d49159b93cd3f5f3b610c",
    "figure_id": "2305.09666v2-Figure1-1",
    "image_file": "2305.09666v2-Figure1-1.png",
    "caption": " (a) An overview of public datasets. AbdomenAtlas-8K stands out from other datasets due to its large number of annotated CT volumes. We have reviewed dataset names (and licenses). (b) Volume distribution of eight organs. The significant variations within and across organs presented in our AbdomenAtlas-8K present challenges for the multi-organ segmentation and the generalizability of models to different domains. More comparisons can be found in Appendix Table 3 and Figure 9.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which organ has the smallest volume?",
    "answer": "The gallbladder.",
    "rationale": "The box plot for the gallbladder in Figure (b) shows that the median volume of the gallbladder is the smallest among all the organs shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.09666v2",
    "pdf_url": null
  },
  {
    "instance_id": "25c3264e919b4c61a38f94408a7fb547",
    "figure_id": "2010.11401v2-Figure6-1",
    "image_file": "2010.11401v2-Figure6-1.png",
    "caption": " Impact of inner update times 𝑘 .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the best performance on the ML1M dataset according to the HR@10 metric?",
    "answer": "GRU4REC-TP (new)",
    "rationale": "The figure shows the performance of different methods on the ML1M dataset according to the HR@10 metric. The GRU4REC-TP (new) method has the highest HR@10 score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.11401v2",
    "pdf_url": null
  },
  {
    "instance_id": "94ad57a5e5604593ad32ecc660ae4ffc",
    "figure_id": "2307.05284v1-Figure3-1",
    "image_file": "2307.05284v1-Figure3-1.png",
    "caption": " Performances of typical algorithms of 7 settings in our benchmark.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the ACS Income (CA->PR) dataset?",
    "answer": "Inprocess-DP",
    "rationale": "The figure shows the performance of different algorithms on various datasets. The bars represent the target accuracy of each algorithm. For the ACS Income (CA->PR) dataset, the Inprocess-DP algorithm has the highest target accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.05284v1",
    "pdf_url": null
  },
  {
    "instance_id": "8aa3d40ad8e44ad3a540a8ea07e4a5ba",
    "figure_id": "2303.05338v2-Figure3-1",
    "image_file": "2303.05338v2-Figure3-1.png",
    "caption": " (a) The audio-visual and approximate uni-modal accuracy. (b) The batch-average uni-modal logit scores. (c-d) The observation of modality-wise weight of each label in norm.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which modality, audio or visual, has a higher weight for the label \"NEU\" in the AV model?",
    "answer": "Visual",
    "rationale": "Comparing the color intensities for \"NEU\" in panels (c) and (d) of the figure, we can see that the intensity is higher in panel (d), which corresponds to the visual modality. This indicates that the visual modality has a higher weight for the label \"NEU\" in the AV model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.05338v2",
    "pdf_url": null
  },
  {
    "instance_id": "83e386ee9979490a8680f33bfe2a46ac",
    "figure_id": "2011.05448v1-Figure7-1",
    "image_file": "2011.05448v1-Figure7-1.png",
    "caption": " Usefulness of Briefs reported by Crowdsourced and Volunteer Fact Checkers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of brief was perceived as most useful by both crowdsourced and volunteer fact checkers?",
    "answer": "Gold QABriefs",
    "rationale": "The figure shows that both crowdsourced and volunteer fact checkers rated Gold QABriefs as the most useful type of brief, with crowdsourced fact checkers giving them a score of 80.0 and volunteer fact checkers giving them a score of 72.0.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.05448v1",
    "pdf_url": null
  },
  {
    "instance_id": "fa546b6ab5fd4df5b7cd22ac50cfa831",
    "figure_id": "1906.03444v4-Figure5-1",
    "image_file": "1906.03444v4-Figure5-1.png",
    "caption": " Effectiveness of feature regeneration units at masking adversarial perturbations in DNN feature maps for images perturbed by universal perturbations (UAP [38], NAG [44], GAP [51] and sPGD [45]). Perturbation-free feature map (clean), different adversarially perturbed feature maps (Row 1) and corresponding feature maps regenerated by feature regeneration units (Row 2) are obtained for a single filter channel in conv1 1 layer of VGG-16 [59], along with an enlarged view of a small region in the feature map (yellow box). Feature regeneration units are only trained on UAP [38] attack examples but are very effective at suppressing adversarial artifacts generated by unseen attacks (e.g., NAG [44], GAP [51] and sPGD [45]).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the different types of adversarial perturbations used in the study?",
    "answer": "UAP, NAG, GAP, and sPGD.",
    "rationale": "The figure shows the effectiveness of feature regeneration units at masking adversarial perturbations in DNN feature maps for images perturbed by universal perturbations (UAP, NAG, GAP, and sPGD).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.03444v4",
    "pdf_url": null
  },
  {
    "instance_id": "4c1b945221584cb7be8f671d4c3e2321",
    "figure_id": "2202.00448v2-Figure11-1",
    "image_file": "2202.00448v2-Figure11-1.png",
    "caption": " Visualization of synthetic-real image matching on some typical objects.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the objects in the image is the most difficult for the algorithms to match?",
    "answer": "The less-textured object, which is the brown box.",
    "rationale": "The less-textured object has fewer features for the algorithms to match, which makes it more difficult to find a match in the real image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.00448v2",
    "pdf_url": null
  },
  {
    "instance_id": "ea6981ed945f47d7af325f707f176f84",
    "figure_id": "2106.02705v1-Figure5-1",
    "image_file": "2106.02705v1-Figure5-1.png",
    "caption": " Pareto frontiers on UCI-Adult dataset. Lower-left indicates better Pareto optimality, i.e. better fairness-accuracy trade-off.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models shown in the figure achieves the best fairness-accuracy trade-off?",
    "answer": "MTA-F",
    "rationale": "The figure shows the Pareto frontier for three different models: Vanilla MTL, Baseline, and MTA-F. The Pareto frontier is a set of points that represent the best possible trade-off between two objectives. In this case, the two objectives are fairness and accuracy. The lower-left point on the Pareto frontier represents the best possible trade-off between these two objectives. As can be seen in the figure, the MTA-F model has the lowest point on the Pareto frontier, which means that it achieves the best fairness-accuracy trade-off.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02705v1",
    "pdf_url": null
  },
  {
    "instance_id": "c2a03941829a4cab9294a39a122632f2",
    "figure_id": "2306.03291v2-Figure8-1",
    "image_file": "2306.03291v2-Figure8-1.png",
    "caption": " Quantitative performance of different SALT models and ARHMMs (averaged over 3 different runs) on the synthetic experiments presented in Section 5.2. The test-set log likelihood is shown as a function of lags in the SALT model, for both (A) the NASCAR® and (B) Lorenz synthetic datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the NASCAR® dataset?",
    "answer": "ARHMM",
    "rationale": "The ARHMM line is consistently above the other lines in the plot for the NASCAR® dataset, indicating that it achieved the highest test log-likelihood for all lag values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.03291v2",
    "pdf_url": null
  },
  {
    "instance_id": "eb99b73198b34c4888cba21e003369d4",
    "figure_id": "2112.04178v2-Figure1-1",
    "image_file": "2112.04178v2-Figure1-1.png",
    "caption": " Comparison of various methods on NTU RGB+D with the cross-subject benchmark in terms of accuracy and number of parameters. The proposed Ta-CNN achieves state-of-the-art performance with a tiny model size.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest accuracy on the NTU RGB+D dataset with the cross-subject benchmark?",
    "answer": "Ta-CNN",
    "rationale": "The figure shows the accuracy of various methods on the NTU RGB+D dataset with the cross-subject benchmark. The x-axis shows the number of parameters, and the y-axis shows the accuracy. The proposed Ta-CNN is shown as a red star and achieves the highest accuracy of all the methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.04178v2",
    "pdf_url": null
  },
  {
    "instance_id": "491ec976e6c641379e6ea0757b008b46",
    "figure_id": "2307.10163v1-Figure9-1",
    "image_file": "2307.10163v1-Figure9-1.png",
    "caption": " We plot the distribution of the datamodels weights for all the experiments. We clearly see that the effect of poisoned samples on other poisoned samples is generally higher than the effect of poisoned samples on clean samples, and than clean samples on each other.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which experiment shows the most significant difference between the effect of poisoned samples on other poisoned samples and the effect of poisoned samples on clean samples?",
    "answer": "Experiment 8.",
    "rationale": "In Experiment 8, the distribution of the datamodel weights for poisoned samples on other poisoned samples is much higher than the distribution of the datamodel weights for poisoned samples on clean samples. This is evident from the taller and more concentrated bars in the blue and orange histograms compared to the green histogram.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.10163v1",
    "pdf_url": null
  },
  {
    "instance_id": "ca50dfe8d8ca4c28a1c149264eeac985",
    "figure_id": "2101.05974v5-Figure7-1",
    "image_file": "2101.05974v5-Figure7-1.png",
    "caption": " Effect of sampling of different tree structures on inductive performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset performs better in terms of AUC?",
    "answer": "Wikipedia",
    "rationale": "The figure shows the AUC for two datasets, UCI and Wikipedia, as a function of the parameter k1. The Wikipedia dataset has a higher AUC than the UCI dataset for all values of k1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.05974v5",
    "pdf_url": null
  },
  {
    "instance_id": "5d8066ce0c2c4576b70ecf880b1344a5",
    "figure_id": "2303.09166v1-Figure10-1",
    "image_file": "2303.09166v1-Figure10-1.png",
    "caption": " Result with pairs of high-dimensional images. As a function of the encoding size of the model, we assess the nonlinear prediction of ground truth factors to quantify how well the learned representation encodes the respective factors. Content factors are denoted in bold, style factors in italic, and modality-specific factors in regular font. Each point denotes the average R2 score across three seeds and bands show one standard deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which factor is the most difficult for the model to predict when there are causal dependencies between the factors?",
    "answer": "Object rotation β",
    "rationale": "In Figure (b), the line for object rotation β is the lowest, indicating that the model has the lowest prediction performance for this factor.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.09166v1",
    "pdf_url": null
  },
  {
    "instance_id": "d1619a7e100a48718a53c39976f6c98c",
    "figure_id": "2302.01312v3-Figure5-1",
    "image_file": "2302.01312v3-Figure5-1.png",
    "caption": " Sample from noise distribution that was used to introduce stochasticity into Hopper and Pendulum.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the shape of the noise distribution?",
    "answer": "The noise distribution is multimodal.",
    "rationale": "The figure shows a histogram of the noise distribution. The histogram has multiple peaks, which indicates that the distribution is multimodal.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.01312v3",
    "pdf_url": null
  },
  {
    "instance_id": "abcd5089b6484b88a8050b67c2a5824d",
    "figure_id": "2004.10289v1-Figure7-1",
    "image_file": "2004.10289v1-Figure7-1.png",
    "caption": " Visual comparison of image synthesis results on the COCO-stuff dataset. We also display the bounding box detection predictions from Faster-RCNN. Other methods generate patterns that are continues throughout instances which makes the instances indistinguishable. Also note that in the last row our method is able to produce detectable car instances in a cluttered scene.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is able to produce detectable car instances in a cluttered scene?",
    "answer": "Our method.",
    "rationale": "The caption states that \"in the last row our method is able to produce detectable car instances in a cluttered scene.\" This is also evident in the figure, where the car is correctly detected by our method but not by the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.10289v1",
    "pdf_url": null
  },
  {
    "instance_id": "35592b1be25d43e49e132f07f434f89d",
    "figure_id": "2008.09435v1-Figure3-1",
    "image_file": "2008.09435v1-Figure3-1.png",
    "caption": " (a) Visualization of the BAS (top) and LAS (bottom) attention matrices that represent average attention alignment scores. Note that the abscissa and ordinate denote indices of input skeletons and predicted skeletons respectively. (b) Reconstruction loss curves when using no attention, BAS, MBAS or LAS for reconstruction.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attention mechanism results in the lowest reconstruction loss?",
    "answer": "LAS",
    "rationale": "The reconstruction loss curves show that the LAS attention mechanism (green line) has the lowest reconstruction loss compared to the other attention mechanisms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.09435v1",
    "pdf_url": null
  },
  {
    "instance_id": "02b7e756a87245a18ab33c7cc2c01480",
    "figure_id": "2306.06712v1-Figure16-1",
    "image_file": "2306.06712v1-Figure16-1.png",
    "caption": " Aggregated confusion matrices on clean CIFAR-10 images for all non-isomorphic networks in NAS-Bench-201.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class is most often confused with class 6?",
    "answer": "Class 9.",
    "rationale": "The confusion matrix shows the number of times each class was predicted to be another class. The value in the cell corresponding to row 6 and column 9 is the highest in row 6, indicating that class 6 was most often confused with class 9.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.06712v1",
    "pdf_url": null
  },
  {
    "instance_id": "5a856ec543664b0ebf0bad4ebe31e439",
    "figure_id": "2211.17228v2-Figure4-1",
    "image_file": "2211.17228v2-Figure4-1.png",
    "caption": " Inter-task correlations for ProxylessNAS (a), MobileNetV3 (b) and ResNet50 (c). We measure SRCC by comparing the performance values of individually trained architectures across all tasks.",
    "figure_type": "Table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task has the highest correlation with semantic segmentation for ResNet50?",
    "answer": "Instance segmentation.",
    "rationale": "The table for ResNet50 in (c) shows the Spearman's rank correlation coefficient (SRCC) between different tasks. The SRCC between semantic segmentation and instance segmentation is 0.951, which is the highest among all the other tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.17228v2",
    "pdf_url": null
  },
  {
    "instance_id": "552d08926dd54d7b90eb36e4a831d401",
    "figure_id": "2009.12547v2-Figure2-1",
    "image_file": "2009.12547v2-Figure2-1.png",
    "caption": " Three basic problems in existing pseudo-masks [63] (dataset: PASCAL VOC 2012 [14]): (a) Object Ambiguity, (b) Incomplete Background, (c) Incomplete Foreground. They usually combine to cause other complications. The context (mean image per class) may provide clues for the reasons.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the three basic problems in existing pseudo-masks is illustrated in the example with the car?",
    "answer": " Incomplete Foreground.",
    "rationale": " The ground-truth mask shows the entire car, but the pseudo-mask only shows the front part of the car. This indicates that the pseudo-mask is incomplete and does not capture the entire foreground object.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.12547v2",
    "pdf_url": null
  },
  {
    "instance_id": "80ae7e0239d048448a185f5130dbd044",
    "figure_id": "2007.03204v2-Figure9-1",
    "image_file": "2007.03204v2-Figure9-1.png",
    "caption": " Orders of magnitude reduction in the number of branching steps which translates to wall-clock improvements as problems get harder. Note, as explain in the paper, Python startup overhead skews results on easy problems.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms is the most efficient in terms of the number of branching steps?",
    "answer": "SharpSAT",
    "rationale": "The figure shows that SharpSAT consistently requires the fewest branching steps to solve the problems, regardless of the problem difficulty.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.03204v2",
    "pdf_url": null
  },
  {
    "instance_id": "6717698895fc4ad08949416c8fb668ad",
    "figure_id": "2304.11005v2-Figure17-1",
    "image_file": "2304.11005v2-Figure17-1.png",
    "caption": " Hyperparameter convergence on the 8-dimensional GP sample for the BoTorch priors. The black dashed line indicates true hyperparameter values. Mean and standard deviation are plotted across 20 repetitions, and a 3 iteration moving average of the plotted moments is applied to increase readability. Lengthscales ℓd ordered smallest (most important) to largest (least important). SCoreBO finds accurate hyperparameters faster, has the most accurate values for all hyperparameters, and has substantially lower variance for all important (i.e. not ℓ6, ℓ7, and ℓ8) hyperparameters except for the noise variance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which hyperparameter has the largest difference in variance between Noisy EI and ScoreBO?",
    "answer": "The noise variance (σ^2).",
    "rationale": "The figure shows the mean and standard deviation of the hyperparameter values across 20 repetitions for both Noisy EI and ScoreBO. The standard deviation of the noise variance is much larger for Noisy EI than for ScoreBO. This can be seen by comparing the width of the gray and red shaded regions for the noise variance plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.11005v2",
    "pdf_url": null
  },
  {
    "instance_id": "ea668e484fd24021b475aee0081523ff",
    "figure_id": "2203.02343v2-Figure6-1",
    "image_file": "2203.02343v2-Figure6-1.png",
    "caption": " Affinity network for the dataset 2017-HSC with a threshold at 10%",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which node has the most connections?",
    "answer": "EM",
    "rationale": "The EM node is connected to four other nodes, while all other nodes are connected to three or fewer nodes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.02343v2",
    "pdf_url": null
  },
  {
    "instance_id": "34b8c9290495453ea24b8a315368ae14",
    "figure_id": "2202.00113v2-Figure8-1",
    "image_file": "2202.00113v2-Figure8-1.png",
    "caption": " Bouncing balls experiment. Left: reported MSE for the proposed InImNet and state-ofthe-art methods, results from other methods are taken from Yıldız et al. (2019) and Vialard et al. (2020). Right: average time consumption per epoch.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of MSE?",
    "answer": "InImNet with p_min = -3.",
    "rationale": "The left panel of the figure shows the MSE for different methods. The line corresponding to InImNet with p_min = -3 is consistently lower than the other lines, indicating that it has the lowest MSE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.00113v2",
    "pdf_url": null
  },
  {
    "instance_id": "33de3fa9229f45e4a9a1dd1bec81e60e",
    "figure_id": "2012.12482v1-Figure7-1",
    "image_file": "2012.12482v1-Figure7-1.png",
    "caption": " Sample results from crowd counting datasets. For each sample we show: the original image, the ground truth density map (A), the baseline density map (by Bayesian loss (Ma et al. 2019)) (B), the topological map by TopoCount (C), and the density map by baseline + TopoCount (D). With the addition of the topological map, the estimated density map (D) has better topological structure and fixes shadowed regions missed by the baseline (Ma et al. 2019) (B).",
    "figure_type": "** Photographs",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the following methods produces a density map that is closest to the ground truth density map?",
    "answer": " Baseline + TopoCount",
    "rationale": " The figure shows that the density map produced by Baseline + TopoCount has the smallest count error and the highest F-score. This indicates that it is closest to the ground truth density map.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.12482v1",
    "pdf_url": null
  },
  {
    "instance_id": "dedc884f91844e75b2e52e2a55afd7aa",
    "figure_id": "2307.03810v2-Figure1-1",
    "image_file": "2307.03810v2-Figure1-1.png",
    "caption": " Zero-shot uncertainty estimates of pretrained models (bars) do not reach the performance of many-shot models yet (dashed line). The URL benchmark aims to guide the field to close this gap. Minimum, average, and maximum R-AUROC across three seeds.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture achieves the highest R-AUROC on downstream test classes when trained with the InfoNCE loss function?",
    "answer": "ResNet 50.",
    "rationale": "The figure shows the R-AUROC for different model architectures and loss functions. The ResNet 50 model trained with the InfoNCE loss function achieves the highest R-AUROC of approximately 0.575.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.03810v2",
    "pdf_url": null
  },
  {
    "instance_id": "02fc942a24ea4d3da80e3e2b43ff6602",
    "figure_id": "1709.06079v2-Figure10-1",
    "image_file": "1709.06079v2-Figure10-1.png",
    "caption": " Results of Riemannian optimization methods to solve OMDSM on MNIST dataset under the 6-layer MLP. We train the model with batch size of 512 and show the training loss curves for different learning rate of ‘EI+QR’, ‘CI+QR’ and ‘CayT’ compared to the baseline ‘plain’ in (a), (b) and (c) respectably. We compare our methods to baselines and report the best performance among all learning rates based on the training loss for each method in (d).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest training loss?",
    "answer": "Our OLM method has the lowest training loss.",
    "rationale": "The training loss curves for each method are shown in (d). Our OLM method is shown in red and has the lowest training loss of all the methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1709.06079v2",
    "pdf_url": null
  },
  {
    "instance_id": "abd73ec5b1a14d058f6223b229ba4008",
    "figure_id": "2303.11502v3-Figure6-1",
    "image_file": "2303.11502v3-Figure6-1.png",
    "caption": " Precision-Recall curves of models trained from different label source under weakly supervised setup.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the highest max F_beta on the SOD dataset?",
    "answer": "Cls+Label+Sketch",
    "rationale": "The table shows the max F_beta for each method on the SOD dataset. Cls+Label+Sketch has the highest value of 0.813.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.11502v3",
    "pdf_url": null
  },
  {
    "instance_id": "d3c5f8a280eb4c50b2dee0e61e6f6710",
    "figure_id": "1803.01682v6-Figure1-1",
    "image_file": "1803.01682v6-Figure1-1.png",
    "caption": " Comparison of related variants of VAE models. Note that user variables are not included in the graphs for clarity. (a) VAE; (b) CVAE-CF with auxiliary variables; (c) Joint Variational Auto-Encoder-Collaborative Filtering (JVAE-CF); (d) JMVAE; and, (e) List-CVAE (ours) with the whole slate as input.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models shown in the figure explicitly accounts for the user's past interactions?",
    "answer": "List-CVAE",
    "rationale": "The figure shows that List-CVAE takes the whole slate of items as input, which represents the user's past interactions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1803.01682v6",
    "pdf_url": null
  },
  {
    "instance_id": "011aefb8e4664033aac61e81a174dbab",
    "figure_id": "1809.06709v2-Figure2-1",
    "image_file": "1809.06709v2-Figure2-1.png",
    "caption": " Document retrieval performance (precision) at different retrieval fractions. Observe different y-axis scales.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which document representation method achieves the highest precision for the IR: 20NS dataset?",
    "answer": "iDeepDNE2",
    "rationale": "The figure shows the precision of different document representation methods for different datasets. The line for iDeepDNE2 is the highest for the IR: 20NS dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.06709v2",
    "pdf_url": null
  },
  {
    "instance_id": "b6c3079ab62047fc9533134548e1d26e",
    "figure_id": "1904.01318v1-Figure13-1",
    "image_file": "1904.01318v1-Figure13-1.png",
    "caption": " Asteroids. Target function: up-fire.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the image show?",
    "answer": "The image shows a field of asteroids.",
    "rationale": "The image shows a large number of small, irregularly shaped objects against a black background. These objects are likely asteroids, which are small, rocky bodies that orbit the Sun.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.01318v1",
    "pdf_url": null
  },
  {
    "instance_id": "58b55d2578544adc9965249dd1e730e6",
    "figure_id": "2104.06873v3-Figure4-1",
    "image_file": "2104.06873v3-Figure4-1.png",
    "caption": " Oracle queries vs. k for single-pass algorithms for the maxcut application on each dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset required the most oracle queries for all values of k?",
    "answer": "er",
    "rationale": "The er plot is the highest of all the plots, meaning that it required the most oracle queries for all values of k.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.06873v3",
    "pdf_url": null
  },
  {
    "instance_id": "aa3b7f1972a44667ac3f07a9b73a0154",
    "figure_id": "1906.00949v2-Figure3-1",
    "image_file": "1906.00949v2-Figure3-1.png",
    "caption": " Average performance of BEAR-QL, BCQ, Naïve RL and BC on medium-quality data averaged over 5 seeds. BEAR-QL outperforms both BCQ and Naïve RL. Average return over the training data is indicated by the magenta line. One step on the x-axis corresponds to 1000 gradient steps.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms performs the best on the Walker2d-v2 environment?",
    "answer": "BEAR-QL",
    "rationale": "The figure shows the average return of each algorithm on the Walker2d-v2 environment. BEAR-QL has the highest average return, which means it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.00949v2",
    "pdf_url": null
  },
  {
    "instance_id": "e39f0866e629495dbe55ce4951d3df0b",
    "figure_id": "2001.06111v1-Figure9-1",
    "image_file": "2001.06111v1-Figure9-1.png",
    "caption": " Performance on different datasets",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest cost for the PC algorithm?",
    "answer": "Wiki-en",
    "rationale": "The figure shows that the PC algorithm has the highest cost for the Wiki-en dataset, which is indicated by the tallest bar in the PC group.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.06111v1",
    "pdf_url": null
  },
  {
    "instance_id": "dd03bf7daf9c4ba0adb762a5d5e53599",
    "figure_id": "2007.05034v3-Figure4-1",
    "image_file": "2007.05034v3-Figure4-1.png",
    "caption": " The probability to go to the left for different algorithms in an environment similar to the maximization bias example from [31]. A lower probability indicates a better policy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in both settings?",
    "answer": "Q-learning",
    "rationale": "The figure shows the probability of taking the left action for different algorithms. A lower probability indicates a better policy. In both settings, Q-learning has the lowest probability of taking the left action.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.05034v3",
    "pdf_url": null
  },
  {
    "instance_id": "d490cf46d34e4ca19369ac9f508d2a6d",
    "figure_id": "1905.04088v2-Figure6-1",
    "image_file": "1905.04088v2-Figure6-1.png",
    "caption": " Comparisons of normal maps and angular error maps (in degrees) on COW (top) and POT1 (bottom) from DILIGENT [28].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest angular error for the COW model?",
    "answer": "CNN-PS",
    "rationale": "The figure shows the angular error maps for different methods. The CNN-PS method has the most red and yellow colors in its error map, which according to the color bar, indicates higher angular errors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.04088v2",
    "pdf_url": null
  },
  {
    "instance_id": "7c27e6505de24759880d56f8d2fdcafb",
    "figure_id": "2307.07929v1-Figure15-1",
    "image_file": "2307.07929v1-Figure15-1.png",
    "caption": " Example pre-training predictions on FUNSD sample 0060024314. For inputs, we visualize masked word boxes, and their text is replace by [MASK]. For predictions, we visualize the predicted word boxes of the masked inputs. Under each box prediction, we also visualize its corresponding word token predictions.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the chemical formula of Fuller G3115 DC?",
    "answer": "Vinyl Acetate",
    "rationale": "The chemical formula of Fuller G3115 DC is shown in the \"CHEMICAL FORMULA\" field of the form.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.07929v1",
    "pdf_url": null
  },
  {
    "instance_id": "a9fb122bde254268a932f0ce82294d65",
    "figure_id": "2104.11213v1-Figure9-1",
    "image_file": "2104.11213v1-Figure9-1.png",
    "caption": " Number of possible tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object has the highest number of tasks?",
    "answer": "Tomato",
    "rationale": "The figure shows the number of tasks for each object. The tomato has the highest bar, indicating it has the most tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.11213v1",
    "pdf_url": null
  },
  {
    "instance_id": "d01b8d5bf8f146ae8462ed3293f3b504",
    "figure_id": "2102.01690v2-Figure16-1",
    "image_file": "2102.01690v2-Figure16-1.png",
    "caption": " Task page for user study on whether a visual cluster consists of coherent clothing styles: one of the options is our algorithm’s discovered cluster, the other is a random grouping of images.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which group of images is more coherent in terms of clothing styles?",
    "answer": "Group A.",
    "rationale": "The images in Group A share a similar color palette and style, while the images in Group B are more diverse and do not seem to belong to the same category.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.01690v2",
    "pdf_url": null
  },
  {
    "instance_id": "b1cb76e546b7463c8dfee4b6fd885c1d",
    "figure_id": "2111.06849v1-Figure10-1",
    "image_file": "2111.06849v1-Figure10-1.png",
    "caption": " More examples of the effectiveness of our method to improve state-of-the-art StyleGAN2 [21] synthesized results (256 × 256) on CUB-12k [47] (11, 788 images, which is small by itself).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following techniques appears to produce the most realistic images of birds?",
    "answer": "Ours (trunc ψ = 0.7)",
    "rationale": "The images in the \"Ours (trunc ψ = 0.7)\" row appear to be the most realistic, with sharper details and more natural-looking colors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.06849v1",
    "pdf_url": null
  },
  {
    "instance_id": "47a5a9130b0a4fbbbcd340355ee7c53f",
    "figure_id": "1912.06989v1-Figure2-1",
    "image_file": "1912.06989v1-Figure2-1.png",
    "caption": " (a) RSE on single-manifold data with different number of columns (ρ = 0.5). (b) RSE on multiple-manifold data with different number of manifolds (ρ = 0.5).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on single-manifold data with a small number of columns?",
    "answer": "SRMC",
    "rationale": "In Figure (a), the SRMC line is the lowest for small values of n/m, indicating that it has the lowest RSE and therefore performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.06989v1",
    "pdf_url": null
  },
  {
    "instance_id": "052de4f9452e4b8c83b538fa72a15d06",
    "figure_id": "2210.00364v2-Figure1-1",
    "image_file": "2210.00364v2-Figure1-1.png",
    "caption": " Loss-capacity curves. Empirical loss-capacity curves (see § 4.1) for various representations (see legend), datasets (top: MPI3D-Real, bottom: Cars3D), and probe types (left: multi-layer perceptrons / MLPs, middle: Random Fourier Features / RFFs, right: Random Forests / RFs). The loss was first averaged over factors zj, and then means and 95% confidence intervals were computed over 3 random seeds. Details in § 6.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which representation performs the best on the MPI3D-Real dataset with MLP probes?",
    "answer": "ImageNet.",
    "rationale": "The ImageNet representation has the lowest loss across all capacities compared to the other representations in the top left plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.00364v2",
    "pdf_url": null
  },
  {
    "instance_id": "3b8255c1f2c1404da21ac739fb75f717",
    "figure_id": "1805.09042v2-Figure8-1",
    "image_file": "1805.09042v2-Figure8-1.png",
    "caption": " Higher frequency grid cells",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which grid cells are higher frequency, the ones on the top row or the ones on the bottom row?",
    "answer": "The ones on the top row are higher frequency.",
    "rationale": "The figure shows a set of grid cells with different frequencies. The cells on the top row have a higher frequency than the cells on the bottom row. This can be seen by looking at the size of the squares in each cell. The squares in the cells on the top row are smaller than the squares in the cells on the bottom row. This means that the cells on the top row have a higher frequency.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.09042v2",
    "pdf_url": null
  },
  {
    "instance_id": "97ce48d680be46709487bc79f45142f8",
    "figure_id": "1902.09037v2-Figure7-1",
    "image_file": "1902.09037v2-Figure7-1.png",
    "caption": " Information planes of networks, using different non-saturating activation functions in the hidden layers. However all networks used softmax function in the output layer. Therefore, the shapes of the leftmost lines on all the information planes show less variation. For every activation function 50 random initializations were trained and averaged mutual information values were used for the information planes presented above.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activation function has the least amount of variation in the information plane?",
    "answer": "Softmax.",
    "rationale": "The passage states that all networks used the softmax function in the output layer. This is reflected in the leftmost lines of all the information planes, which show less variation compared to the other lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.09037v2",
    "pdf_url": null
  },
  {
    "instance_id": "3f830604fc3443bf8a92abe1ab314ca9",
    "figure_id": "2210.15491v2-Figure1-1",
    "image_file": "2210.15491v2-Figure1-1.png",
    "caption": " (a) Global self-attention token mixing [8]. (b) Selfattention mixing along H and W axes [9]. (c) Convolutionmixing along H and W axes [6] (d) Heterogeneous multi-axial mixer (ours).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the panels in the figure depicts a heterogeneous multi-axial mixer?",
    "answer": "Panel (d)",
    "rationale": "The caption states that panel (d) depicts a heterogeneous multi-axial mixer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.15491v2",
    "pdf_url": null
  },
  {
    "instance_id": "1e041af110cd47b4a9d5b6c84079865a",
    "figure_id": "2102.11137v2-Figure6-1",
    "image_file": "2102.11137v2-Figure6-1.png",
    "caption": " Effect of varying the number of samples m on our approach, evaluated on the box-world over 5 random seeds. Mean and variance of (a) the average reward, (b) the average finishing time on the test tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two metrics, reward or finish time, is more sensitive to the number of samples used?",
    "answer": "Finish time.",
    "rationale": "The variance of the finish time is much higher than the variance of the reward, as shown by the larger y-axis range for the variance of finish time compared to the reward. This indicates that the finish time is more sensitive to the number of samples used.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.11137v2",
    "pdf_url": null
  },
  {
    "instance_id": "c4adb4897ccc4315b23ffce0ce750be6",
    "figure_id": "2002.11576v1-Figure6-1",
    "image_file": "2002.11576v1-Figure6-1.png",
    "caption": " F1 scores for prediction of female sex using embeddings from models trained on datasets with varying proportions of white and black individuals.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best for predicting female sex when the training set is composed of mostly white individuals?",
    "answer": "BetaVAE",
    "rationale": "The figure shows the F1 score for different models trained on datasets with varying proportions of white and black individuals. The BetaVAE model has the highest F1 score for white females when the percentage of white individuals in the training set is high.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11576v1",
    "pdf_url": null
  },
  {
    "instance_id": "64e7e227ee77422c90965efc6ecf7070",
    "figure_id": "2112.00305v1-Figure12-1",
    "image_file": "2112.00305v1-Figure12-1.png",
    "caption": " PRD curves on all datasets. kPF is competitive to the other methods in terms of Area Under Curve (AUC)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the CelebA dataset?",
    "answer": "SRAE_{kPF-RBF}",
    "rationale": "The PRD curves for all methods on the CelebA dataset are shown in the rightmost plot. The SRAE_{kPF-RBF} method has the highest AUC, indicating that it performs the best on this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.00305v1",
    "pdf_url": null
  },
  {
    "instance_id": "8addf0eefa1f4afe95f5a1615cb98d44",
    "figure_id": "2104.10116v1-Figure3-1",
    "image_file": "2104.10116v1-Figure3-1.png",
    "caption": " Illustration of labeling and sample selection for the third stage of VED training. Each vertical line represents a video frame. Positive samples are blocks of 3 frames each starting from frames labeled as “hits”. Negative samples are blocks of 3 frames each that are immediately adjacent to positive samples from 6 frames behind to 6 frames ahead of the hit",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many negative samples are there for the hit shown in the figure?",
    "answer": "13",
    "rationale": "The figure shows that negative samples are blocks of 3 frames each that are immediately adjacent to positive samples from 6 frames behind to 6 frames ahead of the hit. Since there are 6 frames before the hit and 7 frames after the hit, there are a total of 13 negative samples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.10116v1",
    "pdf_url": null
  },
  {
    "instance_id": "8f178f3ec3734926a1e8d0b8ae94865f",
    "figure_id": "1908.03265v4-Figure7-1",
    "image_file": "1908.03265v4-Figure7-1.png",
    "caption": " Performance of RAdam, Adam with warmup on CIFAR10 with different learning rates.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performs better, RAdam or Adam with warmup?",
    "answer": "RAdam performs better than Adam with warmup.",
    "rationale": "The figure shows the test accuracy and training loss of RAdam and Adam with warmup for different learning rates and warmup lengths. RAdam consistently achieves higher test accuracy and lower training loss than Adam with warmup.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.03265v4",
    "pdf_url": null
  },
  {
    "instance_id": "9b27ffc45f4447d0ae401d5830b94993",
    "figure_id": "2207.11717v4-Figure6-1",
    "image_file": "2207.11717v4-Figure6-1.png",
    "caption": " Samples from the MC-10 dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the locations in the images is closest to New York City?",
    "answer": "233rd Street station.",
    "rationale": "The figure shows that the 233rd Street station is on the IRT White Plains Road Line of the New York City Subway, which means it is located in New York City. The other three locations are in Philadelphia, Chicago, and Boston, respectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.11717v4",
    "pdf_url": null
  },
  {
    "instance_id": "ae523f16968a44779dc8e49f4098ef30",
    "figure_id": "2305.16283v4-Figure11-1",
    "image_file": "2305.16283v4-Figure11-1.png",
    "caption": " SG-FRONT dataset statistics for (a) Dining room, (b) bedroom, and (c) Living room scenes. Relationships are provided on the left, and the object occurrences on the right.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common relationship between objects in the bedroom?",
    "answer": "\"Close to\"",
    "rationale": "The bar chart for \"Close to\" is the tallest in the bedroom relationships plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16283v4",
    "pdf_url": null
  },
  {
    "instance_id": "ccec43fe600842709c1a4850e0bbaf9c",
    "figure_id": "1910.09413v3-Figure1-1",
    "image_file": "1910.09413v3-Figure1-1.png",
    "caption": " Sampling setup: J input signals x(j)(t), j = 1 · · · J are mixed using a matrix A and produce signals y(i)(t), i = 1 · · · I . Each y(i)(t) is then sampled using a time encoding machine TEM(i) which produces spike times { t (i) ` , ` = 1 · · ·n(i) spikes } .",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many input signals are there in the sampling setup?",
    "answer": "There are J input signals.",
    "rationale": "The left side of the figure shows J input signals, labeled x(j)(t), j = 1 · · · J.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.09413v3",
    "pdf_url": null
  },
  {
    "instance_id": "e13330576eb44d3ba89a78910acad63f",
    "figure_id": "2106.04756v2-Figure2-1",
    "image_file": "2106.04756v2-Figure2-1.png",
    "caption": " Number of problems solved for MIP Relaxations (top), LP benchmark (middle), and Netlib (bottom) datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best for the MIP Relaxations dataset with a tolerance of 1e-04?",
    "answer": "SCS (matrix-free)",
    "rationale": "The top left plot shows the performance of different algorithms for the MIP Relaxations dataset with a tolerance of 1e-04. The SCS (matrix-free) algorithm solves the highest fraction of problems within the given time limit.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04756v2",
    "pdf_url": null
  },
  {
    "instance_id": "b4b5d40ab7f04cf99c8c46a5c496bc4b",
    "figure_id": "2105.14039v3-Figure14-1",
    "image_file": "2105.14039v3-Figure14-1.png",
    "caption": " HCAM (labeled as HTM) performs better without gating [44] than with gating. On the fast-binding tasks HCAM with gating learns slightly more slowly and generalizes slightly worse than without gating. Gating of memory layers does not appear necessary for TrXL in our tasks, unlike the experiments of Parisotto et al. [44]. However, neither gated nor ungated TrXL are able to extrapolate to the tasks that gated or ungated HCAM does. (3 seeds per condition for main runs, 2 per condition for alternatives.)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the fast-binding tasks?",
    "answer": "HCAM without gating.",
    "rationale": "The figure shows that HCAM without gating has the highest accuracy on all three tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14039v3",
    "pdf_url": null
  },
  {
    "instance_id": "ec6a3714abc94d0bb66074302f6adf63",
    "figure_id": "2110.07560v2-Figure5-1",
    "image_file": "2110.07560v2-Figure5-1.png",
    "caption": " Percentage of parameters selected for the sparse fine-tuning of both languages in a pair.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language pair has the highest percentage of parameters selected for sparse fine-tuning?",
    "answer": "ar-zh",
    "rationale": "The figure shows a heatmap of the percentage of parameters selected for sparse fine-tuning for each pair of languages. The color of each square represents the percentage, with darker colors representing higher percentages. The square corresponding to the ar-zh language pair is the darkest, indicating that this pair has the highest percentage of parameters selected for sparse fine-tuning.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.07560v2",
    "pdf_url": null
  },
  {
    "instance_id": "e7c0fd3a570340ae867aa0002cd03219",
    "figure_id": "2306.04039v1-Figure4-1",
    "image_file": "2306.04039v1-Figure4-1.png",
    "caption": " Distributions of recommendations over log-scaled recommendation frequency buckets. Lower bars in higher buckets indicates that fewer head items are shown.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model shows the most head items are shown?",
    "answer": "Dot product",
    "rationale": "The lower bars in higher buckets indicates that fewer head items are shown. Dot product has the lowest bars in the highest buckets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.04039v1",
    "pdf_url": null
  },
  {
    "instance_id": "6e20d2aa056f4730b6d3be9ba401f989",
    "figure_id": "2010.15768v2-Figure1-1",
    "image_file": "2010.15768v2-Figure1-1.png",
    "caption": " Convergence speed of Smoothed-GDA and the algorithm in [20].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges faster, Smoothed-GDA or the algorithm in [20]?",
    "answer": "Smoothed-GDA converges faster.",
    "rationale": "The figure shows the loss function of both algorithms over time. The loss function of Smoothed-GDA decreases more quickly than the loss function of the algorithm in [20]. This indicates that Smoothed-GDA is converging faster.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.15768v2",
    "pdf_url": null
  },
  {
    "instance_id": "57f3a3bd6e174a21a1d59b4eeaaec9fe",
    "figure_id": "2311.05924v1-Figure2-1",
    "image_file": "2311.05924v1-Figure2-1.png",
    "caption": " A toy schematic to compare the naive aggregation and normalized aggregation of the local updates, where the number of clients is 2 and the local intervals are set as 1. The solid line indicates the client’s local update △i, θi is the angle between the local update and the global update, and the dotted line represents the clients’ contribution on the global update. The red lines are the aggregated global update △. The main difference is ∥△∥, the norm of the global update. When adopting the naive aggregation method, the global norm ∥△∥ = ∑",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method results in a larger norm of the global update?",
    "answer": "Vanilla Aggregation",
    "rationale": "The figure shows that the norm of the global update for Vanilla Aggregation is ∥△∥ = ∑ , while the norm of the global update for Normalized Aggregation is ∥△∥ = 1. Since the number of clients is 2, the norm of the global update for Vanilla Aggregation is larger.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2311.05924v1",
    "pdf_url": null
  },
  {
    "instance_id": "a64c20f74e034b679329cfec35804cc2",
    "figure_id": "2106.04465v2-Figure6-1",
    "image_file": "2106.04465v2-Figure6-1.png",
    "caption": " GoF testing for the SPP using additional scenarios.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which test statistic has the highest ROC AUC score for the IncreasingRate scenario?",
    "answer": "S3 statistic",
    "rationale": "The S3 statistic line is the highest among all the lines for the IncreasingRate scenario.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04465v2",
    "pdf_url": null
  },
  {
    "instance_id": "b988c6778c3c47968e971b79cfc08bdc",
    "figure_id": "2006.14076v2-Figure7-1",
    "image_file": "2006.14076v2-Figure7-1.png",
    "caption": " Survival plots for the results in Section 5. The horizontal dashed line is the upper bound on the number of verifiable images.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which verification method is the fastest for MNIST ConvSmall?",
    "answer": "FastC2V",
    "rationale": "The figure shows the number of images verified as a function of time for different verification methods and datasets. For MNIST ConvSmall, the FastC2V curve reaches the upper bound on the number of verifiable images first, indicating that it is the fastest method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.14076v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd96266bf7c34f56a18278869f11b643",
    "figure_id": "2303.17713v3-Figure8-1",
    "image_file": "2303.17713v3-Figure8-1.png",
    "caption": " Identification of high accuracy regimes for NLP datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows a higher accuracy for all latent factors (LFs) at a distance of 10?",
    "answer": "CivilComments.",
    "rationale": "The plot for CivilComments shows that all LFs have an accuracy of at least 0.7 at a distance of 10, while the plot for hateExplain shows that some LFs have an accuracy below 0.7 at the same distance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.17713v3",
    "pdf_url": null
  },
  {
    "instance_id": "a46d105901634cadab185e4eea7225b0",
    "figure_id": "2210.06284v4-Figure4-1",
    "image_file": "2210.06284v4-Figure4-1.png",
    "caption": " The predictions of C-AVP-v0 vs. C-AVP on (CIFAR10, ResNet18).",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class has the highest percentage of correct predictions for both C-AVP-v0 and C-AVP?",
    "answer": "Airplane",
    "rationale": "The figure shows the percentage of correct predictions for each class. The highest percentage for both C-AVP-v0 and C-AVP is for the \"airplane\" class.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06284v4",
    "pdf_url": null
  },
  {
    "instance_id": "77498f8d150f4dc8b1c81ae1ad642e90",
    "figure_id": "2011.04163v1-Figure3-1",
    "image_file": "2011.04163v1-Figure3-1.png",
    "caption": " Number of books in which the most frequent header formats occur the most frequently",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most frequent header format in the dataset?",
    "answer": "\"TITLE\"",
    "rationale": "The figure shows a bar plot of the frequency of different header formats. The bar for \"TITLE\" is the highest, indicating that it is the most frequent header format.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.04163v1",
    "pdf_url": null
  },
  {
    "instance_id": "e823c96a55f34c029ebb2c119f1bdb6b",
    "figure_id": "2012.03801v2-Figure11-1",
    "image_file": "2012.03801v2-Figure11-1.png",
    "caption": " Layerwise two-dimensional t-SNE embeddings of δc, obtained as a decomposition of {Gl}Ll=1 in (Papyan 2018) show C clusters in each subplot (DenseNet on CIFAR10). The last subplot “full” corresponds to δc of the entire G matrix.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many clusters are there in each layer of the DenseNet on CIFAR10?",
    "answer": "There are C clusters in each layer.",
    "rationale": "The caption states that \"Layerwise two-dimensional t-SNE embeddings of δc, obtained as a decomposition of {Gl}Ll=1 in (Papyan 2018) show C clusters in each subplot (DenseNet on CIFAR10).\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.03801v2",
    "pdf_url": null
  },
  {
    "instance_id": "5b94a33c28ed4c948a38fe955019f541",
    "figure_id": "1811.02921v3-Figure3-1",
    "image_file": "1811.02921v3-Figure3-1.png",
    "caption": " Comparison of NP-Hard rules with our polynomial rules for |C| = 17, |S| = 80, |V | = 51. We cannot scale this graph in the same was Figure 1 due to the high computational cost of computing the winning sets for k−Median and Chamberlin-Courant. However, from this small sample we see that Weights, STV, and AV all outperform Chamberlin-Courant and k−Median in terms of agreement.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which rule performs the best in terms of agreement?",
    "answer": "Weights",
    "rationale": "The Weights line is the highest on the graph, indicating that it has the highest agreement mean.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.02921v3",
    "pdf_url": null
  },
  {
    "instance_id": "29b3abe781d241d28c98f9dadd6a0279",
    "figure_id": "2106.04399v2-Figure6-1",
    "image_file": "2106.04399v2-Figure6-1.png",
    "caption": " The top-k return (mean over 3 runs) in the 4-D Hyper-grid task with active learning. GFlowNet gets the highest return faster.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the highest top-k return after 15 rounds of acquisition?",
    "answer": "GFlowNet",
    "rationale": "The plot shows the top-k return for each algorithm as a function of the number of rounds of acquisition. After 15 rounds, the GFlowNet line is the highest, indicating that it achieves the highest top-k return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04399v2",
    "pdf_url": null
  },
  {
    "instance_id": "fae93afcccfc4360b88ceabe9865716d",
    "figure_id": "2208.06102v2-Figure19-1",
    "image_file": "2208.06102v2-Figure19-1.png",
    "caption": " Cumulative regret of Zeus vs. Grid Search across all workloads.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model shows the least cumulative regret?",
    "answer": "NeuMF",
    "rationale": "The cumulative regret is the lowest for NeuMF as its curve is the lowest on the y-axis across all the subplots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.06102v2",
    "pdf_url": null
  },
  {
    "instance_id": "e0fe42c2adbb43ffb14100e394e51341",
    "figure_id": "1909.03638v1-Figure8-1",
    "image_file": "1909.03638v1-Figure8-1.png",
    "caption": " Example scenarios of the CS task with N = 3 selectable (orange colored) and U = 1 unselectable (green dashed) circles, with K = 2 selected (shaded) circles. The assigned commands are represented by the arrows. The agent receives (a) negative reward if selected circles overlap with unselectable one; (b) zero reward if only selected circles are overlapped with each other; and (c) positive reward if there is no overlap.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens when the selected circles overlap with the unselectable circle?",
    "answer": "The agent receives a negative reward.",
    "rationale": "The figure shows that when the selected circles (shaded) overlap with the unselectable circle (green dashed), the agent receives a negative reward (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.03638v1",
    "pdf_url": null
  },
  {
    "instance_id": "59a2423d5e1241818f3cf60a1abe4a7e",
    "figure_id": "2303.14817v1-Figure11-1",
    "image_file": "2303.14817v1-Figure11-1.png",
    "caption": " Batch Normalization statistics at various layers. TSMST is trained at 16 Frame and both models are evaluated at 16 Frame as well. The statistics are calculated from the fourth stage of ResNet-50.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has a higher variance in the fourth stage of ResNet-50?",
    "answer": "FFN",
    "rationale": "The variance plot shows that the FFN model has a higher variance than the ST model at all layers of the fourth stage of ResNet-50.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.14817v1",
    "pdf_url": null
  },
  {
    "instance_id": "67c044b87e064b18b13a0c5754dedbfe",
    "figure_id": "2110.01899v2-Figure14-1",
    "image_file": "2110.01899v2-Figure14-1.png",
    "caption": " SVM test accuracy using different kernels. Raw Fashion-MNIST dataset (Number of features p = 784). Number of samples n = 1024 fixed, varying number of random features from m = 70 to m = 700. Note that the y-axis is zoomed in to better distinguish the performance of different methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which kernel has the highest test accuracy?",
    "answer": "The TRF kernel has the highest test accuracy.",
    "rationale": "The figure shows the test accuracy of two kernels, Arc-Cosine RF and TRF, on the Raw Fashion-MNIST dataset. The TRF kernel has a higher test accuracy than the Arc-Cosine RF kernel for all values of m.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.01899v2",
    "pdf_url": null
  },
  {
    "instance_id": "6b497f1e59ac45758cdd8260eb41fe43",
    "figure_id": "1909.05479v2-Figure6-1",
    "image_file": "1909.05479v2-Figure6-1.png",
    "caption": " Hermite-SaaS trains faster. We plot the number of outer epochsMO vs. the pseudo-label accuracy across 4 datasets. We consistently observe that the minimum number of outer epochs MO to reach a given value of pseudo-label accuracy is always lower for Hermite-SaaS than ReLU-SaaS.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activation function requires fewer epochs to reach a given pseudo-label accuracy?",
    "answer": "Hermite",
    "rationale": "The figure shows that for each dataset, the minimum number of outer epochs required to reach a given pseudo-label accuracy is lower for Hermite-SaaS than ReLU-SaaS.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.05479v2",
    "pdf_url": null
  },
  {
    "instance_id": "45cc1d4ca424414da3ae989393c0156c",
    "figure_id": "1804.02772v2-Figure5-1",
    "image_file": "1804.02772v2-Figure5-1.png",
    "caption": " MNIST experiment (10 repetitions). The mean performance for each method is reported in Panel (a). We compared all variations of our proposed methods with two baselines: Baseline SGD and ActiveBias [4]. All our methods perform better than the baselines. AnnealPDS performs best. For better visualization, Panel (b) shows the mean and standard deviation of our proposed Anneal PDS comparing with two baselines closely.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in the MNIST experiment?",
    "answer": "AnnealPDS",
    "rationale": "Panel (a) of the figure shows the mean performance of each method, and AnnealPDS has the lowest test error rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.02772v2",
    "pdf_url": null
  },
  {
    "instance_id": "63ffa74e7bf240a99808053f3b1af85d",
    "figure_id": "2306.16780v1-Figure5-1",
    "image_file": "2306.16780v1-Figure5-1.png",
    "caption": " Heatmap of property similarity and statistics of target property sampling times on Tox21.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two nuclear receptors have the highest property similarity?",
    "answer": "NR-AR and NR-LBD",
    "rationale": "The heatmap on the left shows the property similarity between different nuclear receptors. The color of the square indicates the degree of similarity, with red being the most similar and blue being the least similar. The square at the intersection of NR-AR and NR-LBD is the most red, indicating that these two receptors have the highest property similarity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.16780v1",
    "pdf_url": null
  },
  {
    "instance_id": "2982438225ab44f497a48b0038b1dd85",
    "figure_id": "2109.15316v1-Figure1-1",
    "image_file": "2109.15316v1-Figure1-1.png",
    "caption": " MCTS vs RL Fine-Tuning. (a)When the average time budget is on the order of 1-10 seconds, RL Fine-Tuning consistently outperforms MCTS. (b)RL Fine-Tuning also outperforms MCTS in terms of sample efficiency. The shaded area represent the min/max range across 5 seeds. The curves are smoothed with an exponential moving average.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more sample-efficient, MCTS or RL Fine-Tuning?",
    "answer": "RL Fine-Tuning is more sample-efficient than MCTS.",
    "rationale": "Figure (b) shows that RL Fine-Tuning achieves a higher total cumulative reward with fewer samples per step compared to MCTS. This indicates that RL Fine-Tuning is more efficient in its use of samples to learn.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.15316v1",
    "pdf_url": null
  },
  {
    "instance_id": "a41dd9ea19b7420bacc3d54f24f51873",
    "figure_id": "1905.08287v1-Figure5-1",
    "image_file": "1905.08287v1-Figure5-1.png",
    "caption": " Results of rank aggregation experiment using synthetic data.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of Weighted Kendall Tau Distance for rank aggregation?",
    "answer": "Hypergraph.",
    "rationale": "The figure shows that the Hypergraph method has the highest Weighted Kendall Tau Distance for all values of p.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.08287v1",
    "pdf_url": null
  },
  {
    "instance_id": "c99b6009eaf84d8fb1b9f5b6e7e8c3ed",
    "figure_id": "2203.13474v5-Figure9-1",
    "image_file": "2203.13474v5-Figure9-1.png",
    "caption": " Maximum Length of Completion versus Pass Rate.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which average maximum length of completion is associated with the highest pass rate?",
    "answer": "536",
    "rationale": "The bar chart shows that the pass rate is highest for an average maximum length of completion of 536.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.13474v5",
    "pdf_url": null
  },
  {
    "instance_id": "49a771692cf44d338edfe2ee84e91ec8",
    "figure_id": "1905.13633v1-Figure13-1",
    "image_file": "1905.13633v1-Figure13-1.png",
    "caption": " Demonstrating the GDU property in the prototypical setting (as predicted by Theorem 1) with the fully connected layered architecture with three hidden layers on MNIST.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which hidden layer shows the greatest amount of gradient descent update (GDU)?",
    "answer": "The first hidden layer.",
    "rationale": "The GDU is the difference between the gradient descent update and the backpropagation through time (BPTT) update. In the figure, the GDU is shown for each hidden layer. The first hidden layer has the largest GDU, as indicated by the largest difference between the two lines in the top left plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.13633v1",
    "pdf_url": null
  },
  {
    "instance_id": "c62164a1b8684a5290202297ecfe27d6",
    "figure_id": "2303.17867v1-Figure18-1",
    "image_file": "2303.17867v1-Figure18-1.png",
    "caption": " Visual comparison of artistic video style transfer. The odd rows show the stylization effect. The even rows show the temporal error heatmap of adjacent frames.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods shown in the figure produces the most temporally consistent stylization?",
    "answer": "Ours.",
    "rationale": "The temporal error heatmap of adjacent frames for our method is the least intense, indicating that the stylization is more temporally consistent than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.17867v1",
    "pdf_url": null
  },
  {
    "instance_id": "c7439a846daf48a88212a2eebc893fd4",
    "figure_id": "2005.13718v1-Figure3-1",
    "image_file": "2005.13718v1-Figure3-1.png",
    "caption": " Selected cases of the dependencies between user characteristics and the RMSE delta (higher value means higher quality loss forminimization) incurred byminimization: (a) Number of ratings in the full profile, kNN, Random, (b) Average similarity to all users in the data, kNN, Random, (c) RMSE of recommendations over full profile, kNN, Random (d) Average rating value in the full profile, SVD, Most Favorite, (e) Average rating value in the full profile, SVD, Least Favorite, (e) Average rating value in the full profile, SVD, Random. In each of the plots, a dot corresponds to one minimizing user.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which user characteristic is most strongly correlated with the RMSE delta for the kNN algorithm?",
    "answer": "The RMSE of recommendations over the full profile.",
    "rationale": "In Figure (c), the dots are clustered more tightly around a trend line than in the other figures for the kNN algorithm (Figures (a) and (b)). This indicates that the RMSE of recommendations over the full profile is more strongly correlated with the RMSE delta than the other user characteristics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.13718v1",
    "pdf_url": null
  },
  {
    "instance_id": "724f86eef6b34720baffa095eda425f7",
    "figure_id": "2201.07877v2-Figure5-1",
    "image_file": "2201.07877v2-Figure5-1.png",
    "caption": " 1D OLO with stochastic losses. The plot shows the negative cumulative loss as a function of T ; higher is better.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which strategy performed the best in terms of cumulative loss over time?",
    "answer": "V^−1/2 (ours)",
    "rationale": "The plot shows the negative cumulative loss for each strategy as a function of time. The higher the line on the plot, the better the performance. We can see that the V^−1/2 (ours) line is generally higher than the other lines, indicating that it had the highest cumulative loss over time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.07877v2",
    "pdf_url": null
  },
  {
    "instance_id": "18a6369636bb4588b07fe27b3a2ee5cc",
    "figure_id": "2010.04941v1-Figure2-1",
    "image_file": "2010.04941v1-Figure2-1.png",
    "caption": " Corpus frequency of all nouns and IP nouns.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the difference in frequency between all nouns and in-pattern nouns?",
    "answer": "In-pattern nouns are less frequent than all nouns.",
    "rationale": "The figure shows that the frequency of in-pattern nouns is lower than the frequency of all nouns at all ranks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.04941v1",
    "pdf_url": null
  },
  {
    "instance_id": "6a3c06bf7f964d3eb6360c17d4e0ba7c",
    "figure_id": "2206.08780v2-Figure19-1",
    "image_file": "2206.08780v2-Figure19-1.png",
    "caption": " Comparison of the evolution of the Wasserstein distance over epochs between SWAE and SSWAE on MNIST (averaged over 5 trainings).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better according to the Wasserstein distance metric?",
    "answer": "SSWAE",
    "rationale": "The Wasserstein distance for SSWAE is consistently lower than that of SWAE in both plots, indicating that SSWAE performs better according to this metric.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.08780v2",
    "pdf_url": null
  },
  {
    "instance_id": "30d7d74cdad54b60afd5d4b566636ec5",
    "figure_id": "2308.02498v1-Figure4-1",
    "image_file": "2308.02498v1-Figure4-1.png",
    "caption": " Examples of synthetic and real-world noise. In each image, blue line is the true segmentation boundary, and all other colors are corresponding noisy boundaries. We removed the random flipping noise in visualization to focus on the boundary.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the most noise?",
    "answer": "Image (c) shows the most noise.",
    "rationale": "The blue line represents the true segmentation boundary, and the other colors represent the noisy boundaries. In image (c), the noisy boundaries are further away from the true boundary than in the other images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.02498v1",
    "pdf_url": null
  },
  {
    "instance_id": "0649b3548521480aada0e0142487319e",
    "figure_id": "2112.00059v1-Figure12-1",
    "image_file": "2112.00059v1-Figure12-1.png",
    "caption": " Reconstrcuted dataset under MixUp and Intra-InstaHide against the strongest attack (batch size is 32).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which data augmentation method appears to be the most effective in protecting against adversarial attacks?",
    "answer": "MixUp + GradPrune",
    "rationale": "The figure shows that the reconstructed dataset under MixUp + GradPrune is the most similar to the original dataset, indicating that this method is the most effective in protecting against adversarial attacks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.00059v1",
    "pdf_url": null
  },
  {
    "instance_id": "591bf750f9be45649e22369df364578e",
    "figure_id": "2201.08131v2-Figure3-1",
    "image_file": "2201.08131v2-Figure3-1.png",
    "caption": " Qualitative comparison of GeoFill against other baselines on user-provided images (top 2 rows), RealEstate10K (mid 2 rows), and MannequinChallenge dataset (last two rows).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods in the figure is most successful at inpainting the missing regions in the images?",
    "answer": "GeoFill",
    "rationale": "The figure shows that GeoFill is able to inpaint the missing regions in the images more accurately than the other methods. For example, in the first row of images, GeoFill is able to correctly inpaint the missing part of the stairs, while the other methods produce artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.08131v2",
    "pdf_url": null
  },
  {
    "instance_id": "332d3c06cfea43b6872b132f7350e7a9",
    "figure_id": "2204.08106v1-Figure1-1",
    "image_file": "2204.08106v1-Figure1-1.png",
    "caption": " Accuracy and Efficiency Comparison for Unweighted Dynamic Hypergraphs: The top row shows the relative error in the reported maximum density by Udshp and HWC with respect to Exact when run with the specified parameters. The bottom row plots the average update time taken by Udshp, HWC, and Exact for each reporting intervals. For each dataset (column), the parameter settings are identical.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest relative error in the reported maximum density for the DAWN dataset?",
    "answer": "HWC with epsilon_H = 0.1",
    "rationale": "The top left plot shows the relative error for each algorithm. The blue line, which represents HWC with epsilon_H = 0.1, is consistently lower than the red line, which represents UDSHP with epsilon_H = 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.08106v1",
    "pdf_url": null
  },
  {
    "instance_id": "abb58edf46be4c6d829463ea6692155e",
    "figure_id": "2010.14497v2-Figure5-1",
    "image_file": "2010.14497v2-Figure5-1.png",
    "caption": " Results on the five environments we consider for our experiments. For each environment we plot the average task reward, the average episodic failures, and the cumulative episodic failures. All the plots are for our method with different safety thresholds χ. From the plots it is evident that our method can naturally trade-off safety for task performance depending on how strict the safety threshold χ is set to. In particular, for a stricter χ (i.e. lesser value), the avg. failures decreases, and the task reward plot also has a slower convergence compared to a less strict threshold.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment has the highest average task reward for CSC = 0.9?",
    "answer": "Laikago",
    "rationale": "The plot in the bottom left corner shows the average task reward for each environment with different CSC values. The Laikago environment has the highest average task reward for CSC = 0.9.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.14497v2",
    "pdf_url": null
  },
  {
    "instance_id": "c3e2fe6569a6472d81d03fd99ada911f",
    "figure_id": "2005.02539v2-Figure9-1",
    "image_file": "2005.02539v2-Figure9-1.png",
    "caption": " Examples of how different SQL components can be explained in natural language",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which SQL component allows you to find only the first n rows of a result set?",
    "answer": "limit n",
    "rationale": "The table shows that the `limit n` component only keeps the first n rows of the results in step 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.02539v2",
    "pdf_url": null
  },
  {
    "instance_id": "0bd85a495bc74c129853a3a2452b3677",
    "figure_id": "2208.10061v1-Figure5-1",
    "image_file": "2208.10061v1-Figure5-1.png",
    "caption": " Impact of coefficient 𝛼 .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset resulted in the highest AUC score?",
    "answer": "Movie",
    "rationale": "The plot shows the AUC and F1 scores for three datasets: Book, Movie, and Music. The AUC score for Movie is the highest at 0.925.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.10061v1",
    "pdf_url": null
  },
  {
    "instance_id": "b231804d40b94c9da98e543b2d782c19",
    "figure_id": "2004.03875v2-Figure3-1",
    "image_file": "2004.03875v2-Figure3-1.png",
    "caption": " Examples of original articles, golden headlines and multiple generated outputs by BASE, BASE + Diverse and BASE + AddFuse. Each generated keyphrase is shown at the end of each generated headline.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which system produced the most concise and informative headline for Article #2?",
    "answer": "BASE + AddFuse",
    "rationale": "The figure shows that BASE + AddFuse produced the shortest headline that still included all of the important information from the article, such as the fact that the suspect was on the FBI's most wanted list, was killed in an incident involving Apex police, and that the incident occurred in North Carolina.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.03875v2",
    "pdf_url": null
  },
  {
    "instance_id": "3243ff9a8f854fa1adf779e364d59bd8",
    "figure_id": "2104.08771v2-Figure1-1",
    "image_file": "2104.08771v2-Figure1-1.png",
    "caption": " Overview of our transfer learning experiments, depicting (a) training from scratch, (b) conventional fine-tuning (src+body), (c) fine-tuning cross-attention (src+xattn), (d) fine-tuning new vocabulary (src), (e) fine-tuning cross-attention when transferring target language (tgt+xattn), (f) transfer learning with updating cross-attention from scratch (src+randxattn). Dotted components are initialized randomly, while solid lines are initialized with parameters from a pretrained model. Shaded, underlined components are fine-tuned, while other components are frozen.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which components are fine-tuned in the conventional fine-tuning approach?",
    "answer": "The encoder, decoder, and source input embedding.",
    "rationale": "In the figure, the shaded components are the ones that are fine-tuned. In the conventional fine-tuning approach (b), the encoder, decoder, and source input embedding are shaded.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.08771v2",
    "pdf_url": null
  },
  {
    "instance_id": "c99a8ff7bfb9469ea2273093678b1054",
    "figure_id": "2206.01186v2-Figure4-1",
    "image_file": "2206.01186v2-Figure4-1.png",
    "caption": " Frequency of being selected as a temporary teacher according to epochs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which ResNet model was selected as a temporary teacher most frequently in the early epochs?",
    "answer": "ResNet20",
    "rationale": "The green line, which represents ResNet20, is the highest in the early epochs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.01186v2",
    "pdf_url": null
  },
  {
    "instance_id": "5ca169828e794fafbf67bca63ddaec60",
    "figure_id": "2212.00222v1-Figure19-1",
    "image_file": "2212.00222v1-Figure19-1.png",
    "caption": " Class-wise purity of the cat class for random, full, and foreground (top 1 and 5) at a variety of layers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the highest class-wise purity for the cat class when using the full foreground?",
    "answer": "Layer 16",
    "rationale": "The bar for layer 16 is the highest among all the bars in the \"full\" group.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.00222v1",
    "pdf_url": null
  },
  {
    "instance_id": "b30bf9fa611f4f72bf85fd4ba768344d",
    "figure_id": "2307.05721v1-Figure13-1",
    "image_file": "2307.05721v1-Figure13-1.png",
    "caption": " Trainset and testset distribution of the 42 spatial annotation classes. This includes subject, object, and tool.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object appears most frequently in the testset?",
    "answer": "Hole C4.",
    "rationale": "The figure shows the frequency of each object in the testset. Hole C4 is the object with the highest bar in the testset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.05721v1",
    "pdf_url": null
  },
  {
    "instance_id": "974cfae5a00c4e6b821cb0712cc9047b",
    "figure_id": "2010.08830v1-Figure5-1",
    "image_file": "2010.08830v1-Figure5-1.png",
    "caption": " Cross/Sub-task transfer performance loss of MESA.",
    "figure_type": "** Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which meta-training task resulted in the highest cross-task transfer performance for the Optical Digits task?",
    "answer": " The Protein meta-training task.",
    "rationale": " Looking at the heatmap in (a), we can see that the Optical Digits task achieves the highest cross-task transfer performance (0.803) when trained on the Protein meta-training task. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.08830v1",
    "pdf_url": null
  },
  {
    "instance_id": "321280a44ee54effb3793f20d0d068d7",
    "figure_id": "2108.04775v1-Figure12-1",
    "image_file": "2108.04775v1-Figure12-1.png",
    "caption": " Qualitative results against baseline methods. Even rows: absolute difference between the corresponding image and the ground truth GS image. (a) The original second frame RS images. (b-e) GS images predicted by Zhuang et al. [36], Zhuang et al. [38], Liu et al. [20], and our SUNet method, respectively.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method produced the GS image that is most similar to the original RS image? ",
    "answer": " Our SUNet method. ",
    "rationale": " The figure shows the original RS image and the GS images predicted by four different methods. The GS image predicted by our SUNet method is the most similar to the original RS image, as it has the most detail and the least amount of noise.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.04775v1",
    "pdf_url": null
  },
  {
    "instance_id": "6b1f6e419a684b5a9990b999f5521481",
    "figure_id": "2305.02556v1-Figure8-1",
    "image_file": "2305.02556v1-Figure8-1.png",
    "caption": " An example of the option selection. For each option, we try to generate an entailment tree to prove the option. We select the option based on the state verifier score V (s) and the controller score P (s).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which option has the highest score?",
    "answer": "Option 2: 30 days",
    "rationale": "The score for Option 2 is 0.9645, which is higher than the scores for all other options.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.02556v1",
    "pdf_url": null
  },
  {
    "instance_id": "7e5714acca6c454e971c421462b3753a",
    "figure_id": "2110.15900v1-Figure9-1",
    "image_file": "2110.15900v1-Figure9-1.png",
    "caption": " We visually present the recovered images using 1,000 iterations of FISTA and 16-layer ALISTA and HyperLISTA. We can see that HyperLISTA has much less artifacts at the boundaries of patches.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm produced the image with the least artifacts at the boundaries of patches?",
    "answer": "HyperLISTA",
    "rationale": "The caption states that \"HyperLISTA has much less artifacts at the boundaries of patches.\" The figure shows the recovered images using 1,000 iterations of FISTA, ALISTA, and HyperLISTA. We can see that HyperLISTA has much less artifacts at the boundaries of patches than FISTA and ALISTA.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.15900v1",
    "pdf_url": null
  },
  {
    "instance_id": "82d85e76e8944769adca4fd754fdd520",
    "figure_id": "2106.03227v2-Figure2-1",
    "image_file": "2106.03227v2-Figure2-1.png",
    "caption": " (Left two plots) Estimated testing power from nrun = 500 of the covariance shift test in Figure 1 in R100 and R2. nX = nY = 200, using three statistics: T̂net (net), T̂NTK with test set only bootstrap (ntk1) and with full bootstrap (ntk2) the training and testing splitting is half-half. (Right two plots) Test statistics T̂a (red cross), the empirical distribution of T̂a,null using the test-only bootstrap and the full bootstrap (blue bars), and the estimated threshold (green circle). Computed from NTK kernel at t = 0 and nboot = 400.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which statistic has the highest testing power for a covariance shift of d=100?",
    "answer": "T̂net",
    "rationale": "The left two plots show the testing power of the three statistics for different values of d. For d=100, the blue line representing T̂net is consistently above the other two lines, indicating it has the highest testing power.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03227v2",
    "pdf_url": null
  },
  {
    "instance_id": "0ef6d92f445646778f72c57e1f02917c",
    "figure_id": "1902.03748v3-Figure5-1",
    "image_file": "1902.03748v3-Figure5-1.png",
    "caption": " Activity location prediction with classification and regression on the multi-scale Manhattan Grid. See Section 3.5.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the role of the CNN in the activity location prediction process?",
    "answer": " The CNN extracts features from the input image.",
    "rationale": " The figure shows that the CNN is used to process the input image and generate feature maps. These feature maps are then used by the subsequent classification and regression layers to predict the location of the activity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.03748v3",
    "pdf_url": null
  },
  {
    "instance_id": "64cc5dde434e4331bd5b3315f0d78379",
    "figure_id": "2106.01282v2-Figure4-1",
    "image_file": "2106.01282v2-Figure4-1.png",
    "caption": " First two dimensions of the embeddings Ŷ(1), . . . , Ŷ(20) of the adjacency matrices A(1), . . . ,A(20). The colours indicate different school years while the marker type distinguishes the two school classes within each year.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which class has the most consistent behavior across both days?",
    "answer": "Class 3A",
    "rationale": "The figure shows that the embeddings of Class 3A are clustered together in a similar location on both days. This suggests that the behavior of students in this class is relatively consistent across both days.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01282v2",
    "pdf_url": null
  },
  {
    "instance_id": "aa9605bc33a34accbe8e35b6a2dd9efd",
    "figure_id": "2306.05167v1-Figure2-1",
    "image_file": "2306.05167v1-Figure2-1.png",
    "caption": " The effect of limiting the number of recurrent steps while training our model. Maximum mean rewards achieved are presented per environment: (a) HalfCheetah, (b) Hopper (c) Walker2D",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which environment is the most sensitive to the number of recurrent steps used during training?",
    "answer": " Walker2D",
    "rationale": " The figure shows that the performance of the Walker2D environment varies the most with the number of recurrent steps. For example, the \"Walker expert\" model achieves a maximum mean reward of about 0.8 with a relative maximum length of 0.4, but the \"Walker medium\" and \"Walker medium-replay\" models only achieve maximum mean rewards of about 0.4 and 0.5, respectively, with the same relative maximum length. In contrast, the performance of the HalfCheetah and Hopper environments is relatively stable across different numbers of recurrent steps.\n\n**Figure type:** Plot",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.05167v1",
    "pdf_url": null
  },
  {
    "instance_id": "7c8d4543cb0d4055a4a9a643338deb11",
    "figure_id": "2303.14092v2-Figure4-1",
    "image_file": "2303.14092v2-Figure4-1.png",
    "caption": " Comparison of novel view synthesis with Ref-NeRF [49], PhySG [58], VolSDF [56] and DIFFREC [32]. Zoom in for a better view. NeuFace captures faces with much richer skin details and high-fidelity highlights.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most realistic-looking faces?",
    "answer": "NeuFace.",
    "rationale": "The figure shows that NeuFace captures faces with much richer skin details and high-fidelity highlights, which makes them appear more realistic. This can be seen in the zoomed-in images of the faces, where NeuFace is able to capture fine details such as wrinkles and pores, while the other methods produce smoother, less detailed faces.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.14092v2",
    "pdf_url": null
  },
  {
    "instance_id": "9fb16d16ea8c4ed78469883c012a6197",
    "figure_id": "2306.00006v3-Figure5-1",
    "image_file": "2306.00006v3-Figure5-1.png",
    "caption": " Homophily distribution of normal nodes and abnormal nodes on the rest of eight datasets",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the most significant difference in homophily distribution between normal and abnormal nodes?",
    "answer": "YelpChi-all.",
    "rationale": "The figure shows the homophily distribution of normal and abnormal nodes for eight different datasets. The YelpChi-all dataset has the most significant difference in homophily distribution between normal and abnormal nodes, with normal nodes having a much higher homophily than abnormal nodes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.00006v3",
    "pdf_url": null
  },
  {
    "instance_id": "dda867fe5d13466e89bdf4d586f2dd28",
    "figure_id": "2105.15053v1-Figure3-1",
    "image_file": "2105.15053v1-Figure3-1.png",
    "caption": " Predictive entropy by head for various question properties - lower entropy indicates higher predictive power.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which question property is most predictable by the quantizer head?",
    "answer": "Preposition",
    "rationale": "The figure shows that the entropy for the \"Preposition\" property is the lowest, which means that it is the most predictable by the quantizer head.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.15053v1",
    "pdf_url": null
  },
  {
    "instance_id": "97f68239c2e94d0792f99456ae632827",
    "figure_id": "2306.11247v1-Figure4-1",
    "image_file": "2306.11247v1-Figure4-1.png",
    "caption": " Within-group agreement metrics, by race. IRR shows that Latine raters have significantly more agreement than other races. Negentropy (i.e. negative of entropy) and plurality size (i.e. the fraction of raters who choose the most popular response) show that White raters have significantly more, and Multiracial significantly less, agreement than other races.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which racial group has the highest level of agreement according to the IRR metric?",
    "answer": "Latine",
    "rationale": "The figure shows that the median value of the IRR metric for Latine raters is higher than for all other racial groups. This is also confirmed by the fact that the Latine violin plot has more of its area at higher values than any of the other plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.11247v1",
    "pdf_url": null
  },
  {
    "instance_id": "610143bd63f648649261941cfd16579d",
    "figure_id": "1906.00588v5-Figure3-1",
    "image_file": "1906.00588v5-Figure3-1.png",
    "caption": " Comparison among NN, RIO, and SVGP. The horizontal axis denotes the prediction RMSE of the NN, and the vertical axis the prediction RMSE of RIO (blue dots) and SVGP (yellow dots). Each dot represents an independent experimental run. Since the scales are different, the solid blue line indicates where NN and RIO/SVGP have same prediction RMSE. Thus, a dot below the line means that the method (RIO or SVGP) performs better than the NN, and vice versa. Results of SVGP on the CT dataset are not plotted because its prediction RMSE exceeded the visible scale (i.e. they were >50). RIO consistently reduces the error of the NN, and outperforms SVGP in most cases.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better than NN in most cases?",
    "answer": "RIO.",
    "rationale": "The solid blue line in each plot indicates where NN and RIO/SVGP have the same prediction RMSE. Thus, a dot below the line means that the method (RIO or SVGP) performs better than the NN, and vice versa. We can see that in most cases, the blue dots are below the line, which means that RIO performs better than NN.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.00588v5",
    "pdf_url": null
  },
  {
    "instance_id": "1d96f649bdb647bbaa600363b4751a38",
    "figure_id": "2106.12800v1-Figure5-1",
    "image_file": "2106.12800v1-Figure5-1.png",
    "caption": " F1 scores (%) with regard to the frequencies of labels. We bucket labels by their frequencies into 6 buckets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better for the most frequent labels?",
    "answer": "The Base model.",
    "rationale": "The Base model has a higher F1 score than the Reranked model for the first two buckets, which represent the most frequent labels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.12800v1",
    "pdf_url": null
  },
  {
    "instance_id": "5a7b4639e70c4f1a9e061c841e075b36",
    "figure_id": "1904.00370v3-Figure2-1",
    "image_file": "1904.00370v3-Figure2-1.png",
    "caption": " VAAL performance on classification tasks using CIFAR10, CIFAR100, Caltech-256, and ImageNet compared to Core-set [43], Ensembles w. VarR [1], MC-Dropout [15], DBAL [16], and Random Sampling. Best visible in color. Data and code required to reproduce are provided in our code repository",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on CIFAR10 when only 10% of the data is labeled?",
    "answer": "VAAL",
    "rationale": "The figure shows that VAAL achieves the highest mean accuracy on CIFAR10 when only 10% of the data is labeled.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.00370v3",
    "pdf_url": null
  },
  {
    "instance_id": "2bfff08facb140888068adbe8efd3262",
    "figure_id": "2310.03882v1-Figure2-1",
    "image_file": "2310.03882v1-Figure2-1.png",
    "caption": " IQM for human normalized scores with varying neural network architectures over 20 games, with 3 seeds per experiment. Shaded areas represent 95% stratified bootstrap confidence intervals.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value-based agent performs best with a reduced batch size?",
    "answer": "QR-DQN",
    "rationale": "The figure shows that QR-DQN has the highest human-normalized score with a reduced batch size.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.03882v1",
    "pdf_url": null
  },
  {
    "instance_id": "fbfd488bce304c84851a826bcaf70e2b",
    "figure_id": "2201.06009v7-Figure18-1",
    "image_file": "2201.06009v7-Figure18-1.png",
    "caption": " Finding 2 Large gains on queries asked in English and Punjabi by MemPrompt.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method achieved the highest accuracy on queries asked in English and Punjabi?",
    "answer": " MEMPROMPT (P(fb) = 0.5)",
    "rationale": " The figure shows the accuracy of different methods on queries asked in English and Punjabi. The line for MEMPROMPT (P(fb) = 0.5) is the highest, indicating that this method achieved the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.06009v7",
    "pdf_url": null
  },
  {
    "instance_id": "3c02b3a5a3a94f4e8923dee97e227a7b",
    "figure_id": "1903.11749v1-Figure6-1",
    "image_file": "1903.11749v1-Figure6-1.png",
    "caption": " Accuracy comparison on NDCG and MAP.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed better on the GrQc dataset according to the NDCG metric?",
    "answer": "DistPPR",
    "rationale": "The bar for DistPPR is higher than the bar for Doubling on the GrQc dataset in the NDCG plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.11749v1",
    "pdf_url": null
  },
  {
    "instance_id": "467901ff17364547a9fe19bec0910760",
    "figure_id": "2201.01251v3-Figure17-1",
    "image_file": "2201.01251v3-Figure17-1.png",
    "caption": " Aggregate metrics with 95% CIs for the stochastic games listed in Table 2, following Agarwal et al. (2021). The CIs use percentile bootstrap with stratified sampling.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of median normalized score?",
    "answer": "XTX",
    "rationale": "The figure shows the median normalized score for each method. XTX has the highest median score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.01251v3",
    "pdf_url": null
  },
  {
    "instance_id": "2c35df76cd764134a7eaee77d7118670",
    "figure_id": "2212.02506v1-Figure6-1",
    "image_file": "2212.02506v1-Figure6-1.png",
    "caption": " Quantitative comparison of saliency maps : Median Softmax Information curves",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the best performance in terms of the area under the curve (AUC)?",
    "answer": "Ours.",
    "rationale": "The figure shows the Softmax Information Curve for different methods. The AUC is a measure of how well a method performs, and the higher the AUC, the better the performance. The curve for \"Ours\" has the highest AUC of 0.934.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.02506v1",
    "pdf_url": null
  },
  {
    "instance_id": "3ecde1b4189a453ab2072e847fc9fc5c",
    "figure_id": "2206.00484v2-Figure7-1",
    "image_file": "2206.00484v2-Figure7-1.png",
    "caption": " DEP-MPO is the most robust against all considered perturbations. Ostrich: DEPMPO performs best under stepdown perturbations for varying step heights. The starting distance of the step was randomly varied. For the slopetrotter task, the average achieved distance is largest for DEP-MPO, although with considerable variability. Human: DEP-MPO also performs better for stepdown perturbations. For human-hopstacle, 4 out of 5 seeds of DEP-MPO achieve robust hopping. The remaining seed found a non-hopping solution achieving good returns that is not robust.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods is most robust to perturbations?",
    "answer": "DEP-MPO.",
    "rationale": "The caption states that DEP-MPO is the most robust against all considered perturbations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.00484v2",
    "pdf_url": null
  },
  {
    "instance_id": "29f4403dcc7440e38a87b0ddb1632df0",
    "figure_id": "1906.09458v2-Figure5-1",
    "image_file": "1906.09458v2-Figure5-1.png",
    "caption": " Illustration of all possible cases of Lemma 2 and Lemma 3. Nodes belonging to T0 (see main text) are black, all remaining nodes are white. Leaves and subtrees of T are represented by squares and triangles, respectively. Each node of T ′c∗ is enclosed in a circle. Above: The two possible cases illustrating Lemma 2, that is, j′ 6∈ V (T0) on the left, and j′ ∈ V (T0) on the right. Below: The five cases described in Lemma 3.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the five cases in Lemma 3 illustrates a situation where node $i$ is a leaf node in $T$?",
    "answer": "Case 5.",
    "rationale": "In Case 5, node $i$ is a leaf node in $T$ because it has no children. This is evident from the fact that there are no edges emanating from node $i$ in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.09458v2",
    "pdf_url": null
  },
  {
    "instance_id": "25c5655d44564cd1876889f11e7df83a",
    "figure_id": "1810.00143v4-Figure10-1",
    "image_file": "1810.00143v4-Figure10-1.png",
    "caption": " Learning rate sensitivity experiment with DenseNet on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performed best in this experiment?",
    "answer": "Adam lr:1e-03 performed best in this experiment.",
    "rationale": "The figure shows the training loss and test accuracy of different optimizers on the CIFAR-10 dataset. The Adam optimizer with a learning rate of 1e-03 achieved the highest test accuracy of around 0.90.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.00143v4",
    "pdf_url": null
  },
  {
    "instance_id": "4f83b17b113446e9b6c22e732dd4063c",
    "figure_id": "2303.05251v1-Figure6-1",
    "image_file": "2303.05251v1-Figure6-1.png",
    "caption": " The KL divergence between attention distributions of different query patches at each layer of a pre-trained ViT-B backbone, averaged on all pairs of query patches.",
    "figure_type": "** \nplot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \nWhich method has the highest KL divergence at depth 11?",
    "answer": " \nLocalMIM",
    "rationale": " \nThe figure shows the KL divergence between attention distributions of different query patches at each layer of a pre-trained ViT-B backbone, averaged on all pairs of query patches. The KL divergence is a measure of how different two probability distributions are. In this case, it is used to measure how different the attention distributions of different query patches are. The higher the KL divergence, the more different the attention distributions are. At depth 11, LocalMIM has the highest KL divergence, which means that its attention distributions are the most different from the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.05251v1",
    "pdf_url": null
  },
  {
    "instance_id": "2e0b621226a64e378de6f2a387cfcfa8",
    "figure_id": "2203.06481v1-Figure9-1",
    "image_file": "2203.06481v1-Figure9-1.png",
    "caption": " Sequential GATSBI performance for the Two Moons Model. The energy-based correction (EBM) results in a slight improvement over amortised GATSBI for 1k and 10k simulations, but the inverse importance weights correction does not.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best for the Two Moons Model according to the figure?",
    "answer": "Sequential GATSBI EBM.",
    "rationale": "The figure shows that the C2ST accuracy for Sequential GATSBI EBM is higher than the other methods for all numbers of simulations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.06481v1",
    "pdf_url": null
  },
  {
    "instance_id": "e39cb71808444b23bc6588c8d7b6a078",
    "figure_id": "2103.15798v2-Figure4-1",
    "image_file": "2103.15798v2-Figure4-1.png",
    "caption": " ResNet XD outperforms both baseline and dilated ResNets on PSICOV. At the highest depth we test we also outperform the reported MAE8 of the much deeper Dilated ResNet-258 [1].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which ResNet architecture has the best performance in terms of MAE8 on the PSICOV real-valued distance prediction task?",
    "answer": "ResNet XD",
    "rationale": "The figure shows the MAE8 for different ResNet architectures at different depths. ResNet XD has the lowest MAE8 at all depths tested.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.15798v2",
    "pdf_url": null
  },
  {
    "instance_id": "3e786ff331ec42c09b5104f8f20990c5",
    "figure_id": "2106.05001v2-Figure10-1",
    "image_file": "2106.05001v2-Figure10-1.png",
    "caption": " The classifer weight L2-norm of FedAvg, FedProx and MOON before and after applying CCVR on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest classifier weight L2-norm before applying CCVR?",
    "answer": "FedProx",
    "rationale": "The figure shows that the bars for FedProx are the tallest before applying CCVR.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05001v2",
    "pdf_url": null
  },
  {
    "instance_id": "2cb259b9e4a2455da73d5e5504de5b20",
    "figure_id": "2203.05285v2-Figure12-1",
    "image_file": "2203.05285v2-Figure12-1.png",
    "caption": " Comparisons of VDN-based methods considering the PI and PE properties.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which VDN-based method performs the best in the super hard environment with 6h vs. 8z maps?",
    "answer": "SET-VDN.",
    "rationale": "The figure shows the median test win % for different VDN-based methods in different environments. In the super hard environment with 6h vs. 8z maps, SET-VDN has the highest median test win %.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.05285v2",
    "pdf_url": null
  },
  {
    "instance_id": "127fe98ec4b14d7e98992cfca0ba881b",
    "figure_id": "1911.07140v2-Figure5-1",
    "image_file": "1911.07140v2-Figure5-1.png",
    "caption": " The success rate of un-targeted black-box adversarial attack at different query levels for undefended ImageNet models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of attack is shown in the figure?",
    "answer": "Un-targeted black-box adversarial attack.",
    "rationale": "The caption of the figure states that the figure shows the success rate of un-targeted black-box adversarial attack at different query levels for undefended ImageNet models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.07140v2",
    "pdf_url": null
  },
  {
    "instance_id": "8a549943674c4d62b2af9aaa104fc631",
    "figure_id": "1912.03500v2-Figure6-1",
    "image_file": "1912.03500v2-Figure6-1.png",
    "caption": " Visual comparison of various differentiable proxies for piecewise constant function.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four panels shows the most accurate representation of the original piecewise constant landscape?",
    "answer": "Panel (a)",
    "rationale": "Panel (a) shows the original piecewise constant landscape, while the other panels show different approximations of it. The approximations in panels (b), (c), and (d) are all smoother than the original landscape, which can be seen by comparing the sharpness of the peaks and valleys in each panel.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.03500v2",
    "pdf_url": null
  },
  {
    "instance_id": "0b305da9de8d4324b93aab00bc19f27d",
    "figure_id": "2206.11477v1-Figure3-1",
    "image_file": "2206.11477v1-Figure3-1.png",
    "caption": " Illustration of policy GNN. (a) Graph representation and GNN process; (b) Illustration of MLP in Equation (19,20,21); (c) Computation of classification loss and ranking loss.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two types of loss functions used in the policy GNN?",
    "answer": "Classification loss and ranking loss.",
    "rationale": "The figure shows that the GNN output is used to predict the node labels, which are then compared to the ground truth labels to compute the classification loss. The GNN output is also used to compute the ranking loss, which measures how well the GNN ranks the nodes according to their predicted labels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.11477v1",
    "pdf_url": null
  },
  {
    "instance_id": "1f395abf713e440c845a41122fb616cf",
    "figure_id": "1805.10000v1-Figure5-1",
    "image_file": "1805.10000v1-Figure5-1.png",
    "caption": " R2P over time",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between R2P and time?",
    "answer": "R2P generally decreases over time.",
    "rationale": "The figure shows that the R2P values for both Ground-Truth and Virtual Taobao generally decrease over time, with some fluctuations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.10000v1",
    "pdf_url": null
  },
  {
    "instance_id": "6aa3ce6e2ebb4874990220babba92733",
    "figure_id": "2304.13854v1-Figure7-1",
    "image_file": "2304.13854v1-Figure7-1.png",
    "caption": " Automatic coherence evaluation of different models.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which model achieved the highest accuracy value?",
    "answer": " KIEST",
    "rationale": " The figure shows the accuracy values of different models. The bar for KIEST is the highest, indicating that it achieved the highest accuracy value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.13854v1",
    "pdf_url": null
  },
  {
    "instance_id": "4829a0086d5941c6a480ef923f6b1ba4",
    "figure_id": "2307.09160v1-Figure10-1",
    "image_file": "2307.09160v1-Figure10-1.png",
    "caption": " How Dual-Depth works. Compared with side prediction, the estimated biases of dual-depth decrease more(a). The ratios of depths whose estimated bias decreases are shown in (b) and ratios of Dual-Depth are higher in all intervals.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does Dual-Depth always improve the depth estimation accuracy?",
    "answer": "No.",
    "rationale": "Figure (b) shows the decrease percentage of depth error after using Dual-Depth. Although the decrease percentage of Dual-Depth is higher than CasMVSNet in all intervals, it is still not 100%. This means that there are still some cases where Dual-Depth does not improve the depth estimation accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.09160v1",
    "pdf_url": null
  },
  {
    "instance_id": "42fa0a80d15345d2acd7ca83012593b2",
    "figure_id": "2212.09535v3-Figure1-1",
    "image_file": "2212.09535v3-Figure1-1.png",
    "caption": " Results for zero-shot prompt-based evaluation of natural language inference, commonsense reasoning, anaphora resolution, and paraphrasing tasks. All tasks are evaluated with accuracy measure. Solid lines indicate language adaptation strategies, and dotted lines indicate baselines. × indicate the non-adapted BLOOM model. Both ✓ and ✗ indicate whether the baseline has seen the language during pretraining, except for Guarani (GN) that is unseen for all models. We also ablate BLOOMZ and mT0 from PAWS-X evaluation as the models has been trained on the task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the XNLI DE task?",
    "answer": "mT0",
    "rationale": "The figure shows that the mT0 model has the highest accuracy on the XNLI DE task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.09535v3",
    "pdf_url": null
  },
  {
    "instance_id": "ff824d23333148e89063e303e79a2e42",
    "figure_id": "1805.07925v3-Figure5-1",
    "image_file": "1805.07925v3-Figure5-1.png",
    "caption": " Style transfer examples where BIN produces similar results with IN and BN+IN.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which style transfer technique produces the most similar results to the original image?",
    "answer": "BIN",
    "rationale": "The figure shows that the BIN style transfer technique produces the most similar results to the original image, as the colors and shapes are very similar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.07925v3",
    "pdf_url": null
  },
  {
    "instance_id": "342cd7a07c374f28adb334227391b66b",
    "figure_id": "1911.01030v1-Figure1-1",
    "image_file": "1911.01030v1-Figure1-1.png",
    "caption": " Sorting or Filtering Functions",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which platform allows users to filter tasks by category?",
    "answer": "CrowdSpring",
    "rationale": "The figure shows that CrowdSpring has a \"CATEGORY\" filter, while Amazon MTurk does not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.01030v1",
    "pdf_url": null
  },
  {
    "instance_id": "d6706c243f2c41f0b0be0c958d71b9d5",
    "figure_id": "2204.03281v2-Figure6-1",
    "image_file": "2204.03281v2-Figure6-1.png",
    "caption": " The results of a 7-day online A/B test on WeChat Subscription platform. (B=Billion)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the most stable CTR throughout the 7-day A/B test?",
    "answer": "SSEDS",
    "rationale": "The figure shows the CTR and number of parameters for both DeepFM and SSEDS models over the 7-day test period. The CTR for DeepFM fluctuates throughout the test, while the CTR for SSEDS remains relatively stable.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.03281v2",
    "pdf_url": null
  },
  {
    "instance_id": "48dccb6f5b11405ca422d7c687182118",
    "figure_id": "2306.06712v1-Figure29-1",
    "image_file": "2306.06712v1-Figure29-1.png",
    "caption": " Top-20 architectures with cell kernel parameter count 18 (hence, architectures with exactly 2 times 3× 3 convolutions) according to (top) mean adversarial accuracy and (bottom) mean corruption accuracy on CIFAR-10. See Figure 1 for cell connectivity and operations (1-6).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which operation results in the highest mean adversarial accuracy for architecture #5926?",
    "answer": "Operation 3",
    "rationale": "The figure shows that for architecture #5926, the dot corresponding to operation 3 is the highest on the y-axis, which represents mean adversarial accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.06712v1",
    "pdf_url": null
  },
  {
    "instance_id": "76c443ef44fb4492859599b32285a0ef",
    "figure_id": "2210.01781v2-Figure9-1",
    "image_file": "2210.01781v2-Figure9-1.png",
    "caption": " Additional collision region localization visualizations with collision predictions from the full multi-view COPILOT model on unseen test scenes. Predicted and ground-truth heatmaps are shown from three of the six viewpoints. The model’s prediction of the collision along with the ground-truth is shown at the bottom.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which body parts are most likely to collide with objects in Scene 2?",
    "answer": "The torso and left hand.",
    "rationale": "The predicted and ground-truth heatmaps for Scene 2 show a high probability of collision in the area of the torso and left hand.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.01781v2",
    "pdf_url": null
  },
  {
    "instance_id": "036fef9a411e405ab56ddbd3b1f1a26a",
    "figure_id": "1905.12676v2-Figure2-1",
    "image_file": "1905.12676v2-Figure2-1.png",
    "caption": " Dependency recall relative to arc length on development sets. The corresponding plot for precision shows similar trends (see Figure 7 in Appendix A).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest recall for dependency lengths of 1 and 2?",
    "answer": "GbMIN",
    "rationale": "The figure shows the recall of different models for different dependency lengths. The model with the highest recall for dependency lengths of 1 and 2 is GbMIN, which is represented by the blue triangles.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12676v2",
    "pdf_url": null
  },
  {
    "instance_id": "7219229fcbac439384e05f43a16b0544",
    "figure_id": "2207.08220v2-Figure1-1",
    "image_file": "2207.08220v2-Figure1-1.png",
    "caption": " (a): Comparison with state-of-the-arts on ImageNet. All methods uses ResNet-50 encoders and are measured with Top-1 linear evaluation accuracy. (b): Overview of Fast-MoCo that includes the Split-Encode-Combine pipeline.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most efficient in terms of epochs needed to achieve a certain level of accuracy?",
    "answer": "Fast-MoCo.",
    "rationale": "The plot in (a) shows that Fast-MoCo requires significantly fewer epochs than the other methods to achieve a similar level of accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.08220v2",
    "pdf_url": null
  },
  {
    "instance_id": "0065a9af76104802a1c310d3bd87090f",
    "figure_id": "2007.04640v2-Figure3-1",
    "image_file": "2007.04640v2-Figure3-1.png",
    "caption": " Comparison of the average return as a function of learning epochs achieved by TRPO with MEPOL initialization, TRPO, SAC, SMM, ICM, and Pseudocount over a set of sparse-reward RL tasks. For each task, we report a visual representation and learning curves. (95% c.i. over 8 runs).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms consistently achieves the highest average return across all tasks?",
    "answer": "TRPO with MEPOL initialization",
    "rationale": "The learning curves in the figure show that TRPO with MEPOL initialization consistently achieves the highest average return across all tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.04640v2",
    "pdf_url": null
  },
  {
    "instance_id": "2d85bc662f98434bb68fcd2621789f6c",
    "figure_id": "2001.00187v1-Figure5-1",
    "image_file": "2001.00187v1-Figure5-1.png",
    "caption": " Performance in MPIIGaze dataset",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest angular error?",
    "answer": "CA-Net (ours)",
    "rationale": "The bar graph shows the angular error for each method. The CA-Net (ours) bar is the shortest, indicating that it has the lowest angular error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.00187v1",
    "pdf_url": null
  },
  {
    "instance_id": "0336ee97d0e94df5a3d9f0b1ab4c4b07",
    "figure_id": "2104.05043v2-Figure6-1",
    "image_file": "2104.05043v2-Figure6-1.png",
    "caption": " Performance (normalized distance to goals vs. actor steps) of our GPIM and baselines (RIG, DISCERN, L2, RIG+).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the 2D Navigation (x-y goal) task?",
    "answer": "GPIM",
    "rationale": "The plot shows that GPIM has the lowest normalized distance to the goal for the 2D Navigation (x-y goal) task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.05043v2",
    "pdf_url": null
  },
  {
    "instance_id": "def6e451986648509f105a4bce8cceef",
    "figure_id": "2201.01420v1-Figure2-1",
    "image_file": "2201.01420v1-Figure2-1.png",
    "caption": " Performance of class-incremental learning on the 150-class dataset. The observable ratio of old data is kept as 10%, 90 out of 150 classes are used for initial training, and then 30 domains are added as additional classes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of overall accuracy?",
    "answer": "CoNDA",
    "rationale": "The figure shows the overall accuracy of different methods as a function of the number of new classes. CoNDA has the highest overall accuracy for all numbers of new classes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.01420v1",
    "pdf_url": null
  },
  {
    "instance_id": "8e12939c7dd54e70b7b5f2c19c446da4",
    "figure_id": "2104.14769v1-Figure7-1",
    "image_file": "2104.14769v1-Figure7-1.png",
    "caption": " Real-scanned results. Comparing SampleNet+PU-GAN with PointLIE for PCSR task on real-scanned large scale LiDAR point cloud, it can be observed that our PointLIE can recover more realistic spatial relationship compared with SampleNet+PU-GAN (e.g., the gap between the pedestrian and motorbike).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is better at recovering the spatial relationship between objects in a point cloud?",
    "answer": "PointLIE.",
    "rationale": "The figure shows that PointLIE is able to recover the gap between the pedestrian and motorbike, while SampleNet+PU-GAN is not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.14769v1",
    "pdf_url": null
  },
  {
    "instance_id": "128d707e462d413c9264bbeadd0c1adb",
    "figure_id": "2106.05691v1-Figure3-1",
    "image_file": "2106.05691v1-Figure3-1.png",
    "caption": " Results of depth compression on CoLA, SST2, QNLI and MNLI. Each color denotes a layer mapping function. The number of layers in HSK includes the embedding layer. Full results on seven tasks are shown in Appendix E.1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the accuracy of RoSITA and TinyBERT compare on the CoLA task?",
    "answer": "RoSITA outperforms TinyBERT on the CoLA task.",
    "rationale": "The figure shows the accuracy of RoSITA and TinyBERT on the CoLA task for different numbers of layers. RoSITA consistently achieves higher accuracy than TinyBERT, regardless of the number of layers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05691v1",
    "pdf_url": null
  },
  {
    "instance_id": "28e857bddb8040a0a0b795d6e39dd2b8",
    "figure_id": "2210.15221v1-Figure3-1",
    "image_file": "2210.15221v1-Figure3-1.png",
    "caption": " The EM, F1 and quantities of adversarial samples using different beam size on three victim models.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the EM metric when the beam size is 5?",
    "answer": "BERT.",
    "rationale": "The plot on the left shows the EM score for different beam sizes and models. When the beam size is 5, the blue line (BERT) is higher than the other two lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.15221v1",
    "pdf_url": null
  },
  {
    "instance_id": "94ba78bfc8d14ece8e38894cd18def6e",
    "figure_id": "1908.10940v2-Figure3-1",
    "image_file": "1908.10940v2-Figure3-1.png",
    "caption": " BayesOpt learns to weight features adaptively on ParaCrawl and WMT, respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature is consistently weighted higher by BayesOpt across both datasets?",
    "answer": "The `emb` feature.",
    "rationale": "The figure shows that the `emb` feature has the highest value for both ParaCrawl and WMT, indicating that BayesOpt assigns a higher weight to this feature regardless of the dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.10940v2",
    "pdf_url": null
  },
  {
    "instance_id": "eff63abcd95145e0844d9d04dc5789c8",
    "figure_id": "2103.01050v1-Figure7-1",
    "image_file": "2103.01050v1-Figure7-1.png",
    "caption": " Ablation on studying the effectiveness of λ. The dotted line represents the trend of accuracy change, and the corresponding value of each λ is the average accuracy of the four models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best with the largest λ value?",
    "answer": "DenseNet",
    "rationale": "The figure shows the accuracy of four different models for different values of λ. The model with the highest accuracy for the largest λ value is DenseNet.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.01050v1",
    "pdf_url": null
  },
  {
    "instance_id": "1c22fb0ca4cf46c0a07adbaadb2049aa",
    "figure_id": "2205.11809v2-Figure4-1",
    "image_file": "2205.11809v2-Figure4-1.png",
    "caption": " Assembling results for Mondrian-Square.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods generated the most accurate assembly of the Mondrian-Square?",
    "answer": "Our method.",
    "rationale": "The figure shows the results of four different methods for assembling the Mondrian-Square. The results of our method are the closest to the original image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.11809v2",
    "pdf_url": null
  },
  {
    "instance_id": "6c70127a25ee4a289fd45a9592b0f86c",
    "figure_id": "2309.01507v3-Figure16-1",
    "image_file": "2309.01507v3-Figure16-1.png",
    "caption": " Outlier patterns of first moment in transformer block layer-13 of GPT-2 Medium at epoch 2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which transformer block layer-13 weight matrix in GPT-2 Medium at epoch 2 has the most outliers?",
    "answer": "WQ and WV.",
    "rationale": "The figure shows the outlier patterns of the first moment in the transformer block layer-13 weight matrices of GPT-2 Medium at epoch 2. The outliers are shown as red dots. WQ and WV have the most red dots, indicating they have the most outliers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.01507v3",
    "pdf_url": null
  },
  {
    "instance_id": "b62428f6fa564b649f41789e46108b82",
    "figure_id": "2105.08541v1-Figure6-1",
    "image_file": "2105.08541v1-Figure6-1.png",
    "caption": " Algorithm footprint t-SNE plot of CMA-ES instances showing where CSA outperforms all static policies.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which region of the plot does CSA outperform all static policies?",
    "answer": "The top right corner of the plot.",
    "rationale": "The plot shows the performance of CSA compared to static policies. The x-axis represents the first dimension of the data, and the y-axis represents the second dimension. The points in the top right corner of the plot represent instances where CSA outperforms all static policies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.08541v1",
    "pdf_url": null
  },
  {
    "instance_id": "ab64688ad6a443dc9a7152516f52b0cb",
    "figure_id": "2201.12329v4-Figure13-1",
    "image_file": "2201.12329v4-Figure13-1.png",
    "caption": " Convergence curves of DETR, Conditional DETR, and our DAB-DETR. All models are trained under the R50(DC5) settinng.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest mAP?",
    "answer": "DAB-DETR(DC5) (Ours)",
    "rationale": "The figure shows the mAP of three models over time. The DAB-DETR(DC5) (Ours) curve is the highest at the end of the training process, indicating that it achieves the highest mAP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.12329v4",
    "pdf_url": null
  },
  {
    "instance_id": "5831be6608614b87a8c034ffed23489a",
    "figure_id": "2011.10427v1-Figure1-1",
    "image_file": "2011.10427v1-Figure1-1.png",
    "caption": " Example Tables",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GP practice has the highest number of patients?",
    "answer": "Dr. E Cullen",
    "rationale": "The table labeled S1 shows the number of patients for each GP practice. Dr. E Cullen has the highest number of patients with 1202.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.10427v1",
    "pdf_url": null
  },
  {
    "instance_id": "1deb40a3a98a4f6aae79e60945ba68cf",
    "figure_id": "1711.07656v1-Figure3-1",
    "image_file": "1711.07656v1-Figure3-1.png",
    "caption": " Comparisons of runtime of all recurrent models on the TRAIN-ALL dataset with d = 800.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest runtime?",
    "answer": "AP-BILSTM",
    "rationale": "The bar for AP-BILSTM is the tallest, indicating that it has the highest runtime.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.07656v1",
    "pdf_url": null
  },
  {
    "instance_id": "75912f2ef6ee44a8858fc98ee36daba4",
    "figure_id": "2102.04487v1-Figure5-1",
    "image_file": "2102.04487v1-Figure5-1.png",
    "caption": " Test Accuracy vs the number of bits communicated for Vanilla CNN",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on non-i.i.d data with a fixed learning rate?",
    "answer": "AdaQuantFL",
    "rationale": "The figure shows the test accuracy of different methods on non-i.i.d data with a fixed learning rate. The AdaQuantFL method has the highest test accuracy, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.04487v1",
    "pdf_url": null
  },
  {
    "instance_id": "18ef407d1c8743c5bd25ff28a3b5abcf",
    "figure_id": "1805.09786v1-Figure4-1",
    "image_file": "1805.09786v1-Figure4-1.png",
    "caption": " Left: Comparison of our models with low-capacity on the Sort-of-CLEVR dataset. The “EA” refers to the model that uses hyperbolic attention weights with Euclidean aggregation. Right: Performance of Relation Network extended by attention mechanism in either Euclidean or hyperbolic space on the CLEVR dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the Sort-of-CLEVR dataset?",
    "answer": "The Hyperbolic RN (Softmax) model.",
    "rationale": "The left plot shows the accuracy of different models on the Sort-of-CLEVR dataset. The Hyperbolic RN (Softmax) model has the highest accuracy, as indicated by the green line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.09786v1",
    "pdf_url": null
  },
  {
    "instance_id": "baa9620e8ab14c3f9ab76d86a06a0beb",
    "figure_id": "2305.17221v1-Figure6-1",
    "image_file": "2305.17221v1-Figure6-1.png",
    "caption": " Alternative weighting mechanisms for FedOPT on the dev set of our proposed benchmark. Recall that FedOPT uses a client’s training set size (w/o loss reduction) as its weight, FedOPTlr refers to only using a client’s loss reduction during each round (w/o train set size) as its weight, while FedOPTlorar considers both factors (Eqn. (6)). FedOPTequal means each client gets equal weight.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which weighting mechanism for FedOPT achieves the highest Overall Dev EM after 60 communication rounds?",
    "answer": "FedOPTlorar",
    "rationale": "The figure shows the Overall Dev EM for different weighting mechanisms for FedOPT. FedOPTlorar is the blue line, and it can be seen that it achieves the highest Overall Dev EM after 60 communication rounds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.17221v1",
    "pdf_url": null
  },
  {
    "instance_id": "7b5e79cd57e04663af7913d226a24fd5",
    "figure_id": "2306.17833v2-Figure14-1",
    "image_file": "2306.17833v2-Figure14-1.png",
    "caption": " K = 1. With this value of K, two of the agents (Rainbow – reset and Rainbow – random reset) become identical.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which agent performs the best according to the figure?",
    "answer": "The Rainbow agent performs the best.",
    "rationale": "The figure shows the average episode score for each agent over the course of training. The Rainbow agent has the highest average episode score at the end of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.17833v2",
    "pdf_url": null
  },
  {
    "instance_id": "60ffdd413d38496488319f1f6db17fe5",
    "figure_id": "2301.12584v3-Figure6-1",
    "image_file": "2301.12584v3-Figure6-1.png",
    "caption": " Fit vs. time, Reddit tensor, R = 100. Thick lines are averages 4 trial interpolations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm appears to have the fastest convergence rate?",
    "answer": "STS-CP",
    "rationale": "The STS-CP line has the steepest slope, which indicates that it is converging to the final value more quickly than the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.12584v3",
    "pdf_url": null
  },
  {
    "instance_id": "8e3c55cb017d4c189f3ed9b8e7bca003",
    "figure_id": "2109.04732v1-Figure3-1",
    "image_file": "2109.04732v1-Figure3-1.png",
    "caption": " Distribution of inter-rater consistency scores across target word lists and corpora (based on GloVe). For both target words and gender base pairs, we observe generally low consistency in bias scores across the three scoring rules, regardless of the corpus used.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which corpus has the highest inter-rater consistency for gender base pairs?",
    "answer": "r/AskHistorians",
    "rationale": "The box plot for r/AskHistorians in Figure (b) is the highest, indicating that the inter-rater consistency for gender base pairs is highest for this corpus.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.04732v1",
    "pdf_url": null
  },
  {
    "instance_id": "d01410c86b0a474eb4310bb97af708ee",
    "figure_id": "2002.06620v1-Figure4-1",
    "image_file": "2002.06620v1-Figure4-1.png",
    "caption": " User behavior ratio (pageviews, hits, and dwell time). The y-axis is the session count normalized by the total number of sessions",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which user behavior metric is the most skewed towards lower values?",
    "answer": "Dwell time",
    "rationale": "The dwell time distribution has a long tail, meaning that most sessions have a relatively short dwell time, while a few sessions have a very long dwell time. This is in contrast to the pageviews and hits distributions, which are more evenly distributed.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.06620v1",
    "pdf_url": null
  },
  {
    "instance_id": "a0e7f30d47fc46fbb3ab887a0f3589f2",
    "figure_id": "2007.03838v2-Figure6-1",
    "image_file": "2007.03838v2-Figure6-1.png",
    "caption": " The value distributions of the accumulated gradients across iterations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which iteration has the most values in the range of [0.1, 0.5]?",
    "answer": "The 1st iteration.",
    "rationale": "The figure shows the distribution of the accumulated gradients across iterations. The height of each bar represents the number of values in that range. The bar for the range of [0.1, 0.5] is the highest in the 1st iteration.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.03838v2",
    "pdf_url": null
  },
  {
    "instance_id": "d9c787d6b1bb4465ab6e2a1f7b67a5b2",
    "figure_id": "2003.10942v1-Figure10-1",
    "image_file": "2003.10942v1-Figure10-1.png",
    "caption": " Percentage Improvement in Waiting Times by Origin.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which borough of New York City saw the largest improvement in waiting times?",
    "answer": "Manhattan",
    "rationale": "The figure shows the percentage improvement in waiting times by origin, with darker shades of blue representing greater improvement. Manhattan is the only borough with a dark blue color, indicating that it had the largest improvement in waiting times.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.10942v1",
    "pdf_url": null
  },
  {
    "instance_id": "183afe150d254f94ac55cf8d9a1b0050",
    "figure_id": "2002.11925v2-Figure3-1",
    "image_file": "2002.11925v2-Figure3-1.png",
    "caption": " An example of estimating three cosine distances that are used for computing the n-pair loss. Given video v with only two classes {c, a} and another video v′ with also two classes {c, b}, we first compute their average class features: {h̄c v, h̄ a v} and {h̄c v′ , h̄b v′}. Then, we estimate the cosine distance dccvv′ of h̄c v and h̄c v′ for the shared class c, and the cosine distances dacvv′ and dcbvv′ of the video features for c and the non-shared classes a and b.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following distances is used to compare the video features for the shared class c?",
    "answer": "dccvv′",
    "rationale": "The figure shows that dccvv′ is the cosine distance between the average class features h̄c v and h̄c v′ for the shared class c.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11925v2",
    "pdf_url": null
  },
  {
    "instance_id": "4291a9649c194efca23e448a75c298b1",
    "figure_id": "2111.06537v1-Figure4-1",
    "image_file": "2111.06537v1-Figure4-1.png",
    "caption": " Log-regret of our non-myopic budget-aware BO method (4-B-MS-EI) compared with baseline acquisition functions on a range of problems.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which acquisition function performs the best on the 'alpine1' problem?",
    "answer": "EI-PUC-CC.",
    "rationale": "The figure shows the log-regret of different acquisition functions on the 'alpine1' problem. The EI-PUC-CC line is the lowest, indicating that it has the lowest regret and therefore performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.06537v1",
    "pdf_url": null
  },
  {
    "instance_id": "430b11c138374a6a96bec2e0e01ed4e0",
    "figure_id": "2003.11928v1-Figure6-1",
    "image_file": "2003.11928v1-Figure6-1.png",
    "caption": " Average recall (%) and precision (%) of all the methods with ratio = 0.1, 0.15, ..., 1. From pair 1-2 to 1-6, the graph pairs become more challenging for graph matching.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of precision and recall for the most challenging graph pair?",
    "answer": "ZACR.",
    "rationale": "The figure shows the precision and recall curves for different graph matching methods on different graph pairs. The most challenging graph pair is pair 1-6, and the ZACR method has the highest precision and recall for this pair.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.11928v1",
    "pdf_url": null
  },
  {
    "instance_id": "b3ebff097f0e4f9f8a094fc6eb4f7678",
    "figure_id": "2109.03285v1-Figure2-1",
    "image_file": "2109.03285v1-Figure2-1.png",
    "caption": " The stages in a ML lifecycle from problem formulation to deployment and monitoring with their respective fairness considerations in orange. Clarify can be used at dataset construction and model testing and monitoring to investigate them.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stage of the ML lifecycle is most likely to introduce bias into the model?",
    "answer": "Dataset construction.",
    "rationale": "The figure shows that dataset construction is the first stage in the ML lifecycle, and it is also the stage where data is collected and processed. If the data is not representative of the population that the model will be used on, or if it contains biases, then the model will be biased as well.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.03285v1",
    "pdf_url": null
  },
  {
    "instance_id": "298fb63f7c204eb281cd477778bc2642",
    "figure_id": "1811.10907v2-Figure5-1",
    "image_file": "1811.10907v2-Figure5-1.png",
    "caption": " Speed/accuracy trade-off by using approximate nearest neighbor (ANN) search in truncation and graph construction.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach achieves the highest mAP?",
    "answer": "Using ANN for truncation.",
    "rationale": "The purple line, which represents the mAP achieved using ANN for truncation, is consistently higher than the other two lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.10907v2",
    "pdf_url": null
  },
  {
    "instance_id": "71f456b290da43ad8438a899d8ac2538",
    "figure_id": "2110.03484v3-Figure7-1",
    "image_file": "2110.03484v3-Figure7-1.png",
    "caption": " An example of an indistinguishable label relation structure (“Husky” and “Bulldog”).",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the color purple in the right figure represent?",
    "answer": "Subsuming.",
    "rationale": "The color purple represents subsuming. The arrows point from \"Husky\" and \"Bulldog\" to \"Domestic Animal\", which means that the concepts of \"Husky\" and \"Bulldog\" are subsumed by the concept of \"Domestic Animal\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.03484v3",
    "pdf_url": null
  },
  {
    "instance_id": "cfb020fd138f4ef3803aa0d70720342a",
    "figure_id": "1905.11666v3-Figure4-1",
    "image_file": "1905.11666v3-Figure4-1.png",
    "caption": " Attention maps for the question \"Are there more green blocks than shiny cubes?\" and its accompanying image, the same data used to show attention logit map in Figure 2. (a) and (b) shows the actual softmax-ed textual and visual attention map which used to acquire the control vector and the information vector in MAC and DAFT MAC, respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which word in the question is most important for the model to focus on in order to answer the question correctly?",
    "answer": "\"Green\"",
    "rationale": "The attention maps for the textual and visual attention show that the model focuses most heavily on the word \"green\" in the question. This is likely because the question is asking about the relative number of green blocks and shiny cubes, and the model needs to be able to identify the green blocks in the image in order to answer the question correctly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11666v3",
    "pdf_url": null
  },
  {
    "instance_id": "c1f86e957c0a41aea34470aed06d992e",
    "figure_id": "2309.08644v1-FigureVI-1",
    "image_file": "2309.08644v1-FigureVI-1.png",
    "caption": "Figure VI. Additional Examples of in-the-wild inference (3/8) – Figure skating (Rampant movements and occlusions)",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the challenges of performing pose estimation in the wild?",
    "answer": "Rampant movements and occlusions.",
    "rationale": "The figure shows that the figure skaters are performing complex movements and that their bodies are often occluded by each other. This makes it difficult for a pose estimation algorithm to accurately estimate the pose of each skater.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.08644v1",
    "pdf_url": null
  },
  {
    "instance_id": "51e075ed27df4b4fbf97965c5304789b",
    "figure_id": "2212.10132v2-Figure6-1",
    "image_file": "2212.10132v2-Figure6-1.png",
    "caption": " Qualitative comparison results of the traditional codes BPG [9], neural image compression method Minnen et al. [30] and our method.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image compression method produces the highest quality image?",
    "answer": "Our method.",
    "rationale": "The images produced by our method are sharper and have more detail than the images produced by the other two methods. This can be seen in the zoomed-in portions of the images, where the text on the life jackets is more legible in our method's images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10132v2",
    "pdf_url": null
  },
  {
    "instance_id": "9af31e25a26543b2a8748416bf735d33",
    "figure_id": "2310.12819v2-Figure6-1",
    "image_file": "2310.12819v2-Figure6-1.png",
    "caption": " The ratio of the number of unsolved puzzles to the number of unsolved puzzles by HIPS as a function of the number of node expansions N (x-axis) with multiple values of ε. Values below 1 indicate that the complete search is superior to the high-level search. HIPS-ε outperforms HIPS in every environment except TSP, where high-level actions are sufficient for solving every problem instance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of ε results in the best performance for the complete search compared to the high-level search?",
    "answer": "ε = 0",
    "rationale": "The purple line in the figure represents the performance of the complete search with ε = 0. This line is consistently below 1, indicating that the complete search outperforms the high-level search for all values of N.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.12819v2",
    "pdf_url": null
  },
  {
    "instance_id": "4ffe4443577f403abfc2fa0320864fa6",
    "figure_id": "2204.10530v1-Figure4-1",
    "image_file": "2204.10530v1-Figure4-1.png",
    "caption": " Classification error (on synthetic data) of all competing methods with respect to sample sizes s in each view.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best when the sample size is small?",
    "answer": "MEIB",
    "rationale": "The plot shows the classification error of different methods for different sample sizes. MEIB has the lowest error for all sample sizes, especially for small sample sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.10530v1",
    "pdf_url": null
  },
  {
    "instance_id": "d40032a8a94547b68f3dc36a4ecf27bb",
    "figure_id": "2109.06085v2-Figure3-1",
    "image_file": "2109.06085v2-Figure3-1.png",
    "caption": " Different instantiations of the Multi-head Cross-Modal Attention module (MCMA).",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which fusion method in the figure performs element-wise multiplication between the attention weights of the different modalities?",
    "answer": "Joint Fusion",
    "rationale": "The figure shows that the Joint Fusion method takes the attention weights from each modality and performs element-wise multiplication between them. This is in contrast to the other fusion methods, which either add the attention weights together (Weighted Addition), concatenate them (Hybrid Fusion), or use a stepwise approach (Stepwise Fusion).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.06085v2",
    "pdf_url": null
  },
  {
    "instance_id": "7f96bbfde20a478eaff5612d56961301",
    "figure_id": "2208.11200v1-Figure8-1",
    "image_file": "2208.11200v1-Figure8-1.png",
    "caption": " The memory usage of the FirmCore algorithm, ML-core algorithms, and CoreCube algorithm.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm uses the most memory on the Homo dataset?",
    "answer": "ML-Hybrid",
    "rationale": "The figure shows the memory usage of different algorithms on various datasets. The bars for the ML-Hybrid algorithm are consistently higher than the bars for the other algorithms, indicating that it uses the most memory.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.11200v1",
    "pdf_url": null
  },
  {
    "instance_id": "eb2f2057aa6441959023cf709fbdef2a",
    "figure_id": "2106.10435v1-Figure2-1",
    "image_file": "2106.10435v1-Figure2-1.png",
    "caption": " Training loss and testing accuracy for classification on CIFAR-10 dataset against the number of communication rounds for moderate heterogeneity setting with b = 8 and I = 61.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, FedAvg, SCAFFOLD, or STEM, has the lowest training loss after 500 communication rounds?",
    "answer": "STEM",
    "rationale": "The plot on the left shows the training loss for each method over 500 communication rounds. The STEM method (yellow line) has the lowest training loss at the end of the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.10435v1",
    "pdf_url": null
  },
  {
    "instance_id": "61cbea22a98d4df083cdeb6d4f00c0ea",
    "figure_id": "1912.07116v2-Figure4-1",
    "image_file": "1912.07116v2-Figure4-1.png",
    "caption": " Effects on inversion performance by the number of latent codes used and the feature composition position.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which number of latent codes consistently results in the best performance across all layers?",
    "answer": "30 latent codes.",
    "rationale": "The line for 30 latent codes is consistently the highest across all layers, indicating that it has the highest correlation and thus the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.07116v2",
    "pdf_url": null
  },
  {
    "instance_id": "de267277936d4f1587f8e4b599800c44",
    "figure_id": "1905.12198v1-Figure5-1",
    "image_file": "1905.12198v1-Figure5-1.png",
    "caption": " Examples of replacing templates. Template 1’s are the inital generated templates, while the remaining ones are produced by the authors. We use bold to denote the heads and use italic red to denote mistaken words.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which template produced the most accurate output for Entity ID: Q859415?",
    "answer": "Template 1",
    "rationale": "The figure shows that Template 1 produced the output \"commune in paris, france\", which is identical to the gold standard. The other templates produced outputs that were either incomplete or incorrect.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12198v1",
    "pdf_url": null
  },
  {
    "instance_id": "aaaf29b6c9374df188c9d21f1f5ca599",
    "figure_id": "2207.08806v1-Figure6-1",
    "image_file": "2207.08806v1-Figure6-1.png",
    "caption": " Comparison of different pre-training objective functions on conformation generation tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pre-training objective function achieves the best performance on the QM9 dataset?",
    "answer": "Our method.",
    "rationale": "The figure shows the MAT and COV scores for different pre-training objective functions on the QM9 and Drugs datasets. Our method achieves the highest MAT and COV scores on the QM9 dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.08806v1",
    "pdf_url": null
  },
  {
    "instance_id": "74df5b01b5a54aadaa33cb1ea055f428",
    "figure_id": "2006.03340v2-Figure6-1",
    "image_file": "2006.03340v2-Figure6-1.png",
    "caption": " Decoded trajectories from memory.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the image show?",
    "answer": "The image shows decoded trajectories from memory.",
    "rationale": "The image shows a series of dots, each of which represents a decoded trajectory from memory. The trajectories are colored differently to distinguish between different groups or conditions. The x-axis of the image represents time, and the y-axis represents the position of the decoded trajectory.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.03340v2",
    "pdf_url": null
  },
  {
    "instance_id": "7b63da556a32458b83a55772f924a359",
    "figure_id": "2106.12940v1-Figure2-1",
    "image_file": "2106.12940v1-Figure2-1.png",
    "caption": " Overall framework of MatchVIE. The relevancy evaluation branch predicts the key-value relationship between entities. The entity recognition branch mainly determines the categories of standalone entities. The entity recognition branch is difficult to distinguish numeric categories which are similar in visual and semantic, such as the ‘4.90’ in purple.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which branch of MatchVIE is responsible for determining the categories of standalone entities?",
    "answer": "Entity recognition branch",
    "rationale": "The passage states that \"The entity recognition branch mainly determines the categories of standalone entities.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.12940v1",
    "pdf_url": null
  },
  {
    "instance_id": "b990778c3b7744398bd81db54551c4f1",
    "figure_id": "1911.04004v4-Figure2-1",
    "image_file": "1911.04004v4-Figure2-1.png",
    "caption": " Agent’s action space with axes x1, x2 corresponding to features. xt is the agent’s true feature vector and rt(α) his misreport against α. Actions α, β, γ comprise the learner’s action set.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "If the agent misreports its features as δ, what is the angle between the true feature vector and the misreported feature vector?",
    "answer": "2δ",
    "rationale": "The angle between the true feature vector and the misreported feature vector is twice the angle between the true feature vector and the action α. This is because the misreported feature vector is obtained by reflecting the true feature vector across the action α.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.04004v4",
    "pdf_url": null
  },
  {
    "instance_id": "4c54493ea4a14d3eb4b3dcc46b8fb734",
    "figure_id": "1904.02860v2-Figure6-1",
    "image_file": "1904.02860v2-Figure6-1.png",
    "caption": " Tree routing distribution of live/spoof data. X-axis denotes 8 leaf nodes, and y-axis denotes 15 types of data. The number in each cell represents the percentage (%) of data that fall in that leaf node. Each row is sum to 1. (a) Print Protocol. (b) Transparent Mask Protocol. Yellow box denotes the unknown attacks.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of data is most likely to be classified as a spoof attack in the Transparent Mask Protocol?",
    "answer": "The \"trans. mask\" data type.",
    "rationale": "In the Transparent Mask Protocol (Figure b), the \"trans. mask\" data type has the highest percentage of data that falls in the leaf nodes associated with unknown attacks (yellow box). This suggests that this type of data is more likely to be classified as a spoof attack than other types of data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.02860v2",
    "pdf_url": null
  },
  {
    "instance_id": "3b62fb95cff7437b8758ea03038567ed",
    "figure_id": "2210.16721v1-Figure6-1",
    "image_file": "2210.16721v1-Figure6-1.png",
    "caption": " Quantitative evaluation of the top performed models from Tab. 1 and Tab. 4. We employ t-SNE [29] for dimension reduction of model latent space. We use the extra labels (i.e., tumour and normal) from the STNet dataset for annotations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model from the figure performs the best in terms of PCC@M?",
    "answer": "Ours (e)",
    "rationale": "The figure shows the t-SNE visualizations of the latent space representations of different models, with the PCC@M values reported in the titles. The PCC@M value for our model is 0.202, which is higher than the PCC@M values for all other models shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.16721v1",
    "pdf_url": null
  },
  {
    "instance_id": "ad56cfea622241b986d513884a628201",
    "figure_id": "2109.00122v3-Figure5-1",
    "image_file": "2109.00122v3-Figure5-1.png",
    "caption": " Error case study 1: The net change in the tax position is the sum of the increase and the decrease plus the penalties and interest. The model lacks this finance knowledge, thus the retriever fails to retrieve the correct table rows and sentences. Another challenging point is the table understanding, since in this case, it’s hard to distinguish the retrieved two table rows for the year 2013 or 2014, using our method that regards each table row as basic unit. The model needs to look at the full table to get this global information.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the total balance of unrecognized tax benefits at December 31, 2014?",
    "answer": "$195237",
    "rationale": "The table shows the changes in the company's unrecognized tax benefits, excluding interest and penalties. The balance at December 31, 2014 is $195237.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.00122v3",
    "pdf_url": null
  },
  {
    "instance_id": "e8573ef4c95d47cfad8c17af8ed7d742",
    "figure_id": "2305.19753v2-Figure50-1",
    "image_file": "2305.19753v2-Figure50-1.png",
    "caption": " CKA-similarity across network’s layers for ResNet-34 and VGG-19.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network shows a higher degree of self-similarity across its layers, ResNet-34 or VGG-19?",
    "answer": "ResNet-34",
    "rationale": "The figure shows the CKA similarity across the layers of each network. CKA similarity measures how similar two sets of data are. In this case, it is measuring how similar the representations learned by different layers of the network are. A higher CKA similarity indicates a higher degree of self-similarity. The figure shows that ResNet-34 has a higher CKA similarity across its layers than VGG-19, indicating that ResNet-34 has a higher degree of self-similarity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19753v2",
    "pdf_url": null
  },
  {
    "instance_id": "5ed18bae199b4dca9cdc82edb3d2f152",
    "figure_id": "2106.12997v2-FigureA.6-1",
    "image_file": "2106.12997v2-FigureA.6-1.png",
    "caption": "Figure A.6: (a) Wall clock time for model fitting with multi-task and batched Gaussian processes as a function of the number of function queries. (b) Time for Thompson sampling as a function of the number of function queries. In both cases, the multi-task Gaussian process is faster; training due to using conjugate gradients and Kronecker MVMs while Thompson sampling is faster due to the Matheron’s rule approach that we use.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is faster for model fitting, MTGP or Batched GP?",
    "answer": "MTGP is faster for model fitting.",
    "rationale": "The figure shows the wall clock time for model fitting with MTGP and Batched GP as a function of the number of function queries. The MTGP line is consistently below the Batched GP line, indicating that it takes less time to fit the model using MTGP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.12997v2",
    "pdf_url": null
  },
  {
    "instance_id": "d0c912a057f2456bb26f400bfb5f303b",
    "figure_id": "1912.12179v1-Figure10-1",
    "image_file": "1912.12179v1-Figure10-1.png",
    "caption": " Relationship between the parts score and different measures of similarity. On the left, the parts scores is plotted against the two different measures of similarity. We can see there is a clear trend for all the models: the parts score increases for more semantically similar images, and decreases as the images become more similar pixel-wise. The figure on the right shows Pearson’s correlation coefficient between the metrics for different models",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model family has the highest correlation between parts score and attribute similarity?",
    "answer": "Self-Supervised",
    "rationale": "The figure on the right shows the correlation between parts score and different measures of similarity for different model families. The self-supervised model family has the highest correlation for attribute similarity, as shown by the orange line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.12179v1",
    "pdf_url": null
  },
  {
    "instance_id": "c3253bdb7da6444d83c2e182539f2a37",
    "figure_id": "2304.07302v2-Figure6-1",
    "image_file": "2304.07302v2-Figure6-1.png",
    "caption": " The influence of representation dimension on two datasets FB and MovieLens for HGWaveNet and GRUGCN.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model and dimension combination performs the best on the MovieLens dataset?",
    "answer": "GRUGCN with a dimension of 16.",
    "rationale": "The figure shows the AUC score for different model and dimension combinations on the MovieLens dataset. The highest AUC score for the MovieLens dataset is achieved by GRUGCN with a dimension of 16.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.07302v2",
    "pdf_url": null
  },
  {
    "instance_id": "1986959a8fe640b38ddc8efc331e59ac",
    "figure_id": "2305.04837v4-Figure4-1",
    "image_file": "2305.04837v4-Figure4-1.png",
    "caption": " Comparisons of different gradient based methods",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which gradient-based method has the highest training accuracy?",
    "answer": "ODMcsvmrg",
    "rationale": "The figure shows the training accuracy of different gradient-based methods as a function of training time. The ODMcsvmrg method has the highest training accuracy of all the methods shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.04837v4",
    "pdf_url": null
  },
  {
    "instance_id": "13c9848c725d4c5faf28d569778ec271",
    "figure_id": "2006.14308v1-Figure7-1",
    "image_file": "2006.14308v1-Figure7-1.png",
    "caption": " CED for WFLW testset. NME and FR10% are reported at the legend for comparison. We compare our methods with other state-of-the-arts with source codes available, including LAB [25] and AWing [24].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best according to the CED for WFLW testset?",
    "answer": "PropNet",
    "rationale": "The plot shows the cumulative error distribution (CED) for the WFLW testset. The PropNet method has the lowest FR (failure rate) at 10%, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.14308v1",
    "pdf_url": null
  },
  {
    "instance_id": "47d341e58c1c4dec8249f05fb6785582",
    "figure_id": "2208.08168v2-Figure2-1",
    "image_file": "2208.08168v2-Figure2-1.png",
    "caption": " Figure illustrating size thresholds τ , τ̂ , and the good gZ with respect to agent a.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which size threshold, τ or τ̂ , determines the number of components in Y that will be affected by a perturbation of X through the good gZ ?",
    "answer": "τ̂",
    "rationale": "The figure shows that the good gZ is defined as the minimal set of components in Z that are affected by a perturbation of X with size at least τ . This means that the number of components in Y that will be affected by a perturbation of X through the good gZ is determined by the size threshold τ̂ , which defines the minimal size of a perturbation of Z that affects Y.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.08168v2",
    "pdf_url": null
  },
  {
    "instance_id": "0c1af9df2f9a464380755f64f4d37ad5",
    "figure_id": "2302.13834v2-Figure12-1",
    "image_file": "2302.13834v2-Figure12-1.png",
    "caption": " lnZ estimate as a function of number of steps K - a) Logistic Sonar dataset, b) Brownian motion, c) NICE. Yellow dotted line is MF-VI and dashed magenta is the gold standard.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, DDS, PIS, or SMC, provides the most accurate estimate of lnZ for the NICE dataset?",
    "answer": "SMC",
    "rationale": "The boxplots for SMC are consistently closest to the gold standard (dashed magenta line) for all three datasets. This indicates that SMC provides the most accurate estimate of lnZ.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.13834v2",
    "pdf_url": null
  },
  {
    "instance_id": "96882f88feb346c3b3d9c18b386a8367",
    "figure_id": "1909.04951v1-Figure11-1",
    "image_file": "1909.04951v1-Figure11-1.png",
    "caption": " Precision-recall curve for AnimalWeb and WIDER Face datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset is more difficult for the model to achieve high precision and recall?",
    "answer": "AnimalWeb Faces",
    "rationale": "The AnimalWeb Faces curve is lower than all of the WIDER FACE curves, which indicates that the model achieves lower precision and recall for this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.04951v1",
    "pdf_url": null
  },
  {
    "instance_id": "13404c8cfa7a45d6b68cc132e13a66ab",
    "figure_id": "2106.12027v1-Figure3-1",
    "image_file": "2106.12027v1-Figure3-1.png",
    "caption": " Example complex sentence (Orig), ground truth output (SS 1 and SS 2), and WRG (best seen in color; edge directions and punctuation omitted for readability). Vertices are word tokens and their indices, edges are neighbor (ngbh) and/or dependency relations. Dashed lines represent edges to Break, the green curved line represents an edge to Copy, the open circle node for and-6 is for Drop, and all other parts of the graph get Accept. At bottom left is a fragment of the corresponding Edge Triple Set.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the graph represents the word \"and\" in the original sentence?",
    "answer": "The open circle node labeled \"and-6\".",
    "rationale": "The caption states that \"the open circle node for and-6 is for Drop\". This means that the word \"and\" is not included in either of the simplified sentences (SS 1 and SS 2).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.12027v1",
    "pdf_url": null
  },
  {
    "instance_id": "d98874e18bd6421e9594567bc2aa6795",
    "figure_id": "1911.01030v1-Figure10-1",
    "image_file": "1911.01030v1-Figure10-1.png",
    "caption": " Synthetic Results",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest completion rate (CR) when the sampling rate is 1.5?",
    "answer": "Greedy CS",
    "rationale": "In Figure (a), the bar for Greedy CS at a sampling rate of 1.5 is the tallest, indicating that it has the highest CR among the methods compared.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.01030v1",
    "pdf_url": null
  },
  {
    "instance_id": "0e59113794674afb97f097b2dcdd1436",
    "figure_id": "1902.11268v1-Figure4-1",
    "image_file": "1902.11268v1-Figure4-1.png",
    "caption": " Wide ResNet Model Size Reduction. Compared with baseline models, compressed models achieve similar model size as ResNet-110. Compressed model named like ”48-4” has 48 convolutional layers and widening parameter as 4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the largest number of parameters?",
    "answer": "Baseline",
    "rationale": "The figure shows the number of parameters for different models. The Baseline model has the tallest bar, which indicates it has the largest number of parameters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.11268v1",
    "pdf_url": null
  },
  {
    "instance_id": "37588400ba7d4a8686dd5353b5269f02",
    "figure_id": "2304.05387v2-Figure6-1",
    "image_file": "2304.05387v2-Figure6-1.png",
    "caption": " Figure demonstrating the effect of clustering in MOST. Eliminating the clustering results in noisier outputs.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of clustering in MOST?",
    "answer": "Clustering in MOST reduces noise in the output.",
    "rationale": "The figure shows that when clustering is used, the bounding boxes are more tightly grouped around the objects in the image. This results in a less noisy output, as the bounding boxes are more likely to accurately represent the objects in the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.05387v2",
    "pdf_url": null
  },
  {
    "instance_id": "0ce30c5ff8a44c4e8a235e54b01131d8",
    "figure_id": "1812.05083v2-Figure6-1",
    "image_file": "1812.05083v2-Figure6-1.png",
    "caption": " Flower images generated by GAN-INT-CLS, TAC-GAN, and Text-SeGAN (with easy-to-hard negatives).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models produced the most realistic-looking flowers?",
    "answer": "TAC-GAN.",
    "rationale": "The TAC-GAN flowers have more natural-looking shapes and colors, while the GAN-INT-CLS and Text-SeGAN flowers have some artifacts and inconsistencies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.05083v2",
    "pdf_url": null
  },
  {
    "instance_id": "919f38c5deaf4acca1403ad359c49a4a",
    "figure_id": "2010.03250v3-Figure2-1",
    "image_file": "2010.03250v3-Figure2-1.png",
    "caption": " Efficiency of DiffMG at the evaluation stage. We do not compare against GTN because of OOM on a single GPU.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models is the most efficient?",
    "answer": "DiffMG",
    "rationale": "The figure shows that DiffMG achieves the highest validation AUC in the shortest amount of time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.03250v3",
    "pdf_url": null
  },
  {
    "instance_id": "34100d879dc041c5aa39efe09d8aa917",
    "figure_id": "2005.09207v2-Figure5-1",
    "image_file": "2005.09207v2-Figure5-1.png",
    "caption": " The attention map of 1st attention head at last layer.",
    "figure_type": "\"plot\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which word in the document is most likely to be the focus of the first attention head at the last layer?",
    "answer": "\"Company\"",
    "rationale": "The figure shows the attention map of the first attention head at the last layer. The attention map is a visualization of how much attention the model pays to each word in the document. The darker the color, the more attention the model pays to that word. In this figure, the word \"Company\" is the darkest, which means that the model is paying the most attention to that word.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.09207v2",
    "pdf_url": null
  },
  {
    "instance_id": "d63c63e66df24cfd8c9e6afe55741eeb",
    "figure_id": "1805.09843v1-Figure4-1",
    "image_file": "1805.09843v1-Figure4-1.png",
    "caption": " The test accuracy comparisons between SWEM and CNN/LSTM models on (a) Yahoo! Answers dataset and (b) SNLI dataset, with different proportions of training data (ranging from 0.1% to 100%).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the Yahoo! Answers dataset when only 0.1% of the training data is used?",
    "answer": "SWEM",
    "rationale": "The figure shows that the SWEM model has the highest accuracy (around 25%) when only 0.1% of the training data is used on the Yahoo! Answers dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.09843v1",
    "pdf_url": null
  },
  {
    "instance_id": "96709b92fdf94fa1ab5d9b872055978f",
    "figure_id": "2203.07171v2-Figure8-1",
    "image_file": "2203.07171v2-Figure8-1.png",
    "caption": " (Part 2/3) Reward density in the Atari suite. The rewards of each game are measured with three behavioral policies: (i) fully random (iteration 0: the first training iteration), (ii) an ε-greedy policy using ε = 0.1, with state-action values estimated by a (Lin)DQN agent after completing 5 iterations of training, and (iii) similar to (ii) but after 49 training iterations. All experiments are averaged over 20 independent trials (shaded areas depict standard deviation) and for 1000 steps with a moving average window of 10 steps.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which game shows the most consistent reward density across all three policies?",
    "answer": "Solaris",
    "rationale": "The reward density for Solaris is relatively flat across all three policies, indicating that the agent is able to learn a consistent policy regardless of the amount of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.07171v2",
    "pdf_url": null
  },
  {
    "instance_id": "90f3aaf70f7d42de9b70c4007f97fdd1",
    "figure_id": "2010.15363v2-Figure9-1",
    "image_file": "2010.15363v2-Figure9-1.png",
    "caption": " Average item recall in different item groups on Adressa.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest recall for items in the 0-10 group?",
    "answer": "LightGCN",
    "rationale": "The LightGCN plot in Figure (a) shows that the LightGCN model has a recall of approximately 0.1 for items in the 0-10 group, while the MF model in Figure (b) has a recall of approximately 0.05 for items in the same group.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.15363v2",
    "pdf_url": null
  },
  {
    "instance_id": "5480a8b458614969aba30169a39c49b9",
    "figure_id": "2202.09931v2-Figure4-1",
    "image_file": "2202.09931v2-Figure4-1.png",
    "caption": " Each pie-chart gives a pointwise decomposition of a dataset according to pro le types. Each point is classi ed based on the accuracy pro le of an ImageNet trained classi er (see the end of Section 3.1). e decompositions are similar for both ResNet-50 and DenseNet-121 architectures.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest percentage of easy points?",
    "answer": "ImageNet-A",
    "rationale": "The pie chart for ImageNet-A shows that 85.3% of the points are easy, which is the highest percentage of easy points among all the datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.09931v2",
    "pdf_url": null
  },
  {
    "instance_id": "c6ee85c307034543812d4ea58805d5a2",
    "figure_id": "2303.04766v1-Figure2-1",
    "image_file": "2303.04766v1-Figure2-1.png",
    "caption": " a) Backfilling results for different training objectives and backfilling orderings evaluated on the ImageNet-1k dataset. The FastFill training objective, defined in Eqn. (5), compared to our baseline method (FCT) obtains significantly improved mAP over the course of backfilling when a random ordering is used. The performance is further improved when using the ordering implied by the predicted uncertainties σ2 i . We also compare against FastFill-Cheating, in which the ordering is obtained by computing the training loss (3) on gallery images (hence a cheating setup), and show comparable performance. b) We show that the predicted log σ2 i , the output of ψϑ on gallery images, and logLl2+disc(xi) are well correlated. c) We sort xi once by σ2 i and once by Ll2+disc(xi) to get two orderings. Here, for each xi we plot its order in the first ordering vs its order in the second ordering, and observe great correlation (Kendall-Tau correlation=0.67).",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training objective performs best according to the figure?",
    "answer": "FastFill-Cheating.",
    "rationale": "Figure (a) shows the mAP for different training objectives and backfilling orderings. The FastFill-Cheating training objective achieves the highest mAP across all backfilling percentages.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.04766v1",
    "pdf_url": null
  },
  {
    "instance_id": "f15a5360418743ff940e49d6fd36c28d",
    "figure_id": "2303.01502v1-Figure3-1",
    "image_file": "2303.01502v1-Figure3-1.png",
    "caption": " Examples of output utterances generated by various speaker models. Here, the “Hard Distractors” referenced are selected with hybrid metrics computed with CLIP embeddings. Both training on more difficult distractors and training with ToM leads to more accurate, fluent utterances.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which speaker model generated the most accurate and fluent utterance?",
    "answer": "The ToM speaker trained on hard distractors.",
    "rationale": "The figure shows that the ToM speaker trained on hard distractors generated the utterance \"A group of men are playing baseball outside,\" which is the most accurate and fluent of the three utterances.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.01502v1",
    "pdf_url": null
  },
  {
    "instance_id": "3bdd0a3acf674d1f84cd928c238e0fe7",
    "figure_id": "2105.04949v4-Figure7-1",
    "image_file": "2105.04949v4-Figure7-1.png",
    "caption": " BATS (top) and Google (bottom) results split by high-level categories.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed best on the Lexical category of tasks for the BATS benchmark?",
    "answer": "BERT",
    "rationale": "The figure shows the accuracy of different methods on different categories of tasks for the BATS and Google benchmarks. For the Lexical category of tasks for the BATS benchmark, the BERT method achieved the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.04949v4",
    "pdf_url": null
  },
  {
    "instance_id": "8d81db8539b84d55bee772ceca789de7",
    "figure_id": "2103.10681v2-Figure5-1",
    "image_file": "2103.10681v2-Figure5-1.png",
    "caption": " The quantitative results for different superpixel segmentation methods on the BSDS dataset (top row), DRIVE dataset (middle row) and DME dataset (bottom row). Better view in color and zoom in four times.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which superpixel segmentation method performs the best on the BSDS dataset?",
    "answer": "Our method performs the best on the BSDS dataset.",
    "rationale": "The figure shows the quantitative results for different superpixel segmentation methods on the BSDS dataset. The top row of the figure shows the results for our method, SSN, ERS, LSC, and SNIC. The results for our method are the closest to the ground truth, which indicates that our method performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.10681v2",
    "pdf_url": null
  },
  {
    "instance_id": "541c1d16f048455f8ee8bde4ab736c6c",
    "figure_id": "1907.04835v2-Figure4-1",
    "image_file": "1907.04835v2-Figure4-1.png",
    "caption": " Left to right: Super-resolution output from FSRCNN, SRResNet, mDCSRN, RRDB, RRDB with patch GAN training, and ground truth. Bottom row is magnified portion of the same image region across the different SISR outputs.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the sharpest and most detailed super-resolution image?",
    "answer": "RRDB patch GAN.",
    "rationale": "The bottom row of the figure shows magnified portions of the same image region across the different SISR outputs. The RRDB patch GAN output shows the most detail and sharpest edges, which is closest to the ground truth reference image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.04835v2",
    "pdf_url": null
  },
  {
    "instance_id": "41e7a729003b4d28a22c3d6ea4553b1e",
    "figure_id": "2105.14039v3-Figure9-1",
    "image_file": "2105.14039v3-Figure9-1.png",
    "caption": " The HCAM-based agent selectively attends to relevant memories in the rapid-word-learning generalization tasks. (a) We analyze the relative weight of attention to the first memory from the first learning phase, when the agent is asked to recall a word form the first phase in test 4 vs. when it is asked to recall a word from a later phase in test 3. (b) Across all 4 memory layers, the agent attends more strongly to its memory of this first learning phase when that memory is relevant—in test 4—compared to when that memory is irrelevant—in test 3. (This plot shows relative attention weights—that is, attention weights divided by average attention weight, so that if the agent were attending uniformly to all memories, their relative attention weights would be 1, indicated by the dotted line. This plot shows averages and 95%-CIs across the 93 episodes where the agent made a correct choice in test 4, out of 100 total super-episodes run.)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which memory layer shows the largest difference in relative attention between test 3 and test 4?",
    "answer": "Layer 0",
    "rationale": "The plot in Figure (b) shows that the difference in relative attention between test 3 and test 4 is largest for layer 0. This is because the bars for layer 0 are further apart than the bars for any other layer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14039v3",
    "pdf_url": null
  },
  {
    "instance_id": "123ce8514d42492091177dfe0d041610",
    "figure_id": "2106.03596v1-Figure1-1",
    "image_file": "2106.03596v1-Figure1-1.png",
    "caption": " Error rate in non-separable synthetic bandit experiments showcasing Gappletron against known baselines. The points are the means and the whiskers are minimum and maximum error rate over ten repetitions (details in Section 6).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in the non-separable synthetic bandit experiments?",
    "answer": "Gappletron.",
    "rationale": "The figure shows the error rates for five different algorithms. The error rate for Gappletron is the lowest, which indicates that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03596v1",
    "pdf_url": null
  },
  {
    "instance_id": "7d4d837b87d342b68d491b3e14d70781",
    "figure_id": "2301.05032v2-Figure1-1",
    "image_file": "2301.05032v2-Figure1-1.png",
    "caption": " Average accuracy (%) on CIFAR-100 25-phase, using two data-receiving settings: 1) training-from-half (TFH): a large amount of data is available beforehand to pre-train the encoder; 2) training-from-scratch (TFS): classes come evenly in each phase. Dark blue and orange indicate the baselines and our method, respectively. Light-color circles are confidence intervals. Notice that methods with strong KD losses, e.g., LUCIR [14], AANets [22], and RMM [23], tend to provide worse performance in TFS than TFH, while methods with weak KD losses, e.g., iCaRL [30] and LwF [21], tend to provide worse performance in TFH than TFS. Our method uses an online learning algorithm to produce the key hyperparameters, e.g., the weights that control which KD losses are used. Thus, our method achieves the highest performance in both TFS and TFH.",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best when trained from scratch?",
    "answer": "Ours",
    "rationale": "The figure shows the average accuracy of different methods on the CIFAR-100 25-phase dataset, using two data-receiving settings: training from half (TFH) and training from scratch (TFS). The orange dot, which represents our method, is the highest point on the plot, indicating that it has the highest accuracy when trained from scratch. \n\nFigure type: plot",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.05032v2",
    "pdf_url": null
  },
  {
    "instance_id": "18af50d0fd924dd191718125023fb719",
    "figure_id": "2211.14003v1-Figure13-1",
    "image_file": "2211.14003v1-Figure13-1.png",
    "caption": " Instructions for the PARKING task where student participants learn how to park a car with a joystick controller.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many phases are there in the parking control experiment?",
    "answer": "Three",
    "rationale": "The instructions explicitly state that the experiment consists of three phases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.14003v1",
    "pdf_url": null
  },
  {
    "instance_id": "c6fc2f78f451470ea1a4b5c90bb89a5a",
    "figure_id": "2003.12327v1-Figure2-1",
    "image_file": "2003.12327v1-Figure2-1.png",
    "caption": " SND comparison of different batch whitening methods. We sample 60,000 examples from a Gaussian distribution as the training set. To calculate SND, we use s = 200 and N = 20. We show (a) the SND with respect to the dimensions ranging from 21 to 29, under a batch size of 1024; (b) the SND with respect to the batch size ranging from 27 to 212, under a dimension of 128.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which whitening method is most effective in terms of signal-to-noise and distortion ratio (SND) at higher dimensions?",
    "answer": "CD whitening",
    "rationale": "As shown in the plot (a), CD whitening consistently achieves the highest SND among all the whitening methods, and the SND of CD whitening increases significantly with increasing dimensions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.12327v1",
    "pdf_url": null
  },
  {
    "instance_id": "53eed0d7a1164f8dabe4a9f6288190b6",
    "figure_id": "2311.05067v2-Figure14-1",
    "image_file": "2311.05067v2-Figure14-1.png",
    "caption": " The performance on 3 COG tasks without ICVF.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the \"Grasp from Closed Drawer\" task?",
    "answer": "The Oracle algorithm.",
    "rationale": "The figure shows the normalized return for different algorithms on the \"Grasp from Closed Drawer\" task. The Oracle algorithm achieves the highest normalized return, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2311.05067v2",
    "pdf_url": null
  },
  {
    "instance_id": "725681b4fed142d39cf3998ac0aaa3ef",
    "figure_id": "1907.12743v6-Figure6-1",
    "image_file": "1907.12743v6-Figure6-1.png",
    "caption": " The comparison of t-SNE visualization with source (blue) and target (orange) distributions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods shown in the figure best separates the source and target distributions?",
    "answer": "TA^3N (d)",
    "rationale": "The figure shows the t-SNE visualization of the source and target distributions for four different methods. TA^3N (d) shows the clearest separation between the source and target distributions, with the blue and orange points forming distinct clusters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.12743v6",
    "pdf_url": null
  },
  {
    "instance_id": "2d82e0e53a434f40b48a37ba534af7bf",
    "figure_id": "1805.09298v9-Figure4-1",
    "image_file": "1805.09298v9-Figure4-1.png",
    "caption": " Class-imbalance learning on MNIST.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of using MHE on the class-imbalance learning on MNIST?",
    "answer": "MHE helps to reduce the class imbalance problem.",
    "rationale": "In Figure (a), the data points are clustered around the center, with a few outliers. This indicates that the classes are imbalanced, with some classes having more data points than others. In Figure (b), the data points are more evenly distributed, indicating that the class imbalance problem has been reduced.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.09298v9",
    "pdf_url": null
  },
  {
    "instance_id": "1a400e7bb8bb4dcf80ffb1873acd65ef",
    "figure_id": "1710.00996v2-Figure2-1",
    "image_file": "1710.00996v2-Figure2-1.png",
    "caption": " Experimental results for budget allocation.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of worst-case profit?",
    "answer": "EQUATOR.",
    "rationale": "In both panels (a) and (c), the blue line representing EQUATOR consistently shows the highest worst-case profit across all values of n and |L|.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1710.00996v2",
    "pdf_url": null
  },
  {
    "instance_id": "b6c5bb63a0e24e91822134696dff2952",
    "figure_id": "2306.12230v2-Figure20-1",
    "image_file": "2306.12230v2-Figure20-1.png",
    "caption": " The mean standard deviation computed for the Jaccard index between the sets of weights chosen for removal during the first update of the sparse connectivity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest mean standard deviation for the Jaccard index between the sets of weights chosen for removal during the first update of the sparse connectivity?",
    "answer": "ResNet-56",
    "rationale": "The figure shows the mean standard deviation for the Jaccard index for each model. The values are shown in the color-coded squares. The highest value is in the ResNet-56 column.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.12230v2",
    "pdf_url": null
  },
  {
    "instance_id": "f9f3499f6dc141a5aaabf03f83aa04e1",
    "figure_id": "2203.03605v4-Figure7-1",
    "image_file": "2203.03605v4-Figure7-1.png",
    "caption": " Training convergence curves evaluated on COCO val2017 for DINO and two previous state-of-the-art models with ResNet-50 using multi-scale features.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest AP after 150 epochs of training?",
    "answer": "DINO(Ours)",
    "rationale": "The figure shows the training convergence curves for three different models, DINO(Ours), DN-Deformable DETR, and Deformable DETR. The AP (average precision) is plotted on the y-axis, and the number of epochs is plotted on the x-axis. After 150 epochs of training, DINO(Ours) has the highest AP, as shown by the red line being higher than the green and blue lines at the end of the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.03605v4",
    "pdf_url": null
  },
  {
    "instance_id": "76f12660b9a340a1876e94f32ab13c67",
    "figure_id": "1911.09929v2-Figure4-1",
    "image_file": "1911.09929v2-Figure4-1.png",
    "caption": " Intermediate results for Modular-level Search. The architectures with blue dot are the selected model C0-C5 based on the previous Stage-one. The orange dots are architectures forming the Pareto front found by our algorithm.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which architecture has the highest mAP?",
    "answer": "C5",
    "rationale": "The figure shows that C5 has the highest mAP, which is around 40%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.09929v2",
    "pdf_url": null
  },
  {
    "instance_id": "a7e8be3e1c6b424d830b5f5a5e121cca",
    "figure_id": "2110.03484v3-Figure2-1",
    "image_file": "2110.03484v3-Figure2-1.png",
    "caption": " The one-to-one mapping between label relations and set relations.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the label relations is equivalent to the set relation \"A ⊂ B\"?",
    "answer": "Subsumed",
    "rationale": "The figure shows that the \"Subsumed\" label relation corresponds to the set relation \"A ⊂ B\", which means that all elements of A are also elements of B. This is illustrated by the circle representing A being completely contained within the circle representing B.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.03484v3",
    "pdf_url": null
  },
  {
    "instance_id": "31d418ee21da4daa92026da6c81dff05",
    "figure_id": "2202.00504v1-Figure2-1",
    "image_file": "2202.00504v1-Figure2-1.png",
    "caption": " Compression force on q0 along normal n at q0.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which direction will q0 move when the compression force is applied?",
    "answer": "q0 will move in the direction opposite to n.",
    "rationale": "The caption states that the compression force is applied along the normal n at q0. This means that the force is pushing q0 in the direction of n. However, q0 is constrained by the other tubes, so it will move in the opposite direction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.00504v1",
    "pdf_url": null
  },
  {
    "instance_id": "cea781edda574e93961dc97adad93fcc",
    "figure_id": "2203.00089v1-Figure3-1",
    "image_file": "2203.00089v1-Figure3-1.png",
    "caption": " A comparison of SGDm, Adam, KFAC, Shampoo, and APO-Precond on UCI regression tasks. Across all tasks, APO-Precond achieves lower loss with competitive convergence compared to second-order optimizers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer achieves the lowest training loss on the Slice dataset?",
    "answer": "APO-Precond",
    "rationale": "The figure shows the training loss for different optimizers on the Slice dataset. APO-Precond is the optimizer with the lowest training loss at the end of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.00089v1",
    "pdf_url": null
  },
  {
    "instance_id": "361fe405a0104ab189f94c4123c4f5b8",
    "figure_id": "2202.13296v2-Figure2-1",
    "image_file": "2202.13296v2-Figure2-1.png",
    "caption": " Illustration of the subgraph retrieving process. We expand a path from each topic entity as well as induce a corresponding tree, and then merge the trees from different topic entities to form a unified subgraph.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which person in the figure is both a Canadian citizen and a Turing Award winner?",
    "answer": "Geoffrey Hinton",
    "rationale": "The figure shows two paths, one for each topic entity. The first path shows that Geoffrey Hinton won the Turing Award, and the second path shows that he is a Canadian citizen. The subgraph at the bottom of the figure shows that Hinton is the only person who satisfies both conditions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.13296v2",
    "pdf_url": null
  },
  {
    "instance_id": "45d4909ce888406fb19c2d0c0a4509e6",
    "figure_id": "1902.07849v1-Figure9-1",
    "image_file": "1902.07849v1-Figure9-1.png",
    "caption": " The accuracy and F1 score with 95% confidence interval for Ultrasound.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best in terms of accuracy and F1 score for Ultrasound data?",
    "answer": "STFNet-Filter",
    "rationale": "The figure shows the accuracy and F1 score for different models on Ultrasound data. The STFNet-Filter model has the highest accuracy and F1 score, as shown by the bars in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.07849v1",
    "pdf_url": null
  },
  {
    "instance_id": "4831e9bc376e4b23a709e57007365dd6",
    "figure_id": "2112.02749v1-Figure6-1",
    "image_file": "2112.02749v1-Figure6-1.png",
    "caption": " Compare our results with videos created by the combination of SynObama and FOMM. The combinated method is sensitive to the initial pose, while our method preserves the authentic head motions.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more sensitive to the initial pose, SynObama + FOMM or ours?",
    "answer": "SynObama + FOMM.",
    "rationale": "The caption states that the SynObama + FOMM method is sensitive to the initial pose, while our method preserves the authentic head motions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.02749v1",
    "pdf_url": null
  },
  {
    "instance_id": "19d0d17d9f4e449f9ae9fd597f7ba18e",
    "figure_id": "1911.07323v1-Figure5-1",
    "image_file": "1911.07323v1-Figure5-1.png",
    "caption": " Convergence Curve of Pubmed with different sampling methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampling method converges the fastest to the highest F1-Score?",
    "answer": "LADIES (512)",
    "rationale": "The figure shows the F1-Score of different sampling methods over time. The LADIES (512) method reaches the highest F1-Score in the shortest amount of time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.07323v1",
    "pdf_url": null
  },
  {
    "instance_id": "45004f1359d145cab568d1659e9a44d4",
    "figure_id": "2205.11672v2-Figure8-1",
    "image_file": "2205.11672v2-Figure8-1.png",
    "caption": " CelebA: Distribution of the third highest PCA feature.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which distribution has the highest peak?",
    "answer": "The green distribution.",
    "rationale": "The green distribution has the highest peak, which is located at approximately 0.0 on the x-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.11672v2",
    "pdf_url": null
  },
  {
    "instance_id": "9a788726f8924d54b415f12cfadb7a24",
    "figure_id": "2211.06651v2-Figure11-1",
    "image_file": "2211.06651v2-Figure11-1.png",
    "caption": " Distribution of links by normalized section type for ACL-17. Sections with non-standard names are mapped to the other category.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which section of the research paper is most likely to contain a link?",
    "answer": "Introduction",
    "rationale": "The figure shows the distribution of links by normalized section type for ACL-17. The x-axis shows the frequency of links, and the y-axis shows the section type. The bar for \"Introduction\" is the highest, indicating that the introduction section is most likely to contain a link.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.06651v2",
    "pdf_url": null
  },
  {
    "instance_id": "7361268a080a41548baed10eaa8d07b5",
    "figure_id": "2004.04772v3-Figure2-1",
    "image_file": "2004.04772v3-Figure2-1.png",
    "caption": " The frequency distributions of the datasets (sorted in non-increasing order).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest weight at the beginning of the distribution?",
    "answer": "SO (out)",
    "rationale": "The figure shows the weight of each dataset on the y-axis, and the index in sorted order on the x-axis. The SO (out) dataset is the highest line at the beginning of the distribution, which means it has the highest weight.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.04772v3",
    "pdf_url": null
  },
  {
    "instance_id": "ee7acde5e9944931a12d703a8b15886e",
    "figure_id": "2304.04625v1-Figure6-1",
    "image_file": "2304.04625v1-Figure6-1.png",
    "caption": " The total reward for each episode when using various RL agents. The plotted values are passed through a one-dimensional maximum filter of size 5 for visibility.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which RL agent performs the best in this experiment?",
    "answer": "DDPG",
    "rationale": "The DDPG agent has the highest average reward over the course of the experiment. This can be seen in the plot, where the blue line representing DDPG is consistently higher than the other two lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.04625v1",
    "pdf_url": null
  },
  {
    "instance_id": "82bebc23ca844ba5b7aecd47617ae9ac",
    "figure_id": "2110.09131v2-Figure3-1",
    "image_file": "2110.09131v2-Figure3-1.png",
    "caption": " The gold AMR and the ensemble AMR graph of SPRING, T5, APT and Cai&Lam using the Graphene algorithm for the sentence “They want money, not the face\".",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which AMR graph is most accurate for the sentence \"They want money, not the face\"?",
    "answer": " The gold AMR graph.",
    "rationale": " The gold AMR graph is the ground truth, or the most accurate representation of the sentence. The other AMR graphs are generated by different models, and may not be as accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.09131v2",
    "pdf_url": null
  },
  {
    "instance_id": "0cc2345cce074614adaf4ac6ca01cf8a",
    "figure_id": "2302.00032v1-Figure13-1",
    "image_file": "2302.00032v1-Figure13-1.png",
    "caption": " Visualization of the decomposition of a cell into patches. Each patch is pulled back into a parent space, which is a B-spline knot span. Quadrature and integration happens in this space. The pore shapes are parameterized by radii.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many patches are used to decompose the cell in the figure?",
    "answer": "Four",
    "rationale": "The figure shows four patches labeled \"Patch 1\", \"Patch 2\", \"Patch 3\", and \"Patch 4\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.00032v1",
    "pdf_url": null
  },
  {
    "instance_id": "12d722ad086a4e259a6496dbd1feba6a",
    "figure_id": "2211.06478v1-Figure1-1",
    "image_file": "2211.06478v1-Figure1-1.png",
    "caption": " Detection error trade-off (DET) curves of various KWS systems. The dot on the DET curve of the ASR baseline large model shows its performance without the bigram edit distance scoring method (Section 4.2).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which KWS system has the lowest Equal Error Rate (EER)?",
    "answer": "TT-KWS + MBR large",
    "rationale": "The EER is the point on the DET curve where the False Positive Rate (FPR) is equal to the False Negative Rate (FNR). The TT-KWS + MBR large system has the lowest EER of 3.27%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.06478v1",
    "pdf_url": null
  },
  {
    "instance_id": "3a3ec5cd537046e3a64328cfb44c8f6a",
    "figure_id": "2103.05137v2-Figure12-1",
    "image_file": "2103.05137v2-Figure12-1.png",
    "caption": " Performance of models on the entire image (from left to right: ResNet, Inception3, and MNASNet).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the entire image?",
    "answer": "ResNet.",
    "rationale": "The figure shows the performance of three models on the entire image. The x-axis shows the accuracy percentage, and the y-axis shows the different objects. The ResNet model has the highest accuracy percentage for most of the objects, which means it performed the best overall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.05137v2",
    "pdf_url": null
  },
  {
    "instance_id": "0774d745300b4af8a74e80301c6ddd60",
    "figure_id": "2206.08780v2-Figure2-1",
    "image_file": "2206.08780v2-Figure2-1.png",
    "caption": " Runtime comparison in log-log scale between W, Sinkhorn with the geodesic distance, SW2, SSW2 with the binary search (BS) and uniform distribution (12) and SSW1 with formula (11) between two distributions on S2. The time includes the calculation of the distance matrices.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms has the fastest runtime when the number of samples in each distribution is 10^3?",
    "answer": "SSW2 with the uniform distribution.",
    "rationale": "The figure shows the runtime of each algorithm as a function of the number of samples in each distribution. The y-axis is in log scale, so the algorithm with the lowest line at a given x-value has the fastest runtime. At x = 10^3, the green line (SSW2, Unif, L = 200) is the lowest, indicating that SSW2 with the uniform distribution has the fastest runtime.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.08780v2",
    "pdf_url": null
  },
  {
    "instance_id": "6efe4ee70cd44849a21e7dc2d0d33d91",
    "figure_id": "2104.06873v3-Figure5-1",
    "image_file": "2104.06873v3-Figure5-1.png",
    "caption": " Solution value vs. k for single-pass algorithms for the revmax application on each dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the slashdot dataset?",
    "answer": "AEFSNS and Greedy",
    "rationale": "The figure shows that the AEFSNS and Greedy algorithms have the highest value for all values of k on the slashdot dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.06873v3",
    "pdf_url": null
  },
  {
    "instance_id": "30daaa26fe8948d3bd4b8c91d20f466b",
    "figure_id": "2307.16634v1-Figure4-1",
    "image_file": "2307.16634v1-Figure4-1.png",
    "caption": " The distributions of the predicted labels across the confidence scores using off-the-shelf CLIP on the whole image (global) and snappets (local).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class has the highest number of samples with a confidence score of 0.9 or higher?",
    "answer": "\"tvmonitor\" class",
    "rationale": "The plot for the \"tvmonitor\" class shows that there are more samples with a confidence score of 0.9 or higher than any other class.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.16634v1",
    "pdf_url": null
  },
  {
    "instance_id": "9cc73ad4692e435496b648d74b35479c",
    "figure_id": "1906.08226v6-Figure3-1",
    "image_file": "1906.08226v6-Figure3-1.png",
    "caption": " Different ablations for the ST-DIM model",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the Asteroids game?",
    "answer": "GT-DIM",
    "rationale": "The figure shows the average F1 score for different models on different Atari games. The GT-DIM model has the highest average F1 score for the Asteroids game.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.08226v6",
    "pdf_url": null
  },
  {
    "instance_id": "68903142074c4ef88c3104400e25e425",
    "figure_id": "1906.04556v2-Figure1-1",
    "image_file": "1906.04556v2-Figure1-1.png",
    "caption": " Comparison of PeNFAC, DDPG and deterministic PPO over 60 different seeds for each algorithm in Hopper.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in this experiment?",
    "answer": "PeNFAC",
    "rationale": "The plot shows the undiscounted sum reward during testing for each algorithm. PeNFAC has the highest reward, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04556v2",
    "pdf_url": null
  },
  {
    "instance_id": "b1f8272d866e4f18911d367553ad4017",
    "figure_id": "1804.07927v4-Figure1-1",
    "image_file": "1804.07927v4-Figure1-1.png",
    "caption": " Example QA pairs obtained from the original movie plot and the paraphrased plot. The relevant spans needed for answering the corresponding question are highlighted in blue and red with the respective question numbers. Note that the span highlighting shown here is for illustrative purposes only and is not available in the dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Where does Cole and Railly spend their remaining time together?",
    "answer": "Florida Keys",
    "rationale": "The passage mentions that \"They decide to spend their remaining time together in the Florida Keys before the plague.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.07927v4",
    "pdf_url": null
  },
  {
    "instance_id": "11a45d9572544c0baa3fb46a2e20067f",
    "figure_id": "2210.10605v3-Figure2-1",
    "image_file": "2210.10605v3-Figure2-1.png",
    "caption": " Convergence speed of the different methods, we use 40 images with spatially-varying blur and Gaussian noise with σ = 10/255.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods is the fastest to converge to a PSNR of 23?",
    "answer": "PnP-ADMM + CG",
    "rationale": "The plot shows that the PnP-ADMM + CG method reaches a PSNR of 23 in the shortest amount of time, which is about 10 seconds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.10605v3",
    "pdf_url": null
  },
  {
    "instance_id": "623a29c8e41c4123be1dcd540916fe48",
    "figure_id": "2310.00093v2-Figure18-1",
    "image_file": "2310.00093v2-Figure18-1.png",
    "caption": " Learned synthetic images with different loss functions on the CIFAR10 dataset with IPC 10.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function results in the most realistic images?",
    "answer": "DataDAM",
    "rationale": "The images in (d) are the most visually appealing and realistic. They have sharp edges and clear details, while the images in (a), (b), and (c) are blurry and lack detail.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.00093v2",
    "pdf_url": null
  },
  {
    "instance_id": "49541549deec4aca9f9bb58044beadb1",
    "figure_id": "2111.11297v2-Figure27-1",
    "image_file": "2111.11297v2-Figure27-1.png",
    "caption": " Teaching initial example to be solved by the human.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the provided context, which genus of plants is found only in North America?",
    "answer": "Callirhoe",
    "rationale": "The text states that all nine species of Callirhoe are native to the prairies and grasslands of North America. No information is provided about the geographic distribution of Nothoscordum.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.11297v2",
    "pdf_url": null
  },
  {
    "instance_id": "b1b88b668127406b9cf1d87db7fca4e7",
    "figure_id": "2209.08856v1-Figure12-1",
    "image_file": "2209.08856v1-Figure12-1.png",
    "caption": ": An illustration of the reduction of Theorem D.9. All arcs have weight 2. Red superscripts denote the C2-Borda score of the candidates (after adding dummy candidates).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the difference in the C2-Borda scores of candidates $e_1$ and $e_2$?",
    "answer": "2",
    "rationale": "The figure shows the C2-Borda scores of the candidates. The C2-Borda score of $e_1$ is $\\alpha + 2 - 2t$ and the C2-Borda score of $e_2$ is $\\alpha + 2$. The difference between these two scores is $2$.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.08856v1",
    "pdf_url": null
  },
  {
    "instance_id": "c158a5e1bf6f44e6839b20ef4cf25972",
    "figure_id": "2306.02520v2-Figure4-1",
    "image_file": "2306.02520v2-Figure4-1.png",
    "caption": " A heat map representing the intersection (left) and union (right) of correct predictions on BDD-QA for five large models: NLI-Roberta, KG-Roberta, KG-T5, QA-T5, and Retrieval-T5. Each value in the heat map represents the joint accuracy of the intersection/union of two models.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest accuracy for the overlap of correct predictions with Ro-NLI?",
    "answer": "T5-Re.",
    "rationale": "The heat map on the left shows the accuracy of the overlap of correct predictions for each pair of models. The value in the cell corresponding to Ro-NLI and T5-Re is the highest, indicating that these two models have the highest overlap of correct predictions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.02520v2",
    "pdf_url": null
  },
  {
    "instance_id": "c655ec9e1b044eb5a6c442dcbce71f04",
    "figure_id": "2003.01908v2-Figure23-1",
    "image_file": "2003.01908v2-Figure23-1.png",
    "caption": " The certification results of the Azure API with denoisers trained with STAB+MSE vs. CLF+MSE.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method produces more accurate certified results for ResNet-50, Stab+MSE or CLF+MSE?",
    "answer": "Stab+MSE",
    "rationale": "The figure shows that the certified accuracy for Stab+MSE on ResNet-50 is higher than the certified accuracy for CLF+MSE on ResNet-50 across all values of the radius parameter.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.01908v2",
    "pdf_url": null
  },
  {
    "instance_id": "8f0a5f565090439299b521de7dc943ad",
    "figure_id": "1903.02709v4-Figure3-1",
    "image_file": "1903.02709v4-Figure3-1.png",
    "caption": " Left: mixup (Equation 4), with interpolated points in blue corresponding to line segments between the three points shown in red. Middle: triplet mixup (Equation 6). Right: Bernoulli mixup (Equation 5).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three mixup methods shown in the figure is the most likely to produce a new data point that is similar to one of the original data points?",
    "answer": "Bernoulli mixup.",
    "rationale": "Bernoulli mixup randomly selects one of the original data points as the new data point. This is different from the other two methods, which create new data points by interpolating between the original data points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.02709v4",
    "pdf_url": null
  },
  {
    "instance_id": "5db06248555140eb8d1881bddd614f84",
    "figure_id": "2305.12716v1-Figure8-1",
    "image_file": "2305.12716v1-Figure8-1.png",
    "caption": " A story generation example with our ImageNet [23] fine-tuned SD-IPC-FT.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Where did the robot finally find his friends?",
    "answer": "At the river.",
    "rationale": "The bottom right image in the figure shows the robot with two other robots, and the caption states that the robot went to the river before finding his friends.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.12716v1",
    "pdf_url": null
  },
  {
    "instance_id": "0d6cb5014d044c39b0c314e677b7c98d",
    "figure_id": "1905.10044v1-Figure2-1",
    "image_file": "1905.10044v1-Figure2-1.png",
    "caption": " Accuracy for various models on the BoolQ dev set as the number of training examples varies.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best when trained on 9,000 examples?",
    "answer": "BERTL + MultiNLI",
    "rationale": "The figure shows that the BERTL + MultiNLI model achieves the highest accuracy when trained on 9,000 examples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10044v1",
    "pdf_url": null
  },
  {
    "instance_id": "cbaaf1521e2a4b409d4cb285292b5e46",
    "figure_id": "2103.04039v1-Figure6-1",
    "image_file": "2103.04039v1-Figure6-1.png",
    "caption": " The training curves of Class-Module (Class) and joint training (Joint) of ClassSR-FSRCNN.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method converges faster, Class or Joint?",
    "answer": "Class.",
    "rationale": "The FLOPS curve for Class (b) shows a steeper decline in the beginning compared to Joint (d), indicating that it reaches a lower FLOPS value quicker.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.04039v1",
    "pdf_url": null
  },
  {
    "instance_id": "2fcdbc948fb249c1b4d579c0558dd1c5",
    "figure_id": "1902.05660v1-Figure6-1",
    "image_file": "1902.05660v1-Figure6-1.png",
    "caption": " Examples from our VQA-Rephrasings dataset. The first question (shown in gray) in each block is the original question from VQA v2.0 validation set, the questions that follow (shown in black) are rephrasings collected in VQA-Rephrasings.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following statements about the image is true?\n\na) The horse is running.\nb) The plane is about to land.\nc) The people are playing a game.\nd) The food is healthy.",
    "answer": "c) The people are playing a game.",
    "rationale": "The image shows a group of people playing a game of frisbee. This is the only statement that can be verified from the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.05660v1",
    "pdf_url": null
  },
  {
    "instance_id": "5b40cd8ccd1c49d6839ea3e57f4469fc",
    "figure_id": "2106.15482v2-Figure10-1",
    "image_file": "2106.15482v2-Figure10-1.png",
    "caption": " Reliability diagrams on CIFAR-100 with 50 clients. Best temperature. The last 5 figures are ours.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the best calibration?",
    "answer": "pFedGP-IP-compute-marg.",
    "rationale": "The reliability diagram shows the relationship between the predicted confidence and the actual accuracy. The ideal calibration is represented by the diagonal line. The closer the bars are to the diagonal line, the better the calibration. In this case, pFedGP-IP-compute-marg. has the bars closest to the diagonal line, indicating that it has the best calibration.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15482v2",
    "pdf_url": null
  },
  {
    "instance_id": "416a7ba7491d40749b3ca16aed59537c",
    "figure_id": "1905.07397v1-Figure1-1",
    "image_file": "1905.07397v1-Figure1-1.png",
    "caption": " A typical state tree. The trunk represents the blocks whose rewards have already been collected. The current state (3, 5, 1) of the game is represented by the blocks mined by Miner 1 and Miner 2 and c = 1 because Miner 2 controls the block before the fork.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the current state of the game?",
    "answer": "(3, 5, 1)",
    "rationale": "The current state of the game is represented by the blocks mined by Miner 1 and Miner 2 and c = 1 because Miner 2 controls the block before the fork.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.07397v1",
    "pdf_url": null
  },
  {
    "instance_id": "aafda3d90ee14bbea480835fd67a587b",
    "figure_id": "2202.04887v1-Figure1-1",
    "image_file": "2202.04887v1-Figure1-1.png",
    "caption": " An example of inserting new concepts into an existing taxonomy of computer science terms. For each new concept, we aim to find the relatedness between the concept and each candidate position.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the new concepts is most closely related to Machine Learning?",
    "answer": "Tensor Core",
    "rationale": "The figure shows that Tensor Core is a child of the Machine Learning node in the enlarged taxonomy. This indicates that Tensor Core is a subfield of Machine Learning.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.04887v1",
    "pdf_url": null
  },
  {
    "instance_id": "72432476f05144159061292b99c03f02",
    "figure_id": "1909.01101v1-Figure2-1",
    "image_file": "1909.01101v1-Figure2-1.png",
    "caption": " In this example, four agents decode the similar sentence with different model capacity. (a): At first, each agent is pre-trained to generate the translation independently. (b) The ensemble model is generated by the average prediction from each agent. (c): The One-to-Many learning distills the knowledge from the ensemble model to each agent as necessary. The performance of each agent is improved explicitly in an interactive updating process, through repeating the process (b) and (c).",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In what stage of the process is the knowledge from the ensemble model distilled to each agent?",
    "answer": "One-to-Many Learning stage.",
    "rationale": "The figure shows that in stage (c), One-to-Many Learning, the knowledge from the ensemble model (generated in stage (b)) is distilled to each agent. This is indicated by the dashed arrows pointing from the ensemble model to each agent.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.01101v1",
    "pdf_url": null
  },
  {
    "instance_id": "2309ab5f07ee4e77a4956f84d8bc3721",
    "figure_id": "2110.14248v2-Figure4-1",
    "image_file": "2110.14248v2-Figure4-1.png",
    "caption": " Ablation of PA-SF and visualization of the latent representation via LER metric. All curves represent the mean and one standard deviation across 7 seeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed best in the door training environments, as measured by the angle difference metric?",
    "answer": "PA-SF (w/o AS)",
    "rationale": "The leftmost plot in the figure shows the angle difference for different methods in the door training environments. The PA-SF (w/o AS) method has the lowest angle difference at the end of training, indicating that it performed best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14248v2",
    "pdf_url": null
  },
  {
    "instance_id": "ec3dfbbcd3bd4c88b5614b304aaf55fa",
    "figure_id": "2205.09726v3-Figure3-1",
    "image_file": "2205.09726v3-Figure3-1.png",
    "caption": " Performance/time trade-off across hyperparameters (grid search details in §A.3). RANKGEN reranking significantly improves MAUVE, but need an order of magnitude more time due to overgeneration.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does reranking with 20 hypotheses improve the MAUVE score compared to not reranking?",
    "answer": "Yes.",
    "rationale": "The figure shows that the MAUVE score for the \"no reranking\" condition is lower than the MAUVE score for the \"20 hypotheses\" condition.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.09726v3",
    "pdf_url": null
  },
  {
    "instance_id": "d7aa84a5cd1742caa0aa6653900cc945",
    "figure_id": "2002.00901v1-Figure8-1",
    "image_file": "2002.00901v1-Figure8-1.png",
    "caption": " Left: User activeness across time. Middle: The recovered number of user interactions. Right: The original number of user interactions.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which time slice has the most user interactions?",
    "answer": " Time slice 0.",
    "rationale": " The original number of user interactions (right panel) shows that the first time slice has the most user interactions, indicated by the darkest green color. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.00901v1",
    "pdf_url": null
  },
  {
    "instance_id": "5fe359f4395b46019f6d1e3190e36382",
    "figure_id": "2209.01207v2-Figure4-1",
    "image_file": "2209.01207v2-Figure4-1.png",
    "caption": " Wasserstein distance for three seeds between demonstrator and imitator trajectories on the 3to2 Cheetah task on co-imitation (CoIL) and pure imitation learning algorithms (SAIL, GAIL).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best in terms of Wasserstein distance?",
    "answer": "CoIL (SAIL)",
    "rationale": "The plot shows that CoIL (SAIL) has the lowest Wasserstein distance overall, which indicates that it is the closest to the demonstrator trajectories.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.01207v2",
    "pdf_url": null
  },
  {
    "instance_id": "0280df127a4b487891f96667a7ed317d",
    "figure_id": "1904.01720v1-Figure3-1",
    "image_file": "1904.01720v1-Figure3-1.png",
    "caption": " The drop in repair accuracy of the repair model due to incorrect slot placement.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which threshold value results in the highest repair accuracy for the NoBugNear repair model?",
    "answer": "τ = 0",
    "rationale": "The table in Figure (b) shows the repair accuracy for the NoBugNear repair model for different threshold values. The highest repair accuracy is 88.6%, which occurs when τ = 0.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.01720v1",
    "pdf_url": null
  },
  {
    "instance_id": "91883023a3a742778b26e2c10151710b",
    "figure_id": "1905.10837v5-Figure3-1",
    "image_file": "1905.10837v5-Figure3-1.png",
    "caption": " (a) Hold-out set accuracy as a function of training trials (log scale) for a newly introduced task. Colored lines indicate task ordinal position (cyan = introduced in episode 1; magenta = introduced in episode 10). In all panels, the shaded region represents ±1 standard error of the mean. (b) Hold-out accuracy of the task introduced in episode 1 by number of times it is retrained (black = 1 time, copper = 10 times). (c) Number of trials required to reach the accuracy criterion (log scale) as a function of the number of times a given task is trained (also log scale). As in (a), the colors indicate task ordinal position (the episode in which a task is introduced). (d) Similar to (c) but plotting only the new task introduced in a given episode. (e) Hold-out accuracy attained after a fixed amount of training (22.5k trials) of a given task, graphed as a function of number of times a given task is trained. As in (a), the colors indicate the episode in which a task is introduced. (f) Similar to (e) but plotting only the new task introduced in a given episode.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task required the least number of trials to reach the accuracy criterion?",
    "answer": "The task introduced in episode 1.",
    "rationale": "Panel (c) shows that the task introduced in episode 1 (cyan line) consistently required the least number of trials to reach the accuracy criterion, regardless of how many times it was trained.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10837v5",
    "pdf_url": null
  },
  {
    "instance_id": "8f34bee29fa8425b99819abf307c4e45",
    "figure_id": "2307.12335v1-Figure4-1",
    "image_file": "2307.12335v1-Figure4-1.png",
    "caption": " Distribution of the sampled data. Left: angular offset for views pairs. Middle: explorable distance. Right: path length between source and target viewpoints (computed by the ShortestPathFollower() function).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three distributions is the most skewed?",
    "answer": "The distribution of explorable distance.",
    "rationale": "The distribution of explorable distance is the most skewed because it has a long tail to the right. This means that there are a few very large values of explorable distance, while most of the values are relatively small.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.12335v1",
    "pdf_url": null
  },
  {
    "instance_id": "d2774d1f55664053a85242b954e3d130",
    "figure_id": "2109.06870v1-Figure1-1",
    "image_file": "2109.06870v1-Figure1-1.png",
    "caption": " Word error rate (WER) and average utterance inference time on LibriSpeech (dev-other) of wav2vec 2.0 and our SEW and SEW-D models fine-tuned with 100h labeled data for 100K updates.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best trade-off between word error rate (WER) and inference time?",
    "answer": "SEW-D",
    "rationale": "The figure shows that SEW-D has a lower WER than wav2vec 2.0 and SEW at all inference times.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.06870v1",
    "pdf_url": null
  },
  {
    "instance_id": "8bc64a6e19804d439cea2e638cb19ed9",
    "figure_id": "2211.03041v1-Figure1-1",
    "image_file": "2211.03041v1-Figure1-1.png",
    "caption": " Visualization of calibration (OOTB) between different PLMs and competitive methods on QQP.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most well-calibrated on QQP?",
    "answer": "Vanilla Baseline",
    "rationale": "The figure shows that the Vanilla Baseline line is the closest to the diagonal line, which indicates perfect calibration. This means that the predicted probabilities are closest to the true probabilities for this method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.03041v1",
    "pdf_url": null
  },
  {
    "instance_id": "d738ef663e7045429b04f34ac9646c3b",
    "figure_id": "1904.03485v2-Figure7-1",
    "image_file": "1904.03485v2-Figure7-1.png",
    "caption": " Denoised performance of models trained with AWGN in (b) and mixed AWGN-RVIN in (c). During testing, k = 0 and s = 2.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the most noise?",
    "answer": "Image (a) shows the most noise.",
    "rationale": "Image (a) is labeled as the \"Noisy image,\" and it visually appears to have the most noise compared to the other two images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03485v2",
    "pdf_url": null
  },
  {
    "instance_id": "eedbba5992ac4bbcbbd037a09c696e41",
    "figure_id": "2205.10012v3-Figure1-1",
    "image_file": "2205.10012v3-Figure1-1.png",
    "caption": " Three use cases of short descriptions onWikipedia.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which use case of short descriptions on Wikipedia is shown in each panel of the figure?",
    "answer": "(a) Search, (b) Summary, (c) Read more.",
    "rationale": "The figure shows three panels, each of which depicts a different use case of short descriptions on Wikipedia. Panel (a) shows the search use case, where a user searches for information about beer. The short description provides a concise overview of beer, including its definition and some related topics. Panel (b) shows the summary use case, where a short description of beer is displayed at the top of the article. This summary provides a brief overview of the topic, including its definition, history, and production process. Panel (c) shows the read more use case, where a short description of beer is displayed in the \"Read more\" section of the article. This section provides links to related articles and other resources.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.10012v3",
    "pdf_url": null
  },
  {
    "instance_id": "1a297fe2b641409890adf09e16e8df9b",
    "figure_id": "2109.13087v2-Figure3-1",
    "image_file": "2109.13087v2-Figure3-1.png",
    "caption": " The Impact of database size on Coverage@500 metric of BM25-QS, BE-QS, CFC-QS.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which query selection strategy has the highest Coverage@500 for a database size of 10M?",
    "answer": "BM25-QS.",
    "rationale": "The figure shows the Coverage@500 for three query selection strategies (BM25-QS, BE-QS, and CFC-QS) for different database sizes. At a database size of 10M, the Coverage@500 for BM25-QS is the highest (around 21.3).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.13087v2",
    "pdf_url": null
  },
  {
    "instance_id": "422b513a72ed40a49911d71e1de33544",
    "figure_id": "2302.01115v3-Figure5-1",
    "image_file": "2302.01115v3-Figure5-1.png",
    "caption": " Performance of PEPNet model with different settings and implementations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which setting has the largest impact on the performance of the PEPNet model for the \"Follow in Domain A\" task?",
    "answer": "The coefficients in GateNN.",
    "rationale": "Figure (c) shows that the GAUC for the \"Follow in Domain A\" task increases significantly when the number of coefficients in GateNN is increased from 0.5 to 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.01115v3",
    "pdf_url": null
  },
  {
    "instance_id": "67bc956bc3624b3c9e1e0daec5a8b0fb",
    "figure_id": "2202.05068v1-Figure3-1",
    "image_file": "2202.05068v1-Figure3-1.png",
    "caption": " Adversarial attacks during testing on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attack method is most effective at reducing the accuracy of the PN-Conv model?",
    "answer": "FGSM_0.1",
    "rationale": "The FGSM_0.1 curve shows the largest decrease in accuracy compared to the clean model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.05068v1",
    "pdf_url": null
  },
  {
    "instance_id": "4e8671abab0b4bc196f877b91cdf7cce",
    "figure_id": "1906.06776v2-Figure4-1",
    "image_file": "1906.06776v2-Figure4-1.png",
    "caption": " Plot of −Q(· | β) for g ∝ |x|2.5 for β = 0.1, 0.5, 0.8, 1.2, 1.5, represented by different colored curves. The true parameter is β∗ = 1. Similar to Figure 3, the blue dots correspond to (β,−Q(β | β)) and the red dots correspond to (β+,−Q(β+ | β))",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of beta is the true parameter?",
    "answer": "1",
    "rationale": "The caption states that the true parameter is β∗ = 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.06776v2",
    "pdf_url": null
  },
  {
    "instance_id": "fd5e221bffeb427480cfb67d0e724e54",
    "figure_id": "2106.07998v2-Figure18-1",
    "image_file": "2106.07998v2-Figure18-1.png",
    "caption": " Alternative calibration metrics for IMAGENET-C: negative log-likelihood (NLL) and Brier score. Plotted as in Figure 4. Second and fourth rows show residuals as described in Figure 8.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best calibration for all levels of corruption severity?",
    "answer": "ResNeXt WSL.",
    "rationale": "The figure shows the relationship between the predicted probability and the actual probability for different models and levels of corruption severity. The closer the points are to the diagonal line, the better the calibration. ResNeXt WSL has the points closest to the diagonal line for all levels of corruption severity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07998v2",
    "pdf_url": null
  },
  {
    "instance_id": "6e5767f2b8f2483e91e05f17619f0725",
    "figure_id": "2210.08726v3-Figure15-1",
    "image_file": "2210.08726v3-Figure15-1.png",
    "caption": " Few-shot prompt for the revision model, which uses chain-of-thought prompting.",
    "figure_type": "\"other\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following statements is **not** true according to the image?\n\n* A. The nasal cycle switches about every 45 minutes during sleep. \n* B. The British side in the Battles of Lexington and Concord was led by Lieutenant Colonel Francis Smith.\n* C. The Stanford Prison Experiment was conducted in the basement of Jordan Hall. \n* D. Phoenix Mills Ltd. was established in 1854.\n* E. Phoenix Market City is located on 21 acres of prime property in Pune.",
    "answer": "C.",
    "rationale": "The image shows that the Stanford Prison Experiment was conducted in the basement of **Encina Hall**, not Jordan Hall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.08726v3",
    "pdf_url": null
  },
  {
    "instance_id": "c0adad86966c4d79b6bade16ed21280c",
    "figure_id": "2104.06644v2-Figure5-1",
    "image_file": "2104.06644v2-Figure5-1.png",
    "caption": " Rissanen Data Analysis (Perez et al., 2021) on the GLUE benchmark and PAWS datasets. The lower minimum description length (MDL, measured in kilobits), the better the learning ability of the model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the PAWS dataset?",
    "answer": "PAWS",
    "rationale": "The PAWS model has the lowest minimum description length (MDL) on the PAWS dataset, which indicates that it has the best learning ability.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.06644v2",
    "pdf_url": null
  },
  {
    "instance_id": "cdaffbfcd5714fe8b4e18e60e75ef2fc",
    "figure_id": "1912.09640v2-Figure4-1",
    "image_file": "1912.09640v2-Figure4-1.png",
    "caption": " FLOPs versus accuracy on ImageNet. † means methods use extra techniques like Swish activation and Squeeze-and-Excitation module.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which architecture achieves the highest accuracy on ImageNet?",
    "answer": "AtomNAS+t",
    "rationale": "The figure shows that AtomNAS+t has the highest accuracy on ImageNet, at approximately 77%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.09640v2",
    "pdf_url": null
  },
  {
    "instance_id": "886bf375b48249f0b5ed19564c55cbe6",
    "figure_id": "2010.15054v1-Figure5-1",
    "image_file": "2010.15054v1-Figure5-1.png",
    "caption": " Attribution maps of a network before and after network compression. These figures are examples that the networks are predicting the correct label (airplane, sofa, cat, bird, airplane, cat, person, person, airplane, bottle) before and after compression but produce different attribution maps. The last column of examples comes from the network trained with knowledge distillation and our regularization. The results show that our regularization indeed preserves attribution maps. 16",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method of network compression appears to preserve attribution maps the best?",
    "answer": "KD with Ours.",
    "rationale": "The last column of the figure shows the attribution maps for the network trained with knowledge distillation and the author's regularization. These maps are very similar to the maps for the full network, which suggests that this method of network compression preserves attribution maps the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.15054v1",
    "pdf_url": null
  },
  {
    "instance_id": "6df59c5fab014feba863c24a4d1cc961",
    "figure_id": "1804.05018v1-Figure2-1",
    "image_file": "1804.05018v1-Figure2-1.png",
    "caption": " Two scenes included in our dataset. The letfmost one depicts a ratio 1:4 (3 animals, 12 artifacts, 15 total items), the rightmost one a ratio 2:3 (6, 9, 15).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the ratio of animals to artifacts in the leftmost scene?",
    "answer": "1:4",
    "rationale": "The leftmost scene has 3 animals and 12 artifacts, which gives a ratio of 1:4.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.05018v1",
    "pdf_url": null
  },
  {
    "instance_id": "d53bfbd5b65f49019d8bd0dcca4dde7b",
    "figure_id": "1907.12205v2-Figure2-1",
    "image_file": "1907.12205v2-Figure2-1.png",
    "caption": " Top: convergence comparisons among various vanilla robust aggregation methods and the versions after deploying DETOX under “a little is enough\" Byzantine attack [11]. Bottom: Per iteration runtime analysis of various methods. All results are for ResNet-18 trained on CIFAR-10. The prefix “D-\" stands for a robust aggregation method paired with DETOX.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which robust aggregation method converges fastest to the highest accuracy?",
    "answer": "D-Multi-Krum",
    "rationale": "The top plot shows the convergence of various robust aggregation methods. D-Multi-Krum is the method that reaches the highest accuracy in the fewest iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.12205v2",
    "pdf_url": null
  },
  {
    "instance_id": "4912cca5c2e341f89478d63deb48a75f",
    "figure_id": "2211.04256v1-Figure5-1",
    "image_file": "2211.04256v1-Figure5-1.png",
    "caption": " Additional results for our KD analysis (varying hidden size) on MNLI (without initialization of student layers, 4 hidden layers). We show the MNLI distillation results for SEAT tests 9 and 10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which hidden size results in the highest effect size for the SEAT Age (10) test?",
    "answer": "384",
    "rationale": "The plot shows the effect size for different hidden sizes for the SEAT Age (10) test. The highest point on the line for SEAT Age (10) is at a hidden size of 384.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.04256v1",
    "pdf_url": null
  },
  {
    "instance_id": "24af579cb3684b21bfa6e00577f45ed1",
    "figure_id": "2210.04457v1-Figure7-1",
    "image_file": "2210.04457v1-Figure7-1.png",
    "caption": " The distribution of prompt token pieces’ importance scores on WSC task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which importance score range has the highest frequency of prompt token pieces?",
    "answer": "0.4-0.5",
    "rationale": "The bar chart shows that the frequency of prompt token pieces is highest for the importance score range of 0.4-0.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.04457v1",
    "pdf_url": null
  },
  {
    "instance_id": "f5f563dc7f9942458a0cdda72457cbe5",
    "figure_id": "1904.02651v1-Figure1-1",
    "image_file": "1904.02651v1-Figure1-1.png",
    "caption": " Example of RC-MCQ from RACE dataset",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How did the people who didn't jump out of the window get out of the building?",
    "answer": "They were taken out by the firefighters.",
    "rationale": "The passage states that \"firefighters arrived at last. They fought the fire bravely. Water pipes were used and a ladder was put near the second-floor window. Then the people inside were taken out by the firefighters.\" This indicates that the firefighters were able to rescue the people who were trapped on the second floor.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.02651v1",
    "pdf_url": null
  },
  {
    "instance_id": "2428bdc88a6847fdb46824dcb232d0ec",
    "figure_id": "2306.06138v1-Figure6-1",
    "image_file": "2306.06138v1-Figure6-1.png",
    "caption": " Training loss curves under different neural sessions. (A) Neural Session: M-1. (B) Neural Session: M-3. (C) Neural Session: C-2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which neural session did the cooperative training method achieve the lowest loss?",
    "answer": "Neural Session: C-2.",
    "rationale": "The cooperative training method is represented by the orange line in each plot. In plot C, the orange line is lower than the blue line for most of the epochs, indicating that the cooperative training method achieved a lower loss than the sequential training method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.06138v1",
    "pdf_url": null
  },
  {
    "instance_id": "bfae52e337e540cb97a1e47cddff338f",
    "figure_id": "1909.04391v2-Figure1-1",
    "image_file": "1909.04391v2-Figure1-1.png",
    "caption": " Qualitative comparison against other methods. Our method can reconstruct fine lines with realistic textures.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most realistic textures in the image?",
    "answer": "The proposed method.",
    "rationale": "The figure shows the results of different methods for reconstructing an image. The proposed method produces the most realistic textures, as can be seen in the details of the trees and the building.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.04391v2",
    "pdf_url": null
  },
  {
    "instance_id": "c964e0aaeb06418ab14c4b957296b2bd",
    "figure_id": "2005.03356v2-Figure5-1",
    "image_file": "2005.03356v2-Figure5-1.png",
    "caption": " An example of correct prediction case to answer the question in Difficulty 4. To see the effectiveness of multi-level representation, we present the results of Our(Full) and Our−High in parallel. Scores of visual inputs are colored in orange and scores of scripts are colored in green. We colored final scores in blue. Prediction of Our(Full) is indicated by green checkmark which is ground truth answer, and prediction of Our−High is indicated by red crossmark.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Why does Taejin call Haeyoungl on the phone?",
    "answer": "Because Taejin wants to talk to Haeyoungl.",
    "rationale": "The figure shows the prediction of a model to answer the question \"Why does Taejin call Haeyoungl on the phone?\". The model predicted the correct answer, which is \"Because Taejin wants to talk to Haeyoungl\". This prediction is based on the visual inputs and scripts. The visual inputs show that Taejin is eating and that he is making a phone call. The scripts show that Taejin is asking about Haeyoungl and that he is angry at her.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.03356v2",
    "pdf_url": null
  },
  {
    "instance_id": "6c65598f26b64b7d885cb817063d7fb1",
    "figure_id": "2102.10488v1-Figure3-1",
    "image_file": "2102.10488v1-Figure3-1.png",
    "caption": " BER vs SNR for MATLAB and SDR",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which modulation scheme performs better at a BER of 10^-4?",
    "answer": "D-BPSK (MATLAB)",
    "rationale": "The figure shows that the BER for D-BPSK (MATLAB) is lower than the BER for D-QPSK (MATLAB) at a BER of 10^-4. This means that D-BPSK (MATLAB) performs better at this BER.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.10488v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee579b2bbb634d65ba2efafbdbdcc1c5",
    "figure_id": "2111.01732v1-Figure2-1",
    "image_file": "2111.01732v1-Figure2-1.png",
    "caption": " (a) Log wall-clock time, including any startup costs, across 7 synthetic spatio-temporal datasets with an increasing number of time steps (average across 5 runs). (b) Negative ELBO during training for the small-scale NYC-CRIME dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest for the NYC-CRIME dataset?",
    "answer": "ST-SVGP - Z Trained.",
    "rationale": "The negative ELBO is a measure of how well the model is fitting the data. The lower the negative ELBO, the better the fit. In Figure (b), we can see that the ST-SVGP - Z Trained method has the lowest negative ELBO, which means that it is the best fit for the NYC-CRIME dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.01732v1",
    "pdf_url": null
  },
  {
    "instance_id": "089aeac2ccf946a7b86d7358d597124c",
    "figure_id": "2211.16663v1-Figure5-1",
    "image_file": "2211.16663v1-Figure5-1.png",
    "caption": " Histogram of human and maximum low-level and high-level feature accuracies of various vision models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which vision model has the highest percentage of images with low-level max accuracy greater than 0.8?",
    "answer": "ViT",
    "rationale": "The figure shows that the ViT model has the highest percentage of images with low-level max accuracy greater than 0.8. This can be seen by looking at the green bars in the ViT subplot, which are higher than the green bars in the other subplots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.16663v1",
    "pdf_url": null
  },
  {
    "instance_id": "2b6a92ae23e8440d93a16badbab52219",
    "figure_id": "2105.11210v1-Figure4-1",
    "image_file": "2105.11210v1-Figure4-1.png",
    "caption": " Examples of the output of LayoutLM and StructuralLM on the FUNSD dataset. The division of | means that the two phrases are independent labels.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which region has the most REPs?",
    "answer": "Seattle South",
    "rationale": "The figure shows that Seattle South has 7 REPs, which is the highest number of REPs for any region.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.11210v1",
    "pdf_url": null
  },
  {
    "instance_id": "9d17a90577fc4b7f827dddcbdd7df614",
    "figure_id": "2004.06409v1-Figure3-1",
    "image_file": "2004.06409v1-Figure3-1.png",
    "caption": " Average PSNR results of each method for different extreme sampling rates (from 1% to 8% of pixels).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best for all sampling rates?",
    "answer": "ADEFAN",
    "rationale": "The figure shows that ADEFAN consistently achieves the highest average PSNR for all sampling rates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.06409v1",
    "pdf_url": null
  },
  {
    "instance_id": "1b49f5bcca164624bfdd31adddf22a8d",
    "figure_id": "2110.14432v5-Figure18-1",
    "image_file": "2110.14432v5-Figure18-1.png",
    "caption": " Omniscient parameterized teaching for MLP Learners on multi-class classification on MNIST digits.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better, SGD or LAST?",
    "answer": "LAST performs better than SGD.",
    "rationale": "The figure shows that the distance between the weights w and w* decreases faster for LAST than for SGD. This means that LAST is able to learn the optimal weights more quickly than SGD.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14432v5",
    "pdf_url": null
  },
  {
    "instance_id": "fa04bcb3aea549b3b7b73da5c8954e88",
    "figure_id": "2211.11222v1-Figure4-1",
    "image_file": "2211.11222v1-Figure4-1.png",
    "caption": " MOS of naturalness in which input fundamental frequencies were shifted up 12 semitones.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which speech synthesis method has the lowest MOS of naturalness?",
    "answer": "PN-sg",
    "rationale": "The bar chart shows the MOS of naturalness for different speech synthesis methods. The PN-sg method has the shortest bar, indicating that it has the lowest MOS of naturalness.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11222v1",
    "pdf_url": null
  },
  {
    "instance_id": "45ab0bc9d6bd4cd7ac846cdf15af4bd0",
    "figure_id": "1911.06949v1-Figure11-1",
    "image_file": "1911.06949v1-Figure11-1.png",
    "caption": " Comparison of ADSP with baselines running a large model",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges the fastest?",
    "answer": "ADACOMM",
    "rationale": "The figure shows that ADACOMM reaches the lowest global loss in the shortest amount of time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.06949v1",
    "pdf_url": null
  },
  {
    "instance_id": "d76b6949609a49c79bd525281644eb2e",
    "figure_id": "2302.04116v1-Figure5-1",
    "image_file": "2302.04116v1-Figure5-1.png",
    "caption": " Statistics of the top ten entities by count in CoNLL2003.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which entity is mentioned the most frequently in the CoNLL2003 dataset?",
    "answer": "USDA",
    "rationale": "The bar chart shows the count of the top ten entities in the CoNLL2003 dataset. The USDA bar is the tallest, indicating that it is the most frequently mentioned entity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.04116v1",
    "pdf_url": null
  },
  {
    "instance_id": "42b1b82f2d874c9d85370566fd0b3203",
    "figure_id": "2303.05812v1-Figure2-1",
    "image_file": "2303.05812v1-Figure2-1.png",
    "caption": " Performance with respect to the amount of labeled data",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best for the Clothing category when there are 2 labeled data points?",
    "answer": "Popularity",
    "rationale": "The plot for Clothing shows that the Popularity line is the highest at 2 labeled data points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.05812v1",
    "pdf_url": null
  },
  {
    "instance_id": "da919681ed3844c8a218ccc33ec224dc",
    "figure_id": "2102.04892v1-Figure1-1",
    "image_file": "2102.04892v1-Figure1-1.png",
    "caption": " Unwrapped phase and linear regression for f = 30 and m = 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following activities resulted in the largest unwrapped phase?",
    "answer": "Waving balloon",
    "rationale": "The unwrapped phase for the waving balloon is the highest among all the activities shown in the figure. This can be seen by comparing the different lines in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.04892v1",
    "pdf_url": null
  },
  {
    "instance_id": "17e6d3a733c84a1eadb0ae6a8a33ae69",
    "figure_id": "2211.11761v3-Figure6-1",
    "image_file": "2211.11761v3-Figure6-1.png",
    "caption": " Classification accuracy with different interaction layers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the most significant increase in accuracy with increasing interaction layers?",
    "answer": "Squirrel.",
    "rationale": "The Squirrel dataset shows a significant increase in accuracy from 0 to 3 interaction layers, while the other datasets show either a slight increase or decrease in accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11761v3",
    "pdf_url": null
  },
  {
    "instance_id": "3a899c0823b94b98b2524479d6ab73d0",
    "figure_id": "2210.12910v1-Figure3-1",
    "image_file": "2210.12910v1-Figure3-1.png",
    "caption": " Example visualizations with MI values from IT and Medical. The more intense the red, the more higher MI value.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which domain has a higher MI value for the statement \"One vial contains 150 mg of omalizumab\"?",
    "answer": "Medical",
    "rationale": "The figure shows that the MI value for the statement \"One vial contains 150 mg of omalizumab\" in the Medical domain is more intense red than the MI value for the statement \"Microsoft Office ; importing password protected files\" in the IT domain. The more intense the red, the higher the MI value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12910v1",
    "pdf_url": null
  },
  {
    "instance_id": "4dbe144128d8491caaeecc7032654d8a",
    "figure_id": "2202.11194v1-Figure4-1",
    "image_file": "2202.11194v1-Figure4-1.png",
    "caption": " Varying noise ratio 𝑝 results in model performances.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the best performance at a noise ratio of 10%?",
    "answer": "CMUdict",
    "rationale": "The figure shows the WER for different datasets at different noise ratios. At a noise ratio of 10%, the CMUdict dataset has the lowest WER, indicating the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.11194v1",
    "pdf_url": null
  },
  {
    "instance_id": "cab2d77b19dd498ba8426d0781e9a3d7",
    "figure_id": "2211.08702v1-Figure9-1",
    "image_file": "2211.08702v1-Figure9-1.png",
    "caption": " Inversion examples generated by our method and existing methods (lamp). Our method reproduces the target more faithfully.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods generated the most faithful reproduction of the target lamp?",
    "answer": "Our method.",
    "rationale": "The figure shows that our method produces a lamp that is closest in shape and size to the target lamp. The other methods produce lamps that are either too small or too large, and they do not have the same level of detail as the lamp produced by our method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.08702v1",
    "pdf_url": null
  },
  {
    "instance_id": "4bfd1683a05844ff90df1d153b8497c5",
    "figure_id": "1904.01318v1-Figure30-1",
    "image_file": "1904.01318v1-Figure30-1.png",
    "caption": " Freeway. Target function: T+.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the number of vehicles change over time?",
    "answer": "The number of vehicles increases over time.",
    "rationale": "The figure shows a freeway with vehicles moving from left to right. In the first frame, there are only a few vehicles on the road. In the second frame, there are more vehicles. In the third frame, there are even more vehicles. In the fourth frame, the road is almost full of vehicles. This shows that the number of vehicles is increasing over time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.01318v1",
    "pdf_url": null
  },
  {
    "instance_id": "bc8a6ba3ab3849f897d9b77296ff908e",
    "figure_id": "2305.19753v2-Figure7-1",
    "image_file": "2305.19753v2-Figure7-1.png",
    "caption": " The tunnel degrades the out-of-distribution performance correlated with the representations’ numerical rank. The accuracy of linear probes (blue) was trained on the out-of-distribution data subset of 10 classes from CIFAR-100. The backbone was trained on CIFAR-10. The shaded area depicts the tunnel, and the red dashed line depicts the numerical rank of representations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest numerical rank for its representations?",
    "answer": "ResNet-34",
    "rationale": "The red dashed line in each plot represents the numerical rank of the representations. We can see that the ResNet-34 plot has the highest peak for the red dashed line, indicating that it has the highest numerical rank.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19753v2",
    "pdf_url": null
  },
  {
    "instance_id": "f944d14639c64350a5634e14e974dd26",
    "figure_id": "1904.01870v1-Figure5-1",
    "image_file": "1904.01870v1-Figure5-1.png",
    "caption": " Qualitative comparison of our results against methods proposed by Eigen et al. [9] and Zheng et al. [59] on KITTI. Ground truth has been interpolated for visualization. To facilitate comparison, we mask out the top regions, where ground truth depth is not available. Our approach preserves more details and yields high-quality depth maps.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods in the figure is most accurate at predicting depth?",
    "answer": "GASDA",
    "rationale": "The figure shows the input images, the ground truth depth maps, and the depth maps predicted by three different methods. The GASDA method produces depth maps that are most similar to the ground truth depth maps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.01870v1",
    "pdf_url": null
  },
  {
    "instance_id": "65c88d97e0144b6591ca8277dc2e9135",
    "figure_id": "2204.09840v1-Figure2-1",
    "image_file": "2204.09840v1-Figure2-1.png",
    "caption": " Overall accuracy comparison for the ablation choices defined in Representation Ablation, Section 3.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which ablation choice leads to the highest overall accuracy?",
    "answer": "Proposal.",
    "rationale": "The plot shows the overall accuracy for each ablation choice. The Proposal line is the highest of all the lines, indicating that it has the highest overall accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.09840v1",
    "pdf_url": null
  },
  {
    "instance_id": "fd54e33e640d4460b67f010dbb8c6787",
    "figure_id": "2204.08807v1-Figure4-1",
    "image_file": "2204.08807v1-Figure4-1.png",
    "caption": " Effect of ablation study.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which ablation study variant of MCCLK performed the best for the Movie domain?",
    "answer": "MCCLK without G.",
    "rationale": "The bar chart for the Movie domain shows that MCCLK without G has the highest AUC and F1 scores compared to the other two variants.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.08807v1",
    "pdf_url": null
  },
  {
    "instance_id": "5e4054ce25fe4613a03507808e6cf8e1",
    "figure_id": "2012.08508v3-Figure5-1",
    "image_file": "2012.08508v3-Figure5-1.png",
    "caption": " The video for an example counterfactual question that can be answered as if it were a descriptive question. The question is: if the brown rubber sphere is removed, what will not happen?",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "If the brown rubber sphere is removed, what will not happen?",
    "answer": "The green block will not fall over.",
    "rationale": "The image shows that the brown sphere is on a collision course with the green block. If the sphere is removed, it will not collide with the block and the block will not fall over.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.08508v3",
    "pdf_url": null
  },
  {
    "instance_id": "bc2ccbc8c44349668dd1ca0f7b6b7188",
    "figure_id": "2108.13264v4-FigureA.13-1",
    "image_file": "2108.13264v4-FigureA.13-1.png",
    "caption": "Figure A.13: Runs can be different from using fixed random seeds. We find that correlation between two sets of 100 runs of DER on Atari 100k using the same set of random seeds, that is, using a fixed random seed per run for Python, NumPy and JAX, is quite small. Small values of correlation coefficient highlight that fixing seeds does not ensure deterministic results due to non-determinism in GPUs. Similarly, setting random seed in PyTorch ensures reproducibility only on the same hardware.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which game has the highest correlation in scores between two independent sets of 100 runs/game with the same seeds?",
    "answer": "Freeway",
    "rationale": "The figure shows the Pearson correlation coefficient for each game, which measures the linear relationship between two variables. The game with the highest bar in the figure is Freeway, which indicates that it has the highest correlation coefficient.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.13264v4",
    "pdf_url": null
  },
  {
    "instance_id": "f8748437b27f4d51869fff8b6ec88e90",
    "figure_id": "1908.06917v1-Figure6-1",
    "image_file": "1908.06917v1-Figure6-1.png",
    "caption": " Alternative entity example that demonstrates a missing answer when only a single correct entity URI is considered (dbr:Rome and not dbr:Pantheon,Rome). LC-QuAD question #261: “Give me a count of royalties buried in Rome?” (correct answer: dbr:Augustus; missing answer: dbr:Margherita_of_Savoy). QAmpwas able to retrieve this false negative sample due to the string matching function and retaining a list of alternative URIs per entity mention.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two royalty are shown in the image to be buried in Rome?",
    "answer": "Augustus and Margherita of Savoy",
    "rationale": "The image shows two entities linked to the \"Resting place\" entity with the label \"Rome\": \"Augustus\" and \"Margherita of Savoy\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.06917v1",
    "pdf_url": null
  },
  {
    "instance_id": "f1825053fe794e59b89989fd8f4f713a",
    "figure_id": "1912.05270v3-Figure2-1",
    "image_file": "1912.05270v3-Figure2-1.png",
    "caption": " Application of mining in conditional setting (on BigGAN [4]). We apply an additional miner network to estimate the class embedding. DT : target data, E: class embedding, l: label.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the role of the miner network in the conditional MineGAN?",
    "answer": "The miner network estimates the class embedding.",
    "rationale": "The figure shows that the miner network (M) takes the label (l) as input and outputs the class embedding (E). This embedding is then used by the generator (G) to generate images that are conditioned on the label.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.05270v3",
    "pdf_url": null
  },
  {
    "instance_id": "cddae8b9ddab48a6b33ba375162c03d1",
    "figure_id": "2306.12306v3-Figure8-1",
    "image_file": "2306.12306v3-Figure8-1.png",
    "caption": " CAMELYON17-WILDS: Average accuracy vs. sECE on the o.o.d. test split. The models that use no running statistics (static BN) are significantly more accurate and better calibrated, while exhibiting a smaller variance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is the most accurate and best calibrated on the o.o.d. test split?",
    "answer": "MAP (static BN)",
    "rationale": "The figure shows that the MAP (static BN) model has the highest average accuracy and the lowest sECE, which indicates that it is the most accurate and best calibrated model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.12306v3",
    "pdf_url": null
  },
  {
    "instance_id": "b1f2f800d99647c19e566a96a00562ae",
    "figure_id": "2106.02940v2-Figure6-1",
    "image_file": "2106.02940v2-Figure6-1.png",
    "caption": " Final performance for different OWL policy selection strategies and Exp Replay.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which policy selection strategy performed the best on average across all tasks?",
    "answer": "SC + DK",
    "rationale": "The average rank of each policy selection strategy is shown in the rightmost panel of the figure. SC + DK has the lowest average rank, which indicates that it performed the best on average.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02940v2",
    "pdf_url": null
  },
  {
    "instance_id": "6ea7ed85a78341bb8f1db87e6cc14c0f",
    "figure_id": "2301.09604v2-Figure9-1",
    "image_file": "2301.09604v2-Figure9-1.png",
    "caption": " Additional results for CIFAR-10 dataset. Mean and standard deviation from experiments with 5 different random seeds. The shaded areas show the standard deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization algorithm has the highest training accuracy after 500 training rounds?",
    "answer": "SCAFFOLD",
    "rationale": "The figure shows the training accuracy of different optimization algorithms over 500 training rounds. The green line, which represents SCAFFOLD, is the highest at the end of the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.09604v2",
    "pdf_url": null
  },
  {
    "instance_id": "7074fc39112544a6bfca0e0e5c25cf8a",
    "figure_id": "2203.08913v1-Figure7-1",
    "image_file": "2203.08913v1-Figure7-1.png",
    "caption": " Difference in loss for each token in a randomly chosen paper, using the same model once with a memory size of 8K and once with 32K. Higher numbers mean the longer memory helped in comparison to the shorter memory. This paper is 22K tokens long.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "At which token does the difference in loss between the 8K and 32K models become most significant?",
    "answer": "Around the 10,000th token.",
    "rationale": "The figure shows that the difference in loss between the two models is relatively small for the first 10,000 tokens, but then it starts to increase rapidly. This suggests that the longer memory of the 32K model is particularly helpful for tokens in the second half of the paper.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.08913v1",
    "pdf_url": null
  },
  {
    "instance_id": "ef2cf190d1b04900af85eedc53901b63",
    "figure_id": "2309.17218v1-Figure5-1",
    "image_file": "2309.17218v1-Figure5-1.png",
    "caption": " Comparison of reconstructed results with state-of-the-art methods [31, 9] on DTU evalution set [1].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most accurate reconstruction of the scene?",
    "answer": "Ground Truth",
    "rationale": "The Ground Truth image shows the actual scene that the other methods are trying to reconstruct. The other methods all have some errors in their reconstructions, such as missing or misplaced objects.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.17218v1",
    "pdf_url": null
  },
  {
    "instance_id": "7aee25bfc8cb42bd9ce70bcc669000e2",
    "figure_id": "1810.13400v3-Figure4-1",
    "image_file": "1810.13400v3-Figure4-1.png",
    "caption": " Learning results on the (simple) pendulum and cartpole environments. We select the best validation loss observed during the training run and report the best test loss.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the pendulum environment when trained with 100 trajectories?",
    "answer": "mpc.cost.dx",
    "rationale": "The figure shows the imitation loss for different methods on the pendulum environment. The mpc.cost.dx method has the lowest imitation loss when trained with 100 trajectories.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.13400v3",
    "pdf_url": null
  },
  {
    "instance_id": "caa9fd188a984615abdc59e9f92de2b2",
    "figure_id": "2301.08834v2-Figure3-1",
    "image_file": "2301.08834v2-Figure3-1.png",
    "caption": " Continuous learning on new patients",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which algorithm performs best when the number of training patients is low?",
    "answer": " Base",
    "rationale": " The Base algorithm has the highest accuracy, Cohen's kappa, and average F1 score when the number of training patients is low. This can be seen in the figure, where the Base line is consistently above the other lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.08834v2",
    "pdf_url": null
  },
  {
    "instance_id": "9a46a0d952f2456584a5c520c5314fa9",
    "figure_id": "1907.08549v2-Figure2-1",
    "image_file": "1907.08549v2-Figure2-1.png",
    "caption": " Sine wave generation. a) Schematic showing conversion of static input specifying a command frequency, ω, for the sine wave output sin(2πωt). b) PCA plots showing trajectories using many evenly divided command frequencies delivered one at a time (blue: smallest ω, yellow: largest ω). c) MDS plots based on SVCCA network-network distances, layout as in Fig. 1d. d) Left, fixed points (colored circles, with color indicating ω, one fixed point per command frequency) showing a single fixed point in the middle of each oscillatory trajectory. Right, the complex eigenvalues of all the linearized systems, one per fixed point, overlayed on top of each other, with primary oscillatory eigenvalues colored as in panel b. e) MDS network-network distances based on fixed point topology, assessing systematic differences in the topology of the input-dependent fixed points (layout as in Fig. 1d). f) Summary analysis showing the frequency of the oscillatory mode in the linearized system vs. command frequency for different architectures (left) and activations (right). Solid line and shaded patch show the mean ± standard error over networks trained with different random seeds. Small, though systematic, variations exist in the frequency of each oscillatory mode.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activation function appears to have the most linear relationship between the input frequency and the frequency of the oscillatory mode in the linearized system?",
    "answer": "ReLU",
    "rationale": "This can be seen in panel (f) of the figure, which shows that the ReLU activation function has the most linear relationship between the input frequency and the frequency of the oscillatory mode in the linearized system.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.08549v2",
    "pdf_url": null
  },
  {
    "instance_id": "397e54378cf1414a8978b4db3140dbaa",
    "figure_id": "2107.09562v2-Figure5-1",
    "image_file": "2107.09562v2-Figure5-1.png",
    "caption": " Generalization performance for different backbone architectures for varying distribution shifts on CUB200-2011. We show absolute Recall@1 performances averaged over 5 runs for each train-test split. Other datasets show similar results and are provided in the supplementary.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which backbone architecture has the highest Recall@1 performance on the CUB200-2011 dataset?",
    "answer": " RN101",
    "rationale": " The figure shows the Recall@1 performance for different backbone architectures on the CUB200-2011 dataset. The RN101 architecture has the highest Recall@1 performance for all four distribution shifts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.09562v2",
    "pdf_url": null
  },
  {
    "instance_id": "dade999f119c42a68feb7d0b3ebbf9bf",
    "figure_id": "2110.10082v2-Figure3-1",
    "image_file": "2110.10082v2-Figure3-1.png",
    "caption": " The performance of our approach with different combinations of R1 and R2 (the number of location and sociability factors respectively). The total number of factors R is fixed to 11 (R = R1 +R2).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which combination of R1 and R2 resulted in the best performance in terms of MSE for the ALog method?",
    "answer": "R1 = 1 and R2 = 10",
    "rationale": "The plot in Figure (a) shows that the MSE is lowest when R1 = 1 and R2 = 10.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.10082v2",
    "pdf_url": null
  },
  {
    "instance_id": "7a8996fe5d35488788f935162c9521ff",
    "figure_id": "2010.02562v1-Figure4-1",
    "image_file": "2010.02562v1-Figure4-1.png",
    "caption": " Classification results, with methods grouped according to the type of cross-lingual resources required. For some methods, average performance (rightmost column) is in parentheses because it is computed on a subset of languages. Across all datasets, CLTS outperforms other methods that require similar types of cross-lingual resources; in many cases (red) CLTS outperforms even more expensive state-of-the-art approaches.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best on the MLDDoc dataset?",
    "answer": "ST-MultiBERT",
    "rationale": "The table in Figure (a) shows the accuracy results on the MLDDoc dataset. The ST-MultiBERT method achieved the highest accuracy of 90.0%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02562v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee60fee4b3934bda81d703c74e0d4fbc",
    "figure_id": "2305.15805v2-Figure15-1",
    "image_file": "2305.15805v2-Figure15-1.png",
    "caption": " Attention weight for different layers for the context switch example in Table 2. Color here indicates that the token can be attended to and does not correspond to the actual attention weight. Notice the casual masking and the three emerging dense triangular sub-matrices, especially in layers 7, 8, 9 and 10.",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layers of the model show the most evidence of casual masking?",
    "answer": "Layers 7, 8, 9, and 10.",
    "rationale": "Casual masking is a technique used in transformer models to prevent the model from attending to tokens that are ahead of the current token in the sequence. This is typically done by masking out the attention weights for these tokens. In the figure, we can see that layers 7, 8, 9, and 10 have a clear triangular pattern in the attention weights, which indicates that the model is not attending to tokens that are ahead of the current token.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.15805v2",
    "pdf_url": null
  },
  {
    "instance_id": "271426d4047a43f0aae291df02cca8de",
    "figure_id": "2111.03189v2-Figure9-1",
    "image_file": "2111.03189v2-Figure9-1.png",
    "caption": "Figure 9",
    "figure_type": "Plot and photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object is the easiest to pick up?",
    "answer": "The coffee cup.",
    "rationale": "The graph shows that the probability of success for picking up the coffee cup is consistently higher than for any other object.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.03189v2",
    "pdf_url": null
  },
  {
    "instance_id": "32ceca1cbe1c43adaa3be37cff00f563",
    "figure_id": "2302.06061v1-Figure3-1",
    "image_file": "2302.06061v1-Figure3-1.png",
    "caption": " The ratio of total reward to budget, with ρ = 0.6.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three algorithms is most efficient in terms of reward per unit of budget?",
    "answer": "GCRM",
    "rationale": "The figure shows that GCRM has the highest ratio of total reward to budget for all values of n.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.06061v1",
    "pdf_url": null
  },
  {
    "instance_id": "b3e6b652f48248fa9edfb9f07ba936b4",
    "figure_id": "2304.00445v1-Figure2-1",
    "image_file": "2304.00445v1-Figure2-1.png",
    "caption": " Average accuracy comparisons of different SOTA methods on various SNR on the RML2016.10a (a), RML2016.10b (b).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on average across all SNR levels on the RML2016.10a dataset?",
    "answer": "AMC-Net",
    "rationale": "The figure shows the overall accuracy of different methods on the RML2016.10a dataset for various SNR levels. The AMC-Net line is consistently above the other lines, indicating that it achieves the highest accuracy on average across all SNR levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.00445v1",
    "pdf_url": null
  },
  {
    "instance_id": "bcbe635d10184921b415ec4d0ac3a957",
    "figure_id": "1809.04040v3-Figure1-1",
    "image_file": "1809.04040v3-Figure1-1.png",
    "caption": " Convergence in HUNL Subgame1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the fastest convergence rate in the HUNL Subgame 1?",
    "answer": "LCFR+",
    "rationale": "The figure shows the exploitability of different algorithms as a function of the number of iterations. The exploitability of LCFR+ decreases the fastest, indicating that it has the fastest convergence rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.04040v3",
    "pdf_url": null
  },
  {
    "instance_id": "9596a8537e384e0996700809fc0d6709",
    "figure_id": "2106.01199v1-Figure3-1",
    "image_file": "2106.01199v1-Figure3-1.png",
    "caption": " The CDF of model’s predicted energy errors. We see that for 99% of the cases, the error is under 16%",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of the model's predictions have an error of less than 10%?",
    "answer": "Approximately 60%.",
    "rationale": "The cumulative probability at 10% error on the x-axis is approximately 0.6 on the y-axis. This means that 60% of the predictions have an error of less than 10%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01199v1",
    "pdf_url": null
  },
  {
    "instance_id": "49969ddfc7444d61acdbef47f25bf18d",
    "figure_id": "2108.00981v3-Figure5-1",
    "image_file": "2108.00981v3-Figure5-1.png",
    "caption": " Performance of using different GAN models for imputation in the cold start experiment. Markers denote the mean NRMSE averaged over datasets and error bars the 68% confidence interval over ten runs. Note that this figure only shows the error of the cold start time series. Overall, TIMEGAN and PSA-GAN improve the NRMSE over DeepAR in this setup, while PSA-GAN is the best method when 30% of time series are cold starts.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GAN model performs the best when 30% of time series are cold starts?",
    "answer": "PSA-GAN",
    "rationale": "The figure shows that PSA-GAN has the lowest mean NRMSE when 30% of time series are cold starts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.00981v3",
    "pdf_url": null
  },
  {
    "instance_id": "62983dc733da450daef69c12408af988",
    "figure_id": "2006.04635v4-Figure11-1",
    "image_file": "2006.04635v4-Figure11-1.png",
    "caption": " Descriptive behavioural statistics of the different networks, as well as human play in the datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network has the highest cross-support move success rate?",
    "answer": "A2C.",
    "rationale": "The cross-support move success rate is shown on the y-axis of the top right plot. A2C is the point furthest to the top right of the plot, indicating that it has the highest cross-support move success rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04635v4",
    "pdf_url": null
  },
  {
    "instance_id": "a5f34ba33fdb44d685e92eeea40b78f7",
    "figure_id": "2103.16561v2-Figure2-1",
    "image_file": "2103.16561v2-Figure2-1.png",
    "caption": " Performance gap between masking and replacing object tokens from instructions. If ∆ > 0, then replacing object tokens leads to worse navigation performance, which suggests that the agent has a better understanding of the object tokens.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the plots in Figure 2, which task shows the largest performance gap between masking and replacing object tokens from instructions?",
    "answer": "Touchdown.",
    "rationale": "Figure 2 shows the performance gap between masking and replacing object tokens from instructions for different tasks. For the Touchdown task, the difference between the curves of all three methods (RCONCAT, VLN-Transformer, and ARC) is the largest. This suggests that the agent has a better understanding of the object tokens for the Touchdown task compared to the other tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.16561v2",
    "pdf_url": null
  },
  {
    "instance_id": "220965ce70324efdbd3bcb750a4348a1",
    "figure_id": "2206.01923v1-Figure3-1",
    "image_file": "2206.01923v1-Figure3-1.png",
    "caption": " Four qualitative results from visual question answering.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four images shows a person in need of help?",
    "answer": "The image of the person sleeping on the bench.",
    "rationale": "The image shows a person sleeping on a bench with a blanket. This suggests that the person is homeless and may be in need of help.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.01923v1",
    "pdf_url": null
  },
  {
    "instance_id": "8ec62dfa9d5547bcb8cae73b157606ce",
    "figure_id": "2202.03734v2-Figure11-1",
    "image_file": "2202.03734v2-Figure11-1.png",
    "caption": " Mean base rate for the privileged and unprivileged groups across different number of interventions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which group has a higher mean base rate across all intervention levels?",
    "answer": "The privileged group.",
    "rationale": "The figure shows that the blue bars, representing the privileged group, are consistently higher than the orange bars, representing the unprivileged group. This indicates that the privileged group has a higher mean base rate across all intervention levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.03734v2",
    "pdf_url": null
  },
  {
    "instance_id": "e8b335a642484aa69607786c819101bd",
    "figure_id": "1901.03396v1-Figure4-1",
    "image_file": "1901.03396v1-Figure4-1.png",
    "caption": " Histograms or recovery errors on train D and validation T datasets from CelebA-HQ showing that overfitting is not happening for PGGAN and MECSH generators on the training dataset, but is for GLO-N and AEGAN-N when training for a small dataset N < 8192.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which generators show signs of overfitting on the training dataset when trained on a small dataset (N < 8192)?",
    "answer": " GLO-N and AEGAN-N.",
    "rationale": " The histograms for GLO-N and AEGAN-N show a clear difference in the distribution of recovery errors between the train and test datasets when N < 8192. This suggests that the generators are overfitting to the training data, meaning they are learning the specific features of the training data too well and are not able to generalize well to unseen data. In contrast, the histograms for PGGAN and MECSH show a more similar distribution of recovery errors between the train and test datasets, suggesting that these generators are not overfitting to the training data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.03396v1",
    "pdf_url": null
  },
  {
    "instance_id": "c662331959bc4f91937e9c9ca2f166f6",
    "figure_id": "1909.00475v1-Figure5-1",
    "image_file": "1909.00475v1-Figure5-1.png",
    "caption": " FacePlace PSNR for all methods (vertical projection on top, horizontal on bottom, max signal PSNR (deprojection estimate) on left, mean projection PSNR on right) with varying sample size for 100 test projections. Our method yields higher maximum signal PSNR than all baselines. DET has a higher expected signal PSNR for one sample because it tends to return a blurry average over many signals. LMMSE has infinite projection PSNR because it captures the exact linear signal-projection relationship by construction.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest expected signal PSNR for one sample?",
    "answer": "DET",
    "rationale": "The caption states that \"DET has a higher expected signal PSNR for one sample because it tends to return a blurry average over many signals.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.00475v1",
    "pdf_url": null
  },
  {
    "instance_id": "a6ff363c7b2141219faac76e96e76794",
    "figure_id": "2202.10842v3-Figure5-1",
    "image_file": "2202.10842v3-Figure5-1.png",
    "caption": " Performance of eight methods on the MTG setting by varying the data density and exposure strategy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampling method consistently achieves the highest average treatment effect (ATE) across all data densities?",
    "answer": "EAR",
    "rationale": "The left-hand column of the figure shows the ATE for each method. The EAR method is represented by the blue circle line and consistently achieves the highest ATE for all three sampling methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.10842v3",
    "pdf_url": null
  },
  {
    "instance_id": "3f6c79ff2d5747c6ae23996b16baad2e",
    "figure_id": "2309.14183v3-Figure4-1",
    "image_file": "2309.14183v3-Figure4-1.png",
    "caption": " Clip-retrieval process of Species16-U from Species196-L. Displaying similarity scores in descending order, we show items No. 100, 500, 1000, and 5000.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image retrieved from LAION5B is the most similar to the sample image of Species196-L?",
    "answer": "Image No. 100.",
    "rationale": "The figure shows that the retrieved images are displayed in descending order of similarity score. This means that the images at the top of the list are the most similar to the sample image. Image No. 100 is the first image in the list, so it is the most similar to the sample image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.14183v3",
    "pdf_url": null
  },
  {
    "instance_id": "a6056e3d86f045c8bd72ebd5aac9b149",
    "figure_id": "2206.01311v2-Figure29-1",
    "image_file": "2206.01311v2-Figure29-1.png",
    "caption": " Average normalized accruals (averaged across 5 training seeds) for ExiD lane change environment. The X-axis and Y-axis are lateral velocity v in ms−1 and signed distance to the the center line of the target lane d in m respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method appears to have the highest probability of success in completing the lane change maneuver?",
    "answer": "ICL (β = 5)",
    "rationale": "The ICL (β = 5) method has the highest concentration of accruals in the top right corner of the plot, which indicates that the agent is more likely to successfully complete the lane change maneuver with a high lateral velocity and a positive signed distance to the center line of the target lane.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.01311v2",
    "pdf_url": null
  },
  {
    "instance_id": "00f359e7a532464e847d2ab1fcc4ddf9",
    "figure_id": "2005.01014v1-Figure5-1",
    "image_file": "2005.01014v1-Figure5-1.png",
    "caption": " Comparison results with the impact of density difference and noise. The left column shows the comparison results on 10 times density difference. The right column shows the comparison results under Gaussian noise with signal-to-noise ratio(SNR) 0.01.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods performs best in terms of RMSE when the initial rotation angle is 45 degrees and the density difference is 10?",
    "answer": "Ours.",
    "rationale": "The left plot in the figure shows the comparison results on 10 times density difference. When the initial rotation angle is 45 degrees, the purple line (Ours) has the lowest RMSE value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.01014v1",
    "pdf_url": null
  },
  {
    "instance_id": "d0b89c72d0c74b93bd587fd01cdc898a",
    "figure_id": "2006.06676v2-Figure8-1",
    "image_file": "2006.06676v2-Figure8-1.png",
    "caption": " (a) We report the mean and standard deviation for each comparison method, calculated over 3 training runs. (b) FID as a function of discriminator capacity, reported as median/min/max over 3 training runs. We scale the number of feature maps uniformly across all layers by a given factor (x-axis). The baseline configuration (no scaling) is indicated by the dashed vertical line.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best according to the FID metric?",
    "answer": "ADA.",
    "rationale": "The FID metric is shown in the right-hand side of the figure. Lower values of FID indicate better performance. ADA has the lowest FID values across all discriminator capacities.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06676v2",
    "pdf_url": null
  },
  {
    "instance_id": "3e57ec4d12234127b5660a579525a23b",
    "figure_id": "2308.15081v1-Figure13-1",
    "image_file": "2308.15081v1-Figure13-1.png",
    "caption": " The F1-score curves of the different components of KL-Teacher in T-HOneCls.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which component of KL-Teacher performs the best for Cotton in HongHu?",
    "answer": "EMA + c_w^2",
    "rationale": "The F1-score curves for Cotton in HongHu are shown in subfigures (a) and (d). We can see that the curve for EMA + c_w^2 is consistently higher than the curves for the other components, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.15081v1",
    "pdf_url": null
  },
  {
    "instance_id": "69172793b12b43f69e032a35f0f256fb",
    "figure_id": "1906.05948v4-Figure7-1",
    "image_file": "1906.05948v4-Figure7-1.png",
    "caption": " MNIST recall. A random sequence of images followed by a repeat (green), output the class of the next image (red).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the next image in the sequence?",
    "answer": "2",
    "rationale": "The green box shows the repeated image in the sequence, and the red box shows the next image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.05948v4",
    "pdf_url": null
  },
  {
    "instance_id": "fdba0113f5394701af9ef9f4a26dba54",
    "figure_id": "2105.07698v1-Figure1-1",
    "image_file": "2105.07698v1-Figure1-1.png",
    "caption": " Macro F1 scores when removing evidence from either the top or bottom of the evidence snippet ranking.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best when evidence is removed from the top of the ranking for the PolitiFact dataset?",
    "answer": "BERT",
    "rationale": "The figure shows that the BERT model has the highest F1 score when evidence is removed from the top of the ranking for the PolitiFact dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.07698v1",
    "pdf_url": null
  },
  {
    "instance_id": "dbf7d9f650d14a36b706a8e9dacd535f",
    "figure_id": "2210.08635v2-Figure1-1",
    "image_file": "2210.08635v2-Figure1-1.png",
    "caption": " Distribution of regional identities among sense entries found in the English Wiktionary. See Appendix A for the detailed experimental setup.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common regional identity for slang terms in the English Wiktionary?",
    "answer": "Shared",
    "rationale": "The figure shows that 55% of slang terms in the English Wiktionary have a shared regional identity. This is the largest percentage for any category of slang terms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.08635v2",
    "pdf_url": null
  },
  {
    "instance_id": "e8698934ec4c4d2bb7da5379bdff1694",
    "figure_id": "2006.08173v3-FigureA.22-1",
    "image_file": "2006.08173v3-FigureA.22-1.png",
    "caption": "Figure A.22: The number of elements of each of the two modes of the gradients for ResNet18 trained on Cifar10, on various layers. Notice that the y-axis is in log scale. Different layers vary in behaviour, but the dominant effect is the increase in the proportion of the left mode, indicating that a large portion of the values are mapped to zero by the consequent ReLU layer, because the left modes’ values originate from zeros values in the ReLU gradients. Up to 80% of values might be in the left mode. This has major consequences when trying to prune the bi-modal distributions to values lower than that using our algorithm. However this is less of a problem when pruning to high sparsity levels. The numbering of the layers here is from the deepest to the shallowest, according to the order of the gradients flow in the backward-pass.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the largest difference in the number of elements between the left and right modes?",
    "answer": "Layer #8.",
    "rationale": "The figure shows that the left mode has a significantly larger number of elements than the right mode for all layers. However, the difference is most pronounced for layer #8, where the left mode has almost 10 times more elements than the right mode.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.08173v3",
    "pdf_url": null
  },
  {
    "instance_id": "2b113a1f366d4d99888e38c4d30f1ba2",
    "figure_id": "2205.02724v1-Figure4-1",
    "image_file": "2205.02724v1-Figure4-1.png",
    "caption": " Distribution of polarity scores for adjectives and their negation bigrams. p-adj and n-adj refer to the positive and negative adjectives respectively. [-] refers to the negation operation (prepending the word “not”). Circles refer to outliers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is most likely to assign a positive polarity score to a positive adjective?",
    "answer": "MVM-L",
    "rationale": "The box plot for MVM-L in (c) shows that the median polarity score for positive adjectives (p-adj) is above 0, indicating that the model is more likely to assign a positive polarity score to positive adjectives.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.02724v1",
    "pdf_url": null
  },
  {
    "instance_id": "d0dabda3199845909ee7fc72655413f1",
    "figure_id": "2009.03300v3-Figure43-1",
    "image_file": "2009.03300v3-Figure43-1.png",
    "caption": " A High School Statistics example.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What would happen to Jonathan's percentile rank if everyone in the class received five additional points on their exams?",
    "answer": "Jonathan's percentile rank would remain the same.",
    "rationale": "Percentile rank is a measure of how well a student performs compared to others in the class. Adding the same number of points to everyone's score would not change the relative order of the scores, and therefore Jonathan's percentile rank would remain unchanged.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.03300v3",
    "pdf_url": null
  },
  {
    "instance_id": "c5f11b512b264ab4aae295e871574221",
    "figure_id": "2004.02546v3-Figure4-1",
    "image_file": "2004.02546v3-Figure4-1.png",
    "caption": " Illustration of the significance of the principal components as compared to random directions in the intermediate latent space W of StyleGAN2. Fixing and randomizing the early principal components shows a separation between pose and style (a, b). In contrast, fixing and randomizing randomly-chosen directions does not yield a similar meaningful decomposition (c, d).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which subfigure demonstrates that fixing and randomizing the early principal components leads to a separation between pose and style?",
    "answer": "Subfigures (a) and (b).",
    "rationale": "Subfigure (a) shows that fixing the first 8 PCA coordinates while randomizing the remaining 504 leads to changes in the cat's appearance (e.g., fur color, eye color) while the pose and camera angle remain relatively fixed. Subfigure (b) shows that randomizing the first 8 PCA coordinates while fixing the remaining 504 leads to changes in the cat's pose while the appearance remains relatively fixed. This suggests that the early principal components capture information about the cat's pose and style.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.02546v3",
    "pdf_url": null
  },
  {
    "instance_id": "b9b2cdf263a2437a84aff2db003ead12",
    "figure_id": "2011.00869v2-Figure4-1",
    "image_file": "2011.00869v2-Figure4-1.png",
    "caption": " This figure depicts maximum pooling and continuous time pooling using the LeNet-5 network on the pixel distance estimation task. The blue plot depicts test errors for the regular LeNet-5 architecture, meanwhile the red curve shows mean squared errors for continuous pooling. The two lines are averaged results over 20 independent trainings and the shaded intervals depict minimal and maximal values in the trainings.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pooling method achieves lower mean squared error on the pixel distance estimation task?",
    "answer": "Continuous time pooling",
    "rationale": "The red curve, which represents continuous time pooling, is consistently lower than the blue curve, which represents maximum pooling. This indicates that continuous time pooling achieves lower mean squared error on the pixel distance estimation task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.00869v2",
    "pdf_url": null
  },
  {
    "instance_id": "f77daf5df36641ae8e2478f7e245e251",
    "figure_id": "1805.07513v1-Figure3-1",
    "image_file": "1805.07513v1-Figure3-1.png",
    "caption": " Effect of clusters. ROBUSTTC-SA and ROBUSTTC-Intent: the performance of our ROBUSTTC clusters on the sentiment and intent classification tasks. ASAP-MT-LR-SA: the state-of-the-art ASAP-MT-LR clusters on the sentiment-analysis tasks (the method is not applicable to the intent-classification tasks).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the sentiment analysis task when the number of clusters is 15?",
    "answer": "RobustTC-SA",
    "rationale": "The plot shows that the Macro Accuracy of RobustTC-SA is highest when the number of clusters is 15.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.07513v1",
    "pdf_url": null
  },
  {
    "instance_id": "8e8e7146f4004a09b1d89e61f2bef627",
    "figure_id": "1805.02971v2-Figure2-1",
    "image_file": "1805.02971v2-Figure2-1.png",
    "caption": " Deviation of item utility vector (upper figure) and linear utility function parameter vector (lower figure) over time horizon.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of minimizing the deviation of the utility function parameter vector?",
    "answer": "LUMB",
    "rationale": "The lower figure shows the deviation of the utility function parameter vector for each algorithm. The LUMB algorithm has the lowest deviation, which means it performs the best in terms of minimizing the deviation of the utility function parameter vector.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.02971v2",
    "pdf_url": null
  },
  {
    "instance_id": "3f6bfeaa65aa4e6f9f19eb2c702e2832",
    "figure_id": "2004.14975v2-Figure6-1",
    "image_file": "2004.14975v2-Figure6-1.png",
    "caption": " Performance on finetuning tasks after reinitializing an individual layer of BERT. Error bars are ±2 standard deviations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which fine-tuning task is most affected by reinitializing a layer of BERT?",
    "answer": "SST-2",
    "rationale": "The figure shows the performance of different fine-tuning tasks after reinitializing a layer of BERT. The SST-2 task has the largest drop in accuracy when a layer is reinitialized, indicating that it is the most affected by this change.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14975v2",
    "pdf_url": null
  },
  {
    "instance_id": "41d873f2b1b4450b87d3841b2c334edf",
    "figure_id": "2306.10658v1-Figure6-1",
    "image_file": "2306.10658v1-Figure6-1.png",
    "caption": " Confusion on Discourse Relations. We use entropy as the metric for filtering most confusing examples. We use the top-3 predictions of the 20 most confusing examples to show the entanglement between relations. We use accumulated 𝑝(𝑟𝑖) · 𝑝(𝑟 𝑗 ) as weights for a pair of relations 𝑟𝑖 , 𝑟 𝑗 . Note that implausible predictions are suppressed to ignore model errors.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which discourse relation is the most confused with \"Concession\"?",
    "answer": "\"Contrast\"",
    "rationale": "The figure shows the confusion between different discourse relations. The thickness of the line between two relations indicates the degree of confusion. The line between \"Concession\" and \"Contrast\" is the thickest, indicating that these two relations are the most confused.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.10658v1",
    "pdf_url": null
  },
  {
    "instance_id": "0f3ab9a3b03648bd8f70edb3bc34ff8e",
    "figure_id": "1804.05493v2-Figure3-1",
    "image_file": "1804.05493v2-Figure3-1.png",
    "caption": " The relationship between the classifier accuracy and the reward. The correlationship coefficient is 0.8258 while the p-value is 0.0115.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does the figure show a strong positive correlation between reward and accuracy?",
    "answer": "Yes.",
    "rationale": "The figure shows a positive correlation between reward and accuracy. The correlation coefficient is 0.8258, which is close to 1, indicating a strong positive correlation. The p-value is 0.0115, which is less than 0.05, indicating that the correlation is statistically significant.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.05493v2",
    "pdf_url": null
  },
  {
    "instance_id": "d204b21d5ba641c088202685d1304f04",
    "figure_id": "2004.08628v1-Figure7-1",
    "image_file": "2004.08628v1-Figure7-1.png",
    "caption": " Single-step adversarial training: Trend of validation loss during single-step adversarial training, obtained for LeNet+ trained on MNIST dataset. Adversarial validation set is generated using column-1: Model-A, column-2: Model-B, column-3: Model-C and column-4: Model-D.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model appears to be the most robust to adversarial attacks?",
    "answer": "Model-C.",
    "rationale": "Model-C has the smallest difference between the clean validation loss and the adversarial validation loss, indicating that it is the most robust to adversarial attacks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.08628v1",
    "pdf_url": null
  },
  {
    "instance_id": "77a535e857b84458854a8f3dd277e925",
    "figure_id": "2306.13337v1-Figure2-1",
    "image_file": "2306.13337v1-Figure2-1.png",
    "caption": " Top-1 linear classification accuracy on ImageNet-1K with different hyper-parameters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on ImageNet-1K?",
    "answer": "ADCLR (Ours)",
    "rationale": "The plot shows the accuracy of different models on ImageNet-1K. ADCLR (Ours) has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.13337v1",
    "pdf_url": null
  },
  {
    "instance_id": "38468acc38804727b037d0721157de65",
    "figure_id": "2308.11551v2-Figure4-1",
    "image_file": "2308.11551v2-Figure4-1.png",
    "caption": " We compare average performance improvement in percentage(%) for Video-to-Text task on subsets of ActivityNet Captions. It shows how much percentage MeRetriever(avg) is better than CLIP4Clip(mean) on different subsets. Fig. 4a: Video-to-Text results for test-S/M/L/XL; Fig. 4b: Video-to-Text results for test-E1/E2/E3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better on the S subset of ActivityNet Captions for the Video-to-Text task?",
    "answer": "Recall-Average",
    "rationale": "Figure 4a shows the performance improvement in percentage for different methods on subsets of different durations. For the S subset, the Recall-Average bar is higher than the other two bars, indicating that Recall-Average performs better.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.11551v2",
    "pdf_url": null
  },
  {
    "instance_id": "8f5d7b00283e4273b63c174f160eb3e9",
    "figure_id": "2209.00647v1-Figure9-1",
    "image_file": "2209.00647v1-Figure9-1.png",
    "caption": " Task performance under different label choices. Prompting results when using different mask colors (e.g, purple/yellow vs. green/red), when drawing full mask compared to edges only, and when changing the mask texture. Compared to other alternatives, purple/yellow and black/white (see Figure 8) masks works best.",
    "figure_type": "** Photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the effect of using different mask colors on task performance?",
    "answer": " Purple/yellow and black/white masks work best.",
    "rationale": " The figure shows that the prompting results are better when using purple/yellow and black/white masks compared to other alternatives, such as green/red masks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.00647v1",
    "pdf_url": null
  },
  {
    "instance_id": "d8e43cfd293d43a1b7615c715d805edc",
    "figure_id": "2111.11297v2-Figure31-1",
    "image_file": "2111.11297v2-Figure31-1.png",
    "caption": " The LIME-Teaching feedback after answering teaching question.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of film is \"Baby Driver\"?",
    "answer": "\"Baby Driver\" is an action crime comedy film.",
    "rationale": "The figure shows that the AI incorrectly identified the film as a musical, and the explanation points out that the film is actually an action crime comedy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.11297v2",
    "pdf_url": null
  },
  {
    "instance_id": "ff3d567a104c4b77bb51c77138882df9",
    "figure_id": "2308.16900v3-Figure3-1",
    "image_file": "2308.16900v3-Figure3-1.png",
    "caption": " Examples of images. The viewpoint, lighting, and composition vary across images.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following wines is a Chardonnay?",
    "answer": "La Mascota",
    "rationale": "The image shows a variety of wines, including La Mascota, which is labeled as a Chardonnay.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.16900v3",
    "pdf_url": null
  },
  {
    "instance_id": "96b0ca09818c4c50836565c5c9219c72",
    "figure_id": "1909.10838v2-Figure2-1",
    "image_file": "1909.10838v2-Figure2-1.png",
    "caption": " Statistics of the Talk2Car dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common number of commands in the Talk2Car dataset?",
    "answer": "1,186",
    "rationale": "This can be seen in Figure (b), which shows the distribution of the number of commands in the dataset. The bar for the 15-20 range is the highest, indicating that this is the most common number of commands.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.10838v2",
    "pdf_url": null
  },
  {
    "instance_id": "0d24cef477524f2ea7abd3c909f8b41d",
    "figure_id": "2305.15871v3-Figure5-1",
    "image_file": "2305.15871v3-Figure5-1.png",
    "caption": " MMD between the NPE-RS posteriors and (left) the prior in the misspecified setting, (middle) the NPE posteriors in the misspecified setting, (right) the NPE posteriors in the wellspecified setting, for different values of λ.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the MMD between the NPE-RS posteriors and the prior in the misspecified setting change with increasing values of λ?",
    "answer": "The MMD decreases with increasing values of λ.",
    "rationale": "The left panel of the figure shows that the MMD between the NPE-RS posteriors and the prior in the misspecified setting decreases as λ increases. This indicates that the NPE-RS posteriors become closer to the prior as λ increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.15871v3",
    "pdf_url": null
  },
  {
    "instance_id": "753d5660db6d47b8948b1be3280dd1f0",
    "figure_id": "2309.07926v1-Figure3-1",
    "image_file": "2309.07926v1-Figure3-1.png",
    "caption": " Overall architecture of our COMPASS. It consists of a base layer (BL) depicted in the sky blue box and one or more enhancement layers (ELs) depicted in the light purple boxes which operate in an iterative manner. Note that we exploit the shared modules (LIFF and residual compression) for multiple ELs.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many enhancement layers are used in the COMPASS architecture?",
    "answer": "One or more.",
    "rationale": "The caption states that \"one or more enhancement layers (ELs)\" are used in the COMPASS architecture.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.07926v1",
    "pdf_url": null
  },
  {
    "instance_id": "60d4ebbac6bb4262a22676eb2a9ffe48",
    "figure_id": "2005.06251v1-Figure1-1",
    "image_file": "2005.06251v1-Figure1-1.png",
    "caption": " An instance from the imSitu dataset. Given an input image, the task it to identify the activity depicted in the image as well as the objects (noun) and their semantic role.",
    "figure_type": "** Photograph(s) and schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the semantic role of the \"chest\" in the image?",
    "answer": " Agent-part",
    "rationale": " The figure shows a man carrying a baby in a chest carrier. The word \"chest\" is connected to the \"agent-part\" box, which indicates that the chest is the part of the agent (the man) that is involved in the carrying activity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.06251v1",
    "pdf_url": null
  },
  {
    "instance_id": "3b3a1f48bb0f4fa284187dfcc8acd73c",
    "figure_id": "2210.01776v2-Figure2-1",
    "image_file": "2210.01776v2-Figure2-1.png",
    "caption": " “DIFFDOCK top-1” refers to the sample with the highest confidence. “DIFFDOCK samples” to the other diffusion model samples. Left: Visual diagram of the advantage of generative models over regression models. Given uncertainty in the correct pose (represented by the orange distribution), regression models tend to predict the mean of the distribution, which may lie in a region of low density. Center: when there is a global symmetry in the protein (aleatoric uncertainty), EquiBind places the molecule in the center while DIFFDOCK is able to sample all the true poses. Right: even in the absence of strong aleatoric uncertainty, the epistemic uncertainty causes EquiBind’s prediction to have steric clashes and TANKBind’s to have many self-intersections.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most likely to produce a model with steric clashes?",
    "answer": "EquiBind",
    "rationale": "The right panel of the figure shows that EquiBind's prediction has steric clashes, which are indicated by the red and green atoms overlapping.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.01776v2",
    "pdf_url": null
  },
  {
    "instance_id": "9d5ef67d6a1f4a45859d94e7febdde95",
    "figure_id": "2203.11197v2-Figure13-1",
    "image_file": "2203.11197v2-Figure13-1.png",
    "caption": " This shows sample efficiency in the grounding phase, similar to the advice-efficiency plot in Fig 4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which advice strategy is the most sample-efficient in the Point Maze task?",
    "answer": "The \"Ours: Direction/Action Advice\" strategy.",
    "rationale": "The plot shows that the \"Ours: Direction/Action Advice\" strategy (red line) reaches a success rate of 1.0 with the fewest number of samples in the Point Maze task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.11197v2",
    "pdf_url": null
  },
  {
    "instance_id": "55bd465b3ef34bc081f5f9f57c214287",
    "figure_id": "1809.05343v3-Figure3-1",
    "image_file": "1809.05343v3-Figure3-1.png",
    "caption": " (a) Training time per epoch on Pubmed and Reddit. (b) Accuracy curves of testing data on Cora for our Adapt method and its variant by adding skip connections.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is faster to train, Adapt or Full?",
    "answer": "Adapt is faster to train than Full.",
    "rationale": "The figure shows the training time per epoch for different methods on Pubmed and Reddit datasets. The training time for Adapt is lower than that of Full for both datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.05343v3",
    "pdf_url": null
  },
  {
    "instance_id": "f306fd1a5ca847e2afab628c5963fccd",
    "figure_id": "2112.07924v2-Figure4-1",
    "image_file": "2112.07924v2-Figure4-1.png",
    "caption": " Analysis of models with different knowledge sources on REDIAL. .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better in terms of BLEU-4 score when the percentage of golden knowledge is low?",
    "answer": "TS",
    "rationale": "The BLEU-4 score for the TS model is higher than the BLEU-4 score for the Ours model when the percentage of golden knowledge is low. This can be seen in the plot in subfigure (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.07924v2",
    "pdf_url": null
  },
  {
    "instance_id": "40293d2bb71247acba3684e43ba38cb1",
    "figure_id": "2103.00065v3-Figure15-1",
    "image_file": "2103.00065v3-Figure15-1.png",
    "caption": " Standard parameterization: summary statistics. Left: For each network width, we plot the mean and standard deviation (over the five different random initializations) of the maximum sharpness λmax along the gradient flow trajectory. Observe that λmax tends to decrease in expectation as the width increases, though it is not clear whether this pattern still holds when moving from width 512 to width 1024 — more samples are needed. Right: For each network width, we plot the mean and standard deviation (over the five different random initializations) of the maximum sharpness gain λmax/λ0 along the gradient flow trajectory. Observe that λmax/λ0 decreases in expectation as the width increases. It is not clear whether or not λmax/λ0 is deterministically tending to 1, but that does seem possible.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, does the maximum sharpness tend to decrease as the width increases?",
    "answer": "Yes.",
    "rationale": "The left plot in the figure shows that the mean of the maximum sharpness decreases as the width increases. This is evident from the downward trend of the horizontal lines in the box plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.00065v3",
    "pdf_url": null
  },
  {
    "instance_id": "33cb6d05991c4033a8fff03c817f2cde",
    "figure_id": "2103.04263v2-Figure2-1",
    "image_file": "2103.04263v2-Figure2-1.png",
    "caption": " (a) Distribution of generation tools used for creating DF-W YouTube (b) CDF of DF-W video upload dates (c) distribution of generation tools used for creating DF-W YouTube (d) CDF of number of DF-W videos contributed by channels.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common deepfake generation method used to create DF-W YouTube videos?",
    "answer": "FakeApp",
    "rationale": "Figure (a) shows that FakeApp is used to create 77.3% of DF-W YouTube videos, which is more than any other method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.04263v2",
    "pdf_url": null
  },
  {
    "instance_id": "e26f1b3d0d634ce4beef0005f62ae038",
    "figure_id": "1909.04625v1-Figure3-1",
    "image_file": "1909.04625v1-Figure3-1.png",
    "caption": " Comparison of models’ expectation preferences for singular vs. plural predicate in English and French Simple Coordination experiments.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best for French or-coordination?",
    "answer": "LSTM (frWaC)",
    "rationale": "The figure shows the results of different models for French or-coordination. The model with the highest S(sg)-S(pl) value is LSTM (frWaC).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.04625v1",
    "pdf_url": null
  },
  {
    "instance_id": "9e8b27211d94447aad1d61af696da9c5",
    "figure_id": "2210.04398v1-Figure7-1",
    "image_file": "2210.04398v1-Figure7-1.png",
    "caption": " Generative modeling performance of four TPMs on three natural image datasets. For each method, we report the test set bits-per-dimension (y-axis) in terms of the number of parameters (x-axis) for different numbers of latent states.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the lowest bits-per-dimension on the ImageNet64 dataset?",
    "answer": "RAT-SPN",
    "rationale": "The figure shows that the RAT-SPN method has the lowest curve on the ImageNet64 plot, indicating that it achieves the lowest bits-per-dimension for any given number of parameters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.04398v1",
    "pdf_url": null
  },
  {
    "instance_id": "053ce7e554b8481ba822a6d198a8981f",
    "figure_id": "2210.13439v2-Figure5-1",
    "image_file": "2210.13439v2-Figure5-1.png",
    "caption": " Percentage difference of examples in heuristic set, H25, and the remaining examples, D \\H25, labeled as having a qualitative property. Examples in the heuristic set are less valid & require more word matching based on explicitly stated information.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which question validity category has the largest difference in percentage of examples between the heuristic set, H25, and the remaining examples, D \\ H25?",
    "answer": "Valid questions.",
    "rationale": "The bar chart for question validity shows that the category \"Valid\" has the largest difference in percentage of examples between the heuristic set, H25, and the remaining examples, D \\ H25. This difference is approximately 0.06.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.13439v2",
    "pdf_url": null
  },
  {
    "instance_id": "09f4b793720c41be9c269cd341c1a5dd",
    "figure_id": "2009.13044v3-Figure2-1",
    "image_file": "2009.13044v3-Figure2-1.png",
    "caption": " Training and testing accuracy of ResNet-20, vanilla ANN-20 and the proposed PKKD ANN-20 models on CIFAR-10 and CIFAR-100 datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the CIFAR-100 dataset?",
    "answer": "KD AdderNet",
    "rationale": "The figure shows the training and testing accuracy of different models on the CIFAR-100 dataset. The KD AdderNet model has the highest testing accuracy of all the models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.13044v3",
    "pdf_url": null
  },
  {
    "instance_id": "9980c6d2dc844f49a271bd0b60594339",
    "figure_id": "2102.05288v1-Figure3-1",
    "image_file": "2102.05288v1-Figure3-1.png",
    "caption": " Number of frames of sound events on development set used for our experiments",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sound event is the most common in the office?",
    "answer": "Keyboard typing",
    "rationale": "The figure shows the number of frames of each sound event in different environments. The bar for keyboard typing in the office environment is the highest, indicating that it is the most common sound event in that environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.05288v1",
    "pdf_url": null
  },
  {
    "instance_id": "4a9fd40c93054c88838f1b12ed180a94",
    "figure_id": "2012.11207v4-Figure12-1",
    "image_file": "2012.11207v4-Figure12-1.png",
    "caption": " Targeted universal adversarial perturbations (ε = 16) for different classes.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object in the figure is most likely to be misclassified by a classifier?",
    "answer": "The matchstick.",
    "rationale": "The matchstick has the most visually distinctive features, which are likely to be targeted by the adversarial perturbation. This makes it more likely to be misclassified by a classifier.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.11207v4",
    "pdf_url": null
  },
  {
    "instance_id": "3432a344a1354a7d8def1d96de945159",
    "figure_id": "2004.00917v1-Figure3-1",
    "image_file": "2004.00917v1-Figure3-1.png",
    "caption": " Analysis of speeding up Newton’s iteration. The entries of proxy matrix Z ∈ R64×256 are sampled from the Gaussian distribution N(3, 1). (a) Comparison of convergence; (b) Comparison of the distribution of the eigenvalues of WWT at iteration t = 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method converges the fastest?",
    "answer": "ONI+Center+CSB",
    "rationale": "The plot in (a) shows the convergence of the different methods. The ONI+Center+CSB method converges to a lower value of ||WWT - I||F in fewer iterations than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.00917v1",
    "pdf_url": null
  },
  {
    "instance_id": "d22aeaa32d9d4d3c8509b6b3d8648913",
    "figure_id": "2101.06561v4-Figure1-1",
    "image_file": "2101.06561v4-Figure1-1.png",
    "caption": " The GENIE architecture for evaluating text generation tasks, with a summarization example. Similar to automatic leaderboards, model developers submit their predictions (top). GENIE then evaluates with a standard human evaluation as well as with automatic metrics (center). These scores are then used to rank and track systems’ performance across time (bottom).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two main methods used to evaluate the performance of text generation models in the GENIE architecture?",
    "answer": "Human evaluation and automatic evaluation.",
    "rationale": "The figure shows that the GENIE architecture uses both human evaluation and automatic evaluation to assess the performance of text generation models. Human evaluators provide feedback on the quality of the generated summaries, while automatic metrics such as BLEU, BERTScore, and ROUGE are used to measure the similarity between the generated summaries and the reference summaries.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.06561v4",
    "pdf_url": null
  },
  {
    "instance_id": "d1f0e994bd484b4993a3c323169ea40f",
    "figure_id": "1804.00104v3-Figure11-1",
    "image_file": "1804.00104v3-Figure11-1.png",
    "caption": " InfoGAN.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three types of disentangled latent factors shown in the figure?",
    "answer": "Angle, thickness and width, and digit type.",
    "rationale": "The figure shows three different types of disentangled latent factors: angle, thickness and width, and digit type. The angle factor controls the orientation of the digit, the thickness and width factor controls the thickness and width of the digit, and the digit type factor controls the type of digit that is generated.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.00104v3",
    "pdf_url": null
  },
  {
    "instance_id": "f12ef24329944009a7d57c75a67eac1e",
    "figure_id": "2301.12860v1-Figure3-1",
    "image_file": "2301.12860v1-Figure3-1.png",
    "caption": " JFT-4B ViT-L/32 latency in terms of FLOPs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the HET-based methods achieves the highest throughput?",
    "answer": "HET-XL.",
    "rationale": "The figure shows the throughput of different HET-based methods as a function of the number of MC samples. HET-XL consistently achieves the highest throughput for all tested numbers of MC samples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.12860v1",
    "pdf_url": null
  },
  {
    "instance_id": "1ee03db454c0475da55f5fde90c703c5",
    "figure_id": "2105.12660v2-Figure9-1",
    "image_file": "2105.12660v2-Figure9-1.png",
    "caption": " Qualitative results of our method on real image editing. The images are taken from FFHQ.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following edits is the most realistic?",
    "answer": "Pose+.",
    "rationale": "The Pose+ edits are the most realistic because they preserve the original lighting and facial features of the subject. The other edits, such as Old and Male, introduce artifacts that are not present in the original image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.12660v2",
    "pdf_url": null
  },
  {
    "instance_id": "37721273fe3f4831bd9adad233196ed6",
    "figure_id": "2310.20145v2-Figure6-1",
    "image_file": "2310.20145v2-Figure6-1.png",
    "caption": " The robot’s initial locations and push times found by different algorithms",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm produced the most consistent push times across all robots?",
    "answer": "M-skl",
    "rationale": "The figure shows the distribution of push times for each robot and algorithm. The M-skl algorithm has the tightest distribution of push times, indicating that it produced the most consistent results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.20145v2",
    "pdf_url": null
  },
  {
    "instance_id": "6ae9216ee4114f99a28d10742838cfeb",
    "figure_id": "2007.05033v3-Figure3-1",
    "image_file": "2007.05033v3-Figure3-1.png",
    "caption": " Inference accuracies (vertical axes) obtained across datasets, using the AGM model (experiment I), where the variable being varied in every plot is the ensemble size M (horizontal axes) used during inference. Values used for M were 10, 100, 1000, 10000, indicated in logarithm base 10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest accuracy when using an ensemble size of 1000?",
    "answer": "MNIST",
    "rationale": "The figure shows that the MNIST dataset has the highest accuracy when using an ensemble size of 1000. This is because the line for MNIST is the highest at the point where the x-axis value is 3 (which corresponds to an ensemble size of 1000).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.05033v3",
    "pdf_url": null
  },
  {
    "instance_id": "f160569ac1554f3a91dbf88fc72514b2",
    "figure_id": "2012.04858v2-Figure8-1",
    "image_file": "2012.04858v2-Figure8-1.png",
    "caption": " Empirical sample complexity. The figure shows the correlation between model-predicted behavioral measures, and actual behavior, for the Subj-DNN model on the MaxProd task. The correlations are calculated over a fixed set of 100 subjects included in both training & validation data, as we add additional subjects to the training data alone. Curves represent mean±SEM of correlation over 50 runs of the evaluation. Panels A-C show correlations for average # moves, accuracy, and average scores, respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three model-predicted behavioral measures has the highest correlation with actual behavior?",
    "answer": "Accuracy.",
    "rationale": "The figure shows that the correlation between model-predicted accuracy and actual accuracy is higher than the correlation between model-predicted number of moves and actual number of moves, and the correlation between model-predicted scores and actual scores.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.04858v2",
    "pdf_url": null
  },
  {
    "instance_id": "fddf957b40e04fe393127b3d9c8e8434",
    "figure_id": "1810.12281v1-Figure2-1",
    "image_file": "1810.12281v1-Figure2-1.png",
    "caption": " Test accuracy as a function of training epoch for SGD and Adam on CIFAR-100 with different weight decay regularization schemes. baseline is the model without weight decay; wd-conv is the model with weight decay applied to all convolutional layers; wd-all is the model with weight decay applied to all layers; wd-fc is the model with weight decay applied to the last layer (fc). Most of the generalization effect of weight decay is due to applying it to layers with BN.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer and weight decay scheme combination achieved the highest test accuracy on VGG16?",
    "answer": "ADAM with wd-fc",
    "rationale": "The plot shows the test accuracy for different combinations of optimizers and weight decay schemes. The highest test accuracy for VGG16 is achieved by the combination of ADAM and wd-fc, as shown by the blue line in the top right plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.12281v1",
    "pdf_url": null
  },
  {
    "instance_id": "002f1c4c7f7e4ffbae83695ebb6abba0",
    "figure_id": "2012.15701v2-Figure12-1",
    "image_file": "2012.15701v2-Figure12-1.png",
    "caption": " Loss landscape visualizations w.r.t FFN-Mid parameters of the 1st and 2nd Transformer layers on MRPC.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the lowest training loss?",
    "answer": "The full-precision model.",
    "rationale": "The training loss is shown on the vertical axis of the plots. The full-precision model has the lowest training loss because its surface is the lowest among all the models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.15701v2",
    "pdf_url": null
  },
  {
    "instance_id": "a81b6297b3e647dd9c109a48fc78bf3a",
    "figure_id": "2204.03236v1-Figure3-1",
    "image_file": "2204.03236v1-Figure3-1.png",
    "caption": " The results of comparing the optimality gap of TSP instances generated by a uniform generator and our proposed hardness-adaptive generator. Our proposed generator can continuously generate instances with more diverse hardness levels. (Best viewed in color)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which generator produces TSP instances with a wider range of optimality gaps?",
    "answer": "The hardness-adaptive generator.",
    "rationale": "The figure shows the distribution of optimality gaps for TSP instances generated by two different generators. The hardness-adaptive generator produces instances with a wider range of optimality gaps, as evidenced by the broader distribution of its data points in the figure. This suggests that the hardness-adaptive generator is better at generating instances with varying levels of difficulty.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.03236v1",
    "pdf_url": null
  },
  {
    "instance_id": "625f2453d8624ea6b28b6eca993a8bc0",
    "figure_id": "2201.12585v2-Figure4-1",
    "image_file": "2201.12585v2-Figure4-1.png",
    "caption": " Simulation results on comparing the proposed algorithm with other baseline methods under different noise levels (uncertainty weights), where x-axis represents the uncertainty weight, and y-axis represents the mean of individualized treatment effects (ITE) normalized by the average outcome of control group on the synthetic RCT datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best when the uncertainty weight is 10?",
    "answer": "CTS.",
    "rationale": "The bar for CTS is the highest among all methods when the uncertainty weight is 10.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.12585v2",
    "pdf_url": null
  },
  {
    "instance_id": "e68f20f5ac144e3a998be4141587fad4",
    "figure_id": "1912.07161v2-Figure2-1",
    "image_file": "1912.07161v2-Figure2-1.png",
    "caption": " tSNE [52] visualizations of unseen 3D point cloud features of (a) ModelNet10 [56] (b) McGill [47] and unseen 2D image features of (c) AwA2 [57] (d) CUB [53] . The cluster structure in the 2D feature space is much better defined, with tighter and more separated clusters than those in the 3D point cloud.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the datasets shown in the figure has the most well-defined cluster structure in the 2D feature space?",
    "answer": "The CUB dataset.",
    "rationale": "The caption states that \"the cluster structure in the 2D feature space is much better defined, with tighter and more separated clusters than those in the 3D point cloud.\" This can be seen in the figure, where the CUB dataset (d) has the most distinct and separated clusters of points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.07161v2",
    "pdf_url": null
  },
  {
    "instance_id": "b12849bbf7244e7b8b85ac9f949da106",
    "figure_id": "2112.15311v1-Figure2-1",
    "image_file": "2112.15311v1-Figure2-1.png",
    "caption": " Top: Results on synthetic problems that adapt widely used synthetic test functions into function networks. Bottom: Results on realistic problems: manufacturing line, design of testing protocols for COVID-19, fetch-and-reach with a robotic arm, and calibration of an epidemic model. EI-FN substantially improves over benchmark methods, with larger improvements for problems with higher-dimensional decision vectors and more nodes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization method performed the best on the fetch-and-reach problem?",
    "answer": "EI-FN",
    "rationale": "The figure shows the best objective value achieved by each optimization method as a function of the number of evaluations. For the fetch-and-reach problem, EI-FN consistently achieves the best objective value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.15311v1",
    "pdf_url": null
  },
  {
    "instance_id": "9488232d4e4b45198093cc1173e565e9",
    "figure_id": "2209.08433v1-Figure1-1",
    "image_file": "2209.08433v1-Figure1-1.png",
    "caption": " Examples of near duplicate image pairs.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following pairs of images is most likely to be misclassified as duplicates by an image recognition algorithm?",
    "answer": "The pair of images of the woman in the pink coat.",
    "rationale": "The two images of the woman in the pink coat are very similar, but there are subtle differences between them. For example, the woman's pose is slightly different in the two images, and the background is slightly different as well. An image recognition algorithm might not be able to detect these subtle differences, and it might therefore misclassify the two images as duplicates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.08433v1",
    "pdf_url": null
  },
  {
    "instance_id": "1042dd90fc4e4ba4b1901fbfaa2340c5",
    "figure_id": "1911.09801v1-Figure2-1",
    "image_file": "1911.09801v1-Figure2-1.png",
    "caption": " Model Accuracy in terms of Answer Length",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best when the answer length is between 100 and 200 words?",
    "answer": "AP-LSTM",
    "rationale": "The figure shows the accuracy of different models as a function of answer length. The AP-LSTM model has the highest accuracy for answer lengths between 100 and 200 words.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.09801v1",
    "pdf_url": null
  },
  {
    "instance_id": "0d28d714e6ab469f9d56b31fa5a91419",
    "figure_id": "1910.09447v2-Figure4-1",
    "image_file": "1910.09447v2-Figure4-1.png",
    "caption": " E and C statistics for admissible methods. The plot shows mean (filled black circle) and 66% confidence ellipse, showing covariance of E and C values for each method. Notice: E and C are positively correlated, suggesting some dependence on either style (compare Fig. 7) or optimization difficulties; XLCM and GAL achieve better E, and universal achieves better C; controls are where expected (style control gets excellent E, weak C; content control weak E, excellent C).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the best E statistic?",
    "answer": "The XLCM method achieves the best E statistic.",
    "rationale": "The plot shows that the mean E statistic for the XLCM method is the lowest of all the methods. This is indicated by the filled black circle for the XLCM method being the lowest on the y-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.09447v2",
    "pdf_url": null
  },
  {
    "instance_id": "dfe9fe94274d4d0a96ae3cf25f3b0669",
    "figure_id": "1905.01639v1-Figure6-1",
    "image_file": "1905.01639v1-Figure6-1.png",
    "caption": " User study results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which video category received the most votes in total?",
    "answer": "Car-turn",
    "rationale": "The \"Total\" bar for Car-turn is the tallest, indicating it received the most votes overall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.01639v1",
    "pdf_url": null
  },
  {
    "instance_id": "94cb192607ba4e23b43261ee361ca088",
    "figure_id": "2106.14405v2-Figure23-1",
    "image_file": "2106.14405v2-Figure23-1.png",
    "caption": " Overview of all the high level planner actions with the pre-conditions (right), post-conditions (left), and an intermediate state when executing the action.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the actions shown in the figure requires the robot to be in a specific location?",
    "answer": "Navigate.",
    "rationale": "The \"Navigate\" action is the only one that explicitly mentions the robot's location as a pre-condition. The other actions only require the robot to be in a position to interact with the objects involved in the action.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.14405v2",
    "pdf_url": null
  },
  {
    "instance_id": "846788a50c644b3e8926682ad6993c11",
    "figure_id": "2004.14340v5-Figure7-1",
    "image_file": "2004.14340v5-Figure7-1.png",
    "caption": " Comparing one-shot sparsity results for WoodTaylor and baselines on the partially trained MLPNET on MNIST.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best at a target sparsity level of 0.2?",
    "answer": "WoodTaylor Joint",
    "rationale": "The figure shows that the WoodTaylor Joint line is the highest at a target sparsity level of 0.2. This means that this method has the highest test accuracy at this sparsity level.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14340v5",
    "pdf_url": null
  },
  {
    "instance_id": "555d6bc697c140c6abf71416d9d98522",
    "figure_id": "1902.09347v1-Figure1-1",
    "image_file": "1902.09347v1-Figure1-1.png",
    "caption": " An example of path scores. Suppose xi is labeled as {c11, c 2 2}. Si j for j = 1, . . . , 6 is shown in the figure.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the path score of path p2?",
    "answer": "The path score of path p2 is 2.",
    "rationale": "The path score is shown below each path in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.09347v1",
    "pdf_url": null
  },
  {
    "instance_id": "76798481e4994390ba20b4069bf8a3f5",
    "figure_id": "1908.05806v2-Figure2-1",
    "image_file": "1908.05806v2-Figure2-1.png",
    "caption": " The length proportion of each defined “bones” for different classes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which species has the highest relative length proportion of bones in the 12th position?",
    "answer": "Humans.",
    "rationale": "The figure shows the relative length proportion of bones for different species. The x-axis shows the index of bones, and the y-axis shows the relative length proportion. The line for humans is the highest at the 12th position.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.05806v2",
    "pdf_url": null
  },
  {
    "instance_id": "e6ba3939e4d44ae49b5c7e39c647221f",
    "figure_id": "2104.12690v1-Figure5-1",
    "image_file": "2104.12690v1-Figure5-1.png",
    "caption": " Over-optimistic results from workers with uniform noise. Human workers tend to make “structured” mistakes. Simulated workers with uniform label noise (blue) can result in overoptimistic annotation performance. Experiments under workers with structured noise reflect real-life performance better.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of noise better reflects real-life performance of human workers?",
    "answer": "Structured noise",
    "rationale": "The figure shows that the accuracy of workers with structured noise is lower than the accuracy of workers with uniform noise. This suggests that structured noise is a better reflection of real-life performance, as human workers tend to make \"structured\" mistakes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.12690v1",
    "pdf_url": null
  },
  {
    "instance_id": "c572fecaa09d4034bc847573f40ed2f6",
    "figure_id": "1912.09256v1-Figure13-1",
    "image_file": "1912.09256v1-Figure13-1.png",
    "caption": " CONFIRM analysis for K-Means and TPC-DS Q65 on Google Cloud and HPCCloud. Median estimates (blue thick curve), 95% nonparametric confidence intervals (light blue filled space), and 1% error bounds (red dotted curves). Vertical axis not starting at 0 for visibility.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two benchmarks, K-Means or TPC-DS Q65, shows a more stable performance over the course of the experiment?",
    "answer": "TPC-DS Q65 shows a more stable performance.",
    "rationale": "The blue curve for TPC-DS Q65 in the bottom plot shows a flat and smooth line with very little variation, whereas the blue curve for K-Means in the top plot fluctuates quite a bit. This indicates that the performance of TPC-DS Q65 is more consistent across different repetitions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.09256v1",
    "pdf_url": null
  },
  {
    "instance_id": "87ba56a4b2c24b169ce072545d5bc5c2",
    "figure_id": "2205.12247v2-Figure4-1",
    "image_file": "2205.12247v2-Figure4-1.png",
    "caption": " Multilingual PLMs’ performance averaged over countries when using multilingual prompts. “en”, “zh”, “hi”, “fa”, and “sw” denote English, Chinese, Hindi, Persian, and Swahili. Complete results are shown in Appendix E.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the highest performance for mBERT, XLM, and XLM-R family?",
    "answer": "Hindi",
    "rationale": "The figure shows the performance of different multilingual PLMs for different languages. The performance is measured on the y-axis, and the language is measured on the x-axis. For the mBERT, XLM, and XLM-R family, the line for Hindi is the highest, which indicates that it has the highest performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.12247v2",
    "pdf_url": null
  },
  {
    "instance_id": "915aee46e7224f34b36eb5447c0822bf",
    "figure_id": "2305.15001v3-Figure9-1",
    "image_file": "2305.15001v3-Figure9-1.png",
    "caption": " Visualization on grouping performance for CAE, CAE++ and CtCAE on Tetrominoes. CtCAE shows a larger spread in phase values for different clusters (column 5) compared to CAE. We hypothesize that this facilitates CtCAE to perform better grouping in the scenario of multiple object instances with the same color.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better at grouping multiple object instances with the same color?",
    "answer": "CtCAE",
    "rationale": "The figure shows that CtCAE has a larger spread in phase values for different clusters than CAE. This suggests that CtCAE is better able to distinguish between different object instances, even if they have the same color.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.15001v3",
    "pdf_url": null
  },
  {
    "instance_id": "047d75a9239c405b9092a21787a6273d",
    "figure_id": "2002.12292v2-Figure15-1",
    "image_file": "2002.12292v2-Figure15-1.png",
    "caption": " Performance on a version of the MiniGridRoomN7S4 in which the colors of the walls and goals are randomly picked from a set of 4 colors at the beginning of each episode.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in this environment?",
    "answer": "RIDE",
    "rationale": "The plot shows that RIDE achieves the highest average return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.12292v2",
    "pdf_url": null
  },
  {
    "instance_id": "566abb67865d4644a19b065f508e113a",
    "figure_id": "2309.07867v3-Figure8-1",
    "image_file": "2309.07867v3-Figure8-1.png",
    "caption": " Analogy to Figure 5 for the mixture of continuous range-bounded data and point masses.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which distance metric decreases the fastest?",
    "answer": "The Wasserstein distance.",
    "rationale": "The Wasserstein distance (blue line) decreases the fastest, as it reaches a value of 0.01 after only 10^4 iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.07867v3",
    "pdf_url": null
  },
  {
    "instance_id": "928b0272b718486780d95040a6dde431",
    "figure_id": "2205.02485v1-Figure7-1",
    "image_file": "2205.02485v1-Figure7-1.png",
    "caption": " Community size distribution in 𝐺5",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which graph has the most communities with a size greater than 1000?",
    "answer": "The crawled graph.",
    "rationale": "The black line representing the crawled graph is higher than the other two lines for sizes greater than 1000.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.02485v1",
    "pdf_url": null
  },
  {
    "instance_id": "15c67617c4a14a36948a8be2ac15b5dd",
    "figure_id": "1707.03631v2-Figure3-1",
    "image_file": "1707.03631v2-Figure3-1.png",
    "caption": " Histograms of the activation values and the mean activation values from a hidden layer of autoencoders in 1,000 MNIST test images. All values are converted by the log scale for the comparison.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of dropout results in the lowest activation values?",
    "answer": "Standard dropout.",
    "rationale": "The green bars in the top panel represent the activation values for standard dropout. These bars are consistently lower than the blue and red bars, which represent the activation values for the plane and adversarial dropout, respectively. This indicates that standard dropout results in the lowest activation values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1707.03631v2",
    "pdf_url": null
  },
  {
    "instance_id": "d47e38175a414afea8c7d88eccc90aee",
    "figure_id": "2107.00644v2-Figure6-1",
    "image_file": "2107.00644v2-Figure6-1.png",
    "caption": " (Left) Comparison with additional DrQ baselines. We compare SVEA implemented with DrQ [K=1,M=1] as base algorithm to DrQ with varying values of its K,M hyperparameters. All methods use the conv augmentation (in addition to shift augmentation used by DrQ). Results are averaged over 5 seeds for each of the 5 tasks from DMControl-GB [21] and shaded area is ±1 std. deviation across seeds. Increasing values of K,M improve sample efficiency of DrQ, but at a high computational cost; DrQ uses approx. 6x wall-time to match the sample efficiency of SVEA. (Right) DistractingCS. Episode return as a function of randomization intensity at test-time, aggregated across 5 seeds for each of the 5 tasks from DMControl-GB. See Appendix E for per-task comparison.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most sample-efficient?",
    "answer": "SVEA with convolutional augmentation.",
    "rationale": "The left plot shows that SVEA with convolutional augmentation achieves the highest episode return in the shortest amount of time (wall-time).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.00644v2",
    "pdf_url": null
  },
  {
    "instance_id": "5d870552ec1d45ef82a233addad2df90",
    "figure_id": "2202.03514v1-Figure2-1",
    "image_file": "2202.03514v1-Figure2-1.png",
    "caption": " Average validation accuracy over 5 folds of ESC-50 over 25 epochs for Xception and Xception-small models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest validation accuracy after 25 epochs?",
    "answer": "Xception 6",
    "rationale": "The figure shows the average validation accuracy over 5 folds of ESC-50 over 25 epochs for Xception and Xception-small models. The highest validation accuracy is achieved by Xception 6, which has a validation accuracy of approximately 0.95.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.03514v1",
    "pdf_url": null
  },
  {
    "instance_id": "22e5a2c07ef4436a87f26f1233d47015",
    "figure_id": "2004.02015v3-Figure7-1",
    "image_file": "2004.02015v3-Figure7-1.png",
    "caption": " The AOPC and log-odds for CNN on the SST dataset.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest AOPC for all values of k?",
    "answer": "Leave-one-out.",
    "rationale": "The figure shows the AOPC for different methods as a function of k. The Leave-one-out method has the highest AOPC for all values of k.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.02015v3",
    "pdf_url": null
  },
  {
    "instance_id": "afcc8d7e6d1844e3b2eafd1bea8299e0",
    "figure_id": "2011.01122v2-Figure7-1",
    "image_file": "2011.01122v2-Figure7-1.png",
    "caption": " Failure on objects with textureless background. The network smooths the depth prediction of the sky with the foreground object and results to ambiguous contour of the object.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the failure of the network to correctly predict the depth of the object?",
    "answer": "Image (d)",
    "rationale": "The network smooths the depth prediction of the sky with the foreground object, resulting in an ambiguous contour of the object. This can be seen in image (d), where the object is not clearly distinguished from the background.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.01122v2",
    "pdf_url": null
  },
  {
    "instance_id": "4c445ac903f142cb83e3b282f0e157d3",
    "figure_id": "2208.03789v1-Figure4-1",
    "image_file": "2208.03789v1-Figure4-1.png",
    "caption": " Social Experience in Pragmatic Society.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm leads to the highest social experience in a pragmatic society?",
    "answer": "XSIGA.",
    "rationale": "The figure shows that the XSIGA algorithm (orange line) leads to the highest social experience in a pragmatic society. This is because the XSIGA algorithm is able to learn and adapt to the changing social environment, which allows it to achieve a higher level of social experience than the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.03789v1",
    "pdf_url": null
  },
  {
    "instance_id": "22d39ce324a341fcb146e629f0322be9",
    "figure_id": "2303.01384v1-Figure6-1",
    "image_file": "2303.01384v1-Figure6-1.png",
    "caption": " Information bottleneck capacity C during training of DAVA for different datasets. Depending on the complexity and structure of the dataset as well as random initialization of the model, a different schedule of C is necessary to achieve best performance. The shaded area denotes one standard deviation across 5 random seeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset exhibits the most stable goal KL-divergence throughout training?",
    "answer": "Mpi3d Toy",
    "rationale": "The figure shows the Goal KL-divergence as a function of training steps for three different datasets. The Mpi3d Toy dataset has the least variance in its Goal KL-divergence across the training steps, as evidenced by the narrow shaded area around the line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.01384v1",
    "pdf_url": null
  },
  {
    "instance_id": "f2000a18fd834839a59d1695be1da31c",
    "figure_id": "2208.13579v2-Figure17-1",
    "image_file": "2208.13579v2-Figure17-1.png",
    "caption": " Outlier detection AUROC without conditional correction. (a) Grayscale image datasets. Blue: log likelihood (LL), uncorrected; Orange: Stirred LL without conditional correction; Green: Shaken LL without conditional correction. (b) Same as in panel (a) but for natural image datasets. Other conventions are the same as in Figures 5-6.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method consistently performs the best across all datasets?",
    "answer": " Shaking",
    "rationale": " The figure shows the outlier detection AUROC for different methods on different datasets. The green bars represent the Shaking method, and they are consistently the highest across all datasets, indicating that this method performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.13579v2",
    "pdf_url": null
  },
  {
    "instance_id": "1ba9d43d44494e268cad387920e33264",
    "figure_id": "2306.02846v3-Figure21-1",
    "image_file": "2306.02846v3-Figure21-1.png",
    "caption": " Trade-off between memory usage and FPR for artificial datasets. All figures are with the seed set to 0. The hyperparameters for PLBFs are N = 1, 000 and k = 5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which filter is the most memory efficient?",
    "answer": "The Bloom filter.",
    "rationale": "The Bloom filter uses the least amount of memory for all values of the false positive rate (FPR).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.02846v3",
    "pdf_url": null
  },
  {
    "instance_id": "ec0ec0fe6d844fb3a9fbe756f19a9f11",
    "figure_id": "2109.13059v4-Figure4-1",
    "image_file": "2109.13059v4-Figure4-1.png",
    "caption": " TENC’s performance against distillation cycles under different base models and tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the STS task in the first cycle?",
    "answer": "BERT-base",
    "rationale": "The plot shows the performance of different models on the STS task in terms of Spearman's rho. In the first cycle, BERT-base has the highest Spearman's rho.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.13059v4",
    "pdf_url": null
  },
  {
    "instance_id": "eeb704dfac4645dbb2a9f8ddd2e247e9",
    "figure_id": "2305.19693v3-Figure7-1",
    "image_file": "2305.19693v3-Figure7-1.png",
    "caption": " “Race” diversity analysis on CelebA64 over 50,000 generated samples by (c) gls-DDIM and (d) DDIM samplers with 5 denoising steps. Results obtained on (a) training set and (b) DDPM using 1000 denoising steps are provided for reference. Corresponding samples obtained by each set are shown on top of the pie charts.",
    "figure_type": "photograph(s) and plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method generated the most diverse set of faces in terms of race?",
    "answer": "gls-DDIM-05",
    "rationale": "The pie charts show the percentage of faces generated for each race. gls-DDIM-05 has the most even distribution of races, with no race exceeding 88.3%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19693v3",
    "pdf_url": null
  },
  {
    "instance_id": "e794924ede614bcf96f1bd72629a525d",
    "figure_id": "2004.04981v1-Figure3-1",
    "image_file": "2004.04981v1-Figure3-1.png",
    "caption": " Spatiotemporal fusion hints obtained from the probability space on Something-Something V2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer-level fusion unit has the highest marginal probability for the spatiotemporal modality?",
    "answer": "B3.L11",
    "rationale": "The figure shows the marginal probability of layer-level fusion units for different modalities. The bar with the highest value for the spatiotemporal modality corresponds to B3.L11.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.04981v1",
    "pdf_url": null
  },
  {
    "instance_id": "e18b7e07d373400b81838010730e90eb",
    "figure_id": "1812.06216v2-Figure3-1",
    "image_file": "1812.06216v2-Figure3-1.png",
    "caption": " Each model in our dataset is composed of multiple patches and feature curves. The two images show the distribution of types of patches (left) and curves (right) over the current dataset (≈ 1M models).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of patch and feature curve is the most common in the dataset?",
    "answer": "Plane and Line",
    "rationale": "The figure shows the distribution of types of patches and feature curves in the dataset. The height of each bar represents the percentage of models that contain that type of patch or curve. The bars for Plane and Line are the highest, indicating that these are the most common types of patches and curves in the dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.06216v2",
    "pdf_url": null
  },
  {
    "instance_id": "dbb82e5bd4094d68a24223edf0ed7025",
    "figure_id": "2111.04906v1-Figure11-1",
    "image_file": "2111.04906v1-Figure11-1.png",
    "caption": " Comparing the testing accuracy curves of DPAdam and DPSGD models across hyperparameter tuning grid from Table 2 with σ = 2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performed the best on the MNIST-TLNN dataset?",
    "answer": "DPAdam",
    "rationale": "The figure shows the testing accuracy curves of different optimizers on different datasets. The MNIST-TLNN dataset is shown in the bottom right panel. The DPAdam curve is the highest of all the curves in this panel, indicating that it achieved the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.04906v1",
    "pdf_url": null
  },
  {
    "instance_id": "0b503f67b98f447ca583ef59162f8acb",
    "figure_id": "2303.11633v1-Figure6-1",
    "image_file": "2303.11633v1-Figure6-1.png",
    "caption": " Visual illustrations from top to bottom are from input images, ground-truth, baseline and baseline+ours. Black regions are ignored during testing.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which regions of the images are ignored during testing?",
    "answer": "The black regions.",
    "rationale": "The caption explicitly states that \"Black regions are ignored during testing.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.11633v1",
    "pdf_url": null
  },
  {
    "instance_id": "0b57a1a2509a4873a87b09d922323151",
    "figure_id": "1706.07230v2-Figure10-1",
    "image_file": "1706.07230v2-Figure10-1.png",
    "caption": " Objects of various colors and sizes used in the environment",
    "figure_type": "Other.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different types of objects are shown in the image?",
    "answer": "Six.",
    "rationale": "The image shows six different types of objects: torches, pillars, key cards, skull keys, armor, and a creature.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1706.07230v2",
    "pdf_url": null
  },
  {
    "instance_id": "59650ed595624b1ab5f49096be3cab13",
    "figure_id": "2110.11430v2-Figure4-1",
    "image_file": "2110.11430v2-Figure4-1.png",
    "caption": " Plots showing the results on the Heart dataset for the alternate method of perturbation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method of perturbation has the lowest error?",
    "answer": "The \"Lower + MDS\" method has the lowest error.",
    "rationale": "This can be seen in Figure (c), where the \"Lower + MDS\" line is the lowest of all the lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.11430v2",
    "pdf_url": null
  },
  {
    "instance_id": "59eba7708c704d9abb403a479dc7a3b8",
    "figure_id": "2103.08307v2-Figure8-1",
    "image_file": "2103.08307v2-Figure8-1.png",
    "caption": " Robustness of AT+CAS against white-box attacks FGSM, PGD-20 and CW∞ under different β. As β increases, the robustness is also improved, especially against the CW∞ attack.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which white-box attack is the most effective against AT+CAS when beta is small?",
    "answer": "FGSM",
    "rationale": "The figure shows that when beta is small (e.g., 0 or 0.5), the accuracy of AT+CAS is the lowest for FGSM compared to the other two attacks. This suggests that FGSM is the most effective attack against AT+CAS when beta is small.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.08307v2",
    "pdf_url": null
  },
  {
    "instance_id": "efdebf3ca9c94e87a3a87e38acf8bd9b",
    "figure_id": "1911.03642v3-Figure3-1",
    "image_file": "1911.03642v3-Figure3-1.png",
    "caption": " Aggregate performance of the NRE model for each relation (left) andmale−female F1 score gender gap for each relation (right). An ideal model maximizes performance and minimizes the gender gap. The experiment is run five times. We give the mean values and standard error bars.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which relation has the highest F1 score gender gap?",
    "answer": "Birthplace",
    "rationale": "The F1 score gender gap is the difference between the F1 score for males and females. The figure on the right shows that the F1 score gender gap for birthplace is the highest, at around 0.05.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.03642v3",
    "pdf_url": null
  },
  {
    "instance_id": "4a25ae8b7286429a8aa21043ee7eea58",
    "figure_id": "1905.09335v2-Figure2-1",
    "image_file": "1905.09335v2-Figure2-1.png",
    "caption": " The rectangular bars and error bars represent the mean normalized return and the standard error, respectively, as measured over 1000 trials. The normalized values have been scaled in such a way that expert and random performance are 1.0 and 0.0, respectively. The x-axis represents the number of available video demonstration trajectories.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four algorithms performs best on the Hopper task when only one demonstration trajectory is available?",
    "answer": "Ours.",
    "rationale": "The figure shows the performance of four algorithms on the Hopper task. When only one demonstration trajectory is available, Ours (red bar) has the highest mean normalized return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.09335v2",
    "pdf_url": null
  },
  {
    "instance_id": "7d617864c7a34012a71d87f72b4f25bc",
    "figure_id": "2310.13545v1-Figure8-1",
    "image_file": "2310.13545v1-Figure8-1.png",
    "caption": " The examples of the generated images by UViT with our proposed LS.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the resolution of the images in the CelebA dataset?",
    "answer": "64 x 64 pixels.",
    "rationale": "The figure shows that the images in the CelebA dataset are 64 x 64 pixels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.13545v1",
    "pdf_url": null
  },
  {
    "instance_id": "6c818081387149e6b16caf89dba751e3",
    "figure_id": "2002.10248v4-Figure26-1",
    "image_file": "2002.10248v4-Figure26-1.png",
    "caption": " High confident MNIST samples generated for each class as predicted by the ADDA model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class of MNIST images has the highest predicted probability?",
    "answer": "Class 0.",
    "rationale": "The top row of the figure shows the predicted probability distribution for each class of MNIST images. The peak of the distribution for class 0 is the highest, indicating that the ADDA model is most confident in its predictions for this class.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.10248v4",
    "pdf_url": null
  },
  {
    "instance_id": "17d95575ac7540e4b482d8ad915697d1",
    "figure_id": "2110.12690v2-Figure2-1",
    "image_file": "2110.12690v2-Figure2-1.png",
    "caption": " Accuracy against PGD attack with 10 iterations in function of the perturbation ε for our CPL networks and its concurrent approaches on CIFAR10 and CIFAR100 datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network is the most robust to PGD attacks on CIFAR100?",
    "answer": "CPL-XL.",
    "rationale": "The figure shows the accuracy of different networks against PGD attacks on CIFAR100. The CPL-XL network has the highest accuracy for all values of epsilon, which indicates that it is the most robust to PGD attacks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.12690v2",
    "pdf_url": null
  },
  {
    "instance_id": "9699b1f912784f84bd40a9d22ff707ac",
    "figure_id": "2207.02098v3-FigureB.6-1",
    "image_file": "2207.02098v3-FigureB.6-1.png",
    "caption": "Figure B.6: Performance curves on all tasks. For the Transformer encoder, we pick the best positional encoding for each task (it is considered as a hyperparameter). The dashed vertical red line is the training range, meaning that sequences to the right have not been seen during training and thus measure generalization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task shows the greatest performance difference between the Transformer and RNN models for sequences longer than 250?",
    "answer": "Binary Addition",
    "rationale": "Looking at Figure B.6, we can see that for Binary Addition, the Transformer model achieves high accuracy on sequences of all lengths, while the RNN models experience a significant drop in accuracy for sequences longer than 250.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.02098v3",
    "pdf_url": null
  },
  {
    "instance_id": "38145f1af16248279f7a69aec8135364",
    "figure_id": "2103.01863v1-Figure3-1",
    "image_file": "2103.01863v1-Figure3-1.png",
    "caption": " Analysis on CNN and Daily Mail article-summary pairs for multi-document setup. Plot shows the statistics of number of samples whose summary spans n documents.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the percentage of CNN article-summary pairs whose summary spans 1 document?",
    "answer": "42%",
    "rationale": "The figure shows that there are 15000 CNN article-summary pairs whose summary spans 1 document, and a total of 35000 CNN article-summary pairs. Therefore, the percentage of CNN article-summary pairs whose summary spans 1 document is 15000/35000 = 42%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.01863v1",
    "pdf_url": null
  },
  {
    "instance_id": "603ab8d268674df3b94ad4777fdc2c9f",
    "figure_id": "2106.06134v4-Figure1-1",
    "image_file": "2106.06134v4-Figure1-1.png",
    "caption": " A heterophilous graph on which GCN achieves perfect performance.",
    "figure_type": "schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the minimum number of nodes that must be removed from the graph in order to disconnect it?",
    "answer": "Two nodes.",
    "rationale": "The graph is a bipartite graph with two connected components. In order to disconnect the graph, we must remove at least one node from each component.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.06134v4",
    "pdf_url": null
  },
  {
    "instance_id": "c168836b8df04ffba7c7704940a89057",
    "figure_id": "1905.10881v3-Figure3-1",
    "image_file": "1905.10881v3-Figure3-1.png",
    "caption": " Recalls based on one-step LPs and one-step DNLPs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest recall after 30 steps?",
    "answer": "Amazon.",
    "rationale": "The blue line representing Amazon has the highest value on the y-axis (recall) at the 30th step on the x-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10881v3",
    "pdf_url": null
  },
  {
    "instance_id": "d6b78c379e364b2c9b3badb0aea5af63",
    "figure_id": "2210.13076v1-Figure3-1",
    "image_file": "2210.13076v1-Figure3-1.png",
    "caption": " Examples of the predicted bounding box in REC. The green and orange boxes indicate the ground truth and predicted boxes, respectively. The images are from RefCOCO+ while the texts are constructed.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the ground truth bounding box for the \"person farthest from the woman with glasses\"?",
    "answer": "Image (q)",
    "rationale": "The image (q) shows a group of people sitting at a table. The woman with glasses is seated on the left side of the table. The person farthest from her is seated on the right side of the table, and their bounding box is indicated in green.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.13076v1",
    "pdf_url": null
  },
  {
    "instance_id": "6e00ee865da5480f8c3c4e15680ed364",
    "figure_id": "1905.10488v5-Figure9-1",
    "image_file": "1905.10488v5-Figure9-1.png",
    "caption": " Denoising results on the synthetic noise images.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most effective at removing Gaussian noise with a sigma of 50?",
    "answer": "BM3D",
    "rationale": "The PSNR of the image denoised with BM3D is the highest (22.98) among all the methods for Gaussian noise with a sigma of 50.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10488v5",
    "pdf_url": null
  },
  {
    "instance_id": "cb034187c873493daffa2e0eb254fdbc",
    "figure_id": "1910.13724v2-Figure4-1",
    "image_file": "1910.13724v2-Figure4-1.png",
    "caption": " In the proposed sampling strategy, half of the pairs contain background noise class.",
    "figure_type": "** table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the probability of sampling a pair that contains two background noise events? ",
    "answer": " 0.25 ",
    "rationale": " The figure shows that the sampling probability for each type of pair is 0.25. The pair type \"Background noise, Background noise\" refers to a pair that contains two background noise events. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.13724v2",
    "pdf_url": null
  },
  {
    "instance_id": "2a698b8d8a014e229c367f8e62052745",
    "figure_id": "2304.04321v2-Figure16-1",
    "image_file": "2304.04321v2-Figure16-1.png",
    "caption": " A toy example of the user interface (UI) for collecting human annotations.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the task type being annotated in the figure?",
    "answer": "The task type being annotated is \"open_draw\".",
    "rationale": "The figure shows a screenshot of the ARNOLD Labeling tool, which is used to collect human annotations for tasks. The \"Task type\" field in the top left corner of the figure shows that the task type being annotated is \"open_draw\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.04321v2",
    "pdf_url": null
  },
  {
    "instance_id": "c479a4534f164754b6537a6928ae4cb2",
    "figure_id": "1809.05793v2-Figure3-1",
    "image_file": "1809.05793v2-Figure3-1.png",
    "caption": " Average runtime per epoch.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which platform generally has the fastest average runtime per epoch?",
    "answer": "PyTorch",
    "rationale": "In all three datasets (N-MNIST, DVS-CIFAR10, and CIFAR10), the blue bars representing PyTorch are consistently shorter than the green bars representing Matlab, indicating a faster runtime.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.05793v2",
    "pdf_url": null
  },
  {
    "instance_id": "580b17684ec247af962ff9314366d6cd",
    "figure_id": "2206.01162v2-Figure4-1",
    "image_file": "2206.01162v2-Figure4-1.png",
    "caption": " (a)-(b) compares the average cumulative reward return achieved by the proposed KSRL (shown in blue) algorithm with MPC-PSRL (Fan and Ming 2021), SAC (Haarnoja et al. 2018), and DDPG (Barth-Maron et al. 2018) for Reacher with and without oracle rewards respectively. (c)-(d) compares the model-complexity. We note that KSRL is able to achieve the maximum average reward at-par with the current state of the art MPC-PSRL with drastically reduced model complexity. Solid curves represent the average across five trials (seeds), shaded areas correspond to the standard deviation amongst the trials",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the highest average reward in the Reacher environment with oracle rewards?",
    "answer": "MPC-PSRL",
    "rationale": "Figure (a) shows the average cumulative reward return for different algorithms in the Reacher environment with oracle rewards. The MPC-PSRL curve is the highest, indicating that it achieves the highest average reward.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.01162v2",
    "pdf_url": null
  },
  {
    "instance_id": "8efdbd8003334ffa853662ae2c9f18ca",
    "figure_id": "1901.02393v2-Figure5-1",
    "image_file": "1901.02393v2-Figure5-1.png",
    "caption": " Importance of considering ∆ > 1. Below these x labels is the cost of fairness ratio. We report the balance for the three largest clusters and include the dotted line at 0.8 because we use δ = 0.2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the cost of fairness ratio for the \"default\" group in the \"bank\" dataset?",
    "answer": "1.03",
    "rationale": "The cost of fairness ratio is shown below the x-axis labels. For the \"default\" group in the \"bank\" dataset, the cost of fairness ratio is 1.03.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.02393v2",
    "pdf_url": null
  },
  {
    "instance_id": "a26f473b71bb4f83af8c2714159d3343",
    "figure_id": "2305.10006v2-Figure1-1",
    "image_file": "2305.10006v2-Figure1-1.png",
    "caption": " Comparison of reconstruction quality (average PSNR in dB on 6 benchmark grayscale datasets) and testing time of several SOTA deep learning based algorithms. Our proposed EfficientSCI achieves higher reconstruction quality with fewer parameters and shorter testing time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the best reconstruction quality?",
    "answer": "EfficientSCI-L",
    "rationale": "The figure shows that EfficientSCI-L has the highest PSNR value, which indicates the best reconstruction quality.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.10006v2",
    "pdf_url": null
  },
  {
    "instance_id": "da61ed227eef4f4a962c98dc03f06366",
    "figure_id": "2203.09693v2-Figure7-1",
    "image_file": "2203.09693v2-Figure7-1.png",
    "caption": " Quantitative comparisons of the performance of Power, TPower and PPower according to the Cosine Similarity for the FashionMNIST dataset and the phase retrieval model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best according to the Cosine Similarity metric?",
    "answer": "PPower",
    "rationale": "The figure shows that PPower has the highest Cosine Similarity values for all values of m.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.09693v2",
    "pdf_url": null
  },
  {
    "instance_id": "28ec7fb1f06c43d1bfd0190f72145072",
    "figure_id": "2104.11832v2-Figure4-1",
    "image_file": "2104.11832v2-Figure4-1.png",
    "caption": " The lottery ticket results of LXMERT on VQA, GQA, and NLVR2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method of pruning appears to be the most effective across all three datasets?",
    "answer": "IMP on Finetuning.",
    "rationale": "The IMP on Finetuning method (magenta line) consistently achieves the highest accuracy across all three datasets (VQA, GQA, and NLVR2) for all sparsity levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.11832v2",
    "pdf_url": null
  },
  {
    "instance_id": "63c266eb37a64ad9a8b0b5fbf412d84f",
    "figure_id": "2209.03592v2-Figure4-1",
    "image_file": "2209.03592v2-Figure4-1.png",
    "caption": " The illustration of spatial attention masks on Character A3 module, BPE A3 module and WordPiece A3 module, respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attention module focuses on individual characters in the input text?",
    "answer": "The Character A3 module.",
    "rationale": "The figure shows the spatial attention masks for three different attention modules: Character A3, BPE A3, and WordPiece A3. The Character A3 module focuses on individual characters, while the BPE A3 and WordPiece A3 modules focus on subwords and words, respectively. This can be seen in the figure by looking at the areas of the image that are highlighted in each attention mask.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.03592v2",
    "pdf_url": null
  },
  {
    "instance_id": "3cb5feead5b6409fb1943d8c03d466c4",
    "figure_id": "2210.02088v1-Figure6-1",
    "image_file": "2210.02088v1-Figure6-1.png",
    "caption": " Visualization of Semantic Segmentation on Cityscapes with different frameworks. Framework WSSS-UDA can achieve results close to UDA methods with fine source labels. However, the results of framework TDOD-WSSS are rough in detail and have many wrong segmentations. Best viewed in color.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which framework produces the most accurate segmentation results?",
    "answer": "E-IAST (fine label)",
    "rationale": "The figure shows that the E-IAST (fine label) framework produces segmentation results that are closest to the ground truth. This can be seen by comparing the segmentation results of each framework to the ground truth image. The E-IAST (fine label) framework correctly segments most of the objects in the image, while the other frameworks produce segmentation results that are less accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.02088v1",
    "pdf_url": null
  },
  {
    "instance_id": "5f14698e2526434e85997829b0d14160",
    "figure_id": "2211.06651v2-Figure7-1",
    "image_file": "2211.06651v2-Figure7-1.png",
    "caption": " Vocabulary overlap on paper abstracts based on the Jaccard metric on lemmas of the datasets in NLPEER.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two datasets have the highest vocabulary overlap?",
    "answer": "ARR-22 and COLING-20.",
    "rationale": "The figure shows the pairwise Jaccard metric for vocabulary overlap on paper abstracts between different datasets in NLPEER. The highest value in the figure is 1.000, which occurs between ARR-22 and COLING-20. This indicates that these two datasets have the highest vocabulary overlap.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.06651v2",
    "pdf_url": null
  },
  {
    "instance_id": "19fe45edb30c40f6b2728ac733f9efac",
    "figure_id": "2210.12485v1-Figure5-1",
    "image_file": "2210.12485v1-Figure5-1.png",
    "caption": " Comparison between HET and DANLI of relative success trajectory path lengths against groundtruth human trajectory lengths. The red dashed line denotes the same length as human trajectories, thus area to its left corresponds to the proportion of model predictions that achieved better efficiency than human performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has a higher proportion of predictions that are more efficient than human performance?",
    "answer": "DANLI",
    "rationale": "The red dashed line denotes the same length as human trajectories, thus the area to its left corresponds to the proportion of model predictions that achieved better efficiency than human performance. The area under the DANLI curve to the left of the red dashed line is greater than the area under the HET curve to the left of the red dashed line. This indicates that DANLI has a higher proportion of predictions that are more efficient than human performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12485v1",
    "pdf_url": null
  },
  {
    "instance_id": "7455da9b2fe14ebbb73c18ac28e2b0a6",
    "figure_id": "2303.08289v1-Figure1-1",
    "image_file": "2303.08289v1-Figure1-1.png",
    "caption": " The figure on the left shows nearness of a feature vector z to the weight vector of the ground truth. The figure on the right depicts how the resulting feature vector of the corresponding adversarial example, z′, is pushed towards the weight vector of wrong class y′ 6= y, thereby increasing the angle between the feature vector of the adversarial example and the weight vector of the true label.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which figure shows the feature vector of the adversarial example?",
    "answer": "The figure on the right.",
    "rationale": "The caption states that the figure on the right depicts the resulting feature vector of the corresponding adversarial example, z′.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.08289v1",
    "pdf_url": null
  },
  {
    "instance_id": "404a66e29d0b474f87ca8f214cbccb08",
    "figure_id": "2209.06794v4-Figure7-1",
    "image_file": "2209.06794v4-Figure7-1.png",
    "caption": " The distribution of ages recognized from the sampled images of WebLI.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common age group recognized from the sampled images of WebLI?",
    "answer": "18-35 years old.",
    "rationale": "The bar corresponding to the 18-35 age group is the highest, indicating that this age group is the most frequently recognized.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.06794v4",
    "pdf_url": null
  },
  {
    "instance_id": "b2c7918f13084d9fae95e5010c24d090",
    "figure_id": "1904.04971v3-Figure2-1",
    "image_file": "1904.04971v3-Figure2-1.png",
    "caption": " On ImageNet validation, increasing the number of experts per layer of our CondConvMobileNetV1 models improves performance relative to inference cost compared to the MobileNetV1 frontier [38] across a spectrum of model sizes. Models with more experts per layer achieve monotonically higher accuracy. We train CondConv models with {1, 2, 4, 8, 16, 32} experts at width multipliers {0.25, 0.50, 0.75, 1.0}.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture achieves the highest accuracy with the lowest computational cost?",
    "answer": "CondConv-MobileNetV1 (0.25x) with 32 experts.",
    "rationale": "The figure shows the trade-off between Top-1 accuracy and multiply adds (a measure of computational cost) for different model architectures. The CondConv-MobileNetV1 (0.25x) with 32 experts model achieves the highest accuracy with the lowest computational cost. This is evident from its position in the upper left corner of the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.04971v3",
    "pdf_url": null
  },
  {
    "instance_id": "a12066b5dcae445c9d08b44132d86c42",
    "figure_id": "1905.12962v3-Figure1-1",
    "image_file": "1905.12962v3-Figure1-1.png",
    "caption": " Results for synthetic experiment showing model recovery of structure of positive examples for low-sparsity data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has a higher error, the symmetric or the non-symmetric model?",
    "answer": "The non-symmetric model has a higher error.",
    "rationale": "The color scale in the figure shows that the error of the non-symmetric model is higher than the error of the symmetric model. The non-symmetric model has a wider range of colors, indicating a wider range of errors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12962v3",
    "pdf_url": null
  },
  {
    "instance_id": "b44bc5c87ca44254bca18713e80cf651",
    "figure_id": "2201.12546v2-Figure2-1",
    "image_file": "2201.12546v2-Figure2-1.png",
    "caption": " The ACC (%) with the number of learned tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the first task?",
    "answer": "The SI method.",
    "rationale": "The SI method has the highest ACC (%) on the first task, as shown in the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.12546v2",
    "pdf_url": null
  },
  {
    "instance_id": "7e492e4ecd85407fb69eb3fc26467b30",
    "figure_id": "2206.03674v3-Figure21-1",
    "image_file": "2206.03674v3-Figure21-1.png",
    "caption": " Training curves for (Left) 28× 28 and (Right) 50× 50.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of successful rate for the 28x28 training data?",
    "answer": "SymGPPP",
    "rationale": "The figure on the left shows the successful rate for each model over the training epochs for the 28x28 data. The SymGPPP model has the highest successful rate at the end of the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.03674v3",
    "pdf_url": null
  },
  {
    "instance_id": "3c9b056cfa394a55b411fc4daa7945e7",
    "figure_id": "2105.01993v1-Figure3-1",
    "image_file": "2105.01993v1-Figure3-1.png",
    "caption": " Answer manifold embedding of is this question type.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces a more compact embedding of the answer manifold?",
    "answer": "Our method.",
    "rationale": "The figure shows that the embeddings produced by our method are more tightly clustered than the embeddings produced by the baseline method. This suggests that our method is better at capturing the underlying structure of the data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.01993v1",
    "pdf_url": null
  },
  {
    "instance_id": "dc53949465b34325bccc48a7bd9c4ec0",
    "figure_id": "2005.13334v1-Figure3-1",
    "image_file": "2005.13334v1-Figure3-1.png",
    "caption": " Transitions available in a top-down transition system (NT-X = Non-Terminal-X).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three transitions available in a top-down transition system?",
    "answer": "The three transitions are Shift, NT-X, and Reduce.",
    "rationale": "The figure shows the three transitions and their corresponding actions. The Shift transition moves the input symbol to the stack. The NT-X transition replaces the non-terminal X on the stack with its corresponding production rule. The Reduce transition replaces the right-hand side of a production rule on the stack with the left-hand side.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.13334v1",
    "pdf_url": null
  },
  {
    "instance_id": "db4bcb340fc746ad9cc4f03f80abf96e",
    "figure_id": "2206.12104v1-Figure2-1",
    "image_file": "2206.12104v1-Figure2-1.png",
    "caption": " Debiasing GAT with explanations given by REFEREE with two different backbones and other baselines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest accuracy when the sampled node ratio is 10%?",
    "answer": "Att",
    "rationale": "The figure shows that the Att model has the highest accuracy when the sampled node ratio is 10%. This can be seen by looking at the (c) Changes of accuracy subfigure and following the lines to where the x-axis is at 10%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.12104v1",
    "pdf_url": null
  },
  {
    "instance_id": "f57b361b010b459da9a579532b818cf7",
    "figure_id": "2204.11171v1-Figure1-1",
    "image_file": "2204.11171v1-Figure1-1.png",
    "caption": " Illustration of a special tournament graph",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between sets A and B in the figure?",
    "answer": "Set A is a subset of set B.",
    "rationale": "The figure shows that all elements of set A are also elements of set B. This is because all the arrows point from elements in set A to elements in set B.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.11171v1",
    "pdf_url": null
  },
  {
    "instance_id": "cd741dbc2e4242828274e905e366632f",
    "figure_id": "2305.18201v1-Figure3-1",
    "image_file": "2305.18201v1-Figure3-1.png",
    "caption": " Pairwise automatic metric correlation.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric has the highest correlation with self-BLEU?",
    "answer": "Rankgen.",
    "rationale": "The correlation coefficient between self-BLEU and Rankgen is 0.75, which is the highest among all the pairs of metrics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.18201v1",
    "pdf_url": null
  },
  {
    "instance_id": "b9fb7983c5eb45aa8f92d96ce9f6a846",
    "figure_id": "2304.05939v1-Figure3-1",
    "image_file": "2304.05939v1-Figure3-1.png",
    "caption": " Here, we present the qualitative results of reconstructions and generations from the proposed method and relevant compared methods on CelebA256 dataset. We observe lower blurriness and higher sharpness for the reconstructed images from the proposed method.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method produces the sharpest reconstructions?",
    "answer": " The proposed method.",
    "rationale": " The reconstructions produced by the proposed method are sharper and less blurry than those produced by the L2 and FFL methods. This can be seen in the figure, where the reconstructions from the proposed method are more detailed and have less noise than the reconstructions from the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.05939v1",
    "pdf_url": null
  },
  {
    "instance_id": "82612ec22fcf4b58b8f93aeefa1cb4c4",
    "figure_id": "1907.03197v1-Figure6-1",
    "image_file": "1907.03197v1-Figure6-1.png",
    "caption": " Average improvement of Local Search over LP-based algorithm for constructing coresets as a function of k.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the largest improvement of Local Search over LP-based algorithm for constructing coresets?",
    "answer": "MNIST-10",
    "rationale": "The plot shows the ratio of solutions found by Local Search to the solutions found by LP-based algorithm for different values of k. The MNIST-10 curve is consistently higher than the other two curves, indicating that Local Search finds more solutions than LP-based algorithm for this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.03197v1",
    "pdf_url": null
  },
  {
    "instance_id": "e281e43eda794c9bb6a5648f4ee2d3df",
    "figure_id": "2201.01221v2-Figure10-1",
    "image_file": "2201.01221v2-Figure10-1.png",
    "caption": " Performance evaluation using MAAC with different types of critics",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which critic performed the best on the Cooperative Treasure Collection task?",
    "answer": "SHC",
    "rationale": "The SHC line is the highest in the Cooperative Treasure Collection plot, indicating that it achieved the highest mean test return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.01221v2",
    "pdf_url": null
  },
  {
    "instance_id": "c90ef2407b1445faa23511cd09d38566",
    "figure_id": "2006.15055v2-Figure16-1",
    "image_file": "2006.15055v2-Figure16-1.png",
    "caption": " Offset in the attention maps or the denominator of the weighted mean for object discovery on CLEVR6 (left) and property prediction on CLEVR10 (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better for object discovery on CLEVR6?",
    "answer": "Epsilon.",
    "rationale": "The figure shows that Epsilon has a higher ARI (Adjusted Rand Index) than Attention and Weighted Mean for object discovery on CLEVR6.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.15055v2",
    "pdf_url": null
  },
  {
    "instance_id": "15e08c4355d742909e1c89fa8d87cf8a",
    "figure_id": "2211.01866v1-Figure11-1",
    "image_file": "2211.01866v1-Figure11-1.png",
    "caption": " ImageNet-X Annotations Examples illustrates three annotation examples from the ImageNet validation set. In panel, A. we see an example of background and pose for Artichoke. In panel B., we see the effect of an object being small and in panel C. we see color and pattern as distinctive varying factors. We provide the full set of annotations with their image paths in our repo.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following images best illustrates the effect of an object being small?",
    "answer": "The image of the sea lion in panel B.",
    "rationale": "The image in panel B shows a sea lion that is relatively small in comparison to the frame. This makes it difficult to identify the sea lion without additional context.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.01866v1",
    "pdf_url": null
  },
  {
    "instance_id": "0054ee0f3f2a49b1aa2a3a3972615c63",
    "figure_id": "2204.04875v2-Figure4-1",
    "image_file": "2204.04875v2-Figure4-1.png",
    "caption": " Results on Dirichlet data for 30 ≤ N ≤ 80. Hamming distanceH between predicted and ground-truth adjacency matrices, averaged over 128 sampled graphs. CSIvA significantly outperforms DCDI and ENCO, both of which are very strong baselines. The difference in performance increases with N .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best for ER-1 graphs with N=80?",
    "answer": "CSIvA",
    "rationale": "The bar for CSIvA is the shortest for ER-1 graphs with N=80, which means that it has the lowest Hamming distance and therefore performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.04875v2",
    "pdf_url": null
  },
  {
    "instance_id": "c42e3451dc92405bb2feef51fbc65611",
    "figure_id": "2305.06984v3-Figure5-1",
    "image_file": "2305.06984v3-Figure5-1.png",
    "caption": " Accuracy of several open-domain QA models on CuratedTREC 2002, computed via regex matching, along with the results of three evaluation mechanisms. Purple points represent the EM accuracy, and green points depict accuracy achieved via BEM, InstructGPT-eval, and human judgment. Classic statistical models from TREC QA 2002 are shown as orange stars. InstructGPT (few shot) outperforms the best of these classic models only under human assessment.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which evaluation mechanism yields the highest accuracy for DPR?",
    "answer": "BEM.",
    "rationale": "The figure shows the accuracy of several open-domain QA models on CuratedTREC 2002, computed via regex matching, along with the results of three evaluation mechanisms. For DPR, the BEM evaluation mechanism yields the highest accuracy, as shown by the green point at 54.0.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.06984v3",
    "pdf_url": null
  },
  {
    "instance_id": "77c0ce314bfa4142b7c64f492ae7b641",
    "figure_id": "1910.10274v1-Figure4-1",
    "image_file": "1910.10274v1-Figure4-1.png",
    "caption": " Qualitative analysis of attention vector. The intensity of the color (red) denotes the strength of the attention weights.",
    "figure_type": "\"other\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What year was the first French version of the scriptures prepared?",
    "answer": "1294",
    "rationale": "The figure shows a passage of text with highlighted phrases. The first highlighted phrase is \"1294,\" and the second highlighted phrase is \"a French version of the scriptures.\" The passage also states that the French version of the scriptures was prepared by a Roman Catholic priest named Guyard de Moulin.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.10274v1",
    "pdf_url": null
  },
  {
    "instance_id": "e5b4dcb46e7e41b686e6073253dff893",
    "figure_id": "2111.11297v2-Figure8-1",
    "image_file": "2111.11297v2-Figure8-1.png",
    "caption": " Performance across LDA topics",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which topic had the highest F1 score?",
    "answer": "Topic 1",
    "rationale": "The figure shows the F1 score for each topic, and topic 1 has the highest bar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.11297v2",
    "pdf_url": null
  },
  {
    "instance_id": "5e285bb6291a41928770f022f78dad4b",
    "figure_id": "2112.07435v1-Figure5-1",
    "image_file": "2112.07435v1-Figure5-1.png",
    "caption": " Situation before player j deviates (with t = 3).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the load on player j before they deviate?",
    "answer": "M + 1",
    "rationale": "The figure shows that player j has a load of M + 1 before they deviate. This is shown by the fact that the bar for player j is taller than the bars for the other players, and the label \"load M + 1\" is below the bar for player j.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.07435v1",
    "pdf_url": null
  },
  {
    "instance_id": "ea30b514621748abb8bce2c2f96d4241",
    "figure_id": "2210.09943v2-Figure10-1",
    "image_file": "2210.09943v2-Figure10-1.png",
    "caption": " Replication of Figure 7 on the CelebA validation dataset with the Disparity in accuracy metric.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the lowest disparity in accuracy between male and female faces?",
    "answer": "swin_base",
    "rationale": "The swin_base model is represented by the red dot in the bottom left corner of the plot. This dot is the closest to the horizontal line at y = 0, which represents zero disparity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.09943v2",
    "pdf_url": null
  },
  {
    "instance_id": "9293f92361be40668e89c6bd3964cb14",
    "figure_id": "1906.04423v4-Figure2-1",
    "image_file": "1906.04423v4-Figure2-1.png",
    "caption": "Figure 2 – Performance of reward during the proxy task, which has been growing throughout the process, indicating that the model of reinforcement learning works.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the performance of the reward change over time?",
    "answer": "The performance of the reward increases over time.",
    "rationale": "The plot shows that the reward increases over time, which indicates that the model is learning.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04423v4",
    "pdf_url": null
  },
  {
    "instance_id": "f5f6f36024394a529ca6ade21a79f960",
    "figure_id": "2306.01977v1-Figure2-1",
    "image_file": "2306.01977v1-Figure2-1.png",
    "caption": " Descriptive Alert Report Screenshots. Visualizes the feature importance, quantiles, distribution and model traffic as examples, with sensitive information removed.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the Feature Importance Histogram, which feature is the most important for the model?",
    "answer": "The feature named \"V11\".",
    "rationale": "The Feature Importance Histogram shows the importance of each feature for the model. The height of each bar represents the importance of the corresponding feature. The bar for the feature named \"V11\" is the tallest, which means that this feature is the most important for the model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01977v1",
    "pdf_url": null
  },
  {
    "instance_id": "6c25224ef6934feebbabe28d394a6d8a",
    "figure_id": "1903.03785v1-Figure7-1",
    "image_file": "1903.03785v1-Figure7-1.png",
    "caption": " Accuracy results for head shape estimation, as cumulative error distributions of the normalized dense vertex errors. a) accuracy results based on the fitted facial meshes to rendered images, b) accuracy results based on the actual ground truth 3D facial meshes. Tables 1 and 2 report additional measures.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the best accuracy for head shape estimation based on the fitted facial meshes to rendered images?",
    "answer": "CFHM-reg",
    "rationale": "The figure shows the cumulative error distributions of the normalized dense vertex errors for different methods. CFHM-reg has the highest curve, which means that it has the lowest error and therefore the best accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.03785v1",
    "pdf_url": null
  },
  {
    "instance_id": "af56fb08913b4763b373cbd110a7ac47",
    "figure_id": "1806.05421v5-Figure12-1",
    "image_file": "1806.05421v5-Figure12-1.png",
    "caption": " First layer neuron importance after learning the second task sorted in descending order according to the first task neuron importance (orange), superimposed on top of figure 11. Left: SNID, Right: SLNID. SLNID allows new neurons to become active and be used for the new task. SNID penalizes all unimportant neurons equally and hence more neurons are re-used then initiated for the first time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm, SNID or SLNID, allows new neurons to become active and be used for the new task?",
    "answer": "SLNID",
    "rationale": "The caption states that \"SLNID allows new neurons to become active and be used for the new task.\" This is also evident in the right plot where there are more blue dots (new neurons) compared to the left plot (SNID).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.05421v5",
    "pdf_url": null
  },
  {
    "instance_id": "f045a2487c804eb0b3e7bc6129c1fd0f",
    "figure_id": "2107.04867v1-Figure3-1",
    "image_file": "2107.04867v1-Figure3-1.png",
    "caption": " Correspondence accuracy for 12 categories in the KeypointNet dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the airplane category?",
    "answer": "Ours",
    "rationale": "The plot for the airplane category shows that the red line (\"Ours\") is higher than the other lines at all Euclidean distances, indicating that our method has the highest correspondence accuracy for this category.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.04867v1",
    "pdf_url": null
  },
  {
    "instance_id": "be18d8df2ba941e1a7822ef749900942",
    "figure_id": "2003.13194v1-Figure5-1",
    "image_file": "2003.13194v1-Figure5-1.png",
    "caption": " The distribution of length of density-ascending path on CIFAR10 (a) and CIFAR100 (b).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has a higher frequency of density-ascending paths with a length of 10?",
    "answer": "CIFAR10.",
    "rationale": "The frequency of density-ascending paths with a length of 10 is approximately 0.14 in CIFAR10 (red bars) and approximately 0.10 in CIFAR100 (blue bars).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.13194v1",
    "pdf_url": null
  },
  {
    "instance_id": "ad9204a12894445d9bb874e382a22645",
    "figure_id": "2110.02456v1-Figure5-1",
    "image_file": "2110.02456v1-Figure5-1.png",
    "caption": " Partition of [0, 1]d into 1/k̃ hypercubes via arrangement of d(k̃ − 1) hyperplanes, where d = 2 and k̃ = 3. Shaded region is [0, 1]d. Dotted region is a cell of the hyperplane arrangement.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many hyperplanes are needed to partition the unit square into 1/3 hypercubes?",
    "answer": "4",
    "rationale": "The caption states that d(k̃ − 1) hyperplanes are needed, where d is the dimension of the space and k̃ is the number of hypercubes per side. In this case, d = 2 and k̃ = 3, so 2(3 - 1) = 4 hyperplanes are needed. The figure shows the four hyperplanes as the four black lines that divide the unit square into nine smaller squares.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02456v1",
    "pdf_url": null
  },
  {
    "instance_id": "9656b1f3d300413bb3798a0d0c3df1f7",
    "figure_id": "1911.08453v1-Figure3-1",
    "image_file": "1911.08453v1-Figure3-1.png",
    "caption": " Comparisons on two vision-based domains that evaluate temporally extended control, with illustrations of the tasks. In 2D Navigation (left), the goal is to navigate around a U-shaped wall to reach the goal. In the Push and Reach manipulation task (right), a robot must first push a puck to a target location (blue star), which may require moving the hand away from the goal hand location, and then move the hand to another location (red star). Curves are averaged over multiple seeds and shaded regions represent one standard deviation. Our method, shown in red, outperforms prior methods on both tasks. On the Push and Reach task, prior methods typically get the hand close to the right location, but perform much worse at moving the puck, indicating an overly greedy strategy, while our approach succeeds at both.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task is more difficult for the robot, 2D Navigation or Push and Reach?",
    "answer": "Push and Reach is more difficult for the robot.",
    "rationale": "The plots show that the final distance to the goal is consistently higher for the Push and Reach task than for the 2D Navigation task. This indicates that the robot is having more difficulty completing the Push and Reach task. Additionally, the caption mentions that prior methods typically get the hand close to the right location in the Push and Reach task, but perform much worse at moving the puck, indicating an overly greedy strategy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.08453v1",
    "pdf_url": null
  },
  {
    "instance_id": "6a8bdc96720d484e82a26a6a49987f2d",
    "figure_id": "1812.05720v2-Figure9-1",
    "image_file": "1812.05720v2-Figure9-1.png",
    "caption": " ROC curves of the CIFAR-10 models on the evaluation datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the best performance on the CIFAR-10 dataset?",
    "answer": "ACET.",
    "rationale": "The ROC curve for ACET is the closest to the top left corner, which indicates that it has the highest true positive rate and the lowest false positive rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.05720v2",
    "pdf_url": null
  },
  {
    "instance_id": "ba50777e9f474af9bf67b088b0455477",
    "figure_id": "2004.09995v3-Figure3-1",
    "image_file": "2004.09995v3-Figure3-1.png",
    "caption": " Evaluation of LSA-3DMM against peer methods: PCA, COMA, and Spiral on test sets.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method performs best on the DFAUST dataset when the latent dimension is 64? ",
    "answer": " The proposed method.",
    "rationale": " The figure shows the L2 error for different methods on the DFAUST and COMA datasets. The x-axis represents the latent dimension, and the y-axis represents the L2 error. The proposed method has the lowest L2 error for all latent dimensions on both datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.09995v3",
    "pdf_url": null
  },
  {
    "instance_id": "bd11faff6b344df19c07ba06351da4ea",
    "figure_id": "1907.08059v3-Figure1-1",
    "image_file": "1907.08059v3-Figure1-1.png",
    "caption": " Federated model: (1) Leaf nodes (L) independently compute local updates asynchronously, (2) The subspace updates are propagated upwards to aggregator nodes (A), (3) The process is repeated recursively until the root node is reached, (4) FPCA returns the global PCA estimate.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many leaf nodes are there in the federated model?",
    "answer": "There are 16 leaf nodes in the federated model.",
    "rationale": "The figure shows that there are 4 leaf nodes at each level of the tree, and there are 4 levels in the tree. 4 * 4 = 16.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.08059v3",
    "pdf_url": null
  },
  {
    "instance_id": "1e042e71b6ab47d683997c4548307e3e",
    "figure_id": "2312.01473v1-Figure7-1",
    "image_file": "2312.01473v1-Figure7-1.png",
    "caption": " Highest RaIR value throughout free play for pure RaIR, RaIR + CEE-US, CEE-US, RND and Dis.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the highest RaIR value during free play?",
    "answer": "Pure RaIR",
    "rationale": "The plot shows the RaIR value over free play iterations for different algorithms. The purple line, which represents pure RaIR, is consistently higher than the other lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2312.01473v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee6ffbc95b63460986cb89a3240b1b8b",
    "figure_id": "2010.15261v1-Figure2-1",
    "image_file": "2010.15261v1-Figure2-1.png",
    "caption": " A qualitative comparison corresponding to our inter-dataset experiment in the last two columns of Table 1. The source shape is from the test set of FAUST, but the target shape is from SCAPE. Our method does not require any postprocessing but still yields the best results.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method yields the best results in the inter-dataset experiment?",
    "answer": "Our method",
    "rationale": "The figure shows that our method produces the most accurate reconstruction of the target shape, which is from SCAPE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.15261v1",
    "pdf_url": null
  },
  {
    "instance_id": "0aebeac804cf45f1b69cc54cde360383",
    "figure_id": "2205.13320v2-Figure11-1",
    "image_file": "2205.13320v2-Figure11-1.png",
    "caption": " Best normalized function value of GRIEWANK ROSENBROCK with std, averaged over 100 runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization strategy performs the best on the GRIEWANK ROSENBROCK function?",
    "answer": "Eagle Strategy.",
    "rationale": "The figure shows the best normalized function value of the GRIEWANK ROSENBROCK function for different optimization strategies. The Eagle Strategy consistently achieves the highest normalized function value, indicating that it performs the best on this function.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.13320v2",
    "pdf_url": null
  },
  {
    "instance_id": "76229f67ba444db98ff4e31263571946",
    "figure_id": "1809.02736v1-Figure12-1",
    "image_file": "1809.02736v1-Figure12-1.png",
    "caption": " This graph shows the average rate savings of our model compared to other codecs. Larger values imply a larger savings, e.g., on average, our method saves 8.41% over BPG (4:4:4) and 35.52% over JPEG2000 (based on the OpenJPEG implementation). These scores are calculated from the RD graph for PSNR on Kodak (see Figure 7) over the shared PSNR range of 27.1–39.9 dB.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which codec does our method save the most rate over?",
    "answer": "JPEG (4:2:0)",
    "rationale": "The bar corresponding to JPEG (4:2:0) is the longest, indicating the largest rate savings.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.02736v1",
    "pdf_url": null
  },
  {
    "instance_id": "b6a2b35308544693ae15727764138986",
    "figure_id": "2106.03471v1-Figure4-1",
    "image_file": "2106.03471v1-Figure4-1.png",
    "caption": " Relative importance of tokens with respect to word class in the GECO dataset. Relative importance is measured as relative fixation duration for humans (top) and as relative gradient-based saliency in the BERT model (bottom). This is the same figure as Figure 2 in the paper but it includes the number of instances per word class on top of the respective bar.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which word class is most important to humans according to the figure?",
    "answer": "Nouns.",
    "rationale": "The figure shows the relative importance of different word classes according to human fixation duration and model saliency. The bar for nouns is the highest in the human fixation subplot, indicating that humans fixate on nouns more than any other word class.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03471v1",
    "pdf_url": null
  },
  {
    "instance_id": "424e8216863a4bcba91e295a04ee7fa7",
    "figure_id": "2101.06262v1-Figure4-1",
    "image_file": "2101.06262v1-Figure4-1.png",
    "caption": " One of the splits of the Movielens 100K dataset. We can see that for small ranks the Fast Local Search solution is better and more stable, but for larger ranks it does not provide any improvement over the Fast Greedy algorithm.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better for small ranks, according to the figure?",
    "answer": "Fast Local Search.",
    "rationale": "The figure shows that the Fast Local Search algorithm has a lower test error than the Fast Greedy algorithm for small ranks. This can be seen in the figure where the green dashed line representing Fast Local Search is below the blue solid line representing Fast Greedy for ranks less than approximately 40.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.06262v1",
    "pdf_url": null
  },
  {
    "instance_id": "3700f105270f47fe8edac6bd2ce55dbe",
    "figure_id": "2207.14288v1-Figure3-1",
    "image_file": "2207.14288v1-Figure3-1.png",
    "caption": " Reconstructing an image with a drastic warp edit. We reconstruct a model-generated sample that is warped drastically by (a) projecting the edited image into the extended latent space proposed by Abdal et al. [2020] or (b) updating the generative model weights directly. The latent projection method can only produce an image with bigger eyes, whereas the weight update method correctly matches the target edits. The latent code from the source sample is used to initialize GAN projection.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is better at matching the target edits, latent projection or weight update?",
    "answer": "Weight update.",
    "rationale": "The figure shows that the latent projection method can only produce an image with bigger eyes, whereas the weight update method correctly matches the target edits. This is because the weight update method directly updates the generative model weights to match the target edits, whereas the latent projection method projects the edited image into the extended latent space, which can only produce limited edits.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.14288v1",
    "pdf_url": null
  },
  {
    "instance_id": "a63b72cd3cf94eefa93448365652224c",
    "figure_id": "2101.07136v1-Figure4-1",
    "image_file": "2101.07136v1-Figure4-1.png",
    "caption": " Overview of Result Plots. On the left, validation time in seconds for each data set. Comparison of continuous behavior of best approach with the baseline. In the middle, answer traces showing the incremental generation of validation results. On the right, diefficiency at time t. A higher value means a steadier answer production.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which engine has the highest validation time for data set D25?",
    "answer": "Engine SHACL259PARQL.",
    "rationale": "The box plot for engine SHACL259PARQL in data set D25 has the highest upper whisker, indicating that it has the highest validation time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.07136v1",
    "pdf_url": null
  },
  {
    "instance_id": "52184fa5c4704c29a77a8faa1cab2bdc",
    "figure_id": "1905.00561v1-Figure4-1",
    "image_file": "1905.00561v1-Figure4-1.png",
    "caption": " Top-1 accuracy on Kinetics when pre-training on different number of labels. Note that the source datasets used in panels (a) and (b) are different, hence the results are not comparable. X-axis is log scale.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does using a larger number of pre-training labels always result in higher Top-1 accuracy?",
    "answer": "No.",
    "rationale": "The figure shows that for the \"I-Verb + noun\" label space, the Top-1 accuracy remains relatively constant as the number of pre-training labels increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.00561v1",
    "pdf_url": null
  },
  {
    "instance_id": "f06b439fc96149e99e923f650d5d826e",
    "figure_id": "2106.07504v3-Figure10-1",
    "image_file": "2106.07504v3-Figure10-1.png",
    "caption": " Fidelity of the fairwashed logistic regression models that are 50% less unfair than the black-box models they are explaining. Results (averaged over 10 fairwashing attacks) are shown for all datasets, black-box models and fairness metrics. The content of each cell is in the form of xy , in which x represents the fidelity of the fairwashed explanation model, and y its percentage change with respect to the fidelity of the unconstrained explainer, used here as a baseline.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which black-box model has the highest fidelity for the COMPAS dataset according to the EOpp metric?",
    "answer": "XgBoost",
    "rationale": "The figure shows the fidelity of the fairwashed logistic regression models for different black-box models and fairness metrics. For the COMPAS dataset and the EOpp metric, the XgBoost model has the highest fidelity of 93.02.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07504v3",
    "pdf_url": null
  },
  {
    "instance_id": "4cead6946c3c4a7da4d19daaf017834a",
    "figure_id": "2207.11122v1-Figure2-1",
    "image_file": "2207.11122v1-Figure2-1.png",
    "caption": " Container CPU usage in percentage at a peak time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which service has the highest average CPU usage?",
    "answer": "Service B.",
    "rationale": "The histogram for Service B is shifted further to the right than the histograms for Services A and C, indicating that the average CPU usage for Service B is higher than for the other two services.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.11122v1",
    "pdf_url": null
  },
  {
    "instance_id": "f3801d5ee2e344d19cab2a027514b823",
    "figure_id": "2102.10543v2-Figure29-1",
    "image_file": "2102.10543v2-Figure29-1.png",
    "caption": " Sketch map of latent space of generative models.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between semantic directions A and B?",
    "answer": "Semantic directions A and B are disentangled.",
    "rationale": "The figure shows that the two semantic directions are orthogonal to each other. This means that they are independent and do not affect each other. The note at the bottom of the figure also explicitly states that A and B are disentangled.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.10543v2",
    "pdf_url": null
  },
  {
    "instance_id": "4c1890b0a63f4ee7995cbbc2e6856ef9",
    "figure_id": "2001.06826v2-Figure4-1",
    "image_file": "2001.06826v2-Figure4-1.png",
    "caption": " Ablation study of the contribution of each loss (spatial consistency loss Lspa, exposure control loss Lexp, color constancy loss Lcol, illumination smoothness loss LtvA ).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss term contributes the most to the image quality?",
    "answer": "The spatial consistency loss Lspa.",
    "rationale": "The image without Lspa is the most distorted, with the buildings and people appearing stretched and warped. The other images without the other loss terms are still recognizable, although they may have some artifacts or color issues.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.06826v2",
    "pdf_url": null
  },
  {
    "instance_id": "3753f6ccaf8c4b01a1e4c296ad2aa18c",
    "figure_id": "2303.03729v3-Figure1-1",
    "image_file": "2303.03729v3-Figure1-1.png",
    "caption": " There are some actions that are hard to recognize because the skeleton representations lack important interactive objects and contexts, which make them easily confused with each other.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which action is most likely to be confused with typing on a keyboard?",
    "answer": "Reading.",
    "rationale": "The figure shows that the skeleton representations for typing on a keyboard and reading are very similar. Both actions involve the person standing in a similar position with their arms outstretched.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.03729v3",
    "pdf_url": null
  },
  {
    "instance_id": "b066a8d4edc04c449ff533a835c821cc",
    "figure_id": "2005.08139v1-Figure2-1",
    "image_file": "2005.08139v1-Figure2-1.png",
    "caption": " The average numbers of 3D points per car (left) and per scene (right). We only include points within the frontal-view camera view and cars whose depths are within 70 meters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the most LiDAR points per car?",
    "answer": "Waymo",
    "rationale": "The bar for Waymo in the left plot is the highest, indicating that it has the most LiDAR points per car.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.08139v1",
    "pdf_url": null
  },
  {
    "instance_id": "61a9ee26cdf34554890316a2f6a6aff4",
    "figure_id": "2302.06229v1-Figure3-1",
    "image_file": "2302.06229v1-Figure3-1.png",
    "caption": " Comparison between the importance given by each model to a symmetric (in green) and antisymmetric (in blue) relation.1",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model gives the highest importance to the member of domain usage relation?",
    "answer": "TransE",
    "rationale": "The figure shows the attention value of each model for two types of relations: member of domain usage (blue bars) and derivationally related form (green bars). The TransE model has the highest blue bar, indicating that it gives the highest importance to the member of domain usage relation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.06229v1",
    "pdf_url": null
  },
  {
    "instance_id": "29f007ccc33442b8b0e682437bbdfb6d",
    "figure_id": "2012.04718v2-Figure3-1",
    "image_file": "2012.04718v2-Figure3-1.png",
    "caption": " Auto-encoding / qualitative – Example decomposition results using Canonical Capsules on the test set, with the aligned setup; we color each decomposition (capsule) with a unique color. For 3D-PointCapsNet [64] and AtlasNetV2 [12], rather than capsules, these correspond to “patches” in the reconstruction network. Our method clearly provides the best qualitative results.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method provides the best qualitative results according to the caption?",
    "answer": "Our method.",
    "rationale": "The caption explicitly states that \"Our method clearly provides the best qualitative results.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.04718v2",
    "pdf_url": null
  },
  {
    "instance_id": "17949d2a9efb4dd6be83e1d80b45dfac",
    "figure_id": "2106.01300v2-Figure7-1",
    "image_file": "2106.01300v2-Figure7-1.png",
    "caption": " Intra-list average distance of news recommended by different methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of intra-list average distance?",
    "answer": "PP-Rec",
    "rationale": "The figure shows that PP-Rec has the highest ILAD values for all Top K values. This means that the items recommended by PP-Rec are more similar to each other than the items recommended by other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01300v2",
    "pdf_url": null
  },
  {
    "instance_id": "a294a7ba67f44b9d98d8114a3f3e0d01",
    "figure_id": "2307.14630v1-Figure6-1",
    "image_file": "2307.14630v1-Figure6-1.png",
    "caption": " Comparing BBox tracking performances of different trackers in terms of dual success rate and angle precision rate under the three distinct attributes of 360VOT.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tracker performs best in terms of success rate and angle precision rate for all three attributes?",
    "answer": "AiATrack-360",
    "rationale": "The figure shows the success rate and angle precision rate for different trackers under three different attributes. AiATrack-360 consistently achieves the highest success rate and angle precision rate across all three attributes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.14630v1",
    "pdf_url": null
  },
  {
    "instance_id": "201ab76858e14a3fadfde9a47182bbe2",
    "figure_id": "2002.04326v3-Figure1-1",
    "image_file": "2002.04326v3-Figure1-1.png",
    "caption": " Performance comparison of state-of-the-art models and humans (graduate students) on EASY and HARD set of ReClor testing set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the EASY set?",
    "answer": "GPT and GPT-2",
    "rationale": "The figure shows the accuracy of different models on the EASY and HARD sets. GPT and GPT-2 achieve the highest accuracy on the EASY set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.04326v3",
    "pdf_url": null
  },
  {
    "instance_id": "ce6698ab97024404a2cdfd4f7be4c5e0",
    "figure_id": "2305.09489v1-Figure3-1",
    "image_file": "2305.09489v1-Figure3-1.png",
    "caption": " Trio infilling scenario: The top plot shows an original validation sample. The central 512 tokens (blue box) were masked out in all tracks, and filled in again by SCHmUBERT (bottom). The tracks are color-coded (red=melody, blue=bass, black=drum). Bass and drum tracks lie in similar pitch regions of the piano roll due to standard MIDI drum encoding.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which track is the melody track?",
    "answer": "The red track is the melody track.",
    "rationale": "The caption states that the tracks are color-coded, and that red represents the melody track.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.09489v1",
    "pdf_url": null
  },
  {
    "instance_id": "cd03099dba294d5baefc2c110f14ab95",
    "figure_id": "2212.06826v1-Figure8-1",
    "image_file": "2212.06826v1-Figure8-1.png",
    "caption": " Visualization of the query frame, memory readout features of STCN [15], XMem [13], our method, and the instanceaware features of the highest resolution P2 from the pixel-decoder.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods, STCN, XMem, Ours, or Pix-dec Feature, best captures the details of the query frame?",
    "answer": "Our method.",
    "rationale": "The memory readout features of our method are more similar to the query frame than the other methods. This is evident in the details of the bike and the rider in the first row, and the people and the dog in the second row.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.06826v1",
    "pdf_url": null
  },
  {
    "instance_id": "2a3ca9a7f5ae4b8582371786eaa9ab35",
    "figure_id": "1904.08242v2-Figure13-1",
    "image_file": "1904.08242v2-Figure13-1.png",
    "caption": " Trajectory plots of GICP, CLS, LOAM, LO-Net and LO-Net+Mapping on KITTI odometry Seq. 06, 07, 09, 10. The ground truth trajectories are available. The results of Seq. 08 are shown in Section 4.2 of our paper.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is the most accurate on KITTI odometry Seq. 06, 07, 09, 10?",
    "answer": "LO-Net+Mapping",
    "rationale": "The trajectory plots show that the LO-Net+Mapping algorithm most closely follows the ground truth trajectory.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.08242v2",
    "pdf_url": null
  },
  {
    "instance_id": "df8ceb8ec7744c898a20bbcf54599855",
    "figure_id": "2310.18526v1-Figure1-1",
    "image_file": "2310.18526v1-Figure1-1.png",
    "caption": " Deletion curves for the MNIST dataset. Larger DEL− is better since it indicates a method finds more negative impact training samples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most robust to data removal?",
    "answer": "The \"tracking\" method.",
    "rationale": "The \"tracking\" method has the highest DEL- score, which means that it is the most robust to data removal.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.18526v1",
    "pdf_url": null
  },
  {
    "instance_id": "6f9e0b5d0b904e1ea4eb2a009cba1fce",
    "figure_id": "1906.03548v2-Figure4-1",
    "image_file": "1906.03548v2-Figure4-1.png",
    "caption": " The complementary effects of Inference Example Weighing (Sec. 3.1) and Ghost Batch Normalization (Sec. 3.2) on CIFAR-100, SVHN, and Caltech-256.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset and ghost batch size combination achieves the highest accuracy?",
    "answer": "SVHN with a ghost batch size of 16.",
    "rationale": "The figure shows the accuracy and loss for different ghost batch sizes on three datasets. The SVHN dataset with a ghost batch size of 16 has the highest accuracy, as shown by the blue line in the top right plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.03548v2",
    "pdf_url": null
  },
  {
    "instance_id": "9ffbb24d4e264f5583db021e8768dc55",
    "figure_id": "1810.05751v2-Figure6-1",
    "image_file": "1810.05751v2-Figure6-1.png",
    "caption": " Transfer performance for the Quadruped example (a) and the Soft-foot Hopper example (b).",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in the Quadruped example?",
    "answer": "SO-CMA.",
    "rationale": "The figure shows the average return for each algorithm in the Quadruped example. The SO-CMA algorithm has the highest average return, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.05751v2",
    "pdf_url": null
  },
  {
    "instance_id": "98eb27fbc5dd4baab991b94e4d12cf96",
    "figure_id": "2302.05549v2-Figure1-1",
    "image_file": "2302.05549v2-Figure1-1.png",
    "caption": " Comparison of weighting methods in bias, variance, balance, and weight stability in three data generating processes (DGP). Each DGP creates 100 simulations with sample size as 100,000. DistEB and DistMS consistently outperform IPW and CBPS. The upper-left panel shows the comparison in biasmeasured by absolutemean bias across all simulations. The upper-right panel shows variance measured by standard deviation. The lower-left and lower-right panel show balance measured by seven balance metrics and weight stability measured by four metrics, respectively. The dotted line is 1% for absolute mean bias and standard deviation and recommended threshold in literature for balance. Y-axis is sqrt transformed for better visualization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which weighting method performs best in terms of bias and variance across all three data generating processes?",
    "answer": "DistEB and DistMS.",
    "rationale": "The upper-left panel of the figure shows the absolute mean bias for each weighting method across all three data generating processes. DistEB and DistMS have the lowest absolute mean bias for all three DGPs. The upper-right panel of the figure shows the standard deviation for each weighting method across all three data generating processes. Again, DistEB and DistMS have the lowest standard deviation for all three DGPs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.05549v2",
    "pdf_url": null
  },
  {
    "instance_id": "8c19eedd1a484ac6b8855e7d95466bf9",
    "figure_id": "2110.03141v2-Figure8-1",
    "image_file": "2110.03141v2-Figure8-1.png",
    "caption": " Cross-entropy loss landscapes of the ResNet18 model respect to adversarial weight perturbations on the CIFAR10 dataset trained with SGD, SAM, and ESAM.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which optimizer results in the smoothest loss landscape?",
    "answer": " ESAM",
    "rationale": " The figure shows the loss landscapes for three different optimizers: SGD, SAM, and ESAM. The loss landscape for ESAM is the smoothest, with fewer sharp peaks and valleys than the other two optimizers. This suggests that ESAM is less likely to get stuck in local minima and more likely to find a global minimum.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.03141v2",
    "pdf_url": null
  },
  {
    "instance_id": "7832c9472f4a4945917419aa6498f5f1",
    "figure_id": "1903.11367v2-Figure5-1",
    "image_file": "1903.11367v2-Figure5-1.png",
    "caption": " Distribution of countries where reviewers work.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which country has the most reviewers?",
    "answer": "The United States has the most reviewers.",
    "rationale": "The bar for the United States is the tallest in the bar chart, indicating that it has the most reviewers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.11367v2",
    "pdf_url": null
  },
  {
    "instance_id": "c23b5b1f31fb4a48afdc4906c95172b4",
    "figure_id": "2306.01650v1-Figure7-1",
    "image_file": "2306.01650v1-Figure7-1.png",
    "caption": " Revert rate vs. anonymous users rate across languages.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Is there a positive or negative correlation between the anonymous user rate and the revert rate?",
    "answer": "Positive.",
    "rationale": "The plot shows a positive linear trend between the two variables, which means that as the anonymous user rate increases, the revert rate also increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01650v1",
    "pdf_url": null
  },
  {
    "instance_id": "942a56281de048f6a38185b07c6f1694",
    "figure_id": "1808.09401v2-Figure4-1",
    "image_file": "1808.09401v2-Figure4-1.png",
    "caption": " On the left, the confusion matrix of C-TLM (Lτ ), and on the right of TL2RTL (Lτce), on TE3‡ for the top-5 most-frequent TLinks (together 95% of data): BEFORE (B), AFTER (A), IS INCLUDED (II), INCLUDES (I), and SIMULTANEOUS (S). Predictions are shown on the x-axis and ground-truth on the y-axis.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of TLink is most likely to be confused with the \"AFTER\" TLink?",
    "answer": "The \"IS INCLUDED\" TLink.",
    "rationale": "The confusion matrix on the right shows that 11.1% of the \"AFTER\" TLinks were classified as \"IS INCLUDED\" TLinks, which is the highest percentage of misclassified \"AFTER\" TLinks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.09401v2",
    "pdf_url": null
  },
  {
    "instance_id": "03e03dec84bc4d2d8a3145e473bab036",
    "figure_id": "2203.00089v1-Figure20-1",
    "image_file": "2203.00089v1-Figure20-1.png",
    "caption": " WideResNet 28-10 on CIFAR-10, using Adam. The shaded regions show the min/max values over 4 random restarts.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer achieves the highest test accuracy on the CIFAR-10 dataset with WideResNet 28-10 architecture?",
    "answer": "Adam-APO",
    "rationale": "The figure shows the test accuracy of three different optimizers: Adam Fixed LR, Adam Decayed LR, and Adam-APO. The Adam-APO optimizer achieves the highest test accuracy of approximately 0.945.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.00089v1",
    "pdf_url": null
  },
  {
    "instance_id": "2b96fe6eab904e6c9d15a711d4bd37e5",
    "figure_id": "1911.01373v2-Figure2-1",
    "image_file": "1911.01373v2-Figure2-1.png",
    "caption": " Panels in the first row show trace plots, obtained by different schemes, across the last 2×104 sampling iterations for the most difficult to sample x100 dimension. The panels in the second row show the estimated values of the diagonal of L obtained by different adaptive schemes. Notice that the real Gaussian target has diagonal covariance matrix Σ = diag(s21, . . . , s 2 100) where si are uniform in the range [0.01, 1].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the adaptive schemes shown in the figure seems to have converged to the true diagonal covariance matrix?",
    "answer": "gadMALAf",
    "rationale": "The panels in the second row of the figure show the estimated values of the diagonal of the covariance matrix L obtained by different adaptive schemes. The true diagonal covariance matrix is Σ = diag(s21, . . . , s 2 100) where si are uniform in the range [0.01, 1]. The estimated values of the diagonal of L obtained by gadMALAf are the closest to the true values, indicating that gadMALAf has converged to the true diagonal covariance matrix.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.01373v2",
    "pdf_url": null
  },
  {
    "instance_id": "ce44402e1c7d41e6a03b63c6b74a1848",
    "figure_id": "2010.13369v1-Figure4-1",
    "image_file": "2010.13369v1-Figure4-1.png",
    "caption": " The L2 distance and cosine similarity of the input and output embeddings for BERT with PostLN and PreLN, at different layers and different steps. We plot the inverse of cosine similarity (arccosine) in degrees, so that for both L2 and arccosine, the lower the more similar.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In the image, at step = 2000, what happens to the L2 norm for PostLN as we move from layer ID 2 to layer ID 12?",
    "answer": "It decreases and then increases again.",
    "rationale": "The L2 norm is shown in the (c) and (d) subplots of the figure. We can see that at step = 2000, the purple dotted line, which represents the L2 norm for PostLN, initially decreases from layer ID 2 to layer ID 6, and then increases from layer ID 6 to layer ID 12.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.13369v1",
    "pdf_url": null
  },
  {
    "instance_id": "4cee956b5dbc469a946a8117091f335b",
    "figure_id": "2204.01668v2-Figure7-1",
    "image_file": "2204.01668v2-Figure7-1.png",
    "caption": " Effective sample size (ESS) per iteration and per second of S3 and the SOTA sampler for some of the datasets in Section 4. The ESS is calculated using one S3 chain of length 10000 iterations with a burn-in of 1000 iterations. The ESS per iteration of the SOTA sampler is omitted from the Left plot as it implements the same Gibbs sampler as S3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampler achieves the highest ESS per second on the Borovecki dataset?",
    "answer": "SOTA Probit",
    "rationale": "The right plot shows the ESS per second of each sampler on each dataset. The highest point for the Borovecki dataset is achieved by the SOTA Probit sampler.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.01668v2",
    "pdf_url": null
  },
  {
    "instance_id": "32ff5f3e71464a5ebf4e2c3b703ff882",
    "figure_id": "2106.15535v2-Figure10-1",
    "image_file": "2106.15535v2-Figure10-1.png",
    "caption": " Results on OGBN-Arxiv. Test accuracy disparity across subgroups by aggregated-feature distance. Each figure corresponds to a model. Bars labeled 1 to 20 represent subgroups with increasing distance to training set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the most consistent performance across all subgroups?",
    "answer": "GCN.",
    "rationale": "The figure shows that GCN has the highest accuracy for all subgroups, while GraphSAGE and MLP have lower accuracy for some subgroups.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15535v2",
    "pdf_url": null
  },
  {
    "instance_id": "9be6f0043e7d4bf48efdb590a4362e10",
    "figure_id": "1911.11120v1-Figure1-1",
    "image_file": "1911.11120v1-Figure1-1.png",
    "caption": " Visualization of the operation Ψ X .",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the value of the element at row 2, column 3 of the resulting matrix after the operation Ψ ⊙ X is performed?",
    "answer": "0.3",
    "rationale": "The operation Ψ ⊙ X is element-wise multiplication. The element at row 2, column 3 of Ψ is 0.5, and the element at row 2, column 3 of X is 0.6. Multiplying these two values gives 0.5 × 0.6 = 0.3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.11120v1",
    "pdf_url": null
  },
  {
    "instance_id": "dfc9db3e473849748d5dbe46f97c3816",
    "figure_id": "1812.02132v3-Figure30-1",
    "image_file": "1812.02132v3-Figure30-1.png",
    "caption": " Case 2, Scenario 1, Qualitative Examples: generated by BBGAN",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many cars are there in the image?",
    "answer": "16.",
    "rationale": "The image shows a 4x4 grid of images, each containing a blue car.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.02132v3",
    "pdf_url": null
  },
  {
    "instance_id": "733a013e975a4c20948970c40fb35b06",
    "figure_id": "2305.20081v2-Figure4-1",
    "image_file": "2305.20081v2-Figure4-1.png",
    "caption": " Performance of different number of actions used in EAS. The experiments are conducted on the nine locomotion tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three tasks seems to benefit the most from increasing the number of actions used in EAS?",
    "answer": "walker2d",
    "rationale": "The plot for walker2d shows the largest increase in normalized return as the number of actions increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.20081v2",
    "pdf_url": null
  },
  {
    "instance_id": "e1e1f0a4b33a425eb2a63833cf3a39af",
    "figure_id": "2301.04604v2-Figure12-1",
    "image_file": "2301.04604v2-Figure12-1.png",
    "caption": " Qualitative results of StyleSpace [53] when manipulating on different regions along with corresponding heatmaps.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which region of the face is most important for StyleSpace to manipulate when it is trying to change the expression of a person's face?",
    "answer": " The mouth.",
    "rationale": " The heatmaps in the figure show that StyleSpace focuses on the mouth region when it is manipulating the expression of a person's face. This is because the mouth is the most expressive part of the face and is responsible for conveying a wide range of emotions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.04604v2",
    "pdf_url": null
  },
  {
    "instance_id": "10272fb913ec42e886cdceb9691a0ede",
    "figure_id": "2106.05445v2-Figure5-1",
    "image_file": "2106.05445v2-Figure5-1.png",
    "caption": " Training error (first two plots) and test error (last two plots) in terms of number of passes over dataset and runtime for Orange dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of training error?",
    "answer": "BFGS",
    "rationale": "The first two plots show the training error in terms of number of passes over the dataset and runtime. BFGS has the lowest training error in both plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05445v2",
    "pdf_url": null
  },
  {
    "instance_id": "095a716bfa72463d9769dc9a0231ff34",
    "figure_id": "1906.01605v1-Figure1-1",
    "image_file": "1906.01605v1-Figure1-1.png",
    "caption": " Neural Projection Skip-gram (NP-SG) model",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the NP-SG model is responsible for learning the transferable representations of words?",
    "answer": "The hidden layers 1 and 2.",
    "rationale": "The figure shows that the hidden layers 1 and 2 are connected to the context window, which contains the words surrounding the target word. This allows the hidden layers to learn the representations of words based on their context. These representations are transferable because they can be used for other tasks, such as sentiment analysis or machine translation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.01605v1",
    "pdf_url": null
  },
  {
    "instance_id": "559a9e0e37c1489d866dd51a12d8d791",
    "figure_id": "1902.07104v3-Figure1-1",
    "image_file": "1902.07104v3-Figure1-1.png",
    "caption": " Concepts have different visual and semantic feature space. (Left) Some categories may have similar visual features and dissimilar semantic features. (Right) Other can possess same semantic label but very distinct visual features. Our method adaptively exploits both modalities to improve classification performance in low-shot regime.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following pairs of objects in the image have similar visual features but different semantic features?",
    "answer": "The ping-pong ball and the egg.",
    "rationale": "The ping-pong ball and the egg are both white and spherical, but they are different objects with different functions. The ping-pong ball is used for playing ping-pong, while the egg is a food item.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.07104v3",
    "pdf_url": null
  },
  {
    "instance_id": "807cdb8da34f467e939113e33ee5bad3",
    "figure_id": "1806.01811v8-Figure6-1",
    "image_file": "1806.01811v8-Figure6-1.png",
    "caption": " The performance of SGD and AdaGrad-Norm in presence of momentum (see Algorithm 3). In each plot, the y-axis is train or test accuracy and x-axis is b0. Left 6 plots are for CIFAR10 using ResNet-18 with disabling learnable parameter in Batch-Norm. Right 6 plots are for CIFAR10 using ResNet-18 with default BatchNorm. The points in the plot are the average of epoch 6-10, epoch 41-45 and epoch 86-90, respectively. The title is the last epoch of the average. Better read on screen.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performs better on CIFAR10 with ResNet-18 with default BatchNorm at epoch 45?",
    "answer": "AdaGrad_Norm",
    "rationale": "The figure shows that at epoch 45, the red line (AdaGrad_Norm) is higher than the black line (SGD_Constant) for both train and test accuracy. This means that AdaGrad_Norm achieves better accuracy than SGD_Constant on CIFAR10 with ResNet-18 with default BatchNorm at epoch 45.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.01811v8",
    "pdf_url": null
  },
  {
    "instance_id": "a1546cf3fd9943a598a27ec6df3a074c",
    "figure_id": "2211.14366v1-Figure3-1",
    "image_file": "2211.14366v1-Figure3-1.png",
    "caption": " Re-Simulation error over Number of Inferences. We highlight our MMN method with the solid, black line, while baseline methods are shown with dashed lines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest re-simulation error?",
    "answer": "MMN",
    "rationale": "The MMN method is represented by the solid black line in each of the four plots. This line is consistently lower than all of the other lines, indicating that the MMN method has the lowest re-simulation error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.14366v1",
    "pdf_url": null
  },
  {
    "instance_id": "5a0cc12d38ba4e8fa53853ee77cdf50e",
    "figure_id": "2109.04404v1-Figure1-1",
    "image_file": "2109.04404v1-Figure1-1.png",
    "caption": " Relative contribution of each dimension to cosine similarity, CC( f i `), (top) paired with its relative influence on model behavior, I(i, `, f ) (bottom). The top and bottom portions of the plots each have 768 bars, one for each dimension in layer 12. The width of the bars corresponds to their relative contribution to each metric. For example, three dimensions (yellow, red, light yellow) dominate cosine similarity in GPT-2, but when we trace those dimensions to the bottom half of the plot, they appear to vanish, meaning their relative influence on model behavior is negligible. While this mismatch is less pronounced for BERT, it is particularly extreme in XLNet, where a single dimension dominates cosine similarity, but is effectively meaningless to the pretraining objective.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the most extreme mismatch between cosine similarity and model behavior?",
    "answer": "XLNet.",
    "rationale": "The figure shows that for XLNet, a single dimension dominates cosine similarity, but this dimension has almost no influence on model behavior. This is a much more extreme mismatch than is seen for the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.04404v1",
    "pdf_url": null
  },
  {
    "instance_id": "bbca6ccfdf84472997fdaac8e0220e75",
    "figure_id": "2012.06979v1-Figure75-1",
    "image_file": "2012.06979v1-Figure75-1.png",
    "caption": " Choices of ψ: PCMAC (k = 1). Top: full experiment. Bottom: Zoom in.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which normalization method results in the lowest mutual information gap?",
    "answer": "NORM-INF",
    "rationale": "The figure shows the mutual information gap for different normalization methods. The NORM-INF line is the lowest of all the lines, indicating that it has the lowest mutual information gap.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.06979v1",
    "pdf_url": null
  },
  {
    "instance_id": "8a15db2706d64c73b19e2f867f806aa3",
    "figure_id": "2107.08391v2-Figure1-1",
    "image_file": "2107.08391v2-Figure1-1.png",
    "caption": " A tiny version of the overall Axial Shifted MLP (AS-MLP) architecture.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the output size of Stage 1 of the AS-MLP architecture?",
    "answer": "48 × H/4 × W/4",
    "rationale": "Stage 1 of the AS-MLP architecture takes an input image of size 3 × H × W and outputs a feature map of size 48 × H/4 × W/4. This can be seen in the first stage of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.08391v2",
    "pdf_url": null
  },
  {
    "instance_id": "e4f0ecd2600847a183bb41cd8eaff4f8",
    "figure_id": "1805.05588v1-Figure1-1",
    "image_file": "1805.05588v1-Figure1-1.png",
    "caption": " A sentence from the ATIS dataset. REs can be used to detect the intent and label slots.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the intent of the sentence \"flights from Boston to Miami\"?",
    "answer": "The intent of the sentence is \"flight\".",
    "rationale": "The figure shows that the sentence \"flights from Boston to Miami\" is tagged with the intent label \"flight\". This means that the speaker is looking for information about flights.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.05588v1",
    "pdf_url": null
  },
  {
    "instance_id": "551bab315bb141fca7b6823161edc3c8",
    "figure_id": "2206.12933v3-Figure4-1",
    "image_file": "2206.12933v3-Figure4-1.png",
    "caption": " Downstream tasks performance versus varied augmentation magnitude β in training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the most consistent performance across different values of β?",
    "answer": "CS.",
    "rationale": "The figure shows that the CS dataset has a relatively flat line, indicating that the performance is not significantly affected by the value of β.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.12933v3",
    "pdf_url": null
  },
  {
    "instance_id": "0b6aeac5f1cb4b378659b3ea04d5a18c",
    "figure_id": "1910.12906v2-Figure6-1",
    "image_file": "1910.12906v2-Figure6-1.png",
    "caption": " Accuracy Analysis of STEP+Aug on E-Gait dataset: Classification results over the 3, 177 gaits. We observe > 75% accuracy for each class.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which emotion class is most accurately predicted by the model?",
    "answer": "Neutral",
    "rationale": "The confusion matrix shows that the model correctly predicted 84.92% of the neutral gaits, which is the highest accuracy for any class.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.12906v2",
    "pdf_url": null
  },
  {
    "instance_id": "43662e4dee784ec8b42054688daa4024",
    "figure_id": "2111.06961v1-Figure2-1",
    "image_file": "2111.06961v1-Figure2-1.png",
    "caption": " Loss vs. iterations in adversarial training. Each attack stage is at most 10 iterations and each defense stage is one iteration. The loss value after the defense step is in green.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of the defense step on the loss function value?",
    "answer": "The defense step reduces the loss function value.",
    "rationale": "The figure shows that the loss function value generally decreases after each defense step (green dots). This indicates that the defense step is effective in reducing the loss function value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.06961v1",
    "pdf_url": null
  },
  {
    "instance_id": "d600a5a26a23495d88378d5304e3d07c",
    "figure_id": "1909.03877v1-Figure5-1",
    "image_file": "1909.03877v1-Figure5-1.png",
    "caption": " (a) Recall-IoU and (b) AR-AN curve on ActivityNet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest recall when the IoU is 0.5?",
    "answer": "BSN",
    "rationale": "The figure shows the recall for different methods at different IoU thresholds. The BSN method has the highest recall when the IoU is 0.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.03877v1",
    "pdf_url": null
  },
  {
    "instance_id": "58847f57ae5542c2bdca8fcc9a36013a",
    "figure_id": "2006.06568v2-Figure5-1",
    "image_file": "2006.06568v2-Figure5-1.png",
    "caption": " Classification loss distribution of positive samples with different IoUs. Higher IoUs mean easier samples. Y-axis denotes the percentage of weighted loss. For example, percentage=20% at IoU=0.85 with SWN-Epoch12 means the the losses of samples whose IoUs fall between 0.8 and 0.9 take up 42% of total loss.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which model and epoch combination achieves the lowest percentage of loss for samples with IoU between 0.8 and 0.9?",
    "answer": "SWN-Epoch12",
    "rationale": "The plot shows the percentage of weighted loss for different IoU ranges. The SWN-Epoch12 curve has the lowest value for the 0.8 to 0.9 IoU range.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06568v2",
    "pdf_url": null
  },
  {
    "instance_id": "59c83ab87c6a4f4eab67239c74d8fab9",
    "figure_id": "2101.05068v2-Figure4-1",
    "image_file": "2101.05068v2-Figure4-1.png",
    "caption": " Can you match the captions to the images? In the COCO annotations, each of the four captions corresponds to (only) one of the four images (Answer: A:b,B:c,C:a,D:d ).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows a baseball player swinging a bat at a ball?",
    "answer": "Image C.",
    "rationale": "Image C shows a baseball player in the middle of his swing, with the bat making contact with the ball.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.05068v2",
    "pdf_url": null
  },
  {
    "instance_id": "5b68fa9a7cec420d9f768acab5b7a580",
    "figure_id": "2304.05387v2-Figure7-1",
    "image_file": "2304.05387v2-Figure7-1.png",
    "caption": " Recall analysis: Comparison of recall values of MOST, MOST+CAD with LOST and LOST+CAD. LOST generates one bounding box per image. MOST+CAD, MOST have higher recall and cover more ground-truth objects for a fixed set of boxes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest recall for a fixed number of boxes on the VOC 07 dataset?",
    "answer": "MOST+CAD",
    "rationale": "The figure shows that the MOST+CAD line is above all other lines for the VOC 07 dataset. This indicates that MOST+CAD has the highest recall for a fixed number of boxes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.05387v2",
    "pdf_url": null
  },
  {
    "instance_id": "bd16bd70380849b4871e042b90d6f9dd",
    "figure_id": "1911.07140v2-Figure11-1",
    "image_file": "1911.07140v2-Figure11-1.png",
    "caption": " One example of adversarial image for attacking Google Cloud Vision API",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image is most likely to be misclassified by the Google Cloud Vision API?",
    "answer": "Image (b)",
    "rationale": "Image (b) is an adversarial image, which means that it has been specifically designed to fool the Google Cloud Vision API. The image contains subtle changes that are imperceptible to the human eye but can cause the API to misclassify the image. In this case, the API is likely to misclassify the image as an \"illustration\" or \"art\" instead of a \"shark.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.07140v2",
    "pdf_url": null
  },
  {
    "instance_id": "2a874fc5330544af93d27caa0e85db80",
    "figure_id": "2302.10657v2-Figure2-1",
    "image_file": "2302.10657v2-Figure2-1.png",
    "caption": " Results on different microphone numbers. All results are on spatialized WSJ0-2MIX with SI-SDRi as metric except for the first column which is on WHAMR! with SDRi metric.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Spatialized WSJ0-2MIX dataset with 4 microphones?",
    "answer": "DasFormer Plus",
    "rationale": "The figure shows the SI-SDRi scores for different models on different datasets and microphone numbers. The DasFormer Plus model has the highest SI-SDRi score (26.7) on the Spatialized WSJ0-2MIX dataset with 4 microphones.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.10657v2",
    "pdf_url": null
  },
  {
    "instance_id": "60393376d4fd45c7af443b32e97daa15",
    "figure_id": "2303.15212v2-Figure5-1",
    "image_file": "2303.15212v2-Figure5-1.png",
    "caption": " Results for Transfer and Non-transfer methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which methods perform better on average, transfer or non-transfer methods?",
    "answer": "Transfer methods perform better on average.",
    "rationale": "The figure shows that the average rank of transfer methods is consistently lower than the average rank of non-transfer methods. This indicates that transfer methods are more effective than non-transfer methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.15212v2",
    "pdf_url": null
  },
  {
    "instance_id": "5a4c89a1d2e94894a6f9ceb3f25677bc",
    "figure_id": "1802.07828v1-Figure1-1",
    "image_file": "1802.07828v1-Figure1-1.png",
    "caption": " An illustration for SNMF. The red points stand for the anchors. All data of X , except for anchors, are denoted as blue points and are contained in the convex hull, generated by the anchors. After any random projection into 1-dimensional space, the geometric information is still partially preserved, where the anchors in the projected space are denoted as yellow points.",
    "figure_type": "** \nSchematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \nHow are the anchors represented in the figure? ",
    "answer": " \nThe anchors are represented by the red points.",
    "rationale": " \nThe caption states that \"the red points stand for the anchors.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1802.07828v1",
    "pdf_url": null
  },
  {
    "instance_id": "78b30bcd80364daa976832a3234200ba",
    "figure_id": "2208.06857v2-Figure8-1",
    "image_file": "2208.06857v2-Figure8-1.png",
    "caption": " Visual comparison of UIE networks with and without pre-trained URanker as additional supervision. Model+U represents the model is optimized by the pre-trained URanker loss.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model produces the most realistic and clear image?",
    "answer": "Ours+U",
    "rationale": "The image produced by Ours+U has the most vibrant colors and sharpest details, making it appear the most realistic and clear.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.06857v2",
    "pdf_url": null
  },
  {
    "instance_id": "6d72cc5b9c1147bca4017613a70d342f",
    "figure_id": "1905.00953v6-Figure7-1",
    "image_file": "1905.00953v6-Figure7-1.png",
    "caption": " Likelihoods on ground-truth attributes predicted by OSNet. Correct/incorrect classifications based on threshold 50% are shown in green/red.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the percentage of likelihood that the person in image (a) is wearing shorts?",
    "answer": "99.9%",
    "rationale": "The figure shows the person in image (a) is wearing shorts and the percentage of likelihood that they are wearing shorts is 99.9%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.00953v6",
    "pdf_url": null
  },
  {
    "instance_id": "d8bbdc75de504c1b8752d09c84ab0ce2",
    "figure_id": "1905.00780v4-Figure5-1",
    "image_file": "1905.00780v4-Figure5-1.png",
    "caption": " Comparison of different neural network saliency methods.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the saliency methods produces the most accurate heatmaps?",
    "answer": "FullGrad",
    "rationale": "The FullGrad heatmaps are the most accurate because they are the most similar to the ground truth heatmaps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.00780v4",
    "pdf_url": null
  },
  {
    "instance_id": "00aa4caf99034719a0e8ffea41783684",
    "figure_id": "2306.01364v1-Figure5-1",
    "image_file": "2306.01364v1-Figure5-1.png",
    "caption": " The spectral distributions of real images and fake images generated by different GANs before and after completion.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GAN model produces images with the most similar spectral distribution to real images?",
    "answer": "MMDGAN.",
    "rationale": "The figure shows the spectral distributions of real images and fake images generated by different GAN models. The MMDGAN curve is the closest to the CelebA curve, which represents the spectral distribution of real images. This suggests that MMDGAN produces images that are more similar to real images in terms of their spectral characteristics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01364v1",
    "pdf_url": null
  },
  {
    "instance_id": "46d1cfcc4e5a4f4ca78328e02118f11e",
    "figure_id": "1810.05751v2-Figure2-1",
    "image_file": "1810.05751v2-Figure2-1.png",
    "caption": " Transfer performance vs Sample number in target environment for the Hopper example. Policies are trained to transfer from DART to MuJoCo.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which policy performs best in the target environment when the dimensionality of the latent space is 2?",
    "answer": "SO-CMA",
    "rationale": "The figure shows the average return of different policies in the target environment as a function of the number of samples. When the dimensionality of the latent space is 2, the SO-CMA policy has the highest average return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.05751v2",
    "pdf_url": null
  },
  {
    "instance_id": "05e7c06559fb45b2b3288b1856b3b9ba",
    "figure_id": "2302.10429v2-Figure3-1",
    "image_file": "2302.10429v2-Figure3-1.png",
    "caption": " Heat maps for different dataset under heterogeneity weight equals to 0.6 for Dirichlet distribution.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the heterogeneity of CIFAR-10 compare to that of CIFAR-100?",
    "answer": "CIFAR-10 is more heterogeneous than CIFAR-100.",
    "rationale": "The heat map for CIFAR-10 shows a wider range of values, with more clients having a high degree of heterogeneity. In contrast, the heat map for CIFAR-100 shows a narrower range of values, with most clients having a low degree of heterogeneity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.10429v2",
    "pdf_url": null
  },
  {
    "instance_id": "3e77e22b24a5400b8e2492bf3ef60b8f",
    "figure_id": "2007.14430v3-Figure1-1",
    "image_file": "2007.14430v3-Figure1-1.png",
    "caption": " Left: Human-normalized mean scores. Right: Human-normalized median scores.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best according to the median score?",
    "answer": "M-IQN",
    "rationale": "The right plot shows the median scores for each algorithm. M-IQN has the highest median score of all the algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.14430v3",
    "pdf_url": null
  },
  {
    "instance_id": "f80e2db999ea4f8697f40bfde1bd2780",
    "figure_id": "2209.03695v3-Figure6-1",
    "image_file": "2209.03695v3-Figure6-1.png",
    "caption": " Three regimes and transitions between them, ConvNet on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which learning rate results in the fastest decrease in training loss?",
    "answer": "5e-4.",
    "rationale": "The red line, which represents the learning rate of 5e-4, shows the fastest decrease in training loss.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.03695v3",
    "pdf_url": null
  },
  {
    "instance_id": "32a1323fc0f94d0fbdacad2e9a2fa079",
    "figure_id": "2106.00305v2-Figure4-1",
    "image_file": "2106.00305v2-Figure4-1.png",
    "caption": " AO-Clevr results Plots of the seen and unseen accuracy and their harmonic mean (yaxis) as the ratio of unseen:seen compositional classes (x-axis) increases. ProtoProp consistently outperforms state-of-the-art methods, especially when the portion of unseen classes grows.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of harmonic performance?",
    "answer": "ProtoProp.",
    "rationale": "The figure shows that the harmonic performance of ProtoProp is consistently higher than that of the other methods, regardless of the ratio of unseen to seen compositional classes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.00305v2",
    "pdf_url": null
  },
  {
    "instance_id": "bcf5fa368f254c7da47b6284538d9124",
    "figure_id": "2106.05933v2-Figure63-1",
    "image_file": "2106.05933v2-Figure63-1.png",
    "caption": " Sparsity over layers for wav2vec-base finetuned for Italian it at 50% sparsity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer of the wav2vec-base model has the highest sparsity?",
    "answer": "Layer 11",
    "rationale": "The figure shows the sparsity of each layer of the wav2vec-base model. The x-axis represents the layer number, and the y-axis represents the sparsity percentage. The bar for layer 11 is the tallest, indicating that it has the highest sparsity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05933v2",
    "pdf_url": null
  },
  {
    "instance_id": "43524421bbbc49628a54cbd3537667c4",
    "figure_id": "2108.13655v2-Figure2-1",
    "image_file": "2108.13655v2-Figure2-1.png",
    "caption": " Comparison of different data augmentation methods, color printing is preferred. (a) augmentation with pretrained MLM (b) augmentation with MELM without linearization (c) augmentation with MELM",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods shown in the figure is most likely to produce the most accurate results?",
    "answer": "(c) augmentation with MELM",
    "rationale": "The figure shows three different methods for data augmentation, and the results of each method are shown in the bottom row. Method (c) is the only method that correctly identifies the entity \"European Union\" in the sentence, which suggests that it is the most accurate method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.13655v2",
    "pdf_url": null
  },
  {
    "instance_id": "c3b6cd582a4c444fa8c4bd5cc2db16d9",
    "figure_id": "2008.08882v2-Figure18-1",
    "image_file": "2008.08882v2-Figure18-1.png",
    "caption": " Test accuracy according to the learning layer in the body.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best on the miniImageNet dataset?",
    "answer": "ANIL",
    "rationale": "The figure shows the test accuracy of different methods on the miniImageNet dataset. The ANIL method has the highest accuracy on this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.08882v2",
    "pdf_url": null
  },
  {
    "instance_id": "5d9d75f356cc4100b37d79d9b43ff59d",
    "figure_id": "2106.04174v1-Figure1-1",
    "image_file": "2106.04174v1-Figure1-1.png",
    "caption": " Published papers as entity records.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is wrong with the table?",
    "answer": "The table is missing information.",
    "rationale": "The table has three missing values: one for the author of the first paper, one for the conference of the second paper, and one for the conference of the third paper.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04174v1",
    "pdf_url": null
  },
  {
    "instance_id": "c9c7c49742a9436893d366e58861cd41",
    "figure_id": "2106.03904v3-Figure9-1",
    "image_file": "2106.03904v3-Figure9-1.png",
    "caption": " Higher uncertainty around peaks 2 0 0",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between peak weeks and edge probabilities?",
    "answer": "Peak weeks have lower edge probabilities.",
    "rationale": "The figure shows that the edge probabilities are lower in the peak weeks (indicated by the white squares) than in the non-peak weeks (indicated by the red squares).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03904v3",
    "pdf_url": null
  },
  {
    "instance_id": "cfbe57e5e952483096e06aa2e1b80329",
    "figure_id": "2106.04399v2-Figure15-1",
    "image_file": "2106.04399v2-Figure15-1.png",
    "caption": " Number of diverse Bemis-Murcko scaffolds (Bemis and Murcko, 1996) found above reward threshold T as a function of the number of molecules seen. Left, T = 7.5. Right, T = 8.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm found the most diverse Bemis-Murcko scaffolds with a reward threshold of 8?",
    "answer": "GFlowNet",
    "rationale": "The figure shows that the GFlowNet line is above the other two lines at a reward threshold of 8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04399v2",
    "pdf_url": null
  },
  {
    "instance_id": "38edfe87ce46474ea9876428418e3911",
    "figure_id": "1910.14673v1-Figure15-1",
    "image_file": "1910.14673v1-Figure15-1.png",
    "caption": " Simple Super Resolution Task result from a 128×128 to a 1024×1024 image for a trained progressive GAN at 19k-th iteration. (a) Ground truth; (b) The result obtained by optimizing a single z; (c) The result obtained by optimizing the best z picked from 5,000 initializations; (d) Results of our algorithm.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which image shows the result of the proposed algorithm?",
    "answer": " Image (d)",
    "rationale": " The caption states that image (d) shows the results of the proposed algorithm. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.14673v1",
    "pdf_url": null
  },
  {
    "instance_id": "b93b47f051cc494ea045c3d53989295c",
    "figure_id": "2210.02019v1-Figure3-1",
    "image_file": "2210.02019v1-Figure3-1.png",
    "caption": " Fairness of the dataset. No statistically significant differences were found in either the accuracy, or the bias of the estimates across the low, medium and high performing algorithms.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which group of algorithms has the lowest median bias?",
    "answer": "The low performing algorithms.",
    "rationale": "The box plots show the distribution of bias for each group of algorithms. The median bias is represented by the horizontal line inside the box. The median bias for the low performing algorithms is lower than the median bias for the medium and high performing algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.02019v1",
    "pdf_url": null
  },
  {
    "instance_id": "de44636d413e4e74a7b75cc38bc6dc3b",
    "figure_id": "1907.03880v1-Figure2-1",
    "image_file": "1907.03880v1-Figure2-1.png",
    "caption": " Swarm scalability e(N1, N2, κ) for the 32×16 scenario. CRW swarms are the most parallelizable (i.e., proportional performance increases are likely at higher values of N ).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which swarm is the most parallelizable?",
    "answer": "CRW swarms",
    "rationale": "The CRW swarm line has the steepest slope, which means that it has the greatest increase in Karp-Flatt value for a given increase in swarm size. This indicates that CRW swarms are the most parallelizable.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.03880v1",
    "pdf_url": null
  },
  {
    "instance_id": "0b9f4185774c4c8996a9861aa7915ca7",
    "figure_id": "2010.08891v1-Figure8-1",
    "image_file": "2010.08891v1-Figure8-1.png",
    "caption": " Greedy performance for CartPole on different dataset sizes. (a) Top row: dataset size 10k (b) Bottom Row: dataset size 50k",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which parameter setting for the MDP build parameter k results in the highest average return for the greedy policy across all three dataset versions and dataset sizes?",
    "answer": "k = 51",
    "rationale": "The figure shows the average return for the greedy policy for different values of k, for three different dataset versions and two different dataset sizes. We can see that for all three dataset versions and both dataset sizes, k = 51 results in the highest average return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.08891v1",
    "pdf_url": null
  },
  {
    "instance_id": "db59740f2cde49949008166215ad025c",
    "figure_id": "2002.06189v2-Figure14-1",
    "image_file": "2002.06189v2-Figure14-1.png",
    "caption": " Dependence of the Lyapunov exponent on ε",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the Lyapunov exponent and ε?",
    "answer": "The Lyapunov exponent decreases as ε increases.",
    "rationale": "This can be seen in the figure, where the Lyapunov exponent is plotted against ε. As ε increases, the Lyapunov exponent decreases, indicating that the system becomes more stable.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.06189v2",
    "pdf_url": null
  },
  {
    "instance_id": "8c184b9a678d419398ea2fcfe1868cb1",
    "figure_id": "1803.09202v5-Figure2-1",
    "image_file": "1803.09202v5-Figure2-1.png",
    "caption": " Predicted (Y axis) versus Ground Truth (X axis) depth heatmaps for different models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model seems to perform the best at predicting depth?",
    "answer": "DepthNet + GAN",
    "rationale": "The DepthNet + GAN heatmap shows the closest relationship between the predicted and actual depth values. This is evident from the diagonal line that runs through the center of the heatmap.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1803.09202v5",
    "pdf_url": null
  },
  {
    "instance_id": "409c25158c6c40509a43874364146241",
    "figure_id": "2106.14747v1-Figure6-1",
    "image_file": "2106.14747v1-Figure6-1.png",
    "caption": " Visual results of different segmentation, saliency detection, co-saliency detection models and our OS-AD on the PAD dataset. OSAD can learn a better capability to perceive the affordance of objects, i.e., segmenting all objects that complete this purpose and suppressing object regions that are not related to affordance. “Container-2” refers to the affordance category that objects can fill liquid.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in segmenting the objects related to the \"Kick\" affordance?",
    "answer": "Our OS-AD model.",
    "rationale": "The figure shows the visual results of different models on the PAD dataset for the \"Kick\" affordance category. The ground truth (GT) shows the object regions that are related to the affordance. Comparing the results of different models, our OS-AD model produces the segmentation mask that is most similar to the ground truth, indicating that it performs the best in segmenting the objects related to the \"Kick\" affordance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.14747v1",
    "pdf_url": null
  },
  {
    "instance_id": "4d5f65a278f743a5bfbbe23e2bd9319e",
    "figure_id": "2210.12696v1-Figure12-1",
    "image_file": "2210.12696v1-Figure12-1.png",
    "caption": " Comparing Latent Concepts of Base models with themselves. X-axis = base model, Y-axis = fine-tuned model",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which base model has the most similar latent concepts to its fine-tuned version?",
    "answer": "ALBERT",
    "rationale": "The figure shows the similarity between the latent concepts of the base models and their fine-tuned versions. The diagonal of each plot shows the similarity of each model to itself. We can see that the diagonal of the ALBERT plot has the highest values, indicating that its latent concepts are most similar to its fine-tuned version.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12696v1",
    "pdf_url": null
  },
  {
    "instance_id": "e320af68e44040849baff2da32142578",
    "figure_id": "2305.14777v2-Figure4-1",
    "image_file": "2305.14777v2-Figure4-1.png",
    "caption": " Visualization of OT Map (x, T (x)) trained on clean Toy data. The comparison suggests that Fixed-µ and UOTM models are closer to the optimal transport map compared to OTM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is closest to the optimal transport map?",
    "answer": "The UOTM model.",
    "rationale": "The figure shows that the UOTM model has the most points that are close to the diagonal line, which represents the optimal transport map.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.14777v2",
    "pdf_url": null
  },
  {
    "instance_id": "dbbc3020bcb44d938b4f32c2d8aac011",
    "figure_id": "2102.11011v4-Figure1-1",
    "image_file": "2102.11011v4-Figure1-1.png",
    "caption": " Effective depth measures the depth of the unrolled network. For example, a network with 3 iterations of its single-layer recurrent module and 4 additional layers has an effective depth of 7.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effective depth of the network shown in the figure?",
    "answer": "7",
    "rationale": "The figure shows a network with 3 iterations of its single-layer recurrent module and 4 additional layers. According to the caption, the effective depth of a network is the depth of the unrolled network. Therefore, the effective depth of the network shown in the figure is 3 + 4 = 7.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.11011v4",
    "pdf_url": null
  },
  {
    "instance_id": "9b4fe595883a4e38b022a1e19d1f4da7",
    "figure_id": "2305.02556v1-Figure6-1",
    "image_file": "2305.02556v1-Figure6-1.png",
    "caption": " Dev results of different planning algorithms broken down by the length of the steps in the gold trees.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which planning algorithm performs best for long steps?",
    "answer": "Beam search",
    "rationale": "The figure shows that Beam search has the highest accuracy for steps of length 3 and >= 4.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.02556v1",
    "pdf_url": null
  },
  {
    "instance_id": "1331041fa147491b94287f97db66d5ec",
    "figure_id": "2104.02008v1-Figure1-1",
    "image_file": "2104.02008v1-Figure1-1.png",
    "caption": " 2-D t-SNE (Maaten & Hinton, 2008) visualization of the style statistics (concatenation of mean and standard deviation) computed from the first residual block’s feature maps of a ResNet-18 (He et al., 2016) trained on four distinct domains (Li et al., 2017). It is clear that different domains are well separated.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four domains is most similar to the cartoon domain?",
    "answer": "The sketch domain.",
    "rationale": "The cartoon and sketch domains are clustered close together in the t-SNE visualization, indicating that they share similar style statistics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.02008v1",
    "pdf_url": null
  },
  {
    "instance_id": "19ea28a6aaf24812a002f2fb9ad3e28a",
    "figure_id": "2108.13753v2-Figure10-1",
    "image_file": "2108.13753v2-Figure10-1.png",
    "caption": " Disentanglement scores before aggregating across different training seeds for DSPRITES. We optimized parameters for each model eight times with different random seeds, whose scores are illustrated by the eight boxes in each plot.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model appears to have the most consistent disentanglement score across different training seeds?",
    "answer": "JointVAE",
    "rationale": "The box plots for JointVAE in Figure (d) are narrower than the box plots for the other models, which indicates that the disentanglement score for JointVAE is more consistent across different training seeds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.13753v2",
    "pdf_url": null
  },
  {
    "instance_id": "6fb57671e4e440fea318d5a08ee0db5a",
    "figure_id": "2307.16895v1-Figure8-1",
    "image_file": "2307.16895v1-Figure8-1.png",
    "caption": " As in Figure 5, but with clipped ACI.",
    "figure_type": "plot and table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models has the highest marginal coverage?",
    "answer": "AR",
    "rationale": "The table in the figure shows that the AR model has a marginal coverage of 0.894, which is the highest among all the models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.16895v1",
    "pdf_url": null
  },
  {
    "instance_id": "3d47dd128649445ea62718a567992425",
    "figure_id": "2305.00286v1-Figure14-1",
    "image_file": "2305.00286v1-Figure14-1.png",
    "caption": " Full timescale results in non-parametric MuJoCo environments",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in the Cheetah-Multi-Task environment?",
    "answer": "MoSS",
    "rationale": "The MoSS line in the Cheetah-Multi-Task plot has the highest average return after 1e8 environment steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.00286v1",
    "pdf_url": null
  },
  {
    "instance_id": "b73ee81a3dc9441b8e066d47e348f69e",
    "figure_id": "1911.07794v5-Figure18-1",
    "image_file": "1911.07794v5-Figure18-1.png",
    "caption": " Atari@200M: Distribution Comparison. Populating Γt from the τ scale is better than from γ scale except at very short timescales. Drawing samples from both scales does best overall.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method of populating the state-action space is the best overall, according to the figure?",
    "answer": "Drawing samples from both scales.",
    "rationale": "The figure shows that drawing samples from both scales results in the lowest MSE, the lowest average norm MSE, and the highest average correlation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.07794v5",
    "pdf_url": null
  },
  {
    "instance_id": "379930626e7d4fdb81c7a057496acb15",
    "figure_id": "2211.11031v5-Figure11-1",
    "image_file": "2211.11031v5-Figure11-1.png",
    "caption": " Comparing editors over time when editing T5 on zsRE.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which editor performs best on the Edit Success metric?",
    "answer": "GRACE",
    "rationale": "The Edit Success metric is shown in panel (c) of the figure. GRACE has the highest bars for all input levels, indicating it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11031v5",
    "pdf_url": null
  },
  {
    "instance_id": "fdd66a3c8bf8484893b72604f15e6d0a",
    "figure_id": "2104.04406v1-Figure5-1",
    "image_file": "2104.04406v1-Figure5-1.png",
    "caption": " Overall Ratio",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest overall ratio for all values of k?",
    "answer": "ProMIPS",
    "rationale": "The figure shows the overall ratio for different values of k for four different methods. ProMIPS has the highest overall ratio for all values of k.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.04406v1",
    "pdf_url": null
  },
  {
    "instance_id": "24ff33420a9c4e7fa5669572a566fb0c",
    "figure_id": "1903.03714v1-Figure4-1",
    "image_file": "1903.03714v1-Figure4-1.png",
    "caption": " An example of a graph between items a and b . r represents a edge type or a relation type.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many paths are there between nodes a and b in the graph?",
    "answer": "4",
    "rationale": "There are four paths between nodes a and b:\n1. a - c - b\n2. a - d - b\n3. a - e - b\n4. a - e - d - b",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.03714v1",
    "pdf_url": null
  },
  {
    "instance_id": "6833a0c2c7ba481cb47ee0a4dc2acf55",
    "figure_id": "2109.09195v3-Figure1-1",
    "image_file": "2109.09195v3-Figure1-1.png",
    "caption": " Score distribution of LS with a 5-point scale across CNN/DM and XSum. Each data point shows the number of times a score was assigned to each system.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many times was a Likert score of 3 assigned to CNN/DM?",
    "answer": "Approximately 60 times.",
    "rationale": "The plot on the left shows the score distribution for CNN/DM. The data point corresponding to a Likert score of 3 on the x-axis and approximately 60 on the y-axis indicates the number of times this score was assigned.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.09195v3",
    "pdf_url": null
  },
  {
    "instance_id": "25dcc3059935448796a5f57123df0268",
    "figure_id": "2204.04826v1-Figure3-1",
    "image_file": "2204.04826v1-Figure3-1.png",
    "caption": " We generate 10 two-player zero-sum games with P1’s matrix payoff entries selected uniformly at random from [0, 1) and run the best of the described approaches for minimizing external regret in the sampled setting. Greedy weights outperforms all methods by almost an order of magnitude. Figure 3a shows the results as a function of the number of iterations used, while Figure 3b shows the same data as a function of time for fair comparison as iterations of greedy weights cost slightly more than previous methods. Exploitability is the distance to a Nash equilibrium. Both axes are logscale and error bars are shown at 95% confidence.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of exploitability?",
    "answer": "Greedy Weights.",
    "rationale": "The figure shows that Greedy Weights has the lowest exploitability across all iterations and times.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.04826v1",
    "pdf_url": null
  },
  {
    "instance_id": "7fc81685ebc642429cd521d19671fbca",
    "figure_id": "1906.03318v2-Figure2-1",
    "image_file": "1906.03318v2-Figure2-1.png",
    "caption": " PAL inference for population data from binomial, Poisson, and negative binomial GPFA models. A. One example simulated neuron (out of 20) are shown for each model. Inferred rate for neurons for PAL inference compared to GPFA (Top). Latent trajectories recovered for each model (bottom). B. Error of recovered latent structure falls to zero with increasing numbers of observed neurons, as expected. C. Example PAL fits for all count-GPFA models compared to standard GPFA for spiking data from an example neuron. Light grey histogram denotes spike-count observations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the count-GPFA models provides the best fit to the true latent trajectory for an example neuron, as shown in panel C?",
    "answer": "NB-GPFA",
    "rationale": "In panel C, the NB-GPFA model (red dashed line) closely tracks the true latent trajectory (black dashed line) with a small mean squared error (MSE) value of 0.003 ± 0.000. The other count-GPFA models, such as B-GPFA (green dashed line) and P-GPFA (yellow dashed line), exhibit larger deviations from the true trajectory with higher MSE values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.03318v2",
    "pdf_url": null
  },
  {
    "instance_id": "c33b10c8a7744c3da00590b6bbfa14e4",
    "figure_id": "2207.08922v1-Figure3-1",
    "image_file": "2207.08922v1-Figure3-1.png",
    "caption": " Implementation metadata example",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the command to run the program?",
    "answer": "./bin/search arg01 arg02 input output",
    "rationale": "The figure shows the command to run the program under the \"cmd\" field.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.08922v1",
    "pdf_url": null
  },
  {
    "instance_id": "aab5dd6fbdb74205a207f3134e97b41a",
    "figure_id": "2210.12316v2-Figure4-1",
    "image_file": "2210.12316v2-Figure4-1.png",
    "caption": " Performance comparison w.r.t. cold-start items on “Office” and “Online Retail” datasets. The bar graph denotes the number of interactions in test data for each group. The line chart denotes the relative improvement ratios compared with the baseline method SASRec (ID).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which recommendation system performs the best in terms of recall for cold-start items on the \"Online Retail\" dataset?",
    "answer": "SASRec (TT)",
    "rationale": "The line chart in the right panel shows that SASRec (TT) has the highest relative improvement ratio compared to the baseline method SASRec (ID) for all cold-start item groups.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12316v2",
    "pdf_url": null
  },
  {
    "instance_id": "49cb06ebf17d4b26ac8c2d9f9e2d4ce9",
    "figure_id": "2003.02819v1-Figure7-1",
    "image_file": "2003.02819v1-Figure7-1.png",
    "caption": " Density plots showing distribution differences between maximum logit (or corresponding to true label) and the average over all logits on different portions of train data and from different label smearing methods. Results using α = 0.2, dataset CIFAR-100, and the ResNet-32 model.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which label smearing method has the highest density of points near a gap of 0.25?",
    "answer": "BASE.",
    "rationale": "The BASE line is the highest at a gap of 0.25 in both plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.02819v1",
    "pdf_url": null
  },
  {
    "instance_id": "90498f1973c34a0eb8e87093b86f6c4b",
    "figure_id": "1910.07860v1-Figure10-1",
    "image_file": "1910.07860v1-Figure10-1.png",
    "caption": " Comparison of our segmentation (left column) against Favreau’s vectorization (right column). Top to Bottom. A simple line sketch from Favreau’s examples; A simple line sketch from Quick-draw dataset; A complicated hatch pattern with zoomed in details.",
    "figure_type": "** schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the following objects is segmented more accurately by our method than by Favreau's method? ",
    "answer": " The traffic cone.",
    "rationale": " Our method segments the traffic cone more accurately because it captures the curved shape of the cone, while Favreau's method produces a more angular segmentation. This can be seen in the top row of the figure, where the left image (our method) shows a smoother, more accurate representation of the cone's shape than the right image (Favreau's method).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.07860v1",
    "pdf_url": null
  },
  {
    "instance_id": "296a92c2caad4827a2b98d3c509c882c",
    "figure_id": "2108.00106v2-Figure10-1",
    "image_file": "2108.00106v2-Figure10-1.png",
    "caption": " Models trained with the NLL loss are overconfident across all three datasets. Adding MMCE as a secondary loss reduces the overconfidence. The MSE loss results in underconfidence and overconfidence for different bins. Models trained with Focal loss are underconfident. Soft Calibration Objectives (S-AvUC, SB-ECE) result in the most visually calibrated reliability plots across all datasets. Note that the high confidence regions have much higher density than the low confidence regions and are thus more critical to the ECE value. In the ECE vs average uncertainty plots, we see that Focal and NLL losses result in the highest and lowest average uncertainties respectively. S-AvUC results in the lowest ECE in all three datasets and SB-ECE is the next best for CIFAR-10 and CIFAR-100. Obtaining lower ECE as well as lower average uncertainty as compared to the NLL loss remains an open challenge.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function results in the most visually calibrated reliability plots across all datasets?",
    "answer": "Soft Calibration Objectives (S-AvUC, SB-ECE)",
    "rationale": "The reliability plots for each loss function are shown in the figure. The plots for Soft Calibration Objectives are closest to the ideal diagonal line, which indicates that the model's confidence is well-calibrated.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.00106v2",
    "pdf_url": null
  },
  {
    "instance_id": "8c931e4f8c134b8f80d2b809fed794ec",
    "figure_id": "1906.02613v2-Figure15-1",
    "image_file": "1906.02613v2-Figure15-1.png",
    "caption": " The training accuracy convergence on CINIC10.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which model architecture achieves the highest accuracy on CINIC10 when trained with SOTA SGD and random initialization?",
    "answer": " ResNet50",
    "rationale": " The figure shows the training accuracy convergence of different model architectures trained with different optimizers and initialization methods. The solid red line in the ResNet50 plot represents the SOTA SGD with random initialization, and it reaches the highest accuracy among all the models shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02613v2",
    "pdf_url": null
  },
  {
    "instance_id": "7bb546a08b734e58804c6a166dc684ea",
    "figure_id": "2203.11437v4-Figure12-1",
    "image_file": "2203.11437v4-Figure12-1.png",
    "caption": " Twenty images estimated to have the highest κ.",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image in the figure has the highest estimated κ value?",
    "answer": "It is impossible to tell from the figure alone.",
    "rationale": "The figure shows twenty images that are estimated to have the highest κ values, but it does not provide any information about the specific κ value for each image. \n\nFigure type: photograph(s)",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.11437v4",
    "pdf_url": null
  },
  {
    "instance_id": "01395a5c875c43dea53a58e3c039c3cd",
    "figure_id": "2105.10497v3-Figure25-1",
    "image_file": "2105.10497v3-Figure25-1.png",
    "caption": " Shape biased models: We conduct the same PatchDrop and Random Shuffle experiments on DeiT models trained on Stylized ImageNet [9] and compare with a CNN trained on the same dataset. All results are calculated over the ImageNet val. set. We highlight the improved performance in the PatchDrop experiments for the DeiT models in comparsion to ResNet50. We also note how the DeiT models’ performance drop with random shuffling is similar to that of the ResNet model.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is most robust to information loss caused by PatchDrop?",
    "answer": "DeiT-S-SIN.",
    "rationale": "The figure shows that DeiT-S-SIN has the highest accuracy for all levels of information loss in the PatchDrop experiments.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.10497v3",
    "pdf_url": null
  },
  {
    "instance_id": "f369818f57eb433e80bbf717f2e594f9",
    "figure_id": "1811.00115v1-Figure6-1",
    "image_file": "1811.00115v1-Figure6-1.png",
    "caption": " quality of different methods on MNIST",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods shown in the figure has the highest 1-precision?",
    "answer": "PCA",
    "rationale": "The 1-precision values are shown in the third column of each row. PCA has the highest value of 0.9528.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.00115v1",
    "pdf_url": null
  },
  {
    "instance_id": "48dce0ea9f3f40f39797ca49155e9b5c",
    "figure_id": "2106.04465v2-Figure4-1",
    "image_file": "2106.04465v2-Figure4-1.png",
    "caption": " GoF testing for the standard Poisson process using different test statistics, measured with ROC AUC (higher is better). See Section 6.1 for the description of the experimental setup.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which test statistic performs best for detecting deviations from the standard Poisson process across all detectability values?",
    "answer": "The S3 statistic.",
    "rationale": "The figure shows the ROC AUC score for different test statistics used to detect deviations from the standard Poisson process. The S3 statistic consistently has the highest ROC AUC score across all detectability values, indicating that it performs best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04465v2",
    "pdf_url": null
  },
  {
    "instance_id": "afc84e2c6fea4121acf7a199712f8546",
    "figure_id": "1904.06882v3-Figure5-1",
    "image_file": "1904.06882v3-Figure5-1.png",
    "caption": " PCK metric calculated for different Viewpoint IDs of the HPatches dataset. The proposed architectures (DGC-NC*) substantially outperform all strong baseline methods with a large margin.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture performed the best according to the PCK metric?",
    "answer": "DGC-NC-UCMD-Net (avg. est.) proposed",
    "rationale": "The figure shows the PCK metric for different network architectures on the HPatches dataset. The proposed DGC-NC-UCMD-Net (avg. est.) outperforms all other networks with a large margin.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.06882v3",
    "pdf_url": null
  },
  {
    "instance_id": "5651570e6a624f2f93802d28d1a9db16",
    "figure_id": "2207.06991v2-Figure3-1",
    "image_file": "2207.06991v2-Figure3-1.png",
    "caption": " Visual explanations of correct PIXEL predictions (for classes contradiction and entailment) for NLI examples with 0% and 80% CONFUSABLE substitutions using method by Chefer et al. (2021), providing qualitative evidence for PIXEL’s robustness to character-level noise and the interpretability of its predictions. Red heatmap regions represent high relevancy.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following sentences is more likely to be true based on the visual explanation of the PIXEL predictions?\n\na) The first Westerner to reach Hawaii was Captain James Cook.\nb) Nobody has a suit.",
    "answer": "b) Nobody has a suit.",
    "rationale": "The visual explanation of the PIXEL prediction for the sentence \"Nobody has a suit\" shows that the model is focusing on the words \"Nobody\" and \"suit,\" which suggests that it is more confident in this sentence being true than the other sentence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.06991v2",
    "pdf_url": null
  },
  {
    "instance_id": "6a3250666aed4b6a93b948e1e11d623b",
    "figure_id": "2204.12679v1-Figure4-1",
    "image_file": "2204.12679v1-Figure4-1.png",
    "caption": " Performances of the classification (in F1 scores) on the development set of different hyperparameter β in Eqn. (6) during the training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best performance when β is 0.8?",
    "answer": "GAIN",
    "rationale": "The figure shows that when β is 0.8, the GAIN model has the highest F1 score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.12679v1",
    "pdf_url": null
  },
  {
    "instance_id": "4be4320dc4df42c69ee29780353bdac6",
    "figure_id": "2009.12756v2-Figure4-1",
    "image_file": "2009.12756v2-Figure4-1.png",
    "caption": " Performance gap between retrieval-free and retrieval-based methods on different QA datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "On which dataset does retrieval-free BART perform the best?",
    "answer": "NQ",
    "rationale": "The figure shows that retrieval-free BART has the highest Answer EM score on the NQ dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.12756v2",
    "pdf_url": null
  },
  {
    "instance_id": "9025c4bc59df48c3aaa85c79ab7cc044",
    "figure_id": "2105.05233v4-Figure14-1",
    "image_file": "2105.05233v4-Figure14-1.png",
    "caption": " Samples from our best 512×512 model (FID: 3.85). Classes are 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the classes has the most images in the figure?",
    "answer": "cheeseburger",
    "rationale": "The caption states that the classes are 933: cheeseburger, 562: fountain, 417: balloon, 281: tabby cat, 90: lorikeet, 992: agaric. Therefore, cheeseburger has the most images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.05233v4",
    "pdf_url": null
  },
  {
    "instance_id": "77fcac74d5f544919bf4337f1038b0c5",
    "figure_id": "2211.09231v2-Figure35-1",
    "image_file": "2211.09231v2-Figure35-1.png",
    "caption": " Architecture search for FERM. The plots show the performance (in terms of discounted reward) of the evaluation policy. The evaluation is performed every 200 training steps. Results are averaged over four runs. Shading denotes standard error.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task is the most difficult for FERM to learn?",
    "answer": "Block in Bowl",
    "rationale": "The \"Block in Bowl\" plot shows the lowest discounted reward values compared to the other tasks, indicating that the agent performs worse on this task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.09231v2",
    "pdf_url": null
  },
  {
    "instance_id": "ce86a1a05cf7481496db663e4e3bffd5",
    "figure_id": "2302.05550v1-Figure9-1",
    "image_file": "2302.05550v1-Figure9-1.png",
    "caption": " Effects of knowledge distillation ratio in W2E.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the metrics is most affected by the distillation ratio?",
    "answer": "Relevance (lexical)",
    "rationale": "The figure shows that the Relevance (lexical) metric has the largest change in value as the distillation ratio increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.05550v1",
    "pdf_url": null
  },
  {
    "instance_id": "366fbb657a8c45c3a0a536219f46452b",
    "figure_id": "2211.11629v3-Figure1-1",
    "image_file": "2211.11629v3-Figure1-1.png",
    "caption": " Distance precision and success rate of the trackers on UAVDT dataset [16]. Compared with offline evaluation, the trackers suffer a lot from their onboard latency in the online setting (30 frames/s (FPS)). Coupled with PVT++, the predictive trackers achieve significant performance gain with very little extra latency, obtaining on par or better results than the offline setting.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tracker achieves the highest success rate in the online setting?",
    "answer": "PVT++ + M",
    "rationale": "The figure shows the success rate and speed of different trackers in the online setting. The tracker with the highest success rate is PVT++ + M, which is represented by the red star.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11629v3",
    "pdf_url": null
  },
  {
    "instance_id": "fdf7214708524d02940513ccffc4ee1e",
    "figure_id": "2104.01291v1-Figure7-1",
    "image_file": "2104.01291v1-Figure7-1.png",
    "caption": " Distribution of length of fingerspelling segments.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has a higher percentage of fingerspelling segments with fewer than 20 frames?",
    "answer": "ChicagoFSWild+",
    "rationale": "The bar for ChicagoFSWild+ in the [0,10) bin is higher than the bar for ChicagoFSWild, indicating that ChicagoFSWild+ has a higher percentage of fingerspelling segments with fewer than 20 frames.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.01291v1",
    "pdf_url": null
  },
  {
    "instance_id": "120d3a6ed0174b20b379e3bb7094ff28",
    "figure_id": "2105.04556v2-Figure4-1",
    "image_file": "2105.04556v2-Figure4-1.png",
    "caption": " Data set Characteristics. Distribution of plans with plan length for home and factory domains. Frequency of interaction of top 10 objects and frequency of actions for top 10 actions. The collected data set contains diverse interactions in complex spaces.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most frequent action in the factory domain?",
    "answer": "Pick and place",
    "rationale": "The figure shows the frequency of actions for the top 10 actions in both the home and factory domains. In the factory domain, the \"pick and place\" action has the highest frequency.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.04556v2",
    "pdf_url": null
  },
  {
    "instance_id": "6b3900df4d0546c3b5252f5681cdac93",
    "figure_id": "2301.05221v2-Figure6-1",
    "image_file": "2301.05221v2-Figure6-1.png",
    "caption": " Qualitative results compared with the state-of-the-art zero-shot semantic segmentation method (Zegformer) on Pascal VOC. While training on our synthetic dataset and real dataset (only PASCAL VOC seen) together, a standard MaskFormer architecture can achieve better performance than Zegformer on unseen categories, i.e. pottedplant, sofa and tvmonitor. Note that, the synthetic data for training MaskFormer are generated from our grounding module, which has only been trained on seen categories, with no extra manual annotation involved whatsoever.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better on unseen categories, Zegformer or Maskformer?",
    "answer": "Maskformer performs better on unseen categories.",
    "rationale": "The figure shows that Maskformer produces more accurate segmentations for unseen categories such as pottedplant, sofa, and tvmonitor, compared to Zegformer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.05221v2",
    "pdf_url": null
  },
  {
    "instance_id": "dcdcfe67207741259c36a214ac64c7ec",
    "figure_id": "2109.05257v2-Figure3-1",
    "image_file": "2109.05257v2-Figure3-1.png",
    "caption": " t-SNE of the input windows of the SWaT test dataset and visualization of corresponding signals. Blue color indicates ground truth (GT) normal while orange, green and red color indicate GT anomaly. Even though (b) is GT anomaly, it shares patterns more with (a), GT normal signal, than (c) abnormal signal.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which signal is more likely to be an anomaly, (b) or (c)?",
    "answer": "Signal (c) is more likely to be an anomaly.",
    "rationale": "The figure shows a t-SNE plot of the input windows of the SWaT test dataset, with blue color indicating ground truth (GT) normal and orange, green and red color indicating GT anomaly. Signal (c) is colored red, indicating that it is a GT anomaly, while signal (b) is colored green, indicating that it is not a GT anomaly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.05257v2",
    "pdf_url": null
  },
  {
    "instance_id": "86ad684be8fe41cd89ba9197afb93dbd",
    "figure_id": "2305.16498v2-Figure34-1",
    "image_file": "2305.16498v2-Figure34-1.png",
    "caption": "Figure 34 | We investigate the weakness of iqlearn and ppil for offline learning (Ant-v2) and highdimensional action spaces (door-v0). Compared to csil, iqlearn and ppil have less stable and effective 𝑄 values, in particular for the expert demonstrations, which results in the policy not inaccurately predicting the demonstration actions (expert_mse).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is the most stable and effective for offline learning and high-dimensional action spaces?",
    "answer": "csil",
    "rationale": "The figure shows that csil has the most stable and effective Q values, particularly for the expert demonstrations. This suggests that csil is the best algorithm for offline learning and high-dimensional action spaces.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16498v2",
    "pdf_url": null
  },
  {
    "instance_id": "28f2268115874caf85e707a764b0a4c1",
    "figure_id": "1809.05504v2-Figure3-1",
    "image_file": "1809.05504v2-Figure3-1.png",
    "caption": " Diverse recommendation predictions. Top to bottom: ground truth, our method’s prediction (by NN2Decision), two stage prediction (by NN2-2Stage)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method makes the most accurate predictions?",
    "answer": "Our method (NN2Decision)",
    "rationale": "The ground truth shows the actual data, and our method's prediction is closest to the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.05504v2",
    "pdf_url": null
  },
  {
    "instance_id": "fc33443a0ed24d3abfed3d2aa52a0ad7",
    "figure_id": "1904.04562v1-Figure3-1",
    "image_file": "1904.04562v1-Figure3-1.png",
    "caption": " An example of constructing three different hierarchical structures in the r-th convolutional layer of a network, denoted asMr , which consists of three disjoint sets of feature maps or units (i.e.,Mr = [Mr 1 ,M r 2 ,M r 3 ] and the number indicates the unit index). The number of tasks and the number of levels of hierarchy are three. Different orders of the units construct the hierarchical structures. Here, hl,j(Mr) is a function that selects the sub-structure ofMr corresponding to the l-th level of hierarchy for the j-th task. S(i, j) returns the level number at which the i-th unit Mr i is added to the hierarchy for the j-th task. (Best viewed in color.)",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " How many levels of hierarchy are there in the example shown in the figure?",
    "answer": " Three",
    "rationale": " The figure shows three levels of hierarchy, labeled as 1st Level, 2nd Level, and 3rd Level. Each level represents a different stage in the hierarchical structure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.04562v1",
    "pdf_url": null
  },
  {
    "instance_id": "923936767f244283b912be3503bde091",
    "figure_id": "2101.00151v2-Figure7-1",
    "image_file": "2101.00151v2-Figure7-1.png",
    "caption": " Experiment results by visual properties: Left: results by the number of objects mentioned in question that are contained in video scenes. Right: results by the relative length of video interval in question.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best when the number of objects mentioned in the question is small?",
    "answer": "Q-type (Freq)",
    "rationale": "The plot on the left shows that the Q-type (Freq) model has the highest accuracy when the number of objects mentioned in the question is 0 or 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.00151v2",
    "pdf_url": null
  },
  {
    "instance_id": "67747fc959fa4c05895c2c95e979b347",
    "figure_id": "2208.03030v1-Figure7-1",
    "image_file": "2208.03030v1-Figure7-1.png",
    "caption": " Prerequisite reasoning skills for ChiQA. From top to the bottom are the question, the image, the reasoning skill, the explanation, and the proportion. Note that the reasoning skills are not mutually exclusive so some examples may require more than one reasoning skill.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the stomach is higher, the pylorus or the cardia?",
    "answer": "The cardia is higher than the pylorus.",
    "rationale": "The image shows a schematic of the stomach, with the cardia located at the top and the pylorus located at the bottom.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.03030v1",
    "pdf_url": null
  },
  {
    "instance_id": "c81599290e2843198367b828f30bf3ea",
    "figure_id": "1901.10623v2-Figure4-1",
    "image_file": "1901.10623v2-Figure4-1.png",
    "caption": " Visualized conversation results on DX dataset. In each table, the first line is a self-report of patient. The results generated by our KR-DS are presented on the left column and the ones produced by the pure DQN method (Wei et al. 2018) on the right. We also present the knowledge graph related to the diagnosis process to the left of each table. To highlight the symptoms and diseases, red boxes used for symptoms from self-report, green for True symptom, gray for False and not sure symptoms in the user goal, blue for the diagnosed disease.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which symptom is most likely to be associated with rhinallergosis?",
    "answer": "Nasal congestion.",
    "rationale": "The knowledge graph shows that nasal congestion is directly connected to rhinallergosis with a high weight of 0.893. This suggests that nasal congestion is a common symptom of rhinallergosis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.10623v2",
    "pdf_url": null
  },
  {
    "instance_id": "0a1cb0c15806429799208e41e36c9f10",
    "figure_id": "2109.04404v1-Figure11-1",
    "image_file": "2109.04404v1-Figure11-1.png",
    "caption": " Average correlation (Spearman’s ρ) with human judgements on each word similarity dataset, with and without postprocessing for BERT",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which post-processing technique generally leads to the highest correlation with human judgements for BERT on each word similarity dataset?",
    "answer": "Standardized post-processing.",
    "rationale": "The figure shows the average correlation with human judgements for BERT on each word similarity dataset, with and without post-processing. The standardized post-processing line (orange dashed line) is generally higher than the other lines for each dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.04404v1",
    "pdf_url": null
  },
  {
    "instance_id": "5cf3d9c74d4345029a0a24f535a05643",
    "figure_id": "2110.08501v4-Figure6-1",
    "image_file": "2110.08501v4-Figure6-1.png",
    "caption": " Data example. We align implicit knowledge from ConceptNet (Speer et al., 2017) between dialogue turns and form each instance in three components.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of knowledge is represented in the figure?",
    "answer": "The figure represents implicit knowledge.",
    "rationale": "The figure shows a dialogue between two people, where one person asks the other what seasonings they are going to use to cook the meat. The other person responds with \"salt and sauce.\" This exchange demonstrates implicit knowledge because the person who asks the question does not explicitly state that they want to know what seasonings will be used, and the person who answers the question does not explicitly state that they are using implicit knowledge to answer the question.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.08501v4",
    "pdf_url": null
  },
  {
    "instance_id": "83d798f39986421a8ebf9e08bb6bd274",
    "figure_id": "2010.12883v1-Figure13-1",
    "image_file": "2010.12883v1-Figure13-1.png",
    "caption": " Plots of class probabilities for images in Dr obtained using q(θ|D), q(θ|Dr), optimized q̃v(θ|Dr;λ = 0) and q̃u(θ|Dr;λ = 0). 22",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which clothing item has the highest probability of being classified correctly by all four methods?",
    "answer": "The pullover.",
    "rationale": "In all four subfigures, the pullover has a probability of 100% of being classified correctly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12883v1",
    "pdf_url": null
  },
  {
    "instance_id": "0b409cd7b56840948be6d805f767e1b5",
    "figure_id": "2010.04081v2-Figure5-1",
    "image_file": "2010.04081v2-Figure5-1.png",
    "caption": " Average and standard deviation of running time in seconds for one iteration (as an average of five) on Sutter and BBC NEWS Data sets by increasing 1) a mode size 2) target rank (R) 3) Sinkhorn iteration. For Figures 5c, 5f we report OT running time and for the rest we present the total running time for one iteration. In Figure 5a for 1250 patients and Figure 5b for R = 40, Figure 5d for 1000 articles, and Figure 5e, execution in Wasserstein TF (direct baseline) failed due to the excessive amount of memory request.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is faster, Wasserstein TF or SWIFT?",
    "answer": "SWIFT is faster than Wasserstein TF.",
    "rationale": "The figure shows the running time of both algorithms on different datasets and with different parameters. In all cases, SWIFT has a lower running time than Wasserstein TF.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.04081v2",
    "pdf_url": null
  },
  {
    "instance_id": "040ea30cf75c47339749088c4011c8c7",
    "figure_id": "2010.08188v2-Figure14-1",
    "image_file": "2010.08188v2-Figure14-1.png",
    "caption": " Qualitative comparisons with extrapolation baselines on the KTH Action dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods produces the sharpest and most accurate predictions?",
    "answer": "Ours.",
    "rationale": "The figure shows the results of different methods for predicting future frames in a video. The \"Ours\" method produces the sharpest and most accurate predictions, as can be seen by comparing the predicted frames to the ground truth frames.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.08188v2",
    "pdf_url": null
  },
  {
    "instance_id": "1c5e4faf9ef8496c89e47872de1b4e61",
    "figure_id": "2303.10904v2-Figure5-1",
    "image_file": "2303.10904v2-Figure5-1.png",
    "caption": " Visualization of the actionlet for a “throw” sequence. The yellow joints are the actionlet. Note that hand movements are mainly selected, indicating that the actionlet is reasonable.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which joints are most important for the \"throw\" actionlet?",
    "answer": " The hand joints.",
    "rationale": " The figure shows that the yellow joints, which represent the actionlet, are primarily located on the hands. This indicates that hand movements are the most important for the \"throw\" actionlet.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.10904v2",
    "pdf_url": null
  },
  {
    "instance_id": "315e4371347a40dbb1b5b31422bc8863",
    "figure_id": "1812.11631v3-Figure6-1",
    "image_file": "1812.11631v3-Figure6-1.png",
    "caption": " ACAM action detection framework running on a surveillance video from VIRAT [26]. Actors are detected by object detectors and tracked over frames by Deep Sort [43]. The generated tubes for each person is analyzed by the ACAM action detector.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the role of Deep Sort in the ACAM action detection framework?",
    "answer": "Deep Sort is used to track actors over frames.",
    "rationale": "The figure shows that the output of the Actor Detector is fed into Deep Sort, which then outputs a set of tubes for each person. These tubes are then analyzed by the ACAM action detector.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.11631v3",
    "pdf_url": null
  },
  {
    "instance_id": "e971143b244a4331b5ef44ebb1c2ae09",
    "figure_id": "2006.03647v2-Figure2-1",
    "image_file": "2006.03647v2-Figure2-1.png",
    "caption": " Evaluation of BREMEN with the existing methods (ME-TRPO, SAC, BCQ, BRAC) under deployment constraints (to 5-10 deployments with batch sizes of 200k and 100k). The average cumulative rewards and their standard deviations with 5 random seeds are shown. Vertical dotted lines represent where each policy deployment and data collection happen. BREMEN is able to learn successful policies with only 5-10 deployments, while the state-of-the-art off-policy (SAC), modelbased (ME-TRPO), and recursively-applied offline RL algorithms (BCQ, BRAC) often struggle to make any progress. For completeness, we show ME-TRPO(online) and SAC(online) which are their original optimal learning curves without deployment constraints, plotted with respect to samples normalized by the batch size. While SAC(online) substantially outperforms BREMEN in sample efficiency, it uses 1 deployment per sample, leading to 100k-500k deployments required for learning. Interestingly, BREMEN achieves even better performance than the original ME-TRPO(online), suggesting the effectiveness of implicit behavior regularization. For SAC and ME-TRPO under deployment-constrained evaluation, their batch size between policy deployments differs substantially from their standard settings, and therefore we performed extensive hyper-parameter search on the relevant parameters such as the number of policy updates between deployments, as discussed in Appendix B.2.1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of cumulative rewards with a batch size of 200,000?",
    "answer": "BREMEN",
    "rationale": "The figure shows the cumulative rewards for different algorithms with different batch sizes. The line for BREMEN is consistently higher than the lines for the other algorithms, indicating that it achieves higher cumulative rewards.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.03647v2",
    "pdf_url": null
  },
  {
    "instance_id": "73afa55ebcd24d5f8bdfbdfa0b5c018a",
    "figure_id": "2308.06058v2-Figure1-1",
    "image_file": "2308.06058v2-Figure1-1.png",
    "caption": " Illustration of the robust convergence of AdaSPS on synthetic data with quadratic loss. SPS has superior performance on the two interpolated problems but cannot converge when the interpolation condition does not hold. DecSPS suffers from a slow convergence on both interpolated problems. AdaSVRPS and SVRG show remarkable performances when solving non-interpolated problems. (Repeated 3 times. The solid lines and the shaded area represent the mean and the standard deviation.)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best on the non-interpolated problems?",
    "answer": "AdaSVRPS and SVRG",
    "rationale": "The figure shows that AdaSVRPS and SVRG have the fastest convergence rates on the non-interpolated problems.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.06058v2",
    "pdf_url": null
  },
  {
    "instance_id": "3140b9c2f2cb431796008efb4070c096",
    "figure_id": "1903.02445v1-Figure5-1",
    "image_file": "1903.02445v1-Figure5-1.png",
    "caption": " An elementary 4-wall.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the dimension of the elementary 4-wall?",
    "answer": "The dimension of the elementary 4-wall is 2.",
    "rationale": "The figure shows that the elementary 4-wall is a two-dimensional object. It has length and width, but no height.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.02445v1",
    "pdf_url": null
  },
  {
    "instance_id": "df1fb4c3f04841a0b5eff0c3426b160e",
    "figure_id": "2306.12356v1-Figure2-1",
    "image_file": "2306.12356v1-Figure2-1.png",
    "caption": " Moving average of evaluation returns of pocomblock for PORL2 and BRIEE",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better, PORL2 or BRIEE?",
    "answer": "PORL2 performs better than BRIEE.",
    "rationale": "The figure shows the average rewards for each algorithm over the total episodes. The average rewards for PORL2 are consistently higher than the average rewards for BRIEE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.12356v1",
    "pdf_url": null
  },
  {
    "instance_id": "f5747844139b416eb470fe181e2ce57b",
    "figure_id": "2305.16577v1-Figure9-1",
    "image_file": "2305.16577v1-Figure9-1.png",
    "caption": " tSNE projections of SR vectors for 608 random names using GPT-2, visualized by frequency in the pre-training corpus, tokenization length, race/ethnicity, and gender associated with the names respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following factors seems to have the least influence on the distribution of SR vectors in the tSNE projection?",
    "answer": "Race/ethnicity",
    "rationale": "The tSNE projection for race/ethnicity shows a high degree of overlap between the different racial/ethnic groups, suggesting that this factor has less influence on the distribution of SR vectors compared to other factors like frequency and gender.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16577v1",
    "pdf_url": null
  },
  {
    "instance_id": "dac8b3cb98564b05b4dd84fff46db695",
    "figure_id": "2205.08803v1-Figure4-1",
    "image_file": "2205.08803v1-Figure4-1.png",
    "caption": " Bayesian parameter estimates. Top left: drift parameters Az . Top right: rates Λzz′ . Bottom: drift parameters bz . I: fluorescence intensity (a.u.).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which parameter has the most significant difference between the blue and orange distributions?",
    "answer": "The drift parameter bz.",
    "rationale": "The blue and orange distributions for bz are almost completely separated, while the distributions for Az and Λzz′ overlap significantly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.08803v1",
    "pdf_url": null
  },
  {
    "instance_id": "8c93cbe3583a4adf8595d80d809ba210",
    "figure_id": "2003.09080v1-Figure4-1",
    "image_file": "2003.09080v1-Figure4-1.png",
    "caption": " Performance of the algorithms. We compare ours (ASKER) against standard IRLS, M-HQ [31], GNC [32], and LM-MOO [34], which are state-of-the-art methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of the best objective value achieved?",
    "answer": "ASKER",
    "rationale": "The figure shows the best objective value achieved by each algorithm over time. ASKER consistently achieves the lowest best objective value, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.09080v1",
    "pdf_url": null
  },
  {
    "instance_id": "9a83c9047d4f4313ba4702347b242e45",
    "figure_id": "1909.02304v1-Figure3-1",
    "image_file": "1909.02304v1-Figure3-1.png",
    "caption": " An generation example of our model based on the same tables in Figure 1. Text that accurately reflects players (Al Jefferson and Kris Humphries) performance is in red.",
    "figure_type": "\"other\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Who was the only other Wizard to reach double-digit points?",
    "answer": "Kris Humphries",
    "rationale": "The passage states that \"The only other Wizard to reach double-digit points was Kris Humphries\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.02304v1",
    "pdf_url": null
  },
  {
    "instance_id": "070a642786be486ebd999dd775c1ade2",
    "figure_id": "2109.11154v2-Figure1-1",
    "image_file": "2109.11154v2-Figure1-1.png",
    "caption": " Convergence of the subgradient method (3.1) with different stepsize rules in the regimes of (a) exact-parameterization k = r, (b) rank overspecication k = 2r, and (c) over-parameterization k = d. We use the following stepsizes: constant stepsizes ηt = η0, geometric diminishing stepsizes ηt = η0 · 0.9t, sublinear diminishing stepsizes ηt = η0/t, the proposed stepsizes (3.2), and the Polyaks’s stepsize.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stepsize rule results in the fastest convergence for the subgradient method in the regime of exact-parameterization?",
    "answer": "The proposed stepsize rule.",
    "rationale": "In Figure (a), which shows the convergence of the subgradient method in the regime of exact-parameterization, the proposed stepsize rule (green line) results in the fastest convergence, followed by the Polyak stepsize rule (cyan line). The other stepsize rules (constant, geometric, and sublinear) converge more slowly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.11154v2",
    "pdf_url": null
  },
  {
    "instance_id": "c423b094ef5149c2a5487639d749ff37",
    "figure_id": "1807.04364v1-Figure5-1",
    "image_file": "1807.04364v1-Figure5-1.png",
    "caption": " Denoised images of the real noisy image Nikon D800 ISO 6400 1 [24] by different methods. This scene was shot 500 times under the same camera and camera setting. The mean image of the 500 shots is roughly taken as the “Ground Truth”.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which denoising method produced the image that is closest to the ground truth?",
    "answer": "TWSC",
    "rationale": "The TWSC image has the highest PSNR and SSIM values, which are metrics used to measure the quality of an image. The closer the PSNR and SSIM values are to 1, the better the image quality.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1807.04364v1",
    "pdf_url": null
  },
  {
    "instance_id": "eb6ff468aa9c48dba4ffde815ad47e90",
    "figure_id": "1805.04833v1-Figure6-1",
    "image_file": "1805.04833v1-Figure6-1.png",
    "caption": " Accuracy of prompt ranking. The fusion model most accurately pairs prompt and stories.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of accuracy for prompt ranking?",
    "answer": "The Fusion model.",
    "rationale": "The figure shows the accuracy of different models for prompt ranking. The Fusion model has the highest accuracy of 16.3%, which is higher than the accuracy of the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.04833v1",
    "pdf_url": null
  },
  {
    "instance_id": "6de274c1d47e44919da9d50e538045d5",
    "figure_id": "2202.03712v2-Figure11-1",
    "image_file": "2202.03712v2-Figure11-1.png",
    "caption": " Comparison of different AFO methods for the generic BBO problem of RNA sequence optimization with n = 30.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which AFO method performs the best in terms of minimizing the objective function?",
    "answer": "ECO-G (SA)",
    "rationale": "The plot shows that the ECO-G (SA) method has the lowest minimum value of the objective function over the entire time period.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.03712v2",
    "pdf_url": null
  },
  {
    "instance_id": "8bedc35bd8ca4ed293448bb920fa8bf4",
    "figure_id": "2205.08704v2-Figure2-1",
    "image_file": "2205.08704v2-Figure2-1.png",
    "caption": " Fairness Confusion Matrix performances (FCNNs)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which fairness metric has the highest percentage of True Positive Rate (TPR)?",
    "answer": "BL (Baseline)",
    "rationale": "The figure shows the confusion matrix for each fairness metric. The True Positive Rate (TPR) is the percentage of correctly classified positive examples. The BL metric has the highest percentage of TPR, at 82.7%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.08704v2",
    "pdf_url": null
  },
  {
    "instance_id": "683b6fd08eaa4a08b9b8a64b51bb3fbc",
    "figure_id": "2305.09275v1-Figure9-1",
    "image_file": "2305.09275v1-Figure9-1.png",
    "caption": " Effect of Sampling Strategy on Online Accuracy. Unlike what was observed for CLGM, for CLOC, the mixed sampling performs the best in terms of online accuracy followed followed by FIFO and then Uniform. However, Uniform still performs the best in terms of information retention with a large gap with a large margin.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampling strategy leads to the highest online accuracy for CLOC?",
    "answer": "Mixed sampling.",
    "rationale": "The figure shows that the mixed sampling strategy (green line) has the highest online accuracy for CLOC, followed by FIFO (red line) and then Uniform (purple line).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.09275v1",
    "pdf_url": null
  },
  {
    "instance_id": "a310a9750326417986291c4110f9a294",
    "figure_id": "2211.06143v1-Figure1-1",
    "image_file": "2211.06143v1-Figure1-1.png",
    "caption": " Visualization of attention maps of FAN-Trans via ScoreCAM [26]. The result is achieved by placing the target layer on resolution 4 × 4. From top to bottom, each row denotes an input image, its spatial attention and activated action units, in respect. FAN-Trans shows a potential high concentration at the facial component that indicates its ability to learn where to focus without using explicit attention modules [8] or manual region allocations [22, 11]. For example, AU1 and AU2 emphasize the eyebrow, AU7 is around the eye while AU24 highlight the mouth.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which facial action units (AUs) are activated when a person raises their eyebrows?",
    "answer": "AU1 and AU2.",
    "rationale": "The figure shows that the spatial attention maps for AU1 and AU2 are concentrated around the eyebrows, indicating that these AUs are activated when a person raises their eyebrows.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.06143v1",
    "pdf_url": null
  },
  {
    "instance_id": "96eadeac0a854d98b41b519a7ce2266d",
    "figure_id": "2305.06522v1-Figure4-1",
    "image_file": "2305.06522v1-Figure4-1.png",
    "caption": " Stochastic stability of RSMI.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on the RAcc metric for the IMDb dataset?",
    "answer": "RoBERTa-IMDb",
    "rationale": "The boxplot for RoBERTa-IMDb on the RAcc metric is higher than the boxplot for BERT-IMDb, indicating that RoBERTa-IMDb has a higher median accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.06522v1",
    "pdf_url": null
  },
  {
    "instance_id": "d60763dba1604dc5986612e31536ffc4",
    "figure_id": "2205.15301v1-Figure6-1",
    "image_file": "2205.15301v1-Figure6-1.png",
    "caption": " Impact of masking a PIE noun in the attention on (a) other PIE tokens, (b) other context tokens. Impact of masking a non-PIE noun on (c) PIE tokens and (d) other non-PIE tokens. (e) shows the difference in similarity between lit-wfw and fig-par.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language shows the biggest difference in similarity between lit-wfw and fig-par when a non-PIE noun is masked?",
    "answer": "Dutch",
    "rationale": "The figure shows the difference in similarity between lit-wfw and fig-par for each language. The bars for each language represent the difference in similarity when a PIE noun is masked (left) and when a non-PIE noun is masked (right). The height of the bar indicates the magnitude of the difference. The Dutch bar is the tallest on the right side of the figure, indicating that Dutch has the biggest difference in similarity between lit-wfw and fig-par when a non-PIE noun is masked.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.15301v1",
    "pdf_url": null
  },
  {
    "instance_id": "2f672544442743e8b03f788dd8736bfd",
    "figure_id": "2205.11680v2-Figure2-1",
    "image_file": "2205.11680v2-Figure2-1.png",
    "caption": " CDFs of hand-crafted clinical activity measures of EHR activity logs on a monthly basis grouped by various burnout score ranges from low to high.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Do physicians with higher burnout scores spend more time on EHR tasks after hours?",
    "answer": "Yes.",
    "rationale": "The CDF for after-hour EHR time shows that physicians with higher burnout scores (darker red lines) tend to spend more time on EHR tasks after hours than physicians with lower burnout scores (lighter red lines). For example, the CDF for the highest burnout score group shows that 50% of physicians in this group spend more than 40 hours on EHR tasks after hours, while the CDF for the lowest burnout score group shows that only 25% of physicians in this group spend more than 40 hours on EHR tasks after hours.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.11680v2",
    "pdf_url": null
  },
  {
    "instance_id": "2aa19ac87cf14d5cbdc921d70febd8cb",
    "figure_id": "1907.02189v4-Figure3-1",
    "image_file": "1907.02189v4-Figure3-1.png",
    "caption": " The impact of K on four datasets. To show more clearly the differences between the curves, we zoom in the last few rounds in the upper left corner of the box.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the most consistent performance across different values of K?",
    "answer": "Balanced MNIST.",
    "rationale": "The lines in the plot for Balanced MNIST are the closest together, indicating that the performance of the model is less sensitive to the value of K for this dataset than for the others.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.02189v4",
    "pdf_url": null
  },
  {
    "instance_id": "056c40353ea64cc5aa79648d769a7752",
    "figure_id": "2109.07448v1-Figure4-1",
    "image_file": "2109.07448v1-Figure4-1.png",
    "caption": " 3D reconstruction on ZJU-MoCap. Tested on unseen model’s unseen pose except Neural Body (per-person optimized). NB: Neural Body [33], PVA: Pixel volumetric avatar [36], Pixel-NeRF [52] and ours.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods shown in the figure is able to reconstruct the 3D pose of a person most accurately?",
    "answer": "Ours",
    "rationale": "The figure shows the 3D reconstruction results of different methods on the ZJU-MoCap dataset. The red circles highlight the areas where the methods differ the most. Our method is able to reconstruct the 3D pose of the person more accurately than the other methods, as evidenced by the smoother and more detailed reconstruction in the highlighted areas.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.07448v1",
    "pdf_url": null
  },
  {
    "instance_id": "869885d133704aa59282c3982ef6b0f6",
    "figure_id": "2203.03078v1-Figure4-1",
    "image_file": "2203.03078v1-Figure4-1.png",
    "caption": " Sensitivity sweeps illustrating the sample efficiency (MHNS) and computational complexity (itr/s) trade-off by the choice of (a) projected dimension F (b) M , which affects the speed/recall ratio of HNSW. Up and to the right is better. For parameters that do not affect speed, we plot the distribution of final HNS’s for c) k, and d) α. Orange line indicates median, green diamond the mean. All game scores are first averaged over 10 seeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the plots, what parameters are associated with speed?",
    "answer": "The parameters F and M are associated with speed.",
    "rationale": "This can be seen in plots (a) and (b) where the x-axis represents speed in iterations per second.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.03078v1",
    "pdf_url": null
  },
  {
    "instance_id": "f90758ce464646b5a9e36e415b60a037",
    "figure_id": "2209.10767v2-Figure10-1",
    "image_file": "2209.10767v2-Figure10-1.png",
    "caption": " Comparison of our decoder (LCP) with that of two baselines (CLP and ICL).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three decoders (LCP, CLP, or ICL) uses self-attention?",
    "answer": "LCP",
    "rationale": "The self-attention block is only present in the LCP decoder, as shown in Figure a.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.10767v2",
    "pdf_url": null
  },
  {
    "instance_id": "f2b59b97ca0f4380b7e82961e1cfc372",
    "figure_id": "2006.09994v1-Figure11-1",
    "image_file": "2006.09994v1-Figure11-1.png",
    "caption": " We train models on different-sized subsets of IN-9LB. The largest training set we use is the full IN-9LB dataset, which is 4 times larger than ImageNet-9. While performance on all test datasets improves as the amount of training data increases, the BG-GAP has almost the same size regardless of the amount of training data used.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which test dataset shows the biggest improvement in accuracy when the training data size is increased from 0.25 to 4.00 times the size of ImageNet-9?",
    "answer": "The Mixed-Next test dataset.",
    "rationale": "The figure shows that the Mixed-Next test dataset has the steepest slope, indicating that the accuracy increases the most when the training data size is increased.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.09994v1",
    "pdf_url": null
  },
  {
    "instance_id": "ed08edec8e6a437795f3e66ef3aca1cd",
    "figure_id": "2302.08266v1-Figure5-1",
    "image_file": "2302.08266v1-Figure5-1.png",
    "caption": " The F1@20 and Recall-Disp@20 of FairNeg and its variants on two backbones over the ML1M-4 dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest Recall-Disp@20 on the ML1M-4 dataset?",
    "answer": "UNS",
    "rationale": "The figure shows the Recall-Disp@20 of different methods on the ML1M-4 dataset. UNS has the highest Recall-Disp@20 among all the methods shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.08266v1",
    "pdf_url": null
  },
  {
    "instance_id": "8691fe72ac1a4a898eec6e281069f512",
    "figure_id": "2109.15117v5-Figure1-1",
    "image_file": "2109.15117v5-Figure1-1.png",
    "caption": " Prediction performance of MVNNs vs plain NNs in SRVM (national bidder). The identity is shown in grey.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has better prediction performance, MVNN or NN?",
    "answer": "MVNN has better prediction performance than NN.",
    "rationale": "The figure shows the predicted bundle value vs. the true bundle value for both MVNN and NN. The points for MVNN are closer to the identity line than the points for NN, which indicates that MVNN has better prediction performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.15117v5",
    "pdf_url": null
  },
  {
    "instance_id": "2d687c564c724721a639e9848e8a6605",
    "figure_id": "2203.07504v1-Figure11-1",
    "image_file": "2203.07504v1-Figure11-1.png",
    "caption": " The semantics of context affects representations from the earliest layers of the autoencoders BERT and RoBERTa.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following settings in BERT and RoBERTa is most affected by the semantics of context?",
    "answer": "Misaligned",
    "rationale": "The misaligned setting in both BERT and RoBERTa shows the greatest deviation from the other settings, with a sharp decrease in VAST score in the early layers. This suggests that the misaligned setting is more sensitive to the semantics of context than the other settings.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.07504v1",
    "pdf_url": null
  },
  {
    "instance_id": "c0a929a7eff74bf79a4dd1dde605483a",
    "figure_id": "2112.07194v2-Figure3-1",
    "image_file": "2112.07194v2-Figure3-1.png",
    "caption": " A case study to examine the limitations of MDD-Eval. The ordinal scores of both human and MDD-S are normalized to be within the [0, 1] range, and presented in the yellow box.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which candidate's response is most relevant to the context provided?",
    "answer": "Candidate D",
    "rationale": "The context is \"why haven't you got it?\". Candidate D's response directly addresses this question by providing information about the location of the bus stop.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.07194v2",
    "pdf_url": null
  },
  {
    "instance_id": "9f5e2d1c82ba4f15950f79b6208257f6",
    "figure_id": "2005.00450v1-Figure4-1",
    "image_file": "2005.00450v1-Figure4-1.png",
    "caption": " We evaluate the proposed method on three different vision problems. (a) Object recognition is a standard classification problem consisting in categorizing objects. We evaluate on datasets of increasing difficulty, starting with real world digit recognition and continuing towards increasingly challenging category recognition. (b) AU recognition involves recognizing local, sometimes subtle patterns of facial muscular articulations. Several AUs can be present at the same time, making it a multi-label classification problem. (c) In semantic segmentation, one has to output a dense pixel categorization that properly captures complex semantic structure of an image.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following tasks is the most challenging, based on the information provided in the caption?",
    "answer": "Semantic segmentation",
    "rationale": "The caption states that semantic segmentation requires \"outputting a dense pixel categorization that properly captures complex semantic structure of an image.\" This suggests that semantic segmentation is a more challenging task than object recognition or AU recognition.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.00450v1",
    "pdf_url": null
  },
  {
    "instance_id": "8e1d5753d7e349019c5ab766592a95bb",
    "figure_id": "1904.06487v5-Figure6-1",
    "image_file": "1904.06487v5-Figure6-1.png",
    "caption": " (a) Eigenvalues of the covariance matrix of the features on the target domain. Eigenvalues reduce quickly in our method, which shows that features are more discriminative than other methods. (b) Our method achieves lower entropy than baselines except ENT. (c) Our method clearly reduces domain-divergence compared to S+T.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest A-distance?",
    "answer": "Ours.",
    "rationale": "The A-distance is a measure of domain divergence, and the lower the A-distance, the better. In Figure (c), we can see that the bar for \"Ours\" is the lowest, indicating that our method has the lowest A-distance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.06487v5",
    "pdf_url": null
  },
  {
    "instance_id": "4ff070d719e24839b4d959d24d78fde0",
    "figure_id": "2310.14652v1-Figure5-1",
    "image_file": "2310.14652v1-Figure5-1.png",
    "caption": " Performance improvement (%) of INV-REG over Arcface on fine-grained attributes of CelebA dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which facial attributes are most likely to be mis-labeled as the same one?",
    "answer": "Eyes, Rosy_Cheeks, and Receding_Hairline.",
    "rationale": "The figure shows that the performance improvement of INV-REG over Arcface is highest for these attributes, which suggests that they are most likely to be mis-labeled as the same one.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.14652v1",
    "pdf_url": null
  },
  {
    "instance_id": "1d58a3e7051a44b9b6231a388ce8b321",
    "figure_id": "1906.03499v1-Figure3-1",
    "image_file": "1906.03499v1-Figure3-1.png",
    "caption": " ROC curves of detection methods on CIFAR-10 dataset with ResNet",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which detection method performed the best on the CIFAR-10 dataset with ResNet?",
    "answer": "LID",
    "rationale": "The ROC curves show the performance of different detection methods. The LID method has the highest AUC values for all of the datasets, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.03499v1",
    "pdf_url": null
  },
  {
    "instance_id": "8b1fdb8401564681ace195167d92cf82",
    "figure_id": "2108.02390v2-Figure4-1",
    "image_file": "2108.02390v2-Figure4-1.png",
    "caption": " Illustration of an example where the negation operator of BetaE does not satisfy non-contradiction, i.e. φ(¬q, e) is not monotonically decreasing with regard to φ(q, e).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does the negation operator of BetaE satisfy non-contradiction in this example?",
    "answer": "No.",
    "rationale": "The negation operator of BetaE satisfies non-contradiction if φ(¬q, e) is monotonically decreasing with regard to φ(q, e). In this example, φ(¬q, e) is not monotonically decreasing with regard to φ(q, e). This can be seen from the figure, where the blue dashed line (representing φ(¬q, e)) is not always below the red solid line (representing φ(q, e)).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.02390v2",
    "pdf_url": null
  },
  {
    "instance_id": "f5b435eb5c5a429e9907ae3be8fd0090",
    "figure_id": "2310.18988v1-Figure29-1",
    "image_file": "2310.18988v1-Figure29-1.png",
    "caption": " The effective number of parameters does not increase past the transition threshold on the CIFAR-10 dataset. Plotting ptrain ŝ (orange) and ptest ŝ (green) for the tree (left), boosting (center) and RFF-linear regression (right) experiments, using the original composite parameter axes of [BHMM19].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the most effective number of parameters after the transition threshold?",
    "answer": "Linear Regression.",
    "rationale": "The figure shows the effective number of parameters for three models: trees, boosting, and linear regression. The effective number of parameters for linear regression continues to increase after the transition threshold, while the effective number of parameters for trees and boosting levels off.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.18988v1",
    "pdf_url": null
  },
  {
    "instance_id": "9119c1604df1408797d6779baee3179f",
    "figure_id": "2302.14372v2-Figure14-1",
    "image_file": "2302.14372v2-Figure14-1.png",
    "caption": " The performance changes during fine-tuning. This table reports the score before normalization. The number in bracket is the standard error. Performance was averaged over 10 random seeds.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment has the highest average performance after fine-tuning?",
    "answer": "In7.",
    "rationale": "The table shows the average performance of each environment after fine-tuning. The In7 environment has the highest average performance of 5343.54.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.14372v2",
    "pdf_url": null
  },
  {
    "instance_id": "7cb0401a52154a63af32a319b9de594b",
    "figure_id": "2303.01728v2-Figure10-1",
    "image_file": "2303.01728v2-Figure10-1.png",
    "caption": " Comparison of training reward and test success rate between our method TS2C and other algorithms with shared control.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest test success rate with a Teacher-High setting?",
    "answer": "Importance",
    "rationale": "The top right plot shows the test success rate for different algorithms with a Teacher-High setting. The green line, which represents Importance, has the highest test success rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.01728v2",
    "pdf_url": null
  },
  {
    "instance_id": "3e22e399aaf44f69b2d89888c0597c0c",
    "figure_id": "2308.14713v1-Figure5-1",
    "image_file": "2308.14713v1-Figure5-1.png",
    "caption": " Depth in δ1.25n range for different thresholds on DDAD. We depict the ratio of depth estimates d for which δ(d) < 1.25n where δ(d) = max ( d d∗ , d∗ d ) and d∗ is ground-truth depth. Note that values at n = 1, 2, 3 represent δ1.25, δ1.252 , δ1.253 .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate depth estimates for the entire range of thresholds shown in the plot?",
    "answer": "Ours (Monocular)",
    "rationale": "The plot shows that the Ours (Monocular) method has the highest fraction of depth estimates d for which δ(d) < 1.25n for all values of n shown. This indicates that this method produces the most accurate depth estimates for the entire range of thresholds shown in the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.14713v1",
    "pdf_url": null
  },
  {
    "instance_id": "c387a31dd2f340c39b137178d87a6609",
    "figure_id": "2003.09960v3-Figure1-1",
    "image_file": "2003.09960v3-Figure1-1.png",
    "caption": " Visualization of the dataset via PCA. The left plot shows the transformed data via PCA. The right polt is a 2-dimensional visualization of the dataset using PCA.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of clothing has a larger range of values for PC 1?",
    "answer": "T-shirts/tops",
    "rationale": "The histogram on the left shows the distribution of values for PC 1 for each type of clothing. The T-shirts/tops distribution is wider than the Pullovers distribution, indicating that T-shirts/tops have a larger range of values for PC 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.09960v3",
    "pdf_url": null
  },
  {
    "instance_id": "e948b87e06bb457da04bec36b214bf83",
    "figure_id": "2306.16081v1-Figure2-1",
    "image_file": "2306.16081v1-Figure2-1.png",
    "caption": " Localization error for our proposed methods and baselines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest localization error when using 6 microphones?",
    "answer": "SLF",
    "rationale": "The figure shows the mean localization error for each method with different numbers of microphones. For 6 microphones, the SLF bar is the lowest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.16081v1",
    "pdf_url": null
  },
  {
    "instance_id": "879f0de2ed424beea8038c2cc518b759",
    "figure_id": "2106.10989v4-Figure3-1",
    "image_file": "2106.10989v4-Figure3-1.png",
    "caption": " The CCA similarities between different models, which is normalized to [0, 100].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model combination has the highest CCA similarity for the bottom-layer feature?",
    "answer": "The pre-trained model and the pre-trained model.",
    "rationale": "The figure shows that the pre-trained model and the pre-trained model have the highest CCA similarity for the bottom-layer feature, as the blue bar is the tallest for all datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.10989v4",
    "pdf_url": null
  },
  {
    "instance_id": "a3920c7347a947a4855753c0c9ae1492",
    "figure_id": "2012.11207v4-Figure1-1",
    "image_file": "2012.11207v4-Figure1-1.png",
    "caption": " Transfer success rates of simple transferable attacks using CE or logit loss in the non-targeted and targeted scenarios.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which loss function is most effective for non-targeted attacks? ",
    "answer": " CE loss. ",
    "rationale": " The figure shows that the success rate of non-targeted attacks using CE loss is higher than that of attacks using logit loss. This can be seen by comparing the curves labeled \"CE\" and \"Logit-TMDI\". ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.11207v4",
    "pdf_url": null
  },
  {
    "instance_id": "0d45c69fd64b495593673db52a480ab9",
    "figure_id": "2303.09826v2-Figure10-1",
    "image_file": "2303.09826v2-Figure10-1.png",
    "caption": " Failure cases when LR frames endure severe color distortions around edges.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the SR methods is most successful at restoring the color of the red boot in the Doraemon image?",
    "answer": "VQD-SR",
    "rationale": "The figure shows the original image on the left and the results of different SR methods on the right. The VQD-SR method is the only one that accurately restores the color of the red boot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.09826v2",
    "pdf_url": null
  },
  {
    "instance_id": "9e5f821fa1914148bb7c38be85de3f36",
    "figure_id": "2106.01553v2-Figure7-1",
    "image_file": "2106.01553v2-Figure7-1.png",
    "caption": " Results of image reconstruction. The bottom row shows zoom-in views.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods tested produces the sharpest reconstruction of the original image?",
    "answer": "SPE",
    "rationale": "The bottom row of the figure shows zoom-in views of the reconstructed images. The SPE* image has the sharpest details and is most similar to the original image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01553v2",
    "pdf_url": null
  },
  {
    "instance_id": "4df5a658a47949429172739812523631",
    "figure_id": "2204.06355v1-Figure13-1",
    "image_file": "2204.06355v1-Figure13-1.png",
    "caption": " Ablation study on all games.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the ablation methods generally resulted in the highest scores across all games?",
    "answer": "\"ppo\"",
    "rationale": "The boxplots for \"ppo\" are generally higher than the boxplots for the other two methods, indicating that this method resulted in higher scores.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.06355v1",
    "pdf_url": null
  },
  {
    "instance_id": "59d88c9fcefc46488a0b3a4c618ff371",
    "figure_id": "2109.02762v1-Figure11-1",
    "image_file": "2109.02762v1-Figure11-1.png",
    "caption": " From left to right: Input reference frame, STRIVE, Pix2Pix. Top two are real videos, and bottom two are synthetic videos.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the sharpest and most realistic images?",
    "answer": "STRIVE",
    "rationale": "The STRIVE images are sharper and more realistic than the Pix2Pix images, which are blurry and have artifacts. This is evident in the \"Seeing Survey\" and \"Holt Park\" examples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.02762v1",
    "pdf_url": null
  },
  {
    "instance_id": "ccb7afa9b2f342278f0c6ae0d327b518",
    "figure_id": "2011.11961v4-Figure6-1",
    "image_file": "2011.11961v4-Figure6-1.png",
    "caption": " Comparison on Model Size and Execution Efficiency. fps can be obtained by dividing 1, 000 with the inference time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models has the fastest inference time?",
    "answer": "FDMPA",
    "rationale": "The figure shows the inference time on the x-axis. The model with the smallest value on the x-axis has the fastest inference time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.11961v4",
    "pdf_url": null
  },
  {
    "instance_id": "c3dbc0fd63794a2b940503aaec7be7ac",
    "figure_id": "2110.12344v1-Figure1-1",
    "image_file": "2110.12344v1-Figure1-1.png",
    "caption": " Different random-walk based embedding methods (old and new) classified according to our analytical framework— with process, similarity, and algorithm as main components. A key contribution of this paper is to integrate autocovariance as a similarity metric and show that it outperforms Pointwise Mutual Information (PMI) in link prediction.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three main components of the analytical framework for classifying random-walk based embedding methods?",
    "answer": "Process, similarity, and algorithm.",
    "rationale": "The figure shows a tree diagram with three levels, labeled \"Process\", \"Similarity\", and \"Algorithm\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.12344v1",
    "pdf_url": null
  },
  {
    "instance_id": "628b3c8e2ced4ec88e5e0d9d1067b097",
    "figure_id": "2101.11562v1-Figure1-1",
    "image_file": "2101.11562v1-Figure1-1.png",
    "caption": " Top: An overview of our TDEN with a two-stream decoupled architecture design, consisting of four modules (i.e., the shared object and sentence encoders for contextually encoding each modality inputs, cross-modal encoder for VL understanding, and cross-modal decoder for VL generation. Below: Four VL proxy tasks in TDEN, i.e., MLM, MOC, and ISM for VL understanding, and MSG for VL generation.",
    "figure_type": "** schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What are the four VL proxy tasks in TDEN? ",
    "answer": " MLM, MOC, ISM, and MSG.",
    "rationale": " The four VL proxy tasks are shown in the bottom part of the figure. MLM stands for Masked Language Modeling, MOC stands for Masked Object Classification, ISM stands for Image-Sentence Matching, and MSG stands for Masked Sentence Generation. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.11562v1",
    "pdf_url": null
  },
  {
    "instance_id": "1491dac47df44a9e8104548469248785",
    "figure_id": "2203.07682v3-Figure5-1",
    "image_file": "2203.07682v3-Figure5-1.png",
    "caption": " Visual comparison of the proposed method with various methods for ×4 SR task. Our method restores sharp and complicated structures more accurately.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the sharpest and most accurate image for the \"Comics\" image?",
    "answer": "ACT",
    "rationale": "The figure shows the results of different methods for ×4 SR task. The ACT method produces the sharpest and most accurate image for the \"Comics\" image, as evidenced by the higher PSNR and SSIM values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.07682v3",
    "pdf_url": null
  },
  {
    "instance_id": "d7b18ce4c03d49b7b368697b0fa80a51",
    "figure_id": "1911.07262v1-FigureA.16-1",
    "image_file": "1911.07262v1-FigureA.16-1.png",
    "caption": "Figure A.16: The structure of our end-to-end network which separates highlight, albedo and shading jointly from a image. The structures of H-net and S-Net are shared, which is adopted from (Narihira, Maire, and Yu 2015b) with added batch normalization layers after convolution layers.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How are the H-Net and S-Net connected in the network?",
    "answer": "The H-Net and S-Net share a common structure.",
    "rationale": "The figure shows that the H-Net and S-Net have the same structure, with the same number and type of layers. This is indicated by the \"Shared structure of H-Net and S-Net\" label in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.07262v1",
    "pdf_url": null
  },
  {
    "instance_id": "e0b54973ce964ca99e25e5622ed5c075",
    "figure_id": "2205.12113v2-Figure5-1",
    "image_file": "2205.12113v2-Figure5-1.png",
    "caption": " Accuracy on object control, passive object control, and subject control after prompting with agent and patient questions. Accuracy changes from Fig. 4 are generally consistent within heuristic groups.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the object control task?",
    "answer": "gpt-neo-1.3b",
    "rationale": "The bar for gpt-neo-1.3b is the highest in the \"Object control\" section of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.12113v2",
    "pdf_url": null
  },
  {
    "instance_id": "1ac2a19055454e5bb5ef0e1626c92a7e",
    "figure_id": "2001.09961v1-Figure6-1",
    "image_file": "2001.09961v1-Figure6-1.png",
    "caption": " Empirical study results",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method resulted in the largest improvement in anxiety levels after 7 weeks?",
    "answer": "GD",
    "rationale": "In Figure (d), the line representing the GD method shows the largest decrease in anxiety levels over time, reaching the lowest point at week 7.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.09961v1",
    "pdf_url": null
  },
  {
    "instance_id": "0f5efe6b23364da585180bd4a2fbf8f2",
    "figure_id": "2212.05506v2-Figure4-1",
    "image_file": "2212.05506v2-Figure4-1.png",
    "caption": " Relation between entropy and accuracy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the most consistent relationship between entropy and accuracy?",
    "answer": "SST",
    "rationale": "The figure shows that the accuracy and entropy lines for SST are the most closely aligned, indicating a strong and consistent relationship between the two metrics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.05506v2",
    "pdf_url": null
  },
  {
    "instance_id": "e759bd1b13a0401c9c28d72a3573363a",
    "figure_id": "2002.10876v2-Figure7-1",
    "image_file": "2002.10876v2-Figure7-1.png",
    "caption": " Evaluation curves: shape classification accuracy using different versions of LA over training epochs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which version of LA performs best in terms of shape classification accuracy?",
    "answer": "Full - Eq. (5)",
    "rationale": "The figure shows the accuracy of different versions of LA over training epochs. The Full - Eq. (5) curve is consistently higher than the other two curves, indicating that it has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.10876v2",
    "pdf_url": null
  },
  {
    "instance_id": "6e8eff32e06e49879eaa664008a26499",
    "figure_id": "1909.12077v4-Figure2-1",
    "image_file": "1909.12077v4-Figure2-1.png",
    "caption": " Sample trajectories and learned functions of Task 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models performs the best at predicting the trajectory of the particle?",
    "answer": "SymODEN",
    "rationale": "The figure shows the ground truth trajectory of the particle, as well as the trajectories predicted by the three models. The SymODEN model's trajectory is the closest to the ground truth trajectory, indicating that it is the most accurate model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.12077v4",
    "pdf_url": null
  },
  {
    "instance_id": "10e122123c8f4e16af9b576bb442645e",
    "figure_id": "2204.00926v1-Figure9-1",
    "image_file": "2204.00926v1-Figure9-1.png",
    "caption": " Performance (HT@10) in offline setup on Core User Recommendation of various models on different datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Amazon_CDs dataset?",
    "answer": "SASRec",
    "rationale": "The figure shows the HT@10 performance of different models on different datasets. For the Amazon_CDs dataset, the SASRec model has the highest HT@10 value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.00926v1",
    "pdf_url": null
  },
  {
    "instance_id": "baf19aed75044659abd7324169075b06",
    "figure_id": "2306.07432v1-Figure8-1",
    "image_file": "2306.07432v1-Figure8-1.png",
    "caption": " Fire outperforms SOTA competing algorithms at",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the highest median percentage decrease in test error?",
    "answer": "FIRE",
    "rationale": "The boxplot for FIRE shows the highest median value (represented by the horizontal line within the box) compared to the other methods. This indicates that FIRE achieved a higher percentage decrease in test error than the other methods on average.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.07432v1",
    "pdf_url": null
  },
  {
    "instance_id": "a9965f6a867b404394828f8dece880b0",
    "figure_id": "1909.07877v2-Figure9-1",
    "image_file": "1909.07877v2-Figure9-1.png",
    "caption": " Qualitative comparison of facial attribute transfer. The styles of StarGAN∗ and DMIT are sampled from random noise.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, StarGAN* or DMIT, produces images that are more similar to the input image?",
    "answer": "DMIT.",
    "rationale": "The images produced by DMIT are generally more similar to the input image than the images produced by StarGAN*. This can be seen in the details of the face, such as the eyes, nose, and mouth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.07877v2",
    "pdf_url": null
  },
  {
    "instance_id": "c375feefc9e845a6ac1d710c753b9683",
    "figure_id": "2004.08423v2-Figure3-1",
    "image_file": "2004.08423v2-Figure3-1.png",
    "caption": " Visualization of data in Figure 1 and the corresponding linear regression curves.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest accuracy when the ratio of ACC150/ACC160 is 81.1?",
    "answer": "AccGCN",
    "rationale": "The green line represents the AccGCN method, and it has the highest value on the y-axis when the x-axis value is 81.1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.08423v2",
    "pdf_url": null
  },
  {
    "instance_id": "79dc2358fe064de39235ad20327cecbf",
    "figure_id": "2205.11930v2-Figure1-1",
    "image_file": "2205.11930v2-Figure1-1.png",
    "caption": " According to SPA, scrambling the prompt before using DALL-E creates significantly worse images than using the original prompt, as expected (α = 0.01, Holm-Bonferroni-corrected). The conclusion is the same regardless of whether we ask agents to estimate P [DALL-E-raw DALL-E-scrambled] or vice-versa.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which system is preferred when only one example is shown from each system?",
    "answer": "DALL-E-raw",
    "rationale": "The figure shows that when only one example is shown from each system, the preference probability for DALL-E-raw is higher than the preference probability for DALL-E-scrambled.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.11930v2",
    "pdf_url": null
  },
  {
    "instance_id": "e3748c8482434732b0d3ef89464f8690",
    "figure_id": "1711.06178v1-FigureC.4-1",
    "image_file": "1711.06178v1-FigureC.4-1.png",
    "caption": "Figure C.4: Decision trees trained using λ = 800.0 for a GRU model using Sepsis. The 5 output dimensions are jointly trained.",
    "figure_type": "** Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which feature is most important for predicting in-hospital mortality according to the decision tree?",
    "answer": " Albumin level.",
    "rationale": " The root node of the decision tree for in-hospital mortality is based on the albumin level. This indicates that this feature is the most important for making the initial split in the data and therefore has the greatest influence on the prediction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.06178v1",
    "pdf_url": null
  },
  {
    "instance_id": "300d0184c51a4cd8a91e9b4a6e0d68f0",
    "figure_id": "2010.09676v1-Figure4-1",
    "image_file": "2010.09676v1-Figure4-1.png",
    "caption": " ContactHands dataset statistics. There are 52,050 and 5,893 annotated hand instances in the training and the test set. For each hand instance, we provide contact state annotations by choosing Yes, No, or Unsure.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which contact state is the most common in the training data?",
    "answer": "No-Contact",
    "rationale": "The bar for No-Contact is the tallest in the Train Data plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.09676v1",
    "pdf_url": null
  },
  {
    "instance_id": "10d430908b634b10a7d3b4bda53efb92",
    "figure_id": "2108.00106v2-Figure6-1",
    "image_file": "2108.00106v2-Figure6-1.png",
    "caption": " Accuracy vs Confidence plots for various methods on CIFAR100. NLL is significantly overconfident and NLL + MMCE is somewhat overconfident. While Focal loss is underconfident, augmenting it with Soft Calibration Objectives fixes this issue, resulting in curves closest to the ideal.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most underconfident predictions?",
    "answer": "Focal loss.",
    "rationale": "The plot shows that the curve for Focal loss is below the ideal curve, which means that the model is underconfident in its predictions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.00106v2",
    "pdf_url": null
  },
  {
    "instance_id": "460fe8b3dad844afbb9a00cdf4917e2c",
    "figure_id": "2205.12904v2-FigureA.1-1",
    "image_file": "2205.12904v2-FigureA.1-1.png",
    "caption": "Figure A.1: P-values of the Wilcoxon signed rank test for results on perfect binary trees and decision lists with different parameters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of α leads to the most significant difference in performance between perfect binary trees and decision lists for all tree sizes?",
    "answer": "1.0",
    "rationale": "The p-values in the first column of the heatmap are the lowest for all tree sizes, indicating that the difference in performance is most significant when α = 1.0.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.12904v2",
    "pdf_url": null
  },
  {
    "instance_id": "742e364f50514c1fb3ce92ed01c45d95",
    "figure_id": "2010.02350v2-Figure5-1",
    "image_file": "2010.02350v2-Figure5-1.png",
    "caption": " VAE experiments. We plot the FID scores of tickets on CIFAR-10 and Celeb-A on three models: VAE, β-VAE, and ResNet-VAE. Winning tickets outperform random tickets on all models. Experiments are performed on 5 random runs, and the error bars represent +/− standard deviation across runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the lowest FID score on CIFAR-10 when the sparsity is 90%?",
    "answer": "ResNet-VAE",
    "rationale": "The FID score for ResNet-VAE at 90% sparsity is lower than the FID score for the other models at the same sparsity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02350v2",
    "pdf_url": null
  },
  {
    "instance_id": "dd74a9e1d6194d4294414b7a2010bc25",
    "figure_id": "2305.19366v2-Figure2-1",
    "image_file": "2305.19366v2-Figure2-1.png",
    "caption": " Comparison with the exact posterior distribution, on small graphs with d = 5 nodes. (a) Comparison of the edge features computed with the exact posterior (x-axis) and the approximation given by JSP-GFN (y-axis); each point corresponds to an edge Xi → Xj for each of the 20 datasets. (b) Quantitative evaluation of different methods for joint posterior approximation, both in terms of edge features and cross-entropy of sampling distribution and true posterior P (θ | G,D); all values correspond to the mean and 95% confidence interval across the 20 experiments.",
    "figure_type": "** plot and table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method performs the best in terms of RMSE and Pearson's r?",
    "answer": " JSP-GFN (full)",
    "rationale": " The table in (b) shows the RMSE and Pearson's r values for different methods. JSP-GFN (full) has the lowest RMSE and the highest Pearson's r, indicating that it is the most accurate method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19366v2",
    "pdf_url": null
  },
  {
    "instance_id": "824b9b05d19c4770afd01cfeeb75bb69",
    "figure_id": "1905.02884v1-Figure5-1",
    "image_file": "1905.02884v1-Figure5-1.png",
    "caption": " User study. “Rank x” means the percentage of inpainting results from each approach being chosen as the x-th best.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed best in the user study for fixed region inpainting?",
    "answer": "Ours.",
    "rationale": "The bar for \"Ours\" is the highest for rank 1 in the fixed region inpainting plot. This means that our method was chosen as the best option more often than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.02884v1",
    "pdf_url": null
  },
  {
    "instance_id": "349e087884bc45eda68c43e97015c709",
    "figure_id": "2306.12306v3-Figure16-1",
    "image_file": "2306.12306v3-Figure16-1.png",
    "caption": " CIVILCOMMENTS-WILDS: Detailed results on the o.o.d. evaluation split for the worst group (WG, determined by the accuracy on each group) and averaged over all groups. LL = LastLayer.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of average accuracy?",
    "answer": "LL MultiViVON",
    "rationale": "The table shows the average accuracy for each model. LL MultiViVON has the highest average accuracy of 0.924 ± 0.001.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.12306v3",
    "pdf_url": null
  },
  {
    "instance_id": "37e0778644b640f2a81d43e8e2caebaf",
    "figure_id": "2303.02700v2-Figure5-1",
    "image_file": "2303.02700v2-Figure5-1.png",
    "caption": " Qualitative comparisons on depth estimation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the depth estimation methods in the figure appears to produce the most accurate results?",
    "answer": "Depth_pseudo",
    "rationale": "The Depth_pseudo image most closely resembles the input image, suggesting that it is the most accurate depth estimation. The other two methods, Depth_weak and Depth_dA, produce images that are more distorted and less accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.02700v2",
    "pdf_url": null
  },
  {
    "instance_id": "a8345a4501844025822cf7c6fd52cc84",
    "figure_id": "2112.07435v1-Figure4-1",
    "image_file": "2112.07435v1-Figure4-1.png",
    "caption": " Illustration for the case distinction in the proof of Theorem 3.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens if player j deviates from r1 to r' and r' is not equal to rt?",
    "answer": "The next deviation will be from rt to r' - 1.",
    "rationale": "The figure shows that if player j deviates from r1 to r' and r' is not equal to rt, then the next deviation will be from rt to r' - 1. This is because the figure shows a tree of possible deviations, and the path from the node \"player j deviates from r1 to r'\" to the node \"next dev.: rt -> r' - 1\" is the only path that does not go through the node \"r' = rt\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.07435v1",
    "pdf_url": null
  },
  {
    "instance_id": "e7afbacfdead4c64abcedb96f05f8f90",
    "figure_id": "2206.01379v1-Figure7-1",
    "image_file": "2206.01379v1-Figure7-1.png",
    "caption": " Comparison of propagation time on SBM datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the fastest propagation time on the SBM-10M dataset?",
    "answer": "PPRGo",
    "rationale": "The figure shows that the PPRGo line is the lowest on the SBM-10M plot, which means it has the fastest propagation time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.01379v1",
    "pdf_url": null
  },
  {
    "instance_id": "692e81c9bec242da893bbccd288661ff",
    "figure_id": "2310.00093v2-Figure1-1",
    "image_file": "2310.00093v2-Figure1-1.png",
    "caption": " (a) Data distribution of the distilled images on the CIFAR10 dataset with 50 images per class (IPC50) for CAFE [52] and DataDAM. (b) Performance comparison with state-of-the-art methods on the CIFAR10 dataset for varying IPCs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the CIFAR10 dataset for varying IPCs?",
    "answer": "DataDAM",
    "rationale": "The plot in (b) shows that DataDAM achieves the highest testing accuracy for all IPCs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.00093v2",
    "pdf_url": null
  },
  {
    "instance_id": "13853408b9ea4842a6887f632b08af86",
    "figure_id": "2010.13063v3-Figure5-1",
    "image_file": "2010.13063v3-Figure5-1.png",
    "caption": " Double talk other degradation DMOS of five consecutive test runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model consistently performs the best across all five runs?",
    "answer": "Model 1",
    "rationale": "The plot shows the DMOS score for each model across five runs. Model 1 has the highest DMOS score in all five runs, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.13063v3",
    "pdf_url": null
  },
  {
    "instance_id": "a4244cabbee840aeaa02baeb556cf41d",
    "figure_id": "2206.05871v1-Figure3-1",
    "image_file": "2206.05871v1-Figure3-1.png",
    "caption": " AC@5 for different combinations of rankingmethods and graph construction ones",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which combination of ranking and graph construction methods has the highest median AC@5?",
    "answer": "PC-gsq and RW-Par.",
    "rationale": "The box plot for this combination has the highest median value, as indicated by the horizontal line inside the box.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.05871v1",
    "pdf_url": null
  },
  {
    "instance_id": "62151e24ae9341938587ad15df117721",
    "figure_id": "2211.14029v2-Figure1-1",
    "image_file": "2211.14029v2-Figure1-1.png",
    "caption": " At the beginning of each time slot t ∈ {1, 2, · · · }, a user arrives to choose a path among N + 1 paths in the transportation network in Fig. 1(a). The current travel latency `i(t) of each path i ∈ {0, 1..., N} has linear correlation with last latency `i(t − 1) and evolves according to current user choice in (1) and (2). Path 0 is a safe route and its latency has fixed correlation coefficient α ∈ (0, 1) to change from last round. Yet any risky path i ∈ {1, · · · , N} has a stochastic correlation coefficient αi(t), which alternates between low coefficient stateαL ∈ [0, 1) and high state αH ≥ 1 according to the partially observable Markov chain in Fig. 1(b).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many paths are there in the transportation network?",
    "answer": "There are N + 1 paths.",
    "rationale": "The figure shows a typical parallel transportation network with N + 1 paths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.14029v2",
    "pdf_url": null
  },
  {
    "instance_id": "32909fce94e64f9f88caa114f4e47068",
    "figure_id": "2108.02479v5-Figure5-1",
    "image_file": "2108.02479v5-Figure5-1.png",
    "caption": " Comparison of HJ against other state-of-the-art optimizers in a parallel deployment scenario using 32 workers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer is the fastest to converge to a loss of 0.1 on the NAS-Bench MNIST dataset?",
    "answer": "ASHA.",
    "rationale": "The figure shows the loss of different optimizers over time on the NAS-Bench MNIST dataset. ASHA is the first optimizer to reach a loss of 0.1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.02479v5",
    "pdf_url": null
  },
  {
    "instance_id": "893a138405244a3485a7221e7c0ea503",
    "figure_id": "2210.15097v2-Figure4-1",
    "image_file": "2210.15097v2-Figure4-1.png",
    "caption": " Human evaluation instructions and interface we post to Amazon Mechanical Turk platform.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the criteria used to evaluate the continuations?",
    "answer": "Fluency and coherence.",
    "rationale": "The instructions state that the continuations should be evaluated based on their fluency and coherence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.15097v2",
    "pdf_url": null
  },
  {
    "instance_id": "5b39fde976f24618bf5c8f571cfc46d9",
    "figure_id": "1912.02292v1-Figure6-1",
    "image_file": "1912.02292v1-Figure6-1.png",
    "caption": " SGD vs. Adam. 5-Layer CNNs on CIFAR-10 with no label noise, and no data augmentation. Optimized using SGD for 500K gradient steps, and Adam for 4K epochs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer converges faster, SGD or Adam?",
    "answer": "Adam converges faster than SGD.",
    "rationale": "The plot shows that the training error for Adam decreases more quickly than the training error for SGD.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.02292v1",
    "pdf_url": null
  },
  {
    "instance_id": "e71adc30cf78499f84828e50e920f84f",
    "figure_id": "1811.06072v1-Figure1-1",
    "image_file": "1811.06072v1-Figure1-1.png",
    "caption": " The Complete Results for Dynamic Graph Streams (t = 100 and s = 30)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest communication cost on the Gaussians dataset at time point 50?",
    "answer": "D^2-CAMP",
    "rationale": "The communication cost for each algorithm is shown in Figure (c). At time point 50, the D^2-CAMP algorithm has the lowest communication cost.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.06072v1",
    "pdf_url": null
  },
  {
    "instance_id": "b722c012226c405286bb074efe224e8a",
    "figure_id": "2205.15043v2-Figure5-1",
    "image_file": "2205.15043v2-Figure5-1.png",
    "caption": " Performance comparison under different model sparsity.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the six policies achieved the highest performance when sparsity is high (above 90%)?",
    "answer": "RigL.",
    "rationale": "The figure shows that the purple line corresponding to RigL is the highest among all lines when sparsity is above 90%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.15043v2",
    "pdf_url": null
  },
  {
    "instance_id": "5728e3aa6da04d0f9f56dfb5696cc78b",
    "figure_id": "2304.07313v1-Figure5-1",
    "image_file": "2304.07313v1-Figure5-1.png",
    "caption": " TOP: Different location schedules shown for 8 steps and α = 2.2. Note that Entropy is instance adaptive and we show it for one instance. BOTTOM LEFT: Visualizing how different α look in terms of how much is uncovered after each step (cumulative). BOTTOM RIGHT: Resulting inference bitrate of the various α, shown for different location schedules and different inference steps. Each point is the same MT model evaluated with a different schedule as parameterized by the triplet (α, inference steps, location schedule), where α is shown via the marker.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which location schedule results in the highest bitrate for a fixed number of inference steps?",
    "answer": "Entropy.",
    "rationale": "The bottom right plot shows the bitrate for different location schedules and different inference steps. For a fixed number of inference steps, the Entropy schedule has the highest bitrate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.07313v1",
    "pdf_url": null
  },
  {
    "instance_id": "e674ebc00ec24a25afa9fc909fa14930",
    "figure_id": "2310.18884v1-Figure13-1",
    "image_file": "2310.18884v1-Figure13-1.png",
    "caption": " The distribution of pair-wise cosine similarity calculated by learned representations on randomly sampled node pairs, one-hop neighbors and two-hop neighbors.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest density of similar nodes according to the one-hop neighbors distribution?",
    "answer": "Texas",
    "rationale": "The Texas plot has the highest peak in the one-hop neighbor distribution, indicating that there are more nodes with similar representations in this dataset compared to the others.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.18884v1",
    "pdf_url": null
  },
  {
    "instance_id": "7d6ccd7f0d7046528711e82519e33a62",
    "figure_id": "2010.01367v2-Figure1-1",
    "image_file": "2010.01367v2-Figure1-1.png",
    "caption": " Examples of symmetries on 4-neighbor grids.",
    "figure_type": "** schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the three examples demonstrates rotational symmetry?",
    "answer": " (b) Corridor",
    "rationale": " The figure shows three examples of symmetries on 4-neighbor grids. In the \"Corridor\" example, the grid can be rotated 180 degrees and still look the same. This is because the grid is symmetrical along its vertical axis. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.01367v2",
    "pdf_url": null
  },
  {
    "instance_id": "076817832cbe4d8099d449511cd0b150",
    "figure_id": "2206.04122v2-Figure3-1",
    "image_file": "2206.04122v2-Figure3-1.png",
    "caption": " The tabular version of ESCHER with an oracle value function is competitive with the tabular version of DREAM with an oracle value function and with OS-MCCFR in terms of exploitability (top row). The regret estimator in ESCHER has orders of magnitude lower variance than those of DREAM and OS-MCCFR (bottom row).",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest exploitability and variance in all three games?",
    "answer": "ESCHER.",
    "rationale": "The top row of the figure shows the exploitability of each algorithm as a function of the number of iterations. ESCHER has the lowest exploitability in all three games. The bottom row of the figure shows the variance of the regret estimator for each algorithm. ESCHER has the lowest variance in all three games.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.04122v2",
    "pdf_url": null
  },
  {
    "instance_id": "ff44c4b9e6f54b19b2681107c9753251",
    "figure_id": "2110.08207v3-Figure4-1",
    "image_file": "2110.08207v3-Figure4-1.png",
    "caption": " Results for T0 task generalization experiments compared to GPT-3 (Brown et al., 2020). Each dot is the performance of one evaluation prompt. The baseline T5+LM model is the same as T0 except without multitask prompted training. GPT-3 only reports a single prompt for each dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Winograd task?",
    "answer": "T0 (11B)",
    "rationale": "The figure shows that the T0 (11B) model has the highest score on the Winograd task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.08207v3",
    "pdf_url": null
  },
  {
    "instance_id": "ec8082f72f664ee8b441842df9a776ac",
    "figure_id": "2205.14691v3-Figure5-1",
    "image_file": "2205.14691v3-Figure5-1.png",
    "caption": " Reward and cost of mixture attackers of MC and MR",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three algorithms has the highest reward for the Car-Run task?",
    "answer": "ADV-PPO (MC)",
    "rationale": "The figure shows that the ADV-PPO (MC) algorithm has the highest reward for the Car-Run task, as its line is consistently above the other two lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.14691v3",
    "pdf_url": null
  },
  {
    "instance_id": "e84240cadd4f4ab9a3542099a0ff03c2",
    "figure_id": "2009.04972v3-Figure8-1",
    "image_file": "2009.04972v3-Figure8-1.png",
    "caption": " MOS histograms of the top 3 models and baseline",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best according to the MOS histograms?",
    "answer": "Team 21 ST NE MOS",
    "rationale": "The MOS histograms show the distribution of MOS scores for each model. The higher the MOS score, the better the model performs. The Team 21 ST NE MOS model has the highest MOS scores, as its histogram is shifted furthest to the right.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.04972v3",
    "pdf_url": null
  },
  {
    "instance_id": "6df17d6b139843ae8e20f7a02be44389",
    "figure_id": "2302.09731v1-Figure11-1",
    "image_file": "2302.09731v1-Figure11-1.png",
    "caption": " (a) A toy sample generated by ”A bird in the sky”. (b) A toy sample generated by ”A bird is on the ground” (c) A toy sample generated by ”A plane in the sky”. (d) A toy sample generated by ”A plane is on the ground”",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the images shows a plane on the ground?",
    "answer": "Image (d)",
    "rationale": "Image (d) shows a plane on the ground. The other images show a bird in the sky, a bird on the ground, and a plane in the sky.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.09731v1",
    "pdf_url": null
  },
  {
    "instance_id": "d4489a15e96a4d6f895c9b2ff8157832",
    "figure_id": "1903.08225v2-Figure9-1",
    "image_file": "1903.08225v2-Figure9-1.png",
    "caption": " Example of obtained solution for Make Fish Curry task. Outputs of the classifier are shown in blue. Correctly localized steps are shown in green. False detections are shown in red. Ground truth intervals for the steps are shown in yellow.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which step in the process of making fish curry took the longest amount of time?",
    "answer": "Stirring the mixture.",
    "rationale": "The figure shows that the step of stirring the mixture took the longest amount of time, as indicated by the long yellow bar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.08225v2",
    "pdf_url": null
  },
  {
    "instance_id": "efe35dda1d4343d69a2a6acef0ee79ac",
    "figure_id": "2211.04332v2-Figure3-1",
    "image_file": "2211.04332v2-Figure3-1.png",
    "caption": " Mean runtime of different methods as a function of the input length. The dotted line represents a real-time factor (RTF) of 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest for input lengths of 5 seconds or less?",
    "answer": "DeGLI",
    "rationale": "The plot shows the runtime of each method as a function of the input length. For input lengths of 5 seconds or less, the DeGLI line is below the other lines, indicating that it has the shortest runtime.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.04332v2",
    "pdf_url": null
  },
  {
    "instance_id": "e13197e314d2453ea65adf0cd78d78ac",
    "figure_id": "2009.13827v1-Figure4-1",
    "image_file": "2009.13827v1-Figure4-1.png",
    "caption": " Case studies on synonym discovery. Entities discovered only by SynSetExpan are colored in green.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which entity in the \"War involving USA\" class was discovered only by SynSetExpan?",
    "answer": "Operation Desert Storm.",
    "rationale": "The entity \"Operation Desert Storm\" is colored in green, which indicates that it was discovered only by SynSetExpan.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.13827v1",
    "pdf_url": null
  },
  {
    "instance_id": "883776d794f54b08b559d8cbd8f23a05",
    "figure_id": "1911.06149v2-Figure3-1",
    "image_file": "1911.06149v2-Figure3-1.png",
    "caption": " Mel spectrogram of source speech (top row) and converted speeches (other rows).",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which emotion seems to have the highest frequency components?",
    "answer": "Surprise.",
    "rationale": "The mel spectrogram shows the frequency content of the speech signal over time. The y-axis of the mel spectrogram represents frequency, and the color of the spectrogram represents the intensity of the frequency components. The surprise spectrogram has the most intense high-frequency components, which indicates that it has the highest frequency components.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.06149v2",
    "pdf_url": null
  },
  {
    "instance_id": "4ccb3a18dd574cf6a253d57173a281f7",
    "figure_id": "2105.14953v1-Figure7-1",
    "image_file": "2105.14953v1-Figure7-1.png",
    "caption": " Our attention shows smaller errors than other FCbased and fixed attention methods in PhysioNet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in terms of minimizing the mean squared error (MSE)?",
    "answer": "FC-ACE-Latent-ODE (ODE enc.)",
    "rationale": "The figure shows the MSE for three different models over time. The FC-ACE-Latent-ODE (ODE enc.) model has the lowest MSE throughout the entire testing period.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14953v1",
    "pdf_url": null
  },
  {
    "instance_id": "9735541fab974be380c7e2d3d590082e",
    "figure_id": "2106.00596v2-Figure8-1",
    "image_file": "2106.00596v2-Figure8-1.png",
    "caption": " Our agent completes a Heat & Place task “Put a heated apple next to the lettuce on the middle shelf in the refrigerator” in an unseen environment.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the agent doing in the first step?",
    "answer": "The agent is turning right and facing the sink.",
    "rationale": "The figure shows the agent's view of the kitchen, with the sink directly in front of them. The text box above the image indicates that the agent is turning right.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.00596v2",
    "pdf_url": null
  },
  {
    "instance_id": "5fe88010fa6844adb2a6cfab92f2bd13",
    "figure_id": "1905.09400v1-Figure5-1",
    "image_file": "1905.09400v1-Figure5-1.png",
    "caption": " Synthetic Dataset Samples. Example images taken from the three synthetic datasets proposed in [24].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three synthetic datasets proposed in [24] seems to be the most realistic?",
    "answer": "MBG",
    "rationale": "The MBG dataset contains images of handwritten digits superimposed on natural images, which makes it more realistic than the other two datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.09400v1",
    "pdf_url": null
  },
  {
    "instance_id": "bc19ae1883c34869b4ce5d57afbfe41b",
    "figure_id": "1812.00353v2-Figure4-1",
    "image_file": "1812.00353v2-Figure4-1.png",
    "caption": " Histogram of dropout rates in each layer of VGG16 and ResNet50 (RRBP).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network has a wider range of dropout rates?",
    "answer": "ResNet50",
    "rationale": "The histograms in (b) show a wider range of dropout rates than the histograms in (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.00353v2",
    "pdf_url": null
  },
  {
    "instance_id": "8da07a17d47c49df8968125145146401",
    "figure_id": "2305.16806v4-Figure2-1",
    "image_file": "2305.16806v4-Figure2-1.png",
    "caption": " Annotation Interface Screenshot for Human Evaluation. The translations are randomized b/w the systems under evaluation to prevent biased evaluation.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which translation is more literal, System A or System B?",
    "answer": "System A is more literal.",
    "rationale": "System A uses the same words as the source sentence, while System B paraphrases some of the words. For example, System A translates \"assembly\" as \"Versammlung\", while System B translates it as \"Parlament\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16806v4",
    "pdf_url": null
  },
  {
    "instance_id": "6bb0088b32e04a9c907767a2e6af1bd6",
    "figure_id": "2010.14498v2-FigureA.12-1",
    "image_file": "2010.14498v2-FigureA.12-1.png",
    "caption": "Figure A.12: TD error vs. Effective rank on Atari. We observe that Huber-loss TD error is often higher when there is a larger implicit under-parameterization, measured in terms of drop in effective rank. The results are shown for the data-efficient online RL setting.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which game environment seems to be the most difficult for DQN to learn in terms of TD error?",
    "answer": "Breakout.",
    "rationale": "The Breakout plot shows the highest TD error among all the games.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.14498v2",
    "pdf_url": null
  },
  {
    "instance_id": "673b114f87814af6b31441c4c859a011",
    "figure_id": "2305.12535v1-Figure7-1",
    "image_file": "2305.12535v1-Figure7-1.png",
    "caption": " ALTI-Logit MRR alignment scores across layers (GPT-2 XL). Horizontal dashed lines refer to random alignment.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task is better aligned with ALTI-Logit across layers, Subject-Verb Agreement or Indirect Object Identification?",
    "answer": "Indirect Object Identification",
    "rationale": "The figure shows that the MRR score for Indirect Object Identification is consistently higher than the MRR score for Subject-Verb Agreement across all layers. This indicates that the ALTI-Logit model is better aligned with the Indirect Object Identification task than the Subject-Verb Agreement task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.12535v1",
    "pdf_url": null
  },
  {
    "instance_id": "527d5f94b7264d4782b205ddfa47c7fe",
    "figure_id": "2205.15301v1-Figure17-1",
    "image_file": "2205.15301v1-Figure17-1.png",
    "caption": " The differences in cross-attention between fig-par and lit-wfw visualised per layer, per language.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the largest difference in cross-attention between fig-par and lit-wfw for the Italian language?",
    "answer": "Layer 4",
    "rationale": "The figure shows the difference in cross-attention between fig-par and lit-wfw for each layer and language. The Italian language is represented by the green line. In subplot (c), the green line is highest at layer 4.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.15301v1",
    "pdf_url": null
  },
  {
    "instance_id": "155a5ec727ba4c85a773c9eba57e9e31",
    "figure_id": "2304.03937v1-Figure7-1",
    "image_file": "2304.03937v1-Figure7-1.png",
    "caption": " Ablation study on Affine transformation on SYMSOL I dataset. We plot log likelihood and spread evolving with training iterations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods achieves the highest log likelihood on the SYMSOL I dataset?",
    "answer": "Mobius + Affine (Ours)",
    "rationale": "The figure shows that the Mobius + Affine (Ours) method has the highest log likelihood among the four methods tested. This can be seen by observing the final value of the red line in the left plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.03937v1",
    "pdf_url": null
  },
  {
    "instance_id": "32aa97db78ab4f51ac88a89a1a9ae47c",
    "figure_id": "1909.00303v2-Figure2-1",
    "image_file": "1909.00303v2-Figure2-1.png",
    "caption": " RSMs showing (Spearman’s ρ) correlation between disagreement among layers i and j (VCorrLi−Lj ) and Vtotfix (left) and VY ngve (Right). BERT layers are denoted with numbers from 1 (topmost) to 24 (lowest).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer of BERT shows the highest correlation with Vtotfix and VY ngve?",
    "answer": "Layer 1",
    "rationale": "The RSMs show the correlation between disagreement among layers i and j and Vtotfix and VY ngve. The color of the squares in the RSMs indicates the strength of the correlation, with red indicating a strong positive correlation and blue indicating a strong negative correlation. The squares in the top left corner of each RSM, which correspond to layer 1, are the most red, indicating that layer 1 has the highest correlation with both Vtotfix and VY ngve.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.00303v2",
    "pdf_url": null
  },
  {
    "instance_id": "d92f455df7334744af429aa346d46426",
    "figure_id": "2212.00373v3-Figure3-1",
    "image_file": "2212.00373v3-Figure3-1.png",
    "caption": " Average faithfulness (%) of SOIREDL and RE2RNN on test sets at different noise levels δ.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is more faithful to the original data at higher noise levels?",
    "answer": "SOIREDL.",
    "rationale": "The plot shows that the faithfulness of SOIREDL is consistently higher than that of RE2RNN across all noise levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.00373v3",
    "pdf_url": null
  },
  {
    "instance_id": "67cf0460feb042868847a31c6a9cc80f",
    "figure_id": "2204.12294v1-Figure4-1",
    "image_file": "2204.12294v1-Figure4-1.png",
    "caption": " ROC curve showing relation between true positive rate and false positive rate of the evaluated baseline methods. The IRSEmethod outperforms both the IR and SEmethods by achieving lower false positive rate at most evaluated true positive rates.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods has the best performance?",
    "answer": "The IRSE method.",
    "rationale": "The IRSE method has the highest true positive rate and the lowest false positive rate, as shown by the blue line in the ROC curve. This means that the IRSE method is more accurate than the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.12294v1",
    "pdf_url": null
  },
  {
    "instance_id": "a03d083e7da743888bea4cfbe8398478",
    "figure_id": "1904.09816v1-Figure2-1",
    "image_file": "1904.09816v1-Figure2-1.png",
    "caption": " Regularization term of adversarial dropout indicates three statistical relationships between the outputs of randomly dropped networks and of adversarially dropped networks: (1) the variances of two outputs, (2) the covariance between two outputs, and (3) the distance between the means.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following factors contributes to the distance between the means of the outputs of randomly dropped networks and of adversarially dropped networks?",
    "answer": "The variances of the two outputs.",
    "rationale": "The distance between the means of two distributions is related to the variances of the two distributions. In this case, the variances of the outputs of randomly dropped networks and of adversarially dropped networks are shown in the figure as the widths of the two bell curves. The wider the bell curve, the greater the variance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.09816v1",
    "pdf_url": null
  },
  {
    "instance_id": "781a33d66a7c4c27abd27b8d1678cc53",
    "figure_id": "1904.03525v1-Figure6-1",
    "image_file": "1904.03525v1-Figure6-1.png",
    "caption": " 3D face reconstruction results compared to MoFA [35] on samples from the 300VW dataset [32] (first row) and the CelebA dataset [24] (second row). The reconstructed shapes of MoFA suffer from unnatural surface deformations when dealing with challenging texture, i.e. beard. By contrast, our non-linear coloured mesh decoder is more robust to these variations.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more robust to variations in texture, such as beards?",
    "answer": "CMD.",
    "rationale": "The figure shows that the MoFA method produces unnatural surface deformations when dealing with challenging textures, such as beards. By contrast, the CMD method is more robust to these variations and produces more accurate reconstructions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03525v1",
    "pdf_url": null
  },
  {
    "instance_id": "0cf990329df54156a322d153607635e8",
    "figure_id": "1905.09690v3-Figure4-1",
    "image_file": "1905.09690v3-Figure4-1.png",
    "caption": " Performances for the real datasets. The score of each RNN based model is standardized by subtracting the score of the neural network based model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best for the Music dataset?",
    "answer": "The piecewise constant model.",
    "rationale": "The figure shows the average negative log-likelihood of each model relative to the neural network model. The lower the score, the better the model performed. For the Music dataset, the piecewise constant model has the lowest score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.09690v3",
    "pdf_url": null
  },
  {
    "instance_id": "2384f086720842d2a68f56eb6c7302b4",
    "figure_id": "1809.02156v2-Figure6-1",
    "image_file": "1809.02156v2-Figure6-1.png",
    "caption": " Difference in percentage of sentences with no hallucination for TopDown and FC models when SPICE scores fall into specific ranges. For sentences with low SPICE scores, the hallucination is generally larger for the FC model, even though the SPICE scores are similar, see Section 3.4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "For which SPICE score range is the difference in the percentage of sentences with no hallucination between the TopDown and FC models the greatest?",
    "answer": "0-10",
    "rationale": "The bar for the 0-10 SPICE score range is the longest, indicating that the difference in the percentage of sentences with no hallucination between the TopDown and FC models is the greatest for this range.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.02156v2",
    "pdf_url": null
  },
  {
    "instance_id": "8f18ddd822ab4a6c9437c86be44a4f7e",
    "figure_id": "1905.12052v3-Figure5-1",
    "image_file": "1905.12052v3-Figure5-1.png",
    "caption": " Variance of the ELBO’s gradient’s first dimension for GRG [22], RSVI [18], IRG [4], and MVK (ours) when fitting a variational posterior to Categorical data with 100 dimensions and a Dirichlet prior. They fit a Dirichlet. We fit a MV-Kumaraswamy using K = 100 samples from Uniform(O) to Monte-Carlo approximate the full expectation; this corresponds to linear complexity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest variance in the ELBO's gradient's first dimension?",
    "answer": "MVK",
    "rationale": "The figure shows the variance of the ELBO's gradient's first dimension for different methods. The MVK method has the lowest variance across all values of α.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12052v3",
    "pdf_url": null
  },
  {
    "instance_id": "a88c1ab2cd5f46e1b9425fee7e1a8189",
    "figure_id": "1812.05634v2-Figure9-1",
    "image_file": "1812.05634v2-Figure9-1.png",
    "caption": " Comparison of our approach to state-of-the-art video description approaches (VideoStory [13], Transformer [76], MoveForwardTell [67]). Red/bold indicates content errors, blue/italic indicates repetitive patterns.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the video descriptions is most accurate for the first video story?",
    "answer": "Ground Truth",
    "rationale": "The Ground Truth column provides the most detailed and accurate description of the video story, including the actions of the woman and the dog, as well as the camera movements. The other columns either contain errors or are repetitive.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.05634v2",
    "pdf_url": null
  },
  {
    "instance_id": "3e823fbe576a452cb15572539b5eee3c",
    "figure_id": "1906.07987v1-Figure7-1",
    "image_file": "1906.07987v1-Figure7-1.png",
    "caption": " Atari. For each number of train rollouts, we normalize the MSVE of each algorithm A by MSVE(A)/maxA′ MSVE(A′). Absolute numbers and confidence intervals are in the appendix.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the Breakout game?",
    "answer": "Adaptive TD(3)",
    "rationale": "The figure shows that Adaptive TD(3) has the lowest MSVE for all numbers of train rollouts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.07987v1",
    "pdf_url": null
  },
  {
    "instance_id": "999ffbe9a0a4400285396fc0e58ea5a2",
    "figure_id": "1804.08328v1-Figure5-1",
    "image_file": "1804.08328v1-Figure5-1.png",
    "caption": " Transfer results to normals (upper) and 2.5D Segmentation (lower) from 5 different source tasks. The spread in transferability among different sources is apparent, with reshading among top-performing ones in this case. Task-specific networks were trained on 60x more data. “Scratch” was trained from scratch without transfer learning.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which source task is most similar to the target task of surface normal estimation?",
    "answer": "Reshading.",
    "rationale": "The figure shows that the results for the reshading task are most similar to the ground truth for surface normal estimation. This suggests that the reshading task is most similar to the target task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.08328v1",
    "pdf_url": null
  },
  {
    "instance_id": "5e95b8643dcd42a6ac5bf5cc2ca6252f",
    "figure_id": "1810.12890v1-Figure3-1",
    "image_file": "1810.12890v1-Figure3-1.png",
    "caption": " ImageNet validation accuracy against keep_prob with ResNet-50 model. All methods drop activation units in group 3 and 4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which regularization technique appears to be the most effective in improving the accuracy of the ResNet-50 model on the ImageNet validation set?",
    "answer": "DropBlock scheduling.",
    "rationale": "The figure shows the validation accuracy of the ResNet-50 model for different regularization techniques and keep_prob values. DropBlock scheduling achieves the highest accuracy at all keep_prob values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.12890v1",
    "pdf_url": null
  },
  {
    "instance_id": "d6daa34191704b9a8c687eea985ea565",
    "figure_id": "2209.10767v2-Figure9-1",
    "image_file": "2209.10767v2-Figure9-1.png",
    "caption": " (a) CIDEr score on validation set during training and (b) Mean-IOU score on validation set during training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves higher CIDEr scores on the validation set during training?",
    "answer": "LCP (Ours)",
    "rationale": "The blue line in Figure (a) represents the CIDEr scores for LCP (Ours), and the orange line represents the CIDEr scores for LCP without DTL. The blue line is consistently higher than the orange line, indicating that LCP (Ours) achieves higher CIDEr scores.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.10767v2",
    "pdf_url": null
  },
  {
    "instance_id": "08aacbc0fa104e3184ad5a8d4b06e4cd",
    "figure_id": "2306.03355v1-Figure12-1",
    "image_file": "2306.03355v1-Figure12-1.png",
    "caption": " Impact of neighbor number 𝐾 .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the optimal number of neighbors for this dataset based on the graph?",
    "answer": "500",
    "rationale": "The graph shows that the top-1 accuracy is highest when the number of neighbors is 500.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.03355v1",
    "pdf_url": null
  },
  {
    "instance_id": "1c4c507a1546439cad90b7dc1dd555be",
    "figure_id": "2111.10952v2-Figure3-1",
    "image_file": "2111.10952v2-Figure3-1.png",
    "caption": " Within-family correlations for each dataset in a task family, using models from Table 2. Performance on datasets from some task families are highly correlated (e.g., NLI) whereas other task families have more erratic results across their datasets (e.g., Dialogue).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task family has the most consistent performance across its datasets?",
    "answer": "NLI",
    "rationale": "The figure shows the correlations between the performance of different models on different datasets within each task family. The correlations for NLI are all very high, indicating that the performance of models on different NLI datasets is very consistent.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.10952v2",
    "pdf_url": null
  },
  {
    "instance_id": "42c208c8f249494f87a8e6bb2348bf22",
    "figure_id": "1902.01894v1-Figure8-1",
    "image_file": "1902.01894v1-Figure8-1.png",
    "caption": " Time cost breakdown for different methods.",
    "figure_type": "schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the most time-consuming setup phase?",
    "answer": "PBT.",
    "rationale": "The figure shows the time cost breakdown for each method, and the setup phase for PBT is the longest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.01894v1",
    "pdf_url": null
  },
  {
    "instance_id": "fb638fd5205d463d8e3f293cd3fe405a",
    "figure_id": "2110.02619v2-Figure6-1",
    "image_file": "2110.02619v2-Figure6-1.png",
    "caption": " Worst group error rate with increasing ratio of majority to minority. Shaded region shows one standard error. All estimates are aggregated over three runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest worst-group error rate when the ratio of majority to minority is 500?",
    "answer": "ERM-UW",
    "rationale": "The black line in the figure represents the ERM-UW method, and it has the highest value at the point where the x-axis value is 500.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02619v2",
    "pdf_url": null
  },
  {
    "instance_id": "24f372bfc06b455eab5429e3bd5a59b0",
    "figure_id": "2208.11021v1-Figure3-1",
    "image_file": "2208.11021v1-Figure3-1.png",
    "caption": " Accuracy (%) of baseline (GNN), FT, LRP, ATA and our model for 1/5-shot cross-domain classification on both novel classes and base classes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in the 5-shot setting for novel classes?",
    "answer": "Our model.",
    "rationale": "The figure shows that our model achieves the highest accuracy (82.56%) for novel classes in the 5-shot setting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.11021v1",
    "pdf_url": null
  },
  {
    "instance_id": "f6c69e55f880416d9e7188425446c984",
    "figure_id": "2102.06062v2-Figure1-1",
    "image_file": "2102.06062v2-Figure1-1.png",
    "caption": " (a) Test accuracy (%) on various datasets with LP-MST for T > 2. The curve “CIFAR-10 w/ pre-train” is using CIFAR-100 as public data to pre-train the model. (b) RRWithPrior with priors obtained from histogram query based on clustering in various SSL representations. We also plot recent results (ClusterRR) from Esfandiari et al. [37], which is a clustering based LabelDP algorithm.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest test accuracy on CIFAR-100?",
    "answer": "ClusterRR",
    "rationale": "The figure shows the test accuracy of different methods on CIFAR-100. ClusterRR achieves the highest test accuracy of about 80%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.06062v2",
    "pdf_url": null
  },
  {
    "instance_id": "a69e80dca98840e29e67971f8f1261dc",
    "figure_id": "2306.03291v2-Figure13-1",
    "image_file": "2306.03291v2-Figure13-1.png",
    "caption": " Autoregressive tensors learned by different models (Forward Locomotion and Reversal): (A-C) One-dimensional autoregressive filters learned in two states by SALT, SLDS, ARHMM (identified as forward and reverse), and (D) by a GLM. AVB and RIB are known to mediate forward locomotion, while AVA and AVE are involved in initiating reversals [31, 32, 37, 38].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model learns autoregressive filters that are most consistent with the known roles of AVB, RIB, AVA, and AVE in forward locomotion and reversals?",
    "answer": "The SALT model.",
    "rationale": "The SALT model learns autoregressive filters that are consistent with the known roles of AVB, RIB, AVA, and AVE in forward locomotion and reversals. Specifically, the SALT model learns filters that show strong positive weights for AVB and RIB during forward locomotion and strong positive weights for AVA and AVE during reversals. This is consistent with the known roles of these neurons in mediating forward locomotion and reversals.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.03291v2",
    "pdf_url": null
  },
  {
    "instance_id": "4b142e111f174e5982a60da1118300ca",
    "figure_id": "2109.10052v1-Figure9-1",
    "image_file": "2109.10052v1-Figure9-1.png",
    "caption": " Examples of emotion profiles for the multilingual models. It showcases that these models are much more positive about all social groups in comparison to the monolingual models. Whereas we observed that monolingual models primarily encode negative associations for most groups, associations encoded within the multilingual models are more balanced between positive and negative sentiments.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which social group is most positively viewed by all three models?",
    "answer": "liberals",
    "rationale": "The color of the squares in the figure represents the sentiment of the model towards each social group. Yellow represents positive sentiment, and black represents negative sentiment. The squares for liberals are mostly yellow for all three models, indicating that these models have a positive view of liberals.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.10052v1",
    "pdf_url": null
  },
  {
    "instance_id": "b0cefa740f7e48628d2cfb8ba8655759",
    "figure_id": "2202.03415v1-Figure5-1",
    "image_file": "2202.03415v1-Figure5-1.png",
    "caption": " Prediction MAE under prediction window 1 and 5",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best in terms of MAE for respiratory diseases when the prediction window was set to 1?",
    "answer": "GRU",
    "rationale": "The bar for GRU is the shortest among all the models in the left panel of the figure, which shows the MAE for respiratory diseases when the prediction window is set to 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.03415v1",
    "pdf_url": null
  },
  {
    "instance_id": "c74a84e65b624d3c908d268ad324f774",
    "figure_id": "2201.03720v3-Figure9-1",
    "image_file": "2201.03720v3-Figure9-1.png",
    "caption": " Impact of semantic margin parameter mψ on retrieval for different datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest recall at mψ = 4.0?",
    "answer": "Scholarly",
    "rationale": "The green line in the figure represents the recall for the Scholarly dataset. At mψ = 4.0, the green line is the highest, indicating that the Scholarly dataset has the highest recall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.03720v3",
    "pdf_url": null
  },
  {
    "instance_id": "3601c5bf005546c09b61aca15ddcec93",
    "figure_id": "2206.05825v4-Figure23-1",
    "image_file": "2206.05825v4-Figure23-1.png",
    "caption": " Head-to-head experiments.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which opponent performed the best in the Phantom Tic-Tac-Toe game?",
    "answer": "MMD",
    "rationale": "The box plot for MMD in the Phantom Tic-Tac-Toe game is the highest, indicating that it achieved the highest expected return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.05825v4",
    "pdf_url": null
  },
  {
    "instance_id": "609031fbc4dd4ffb87106d4471905370",
    "figure_id": "2305.10345v2-Figure5-1",
    "image_file": "2305.10345v2-Figure5-1.png",
    "caption": " The visualization of human pose estimation using four modalities.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which modality provides the most detailed information about the person's pose?",
    "answer": "RGB",
    "rationale": "The RGB image shows the person in full detail, including their clothing, facial expression, and body position. The other modalities provide less detailed information, such as the LiDAR image, which only shows the person's silhouette.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.10345v2",
    "pdf_url": null
  },
  {
    "instance_id": "5737db688d9e485bab5c59bf49a93ff9",
    "figure_id": "1905.12156v1-Figure6-1",
    "image_file": "1905.12156v1-Figure6-1.png",
    "caption": " Results on our synthetic dataset. References for the baseline methods could be found in Table 1. “GT” represent ground truth.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the sharpest image?",
    "answer": "Ours",
    "rationale": "The figure shows the results of different methods for image deblurring. The \"Ours\" column shows the results of the proposed method, which produces the sharpest images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12156v1",
    "pdf_url": null
  },
  {
    "instance_id": "74229eaec80a4feb8b33312b0c864245",
    "figure_id": "2204.08106v1-Figure11-1",
    "image_file": "2204.08106v1-Figure11-1.png",
    "caption": " Average Accuracy and Efficiency Comparison for Weighted Incremental Setting: On the left, we plot the average relative error of Wdshp and HWC, and on the right, we compare the average update time of Wdshp, HWC, and Exact. The average is taken over the entire duration.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest average update time?",
    "answer": "Exact",
    "rationale": "The average update time is shown in the right panel of the figure. The Exact method has the lowest average update time, as shown by the green bars.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.08106v1",
    "pdf_url": null
  },
  {
    "instance_id": "63d488101b084cccbd7f00e41e262ab1",
    "figure_id": "2302.13110v1-Figure2-1",
    "image_file": "2302.13110v1-Figure2-1.png",
    "caption": " Instance showing that the optimum of pIM dp cannot be upper bounded in terms of iIM dp .",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the weight of the edge from u2 to vN?",
    "answer": "1",
    "rationale": "The number next to the edge indicates the weight of the edge.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.13110v1",
    "pdf_url": null
  },
  {
    "instance_id": "71a25c2ffd8648aab41f42cf9bd28595",
    "figure_id": "2009.04972v3-Figure6-1",
    "image_file": "2009.04972v3-Figure6-1.png",
    "caption": " P-values of ANOVA test of the top 5 teams.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which team has the most statistically significant difference in performance compared to the other teams?",
    "answer": "Team 21",
    "rationale": "The p-value for Team 21 compared to all other teams is 0.00, which is less than the significance level of 0.05. This means that the difference in performance between Team 21 and the other teams is statistically significant.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.04972v3",
    "pdf_url": null
  },
  {
    "instance_id": "6699ea7d07b4418cb0d3339a9dfc16d4",
    "figure_id": "2110.12485v1-Figure5-1",
    "image_file": "2110.12485v1-Figure5-1.png",
    "caption": " Comparing Heap Search and A∗",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is more efficient in terms of the number of programs it can solve within a given time limit?",
    "answer": "A",
    "rationale": "The plot shows that for any given time limit, A* can solve more programs than Heap Search. For example, at the 1-second mark, A* has solved about 10^4 programs, while Heap Search has only solved about 10^3 programs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.12485v1",
    "pdf_url": null
  },
  {
    "instance_id": "285edb263d834a9380359225497280b7",
    "figure_id": "2306.03899v2-Figure6-1",
    "image_file": "2306.03899v2-Figure6-1.png",
    "caption": " Qualitative results of label-free 2D semantic segmentation on ScanNet dataset. From the left to the right are the input, prediction by SAM, prediction by CLIP, ours w/o CNS, our full method, and ground truth, respectively.",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method produces the most accurate segmentation results?",
    "answer": " Our full method.",
    "rationale": " The figure shows that our full method produces segmentation results that are most similar to the ground truth. This can be seen by comparing the segmentation results in the \"Ours\" column to the ground truth in the \"Ground truth\" column.\n\n**Figure type:** Photograph(s)",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.03899v2",
    "pdf_url": null
  },
  {
    "instance_id": "a1cb826e046244b48dbc7fa05023764b",
    "figure_id": "2208.02108v3-Figure7-1",
    "image_file": "2208.02108v3-Figure7-1.png",
    "caption": " Variation of log likelihoods for different entities on the whole testing dataset (anomalies are highlighted in red, and the blue line is the threshold according to Sck).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which entity has the most anomalies?",
    "answer": "P102",
    "rationale": "The figure shows the variation of log-likelihoods for different entities on the whole testing dataset. Anomalies are highlighted in red. P102 has the most red highlighted regions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.02108v3",
    "pdf_url": null
  },
  {
    "instance_id": "7a18cfaed0494cc2a460a332bb48da92",
    "figure_id": "2210.15897v1-Figure4-1",
    "image_file": "2210.15897v1-Figure4-1.png",
    "caption": " Tone-mapped HDR images comparison between different methods. DrTMO [7] and Deep Recursive HDRI [20] produce artifacts in extremely high dynamic range regions, SingleHDR [23] appears to have checkboard artifacts, while our method can recover details in these regions pleasingly.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most visually pleasing results?",
    "answer": "Our method.",
    "rationale": "The figure shows that DrTMO and Deep Recursive HDRI produce artifacts in extremely high dynamic range regions, SingleHDR appears to have checkboard artifacts, while our method can recover details in these regions pleasingly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.15897v1",
    "pdf_url": null
  },
  {
    "instance_id": "893a18ef17bd4d9ea86ef8868b95296e",
    "figure_id": "1902.08412v1-Figure2-1",
    "image_file": "1902.08412v1-Figure2-1.png",
    "caption": " Comparison with logistic regression baseline on CITESEER.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on CITESEER, Clean or Meta-Self (5%)?",
    "answer": "Clean",
    "rationale": "The figure shows that the Clean model achieves a higher accuracy than the Meta-Self (5%) model on CITESEER for all three methods (CLN, GCN, and Log. reg.).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.08412v1",
    "pdf_url": null
  },
  {
    "instance_id": "52e00ce065434d26a03034731057f3cc",
    "figure_id": "2204.11992v2-Figure3-1",
    "image_file": "2204.11992v2-Figure3-1.png",
    "caption": " Reduction in total cost due to using our approach (µ∗ and α SimAnn+Greedy) for selecting tight pickup windows, compared to using naı̈ve window selection, with three different offline VRP solvers: VROOM ( ), Google OR-Tools ( ), and our greedy and simulated annealing algorithms as offline VRP solvers ( ).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the offline VRP solvers has the most consistent performance in terms of cost reduction?",
    "answer": "Google OR-Tools",
    "rationale": "The box plot for Google OR-Tools is the most compact, indicating that the data is more tightly clustered around the median. This suggests that Google OR-Tools has the most consistent performance in terms of cost reduction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.11992v2",
    "pdf_url": null
  },
  {
    "instance_id": "b60ab1fad7244e94bae3e87ae9c8c8a0",
    "figure_id": "1909.03042v2-Figure1-1",
    "image_file": "1909.03042v2-Figure1-1.png",
    "caption": " Dev set statistics, illustrating median and quartile for each of the 3 categories under our scalar probability scheme. Light / dark shade covers 96% / 50% of each category, and the bar denotes the median. Note that x-axis is logistic to allow fine-grained distinctions near 0.0 and 1.0.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which category has the most spread in its probability distribution?",
    "answer": "NEU",
    "rationale": "The figure shows the median and quartiles for each category. The category with the largest difference between the upper and lower quartiles has the most spread in its probability distribution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.03042v2",
    "pdf_url": null
  },
  {
    "instance_id": "e314b5d3b1c84f0ba975458a696de115",
    "figure_id": "2004.14008v2-Figure4-1",
    "image_file": "2004.14008v2-Figure4-1.png",
    "caption": " Distributions between human scores and automatically computed scores by each method (English).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest correlation with human scores?",
    "answer": "Ours S_R",
    "rationale": "The figure shows that the Ours S_R method has the highest correlation with human scores, as the points are more tightly clustered around a positive linear trend.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14008v2",
    "pdf_url": null
  },
  {
    "instance_id": "67f25b32f9574c2a8573cb6018bdc4af",
    "figure_id": "2208.09788v2-Figure5-1",
    "image_file": "2208.09788v2-Figure5-1.png",
    "caption": " “Inference Cost” denotes the time taken for a single face-swap. FSGAN, with 400× FaceOff’s inference cost, fails to swap the identities fully. DeepFakes and DeepFaceLabs swap the identities successfully but are 9000× less efficient than FaceOff. FaceOff perfectly swaps source identity and expressions. None of the other methods can swap source expressions.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most efficient in terms of inference cost?",
    "answer": "FaceOff.",
    "rationale": "The figure shows the inference cost of each method, with FaceOff having the lowest cost of 1x.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.09788v2",
    "pdf_url": null
  },
  {
    "instance_id": "c05331ef25134ee1aee457e082420a7f",
    "figure_id": "2010.04495v1-Figure6-1",
    "image_file": "2010.04495v1-Figure6-1.png",
    "caption": " Evolution of average accuracy during the continual learning experience with 20 tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the Split CIFAR-100 dataset?",
    "answer": "MC-SGD",
    "rationale": "The figure shows the average accuracy of different methods on the Split CIFAR-100 dataset. MC-SGD has the highest accuracy across all tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.04495v1",
    "pdf_url": null
  },
  {
    "instance_id": "ac56378b666f47fbaa429d117df06b54",
    "figure_id": "2005.00504v1-Figure1-1",
    "image_file": "2005.00504v1-Figure1-1.png",
    "caption": " Plot of the function f(p)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the maximum value of f(p) in the plot?",
    "answer": "0.0255",
    "rationale": "The plot shows the function f(p) as a function of p. The maximum value of f(p) occurs at the point (0.185, 0.0255). This can be seen by observing the peak of the curve in the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.00504v1",
    "pdf_url": null
  },
  {
    "instance_id": "5918260e2a71438e9a1dc0ec944ff95d",
    "figure_id": "2109.04689v1-Figure7-1",
    "image_file": "2109.04689v1-Figure7-1.png",
    "caption": " Each horizontal bar represents a distribution of the first word of a question in a dataset. Each color represents the proportion of the corresponding word as the first word in a question. The 12 words shown here are the most frequent first words in all the dataset. This figure shows that (SC)2QA has more diverse initial words of questions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the most questions that start with the word \"What\"?",
    "answer": "SQuAD",
    "rationale": "The figure shows that the blue bar for SQuAD is the longest, indicating that it has the highest proportion of questions that start with the word \"What\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.04689v1",
    "pdf_url": null
  },
  {
    "instance_id": "c10525a16eb040ac81a0790db8dfaddb",
    "figure_id": "2206.04361v1-Figure10-1",
    "image_file": "2206.04361v1-Figure10-1.png",
    "caption": " Test accuracy on the PubMed dataset under different levels of feature, edge and label sparsity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods performs the best when there is a high level of label sparsity?",
    "answer": "GCN+AIR",
    "rationale": "The figure shows that the GCN+AIR method has the highest test accuracy for all levels of label sparsity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.04361v1",
    "pdf_url": null
  },
  {
    "instance_id": "00e5e97d492547c5932cc712e351221b",
    "figure_id": "2111.01007v1-Figure13-1",
    "image_file": "2111.01007v1-Figure13-1.png",
    "caption": " Uncurated Results for LSUN church (2562). The images are selected randomly given one global random seed. We recommend zooming in for comparison.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GAN architecture generated the most realistic images of churches?",
    "answer": "Projected GAN.",
    "rationale": "The images generated by Projected GAN appear more realistic and detailed than the images generated by StyleGAN2-ADA and FastGAN. For example, the images in the Projected GAN column have more realistic textures and lighting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.01007v1",
    "pdf_url": null
  },
  {
    "instance_id": "10b4dd2b74d34e14a2c45d89dfb466a3",
    "figure_id": "1911.08360v1-Figure5-1",
    "image_file": "1911.08360v1-Figure5-1.png",
    "caption": " Upper- and lower-bounds on the values of the games G(1, 3), G(1, 2), G(2, 2), G(2, 1), and G(3, 1).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which game has the highest value for a budget ratio of 1.5?",
    "answer": "G(1, 3)",
    "rationale": "The figure shows that the upper bound for G(1, 3) is the highest at a budget ratio of 1.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.08360v1",
    "pdf_url": null
  },
  {
    "instance_id": "cb74fd29dcc64e74b284f026c997a692",
    "figure_id": "2305.12283v2-Figure1-1",
    "image_file": "2305.12283v2-Figure1-1.png",
    "caption": " Synthetic data where the underlying distribution is obtained by a combination of sine functions. The solid lines denote predicted means, the shaded area denotes predicted intervals between 97.5% and 2.5% quantiles, and the yellow dots denote a subset of real observations. The leftmost plot gives the real mean as well as oracle quantile values, while the rest two plots are predictions from different calibration models. The middle plot is produced by a Deep Ensemble (Lakshminarayanan et al., 2017) of 5 HNNs trained with 40,000 samples, which is both a common benchmark and a building block for several regression calibration methods. The rightmost plot is produced by our proposed nonparametric calibration method – Algorithm 2 NRC of which the base regression model is an ordinary feed-forward regression network. The detailed setup is given in Appendix E.2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest uncertainty in its predictions?",
    "answer": "Our Algorithm NRC.",
    "rationale": "The shaded area in the rightmost plot is wider than in the other two plots, indicating that the model has a higher degree of uncertainty in its predictions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.12283v2",
    "pdf_url": null
  },
  {
    "instance_id": "d97e596d784542ea88fd2c3849551a67",
    "figure_id": "2212.05055v2-Figure11-1",
    "image_file": "2212.05055v2-Figure11-1.png",
    "caption": " Final upstream and downstream performance for upcycled B/16 vision models with different number of experts per MoE layer. The number of MoE layers is fixed at 6. The upcycled model is trained for an additional 7 epochs (from 14 to 21) relative to the original dense model. The dashed horizontal lines show the performance of the dense model when trained for an additional 7 epochs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on ImageNet 10-shot accuracy when the number of experts is 128?",
    "answer": "The upcycling model.",
    "rationale": "The figure shows that the upcycling model (orange line) has a higher ImageNet 10-shot accuracy than the original dense model (blue line) when the number of experts is 128.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.05055v2",
    "pdf_url": null
  },
  {
    "instance_id": "afe1363c34014496803125593245511a",
    "figure_id": "2210.12048v3-Figure5-1",
    "image_file": "2210.12048v3-Figure5-1.png",
    "caption": " ORCHID curvatures are non-redundant. We show distributions of ORCHID edge curvatures (top) and edge-averaged node curvatures (bottom) using probability measures µEN, µEE, and µWE with smoothing α, for the aggregation functions AGGM (light blue) and AGGA (dark blue) on dblp-v hypergraphs representing top conferences in machine learning and in theoretical computer science.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which smoothing parameter results in the most similar distributions of ORCHID edge curvatures for the two aggregation functions AGGM and AGGA in machine learning conferences?",
    "answer": "0.1",
    "rationale": "The figure shows the distributions of ORCHID edge curvatures for different smoothing parameters and aggregation functions. For machine learning conferences, the distributions for AGGM and AGGA are most similar when the smoothing parameter is 0.1. This can be seen by comparing the two violin plots in the top left panel of Figure (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12048v3",
    "pdf_url": null
  },
  {
    "instance_id": "e6663c4759a74a8d98440d4d70f53bd5",
    "figure_id": "1803.09720v1-Figure3-1",
    "image_file": "1803.09720v1-Figure3-1.png",
    "caption": " The 15 most common medical specialties represented in the dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which medical specialty is most common in the dataset?",
    "answer": "Surgery",
    "rationale": "The plot shows that the percentage of cases for surgery is higher than that of other medical specialties.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1803.09720v1",
    "pdf_url": null
  },
  {
    "instance_id": "5a69eb1776204504ab2602b6692a6368",
    "figure_id": "1810.03370v3-Figure4-1",
    "image_file": "1810.03370v3-Figure4-1.png",
    "caption": " The projected inequalities (13), (17), (20), and (21) define the convex outer approximation on (gli, h l i).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which inequality defines the lower bound of the shaded region?",
    "answer": "Inequality (21)",
    "rationale": "The shaded region is bounded below by the line segment labeled (21).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.03370v3",
    "pdf_url": null
  },
  {
    "instance_id": "d06decf2c194479fbcc4bf9d8c892fe4",
    "figure_id": "2003.02570v6-Figure7-1",
    "image_file": "2003.02570v6-Figure7-1.png",
    "caption": " Validation accuracy of Conv2, Conv4, and Conv13 on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three optimizers, LaPerm (He Uniform), Adam, or LA, achieves the highest validation accuracy on Conv2?",
    "answer": "LaPerm (He Uniform)",
    "rationale": "The figure shows the validation accuracy of three optimizers on Conv2. The blue line, which represents LaPerm (He Uniform), reaches the highest point on the y-axis, indicating the highest validation accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.02570v6",
    "pdf_url": null
  },
  {
    "instance_id": "b3c24d5810db44ec8cd9b9b2756be55c",
    "figure_id": "2012.08512v3-Figure10-1",
    "image_file": "2012.08512v3-Figure10-1.png",
    "caption": " Interpolation results for 2× interpolation on Middleburry test set. The leftmost images in each row repesent the overlayed inputs. The first row in each set represents the interpolated frame, while the second row shows the error maps with respect to the ground truth. IE shows the interpolation error of the method. The interpolation errors for all the baselines are reported on the official leaderboard.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest interpolation error for the basketball image?",
    "answer": "SSMO",
    "rationale": "The figure shows the interpolation results for four different methods on three different images. The interpolation error (IE) is shown below each image. For the basketball image, SSMO has the lowest IE of 5.37.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.08512v3",
    "pdf_url": null
  },
  {
    "instance_id": "f77c9c872ddb49949397feb266868e93",
    "figure_id": "1910.10053v1-Figure2-1",
    "image_file": "1910.10053v1-Figure2-1.png",
    "caption": " Adversarial Patches. Obtained for different optical flow networks. The size is enlarged for visualization purposes.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network seems to be the most vulnerable to adversarial patches?",
    "answer": "Back2Future",
    "rationale": "The adversarial patches for Back2Future are the most distinct and easiest to see, even at the smallest size. This suggests that this network is the most vulnerable to adversarial attacks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.10053v1",
    "pdf_url": null
  },
  {
    "instance_id": "e13893204f1f4dfc943f6dac20de34ec",
    "figure_id": "2102.07804v4-Figure1-1",
    "image_file": "2102.07804v4-Figure1-1.png",
    "caption": " Test accuracy and nodes removed for varying amounts of `1 regularization. The plots correspond to classifiers with different architectures on the (a)-(g) MNIST, (h)-(n) CIFAR-10, and (o)-(u) CIFAR-100 datasets. For each dataset, we keep the ranges of all the axes of the smaller plots same as the bigger plot but hide the ticks for brevity. Networks trained with `1 regularization can be exactly compressed, even when regularization improves accuracy. In light red background, test accuracy is better than with no regularization (blue curve) and exact compression occurs (red curve).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "For which dataset and architecture does the model achieve the highest test accuracy with L1 regularization?",
    "answer": "CIFAR-100 with a 2 x 200 architecture.",
    "rationale": "The highest point on the blue curve for CIFAR-100 is at around 58%, which is higher than the highest point on the blue curve for any of the other datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.07804v4",
    "pdf_url": null
  },
  {
    "instance_id": "1b669a037c4645fe98fd976eb037d01d",
    "figure_id": "1903.12370v4-Figure3-1",
    "image_file": "1903.12370v4-Figure3-1.png",
    "caption": " Long-run Langevin samples of recent energybased models. Probability mass is concentrated on images that have unrealistic appearance. From left to right: Wasserstein-GAN critic on Oxford flowers (Arjovsky, Chintala, and Bottou 2017), WINN on Oxford flowers (Lee et al. 2018), conditional EBM on ImageNet (Du and Mordatch 2019). The W-GAN critic is not trained to be an unnormalized density but we include samples for reference.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is able to generate realistic images?",
    "answer": "The Conditional EBM model.",
    "rationale": "The figure shows that the Conditional EBM model is able to generate images that are more realistic than the W-GAN and WINN models. The images generated by the W-GAN and WINN models are blurry and have unrealistic colors, while the images generated by the Conditional EBM model are sharper and have more realistic colors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.12370v4",
    "pdf_url": null
  },
  {
    "instance_id": "35685bb36a3f4d6ab45ea4bbeb20012f",
    "figure_id": "2003.08560v4-Figure6-1",
    "image_file": "2003.08560v4-Figure6-1.png",
    "caption": " Ablation study of repeated GCN blocks. 1 GCN, 2 GCN, 3 GCN and 4 GCN represent stacking corresponding number of the GCN blocks",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which number of GCN blocks gives the best mean precision, recall, and F1 score?",
    "answer": "4 GCN blocks.",
    "rationale": "The figure shows that the mean precision, recall, and F1 score all increase with the number of GCN blocks, and reach their highest values when 4 GCN blocks are used.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.08560v4",
    "pdf_url": null
  },
  {
    "instance_id": "125c30f561f24cad956a51ceed9ea33b",
    "figure_id": "2101.06561v4-Figure4-1",
    "image_file": "2101.06561v4-Figure4-1.png",
    "caption": " Annotation interfaces for the datasets of four tasks integrated in GENIE.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task in GENIE requires annotators to judge the fluency of summaries?",
    "answer": "Summarization (XSUM).",
    "rationale": "The figure shows the annotation interface for the Summarization task, which includes questions about the fluency of two summaries.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.06561v4",
    "pdf_url": null
  },
  {
    "instance_id": "1c80592ff9164dec8a463bbe9d9abc13",
    "figure_id": "2105.11763v4-Figure3-1",
    "image_file": "2105.11763v4-Figure3-1.png",
    "caption": " Q3 - Cumulative runtime evolution enhancements on incrementality and constrainedness.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the best performance in terms of cumulative exploration time?",
    "answer": "OUSb+SS, caching",
    "rationale": "The blue line in the figure represents the OUSb+SS, caching algorithm. It has the lowest cumulative exploration time for all numbers of literals derived.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.11763v4",
    "pdf_url": null
  },
  {
    "instance_id": "e16519d4764a4384b831e3e6875fc41e",
    "figure_id": "1905.11342v3-Figure30-1",
    "image_file": "1905.11342v3-Figure30-1.png",
    "caption": " Screenshot of question using Archive-It facsimile surrogates for Egypt Revolution and Politics",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following web pages is most likely to be about the January 25th Revolution in Egypt?",
    "answer": "We are all Khaled Said.",
    "rationale": "The description of the web page mentions \"Working against torture and inhuman treatment of Egyptians in their own country. Standing up against corruption in Egypt.\" This is consistent with the goals of the January 25th Revolution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11342v3",
    "pdf_url": null
  },
  {
    "instance_id": "ec0046fa15484e4595436aa0dfdc468b",
    "figure_id": "1911.04146v1-Figure1-1",
    "image_file": "1911.04146v1-Figure1-1.png",
    "caption": " An example instance of MAC corresponding to an instance of NAE3SAT with 4 variables and 2 clauses x1 ∨ x2 ∨ ¬x3 and x1 ∨ ¬x2 ∨ x4: each rectangular represents an action; each line (including the bottom line) in a rectangular represents one or multiple agents, whose names are recorded to the right of the line; the number to the left of a line represents the cost for the agents to take this action; in particular, the number to the left of the top line of a rectangular represents the reward for the action.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the reward for action A1?",
    "answer": "The reward for action A1 is δ.",
    "rationale": "The reward for an action is represented by the number to the left of the top line of the corresponding rectangular. In this case, the reward for action A1 is δ.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.04146v1",
    "pdf_url": null
  },
  {
    "instance_id": "563266a1d8704ceda5ecf3aabf3143c1",
    "figure_id": "2206.03040v1-Figure5-1",
    "image_file": "2206.03040v1-Figure5-1.png",
    "caption": " Performance as a function of the trade-off hyper-parameter _. For sub-figures (1) and (2), we plot the relative degradation from Keep-All (closer to zero, the better).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the unintended task?",
    "answer": "Keep-All.",
    "rationale": "The plot shows that Keep-All has the lowest relative degradation from Keep-All on the unintended task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.03040v1",
    "pdf_url": null
  },
  {
    "instance_id": "417df15a41e94fd894f7fb6be5e393ed",
    "figure_id": "2209.00456v2-Figure5-1",
    "image_file": "2209.00456v2-Figure5-1.png",
    "caption": " Sensitivity analysis of the weight of contrastive loss.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which category is more sensitive to changes in the weight of contrastive loss?",
    "answer": "Beauty",
    "rationale": "The plot for Beauty shows a larger change in Recall@40 as the weight of contrastive loss is varied, compared to the plot for Office. This indicates that the Beauty category is more sensitive to changes in the weight of contrastive loss.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.00456v2",
    "pdf_url": null
  },
  {
    "instance_id": "759ed6cfa8544a518f494d345c1de44a",
    "figure_id": "1904.03736v2-Figure6-1",
    "image_file": "1904.03736v2-Figure6-1.png",
    "caption": " Average success rate of RL models with different reward functions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which reward function leads to the highest average success rate?",
    "answer": "KL + Rep",
    "rationale": "The figure shows that the KL + Rep line is consistently higher than the other lines, indicating that it has the highest average success rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03736v2",
    "pdf_url": null
  },
  {
    "instance_id": "b2d385c1a3f74486967d2962c3bff95e",
    "figure_id": "1909.12127v2-Figure3-1",
    "image_file": "1909.12127v2-Figure3-1.png",
    "caption": " NLL loss for event time prediction without marks (left) and with marks (right). NLL of each model is standardized by subtracting the score of LogNormMix. Lower score is better. Despite its simplicity, LogNormMix consistently achieves excellent loss values.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model consistently performs the best on both event time prediction tasks?",
    "answer": "LogNormMix.",
    "rationale": "The figure shows the standardized test NLL loss for different models on event time prediction tasks without marks (left) and with marks (right). Lower scores are better. The NLL of each model is standardized by subtracting the score of LogNormMix, so a score of 0 indicates that the model performs as well as LogNormMix. The figure shows that LogNormMix consistently achieves the lowest scores, indicating that it performs the best on both tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.12127v2",
    "pdf_url": null
  },
  {
    "instance_id": "c73595484a0d441d9bb19ccdc9608032",
    "figure_id": "1911.06939v4-Figure4-1",
    "image_file": "1911.06939v4-Figure4-1.png",
    "caption": " Distribution of the (pitch, rp) and (yaw, ry) data points (with (yaw, pitch) being the ground truth gaze in head coordinate system (HCS)), for different training folds of a dataset (a) Eyediap. (b) Columbia Gaze (discrete and sparse gaze label). (c) UTMultiview.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the datasets shows the most spread in the pitch and yaw data points?",
    "answer": "UTMultiview.",
    "rationale": "The UTMultiview dataset has the most spread in the pitch and yaw data points, as shown in the figure. The data points are more dispersed in the UTMultiview dataset than in the Eyediap and Columbia Gaze datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.06939v4",
    "pdf_url": null
  },
  {
    "instance_id": "1bf9e3936a48452b804fd046dc214448",
    "figure_id": "2105.05222v2-Figure1-1",
    "image_file": "2105.05222v2-Figure1-1.png",
    "caption": " Evolution of the number of publications referring to sign language in its title from computer science venues and in the ACL anthology. Publications in computer science are extracted from the Semantic Scholar archive (Ammar et al., 2018).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which data source shows a more consistent increase in the number of publications related to sign language?",
    "answer": "All Computer Science",
    "rationale": "The blue line in the plot, which represents the number of publications from all computer science venues, shows a steady increase over time, while the orange line, which represents the number of publications in the ACL Anthology, shows a more erratic pattern.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.05222v2",
    "pdf_url": null
  },
  {
    "instance_id": "f87e739c12954b17bd13c30c57ec2b3c",
    "figure_id": "2301.05674v1-Figure2-1",
    "image_file": "2301.05674v1-Figure2-1.png",
    "caption": " Network of friends for Example 2",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In the figure, how many friends does person 1 have?",
    "answer": "Person 1 has 2 friends.",
    "rationale": "The figure shows a network of friends, where each person is represented by a number.  Lines connect people who are friends.  Person 1 is connected to persons 5 and 2, indicating that they are friends.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.05674v1",
    "pdf_url": null
  },
  {
    "instance_id": "0abb4ca7b5d44aca8a5d257df285f848",
    "figure_id": "2010.12883v1-Figure1-1",
    "image_file": "2010.12883v1-Figure1-1.png",
    "caption": " Plots of EUBO and ELBO when (a) unlearning from De and (b) retraining with Dr.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between EUBO and ELBO when unlearning from De?",
    "answer": "EUBO decreases while ELBO increases.",
    "rationale": "The figure shows that the EUBO curve decreases with increasing iterations while the ELBO curve increases with increasing iterations. This suggests that unlearning from De leads to a decrease in EUBO and an increase in ELBO.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12883v1",
    "pdf_url": null
  },
  {
    "instance_id": "5db1942e0d624764a1282fa4d4dbbe77",
    "figure_id": "2106.08909v3-Figure6-1",
    "image_file": "2106.08909v3-Figure6-1.png",
    "caption": " Performance of all three algorithms with reverse KL regularization across mixtures between halfcheetah-random and halfcheetah-medium. Error bars indicate min and max over 3 seeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best when there is a large amount of medium data and a small amount of random data?",
    "answer": "The one-step algorithm.",
    "rationale": "The figure shows the performance of the algorithms for different mixtures of random and medium data. The x-axis shows the proportion of medium data, and the y-axis shows the normalized return. The one-step algorithm has the highest normalized return when there is a large amount of medium data and a small amount of random data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.08909v3",
    "pdf_url": null
  },
  {
    "instance_id": "814f07ffa263431084700fa7cf11f6dd",
    "figure_id": "2306.09347v2-Figure15-1",
    "image_file": "2306.09347v2-Figure15-1.png",
    "caption": " The qualitative results of different point cloud pretraining approaches pretrained on the raw data of nuScenes [26] and fine-tuned with 1% labeled data. To highlight the differences, the correct / incorrect predictions are painted in gray / red, respectively. Best viewed in color.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pretraining approach performs the best in predicting the location of objects in the scene?",
    "answer": "Seal (Ours)",
    "rationale": "The error map for Seal (Ours) shows the least amount of red, which indicates that it has the fewest incorrect predictions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.09347v2",
    "pdf_url": null
  },
  {
    "instance_id": "dcde33964e694569a0245b608defd480",
    "figure_id": "1903.03096v4-Figure15-1",
    "image_file": "1903.03096v4-Figure15-1.png",
    "caption": " The performance across different shots, with 95% confidence intervals, shown separately for each evaluation dataset. All models had been trained on (the training splits of) all datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the Aircraft dataset?",
    "answer": "The ProtoNet model.",
    "rationale": "The figure shows the performance of different models on different datasets. The ProtoNet model has the highest class precision on the Aircraft dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.03096v4",
    "pdf_url": null
  },
  {
    "instance_id": "0c884358996e4acca982de455f7149b7",
    "figure_id": "2112.05136v1-Figure9-1",
    "image_file": "2112.05136v1-Figure9-1.png",
    "caption": " Exemplar images and questions on conceptual problems",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many tables are in the room?",
    "answer": "3",
    "rationale": "There are three tables in the room, one with brown legs and a cyan top, one with a red top and brown legs, and one with a white top and brown legs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.05136v1",
    "pdf_url": null
  },
  {
    "instance_id": "68e34af53a6e411aab25ba52a6273feb",
    "figure_id": "1902.02907v1-Figure5-1",
    "image_file": "1902.02907v1-Figure5-1.png",
    "caption": " Performance in 3D Gridworld using best tested annealing schedule. Compare to figure 2 (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in the 3D Gridworld environment?",
    "answer": "The TD Source-SR algorithm.",
    "rationale": "The figure shows the performance of different algorithms in the 3D Gridworld environment. The TD Source-SR algorithm has the lowest error rate, which means it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.02907v1",
    "pdf_url": null
  },
  {
    "instance_id": "6db18eb053bb44dc98f950907139db80",
    "figure_id": "2204.02601v1-Figure6-1",
    "image_file": "2204.02601v1-Figure6-1.png",
    "caption": " Sparsity of different layers pruned by regularization-based pruning vs. the sparsity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer is the most sparse when the network sparsity is 50%?",
    "answer": "Layer 1",
    "rationale": "The figure shows that the sparsity of Layer 1 is the highest when the network sparsity is 50%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.02601v1",
    "pdf_url": null
  },
  {
    "instance_id": "d1e189f3bcab47b7b8edaf66d0516e4c",
    "figure_id": "2211.12005v3-Figure3-1",
    "image_file": "2211.12005v3-Figure3-1.png",
    "caption": " The accuracy trend of training a CIFAR-10 ResNet18 using different data protection methods (ε = 2/255, None means no protection). All methods have high training accuracy v.s. low testing accuracy, but SEP, equipped with FA and VR, reduces model’s generalization most effectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which data protection method is the most effective at reducing the model's generalization?",
    "answer": "SEP + FA + VR",
    "rationale": "The figure shows that the SEP + FA + VR method has the lowest testing accuracy, which indicates that it is the most effective at reducing the model's generalization.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.12005v3",
    "pdf_url": null
  },
  {
    "instance_id": "eaefd154e9434ca2a5030134b15b5734",
    "figure_id": "2008.06952v4-Figure1-1",
    "image_file": "2008.06952v4-Figure1-1.png",
    "caption": " Test Error for d = 10 on the neural architectures of Section 3.1",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which neural architecture has the lowest test error for the potential function?",
    "answer": "S3",
    "rationale": "The plot for the potential function shows that the test error for S3 is consistently lower than the test error for S1 and S2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.06952v4",
    "pdf_url": null
  },
  {
    "instance_id": "327f32c55c804585983c315a4edbb9b0",
    "figure_id": "1910.05789v2-Figure13-1",
    "image_file": "1910.05789v2-Figure13-1.png",
    "caption": " Accuracy of various models when used to predict human behavior in all of the human-AI trajectories. The standard errors for DRL are across the 5 training seeds, while for the human models we only use 1 seed. For each seed, we perform 100 evaluation runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in the \"Forced Coord.\" setting?",
    "answer": "SP",
    "rationale": "The figure shows the prediction accuracy of various models in different settings. In the \"Forced Coord.\" setting, the SP model has the highest prediction accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.05789v2",
    "pdf_url": null
  },
  {
    "instance_id": "6a73c1d466314d8abe3a4846fad6b558",
    "figure_id": "2109.01819v1-Figure2-1",
    "image_file": "2109.01819v1-Figure2-1.png",
    "caption": " Results on GLUE dev sets across (a) epochs and (b) days. Each point is a checkpoint pretrained for 1 ≤ n ≤ 5 day(s).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pretraining task results in the highest GLUE score on the small dataset after 5 days of pretraining?",
    "answer": "MLM + NSP",
    "rationale": "The figure shows that the MLM + NSP line is the highest after 5 days of pretraining on the small dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.01819v1",
    "pdf_url": null
  },
  {
    "instance_id": "e77ac60e27ec43b4bc710a481e6cce38",
    "figure_id": "1810.04586v1-Figure2-1",
    "image_file": "1810.04586v1-Figure2-1.png",
    "caption": " FourRoom Env.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the shape of the central obstacle in the FourRoom Env?",
    "answer": "The central obstacle is a cross shape.",
    "rationale": "The image shows a schematic of the FourRoom Env. The central obstacle is clearly visible as a cross shape.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.04586v1",
    "pdf_url": null
  },
  {
    "instance_id": "277ce08fb74d46ff966db027244e970d",
    "figure_id": "2207.08675v2-Figure3-1",
    "image_file": "2207.08675v2-Figure3-1.png",
    "caption": " 2D Darcy Flow: Error on test set during training. We train a NN architecture with the PDE residual loss function (“soft constraint” baseline), and the same NN architecture with our PDE-CL (“hard constraint”). During training, we track error on the test set, which we plot on a log-log scale. (a) PDE residual loss on the test set, during training. This loss measures how well the PDE is enforced. (b) Relative error on the test set, during training. This metric measures the distance between the predicted solution and the target solution obtained via finite differences. Both measures show that our hard-constrained PDE-CL model starts at a much lower error (over an order of magnitude lower) on the test set at the very start of training, and continues to decrease as training proceeds. This is particularly visible when tracking the PDE residual test loss.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method results in a lower PDE residual loss on the test set at the very start of training?",
    "answer": "The hard-constrained PDE-CL model.",
    "rationale": "The figure shows that the hard-constrained PDE-CL model (blue line) starts at a much lower error (over an order of magnitude lower) on the test set at the very start of training, and continues to decrease as training proceeds. This is particularly visible when tracking the PDE residual test loss.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.08675v2",
    "pdf_url": null
  },
  {
    "instance_id": "b2e639223471424abce158c75dcb492d",
    "figure_id": "2208.13298v4-Figure11-1",
    "image_file": "2208.13298v4-Figure11-1.png",
    "caption": " Additional Results from Robotics experiments. See text for details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest success rate for the HandReach task with a single seed?",
    "answer": "DDPG+HER",
    "rationale": "The figure shows the success rate of different algorithms on the HandReach task with a single seed. The DDPG+HER algorithm has the highest success rate, as shown by the red line in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.13298v4",
    "pdf_url": null
  },
  {
    "instance_id": "22712368eb154553b0bf25181514cae6",
    "figure_id": "1906.01747v1-Figure3-1",
    "image_file": "1906.01747v1-Figure3-1.png",
    "caption": " Score distribution for groups in MEPS and CS dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which race group has the highest utilization score in the MEPS dataset?",
    "answer": "Black.",
    "rationale": "The figure shows that the utilization score for the Black group is consistently higher than that of the other race groups, as indicated by the blue line being higher than all other lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.01747v1",
    "pdf_url": null
  },
  {
    "instance_id": "f2eced655b324d70b0dea42d3194b1f1",
    "figure_id": "2012.03107v3-Figure10-1",
    "image_file": "2012.03107v3-Figure10-1.png",
    "caption": " Large data regime – FOOD101 and FOOD101N. (a) Bar plots showing the best mean accuracy, for curriculum (blue), anti-curriculum (purple), random-curriculum (green), and standard i.i.d. training (grey) with three ways of calculating the standard training baseline for FOOD101 standard-time training (left), FOOD101 short-time training (middle), and FOOD101N training (right)4. (b) For FOOD101 standard-time training (left), we plot the means over three seeds for all 540 strategies, and 180 means over three standard training runs (grey). For FOOD101 short-time training (middle), we plot the means over three seeds for all 216 strategies, and 72 means over three standard training runs. For FOOD101N (right), we reported the values for all 216 strategies and 72 standard training runs. See Figure 5 for additional details. In FOOD101 standardtime training, we observe no statistically significant improvement from curriculum, anti-curriculum, or random training. However, for FOOD101 short-time training and FOOD101N training, CL provides a robust benefit. 4 Since we only run one random seed for FOOD101N, we plot the average of all standard runs (standard1) and the best run over 72 strategies (standard2).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which dataset and training time regime did the curriculum learning strategies provide the most robust benefit?",
    "answer": "FOOD101N training.",
    "rationale": "The caption states that \"for FOOD101 short-time training and FOOD101N training, CL provides a robust benefit.\" This is supported by the bar plots in Figure (a), which show that the curriculum learning strategies (blue and purple bars) consistently outperform the standard training baseline (grey bars) in both FOOD101 short-time and FOOD101N training. In contrast, the curriculum learning strategies do not show a clear advantage in FOOD101 standard-time training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.03107v3",
    "pdf_url": null
  },
  {
    "instance_id": "5d4257d301264fc1a5323a51a1bb69ed",
    "figure_id": "1907.10764v4-Figure2-1",
    "image_file": "1907.10764v4-Figure2-1.png",
    "caption": " Illustration Example of Different Perturbation Schemes. (a) Original data. Perturbed data using (b) supervised adversarial generation method and (c) the proposed feature scattering, which is an unsupervised method. The overlaid boundary is from the model trained on clean data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods preserves the original data distribution the most?",
    "answer": "The proposed feature scattering method.",
    "rationale": "The proposed feature scattering method (c) preserves the original data distribution the most because the perturbed data points are still clustered in the same regions as the original data points (a). In contrast, the supervised adversarial generation method (b) results in perturbed data points that are more spread out and less clustered.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.10764v4",
    "pdf_url": null
  },
  {
    "instance_id": "170704bf767b4945a166efb2ca2be71e",
    "figure_id": "2007.02798v5-Figure11-1",
    "image_file": "2007.02798v5-Figure11-1.png",
    "caption": " Training GONs with z0 sampled from a variety of normal distributions with different standard deviations σ, z0 ∼ N (0, σ2I). Approach (a) directly uses the negative gradients as encodings while approach (b) performs one gradient descent style step initialised at z0.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach converges faster, (a) or (b)?",
    "answer": "Approach (b) converges faster than approach (a).",
    "rationale": "The figure shows the training loss of GONs for two different approaches: (a) and (b). Approach (b) has a lower training loss than approach (a) for all values of σ, which indicates that it converges faster.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.02798v5",
    "pdf_url": null
  },
  {
    "instance_id": "2444d66fb5a147b3b1cecf28d47a51ce",
    "figure_id": "2010.03132v2-Figure10-1",
    "image_file": "2010.03132v2-Figure10-1.png",
    "caption": " Qualitative comparison against the state-of-the-art on ImageNet (left 5 columns) and STL (right 5 columns) datasets. Our model generally produces more vibrant and balanced color distributions.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models in the figure produces the most vibrant and balanced color distributions?",
    "answer": "The \"Ours\" model.",
    "rationale": "The figure shows the results of different image colorization models on the ImageNet and STL datasets. The \"Ours\" model generally produces more vibrant and balanced color distributions compared to the other models. This can be seen by comparing the images in the \"Ours\" row to the images in the other rows.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.03132v2",
    "pdf_url": null
  },
  {
    "instance_id": "15f29a995c01407da73b35b7b5b4f087",
    "figure_id": "1804.00325v3-Figure6-1",
    "image_file": "1804.00325v3-Figure6-1.png",
    "caption": " ResNet-32 Trained On CIFAR-100 The training loss and validation accuracy during training on CIFAR-100 for each optimizer.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer has the lowest training loss?",
    "answer": "AggMo",
    "rationale": "The training loss for AggMo is the lowest of all the optimizers shown in the figure. This can be seen by comparing the lines in the plot on the left.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.00325v3",
    "pdf_url": null
  },
  {
    "instance_id": "e10f35f939484f6e943bf9ffbe477597",
    "figure_id": "2306.08731v1-Figure10-1",
    "image_file": "2306.08731v1-Figure10-1.png",
    "caption": " Reconstruction time per video length. We plot both times for the sparse reconstruction and the registration time to obtain the dense camera poses. While the time for registration is almost linear, the reconstruction time increases by the video length non-linearly mainly because of the bundle adjustment operation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which process takes longer, sparse reconstruction or dense registration?",
    "answer": "Dense registration.",
    "rationale": "The figure shows that the dense registration time (orange bars) is consistently higher than the sparse reconstruction time (blue bars) for all video lengths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.08731v1",
    "pdf_url": null
  },
  {
    "instance_id": "13f986784c3644349bc15ac04a4d6368",
    "figure_id": "1905.12776v4-Figure4-1",
    "image_file": "1905.12776v4-Figure4-1.png",
    "caption": " Starting at xt−1, G-OBD first does projection on to the H ′t level set (red dashed line) in the first phase. The projection point is x′t. Then G-OBD moves toward the minimizer to obtain point xt in the second phase. Let the minimizer vt be the origin. Notice that the three points xt−1, x′t, vt defines a plane S. Without loss of generality, we can let axis D2 be parallel to line x′txt−1; and let axis D1 be parallel to the projection hyperplane.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which point in the figure represents the projection of xt−1 onto the H′t level set?",
    "answer": "x′t",
    "rationale": "The figure shows that xt−1 is projected onto the H′t level set (red dashed line) to obtain x′t.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12776v4",
    "pdf_url": null
  },
  {
    "instance_id": "ce53637a224c4ae1ac515098f96c6347",
    "figure_id": "2008.11878v1-Figure4-1",
    "image_file": "2008.11878v1-Figure4-1.png",
    "caption": " Ten Samples from Office-Home Ar→Cl. Y row denotes the ground-truth labels, CN row shows the mis-classified labels, while CP means the correctly prediction.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object was most frequently misclassified?",
    "answer": "Speaker",
    "rationale": "The figure shows that the speaker was misclassified as a radio in two separate instances, while other objects were only misclassified once.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.11878v1",
    "pdf_url": null
  },
  {
    "instance_id": "742a0c829a5247e8874ebc94516d0d3d",
    "figure_id": "2310.18428v1-Figure1-1",
    "image_file": "2310.18428v1-Figure1-1.png",
    "caption": " A summary of equivalences between distribution-dependent definitions of stability (Theorem 1.4). A solid black arrow from A to B means that definition A weakly implies definition B. A dashed blue arrow from A to B means that A weakly implies B only if the domain X is countable. A dotted red arrow from A to B means that A weakly implies B only if the domain X is finite. A double brown arrow from A to B means that every learning rule that satisfies definition A also satisfies definition B.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which definition of stability weakly implies DD KL-Stability when the domain is finite?",
    "answer": "ML-Stability",
    "rationale": "The figure shows that there is a dotted red arrow from ML-Stability to DD KL-Stability, which indicates that ML-Stability weakly implies DD KL-Stability when the domain is finite.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.18428v1",
    "pdf_url": null
  },
  {
    "instance_id": "151657724f1a48c7b274cbbe5144b512",
    "figure_id": "2108.07181v2-Figure7-1",
    "image_file": "2108.07181v2-Figure7-1.png",
    "caption": " Mean-Error comparison of the 5% Hardest Poses.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in terms of MPJPE for the 5% hardest poses?",
    "answer": "Ours",
    "rationale": "The bar graph shows that \"Ours\" has the highest MPJPE value, which indicates the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.07181v2",
    "pdf_url": null
  },
  {
    "instance_id": "f4825fdd4957424a854cdc7e6a0e6952",
    "figure_id": "2007.01760v3-Figure8-1",
    "image_file": "2007.01760v3-Figure8-1.png",
    "caption": " Heatmaps for horses on PASCAL VOC. Here (a) shows anomalous samples ordered from most nominal to most anomalous from left to right, and (b) shows examples that indicate that the model is a “Clever Hans,” i.e. has learned a characterization based on spurious features (watermarks).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image in (a) is the most anomalous according to the model?",
    "answer": "The rightmost image in (a).",
    "rationale": "The heatmaps in (a) show the areas of the image that the model is focusing on when it makes its prediction. The more anomalous the image, the more the heatmap will be concentrated in areas that are not relevant to the task of identifying a horse. In this case, the rightmost image has a heatmap that is concentrated on the person's face and the background, rather than on the horse itself. This suggests that the model is not focusing on the correct features of the image and is therefore more likely to make a mistake.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.01760v3",
    "pdf_url": null
  },
  {
    "instance_id": "4cb9f0e7e8054312b1b1bef79dc52bcc",
    "figure_id": "1904.11727v1-Figure7-1",
    "image_file": "1904.11727v1-Figure7-1.png",
    "caption": " Accuracy of top five legislators who are affected by network factor",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which legislator was most affected by the network factor according to the NIPEN-Tensor model?",
    "answer": "M. Hall",
    "rationale": "The NIPEN-Tensor model is represented by the black bars in the figure. The black bar for M. Hall is the tallest, indicating that this legislator had the highest accuracy according to this model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.11727v1",
    "pdf_url": null
  },
  {
    "instance_id": "0d6f20af5aa74676b3f0d143195d7d19",
    "figure_id": "2004.00305v1-Figure1-1",
    "image_file": "2004.00305v1-Figure1-1.png",
    "caption": " Visualization and comparisons of representative longterm tracking results on VOT2019LT. “ATOM*” is our local tracker based on ATOM [9], “Ours” denotes our long-term tracker with meta-update. “ATOM* LT” means “Ours” without metaupdater. “CLGS” and “SiamDW LT” are the second and third best trackers on VOT2019LT. Please see Sections 3 and 4 for more details.",
    "figure_type": "photograph(s) and table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tracker performs the best on the VOT2019LT dataset according to the table?",
    "answer": "\"Ours\"",
    "rationale": "The table shows the F-score, precision (Pr), and recall (Re) for different trackers on the VOT2019LT dataset. The tracker \"Ours\" has the highest F-score of 0.697, which indicates that it performs the best overall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.00305v1",
    "pdf_url": null
  },
  {
    "instance_id": "17973d3b76d24706a26838fd30e18c15",
    "figure_id": "2304.10351v1-Figure4-1",
    "image_file": "2304.10351v1-Figure4-1.png",
    "caption": " Matrix games. Left: The common-payoff matrix of the Penalty game, where k = [0,−25,−50,−75,−100]. (a1 1, a 2 3), (a 1 2, a 2 2) and (a1 3, a 2 1) are NE points, and (a1 1, a 2 3) is the only SE point. Right: The payoff matrix of the Mixing game. It has only one NE point (a1 2, a 2 2) and only one SE point (a1 1, a 2 1). The SE is Pareto superior to the NE.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which game has a single Nash equilibrium point?",
    "answer": "The Mixing game.",
    "rationale": "The caption states that the Mixing game has only one NE point (a1 2, a 2 2).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.10351v1",
    "pdf_url": null
  },
  {
    "instance_id": "d15ca8f6d6394d4884e9f26c2078456d",
    "figure_id": "2009.07810v2-Figure2-1",
    "image_file": "2009.07810v2-Figure2-1.png",
    "caption": " Top-15 most frequent relations in CODEX-M and FB15K-237.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset contains more information about awards and nominations?",
    "answer": "CODEX-M.",
    "rationale": "The figure shows that the relations related to awards and nominations are more frequent in CODEX-M than in FB15K-237. For example, the relation \"/award/award_nominee/award_nominations\" is mentioned more than 80k times in CODEX-M, while it is mentioned less than 40k times in FB15K-237.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.07810v2",
    "pdf_url": null
  },
  {
    "instance_id": "d0a05fe237d3488296ea6c0544563576",
    "figure_id": "2204.00290v1-Figure3-1",
    "image_file": "2204.00290v1-Figure3-1.png",
    "caption": " Evidence sentence identification. The evidence sentences constitute the positive instances whereas the non-evidence sentences the negative ones.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following sentences is an example of an evidence sentence according to the figure? \na) \"survival was significantly improved\"\nb) \"all patients received aspirin\" \nc) \"the trial was completed on schedule\"",
    "answer": "a) \"survival was significantly improved\"",
    "rationale": "The figure shows that evidence sentences are those that provide evidence for a claim, while non-evidence sentences do not. In this case, the sentence \"survival was significantly improved\" is an evidence sentence because it provides evidence for the claim that a particular treatment is effective. The other two sentences are not evidence sentences because they do not provide evidence for any claim.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.00290v1",
    "pdf_url": null
  },
  {
    "instance_id": "6f38cd9723e842f6afd8fc8bd47591af",
    "figure_id": "2003.10275v1-Figure5-1",
    "image_file": "2003.10275v1-Figure5-1.png",
    "caption": " Feature distribution discrepancy of foregrounds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method results in the smallest discrepancy in feature distribution for the foreground of the \"person\" class?",
    "answer": "Our method.",
    "rationale": "The bar chart shows the proxy A-distance for each method and object class. A lower proxy A-distance indicates a smaller discrepancy in feature distribution. For the \"person\" class, our method has the lowest bar, indicating the smallest discrepancy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.10275v1",
    "pdf_url": null
  },
  {
    "instance_id": "cef00d2334c249dd8746132477017f7a",
    "figure_id": "1902.05826v2-Figure6-1",
    "image_file": "1902.05826v2-Figure6-1.png",
    "caption": " XROC curves, before and after adjustment",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the largest difference between the XROC curves before and after adjustment?",
    "answer": "The German dataset.",
    "rationale": "The XROC curves for the German dataset show the largest gap between the blue and red lines, indicating the largest difference before and after adjustment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.05826v2",
    "pdf_url": null
  },
  {
    "instance_id": "83562ed2b22b450eb682954032eaadd7",
    "figure_id": "2304.04779v1-Figure1-1",
    "image_file": "2304.04779v1-Figure1-1.png",
    "caption": " Linear probing results on ogbn-Products and ogbnPapers100M. GraphMAE2 achieves a significant advantage over previous graph SSL methods on benchmarks with millions of nodes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest accuracy on the ogbn-Papers100M dataset?",
    "answer": "GraphMAE2",
    "rationale": "The figure shows the accuracy of different methods on the ogbn-Papers100M dataset. The bar for GraphMAE2 is the highest, indicating that it achieves the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.04779v1",
    "pdf_url": null
  },
  {
    "instance_id": "62e56eaeae1b4e2085eba6a235b26331",
    "figure_id": "1808.07604v1-Figure2-1",
    "image_file": "1808.07604v1-Figure2-1.png",
    "caption": " The heatmap generated by the learned label graph. The deeper color represents the closer relation. For space, we abbreviate some music style names. We can see that some obvious relations are well captured by the model, e.g., “Heavy Metal Music (Metal)” and “Rock”, “Country Music (Country)” and “Folk”.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which music genres are most closely related according to the heatmap?",
    "answer": "Metal and Rock, Country and Folk.",
    "rationale": "The deeper the color in the heatmap, the closer the relation between two genres. The squares with the darkest blue color are located at the intersection of Metal and Rock, and Country and Folk.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.07604v1",
    "pdf_url": null
  },
  {
    "instance_id": "2690c694530e488bbd4ab22f7d86d442",
    "figure_id": "2007.04298v2-Figure6-1",
    "image_file": "2007.04298v2-Figure6-1.png",
    "caption": " Examples of trees extracted from BERT trained on the SST-2 dataset (a) and the CoLA dataset (b), respectively. Metrics are shown in each non-leaf node.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset produced the tree with the highest B(S) value at the root node?",
    "answer": "The SST-2 dataset.",
    "rationale": "The B(S) value at the root node of the tree in (a) is 5.87, while the B(S) value at the root node of the tree in (b) is 0.41.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.04298v2",
    "pdf_url": null
  },
  {
    "instance_id": "a7a6b4d44bd64f66ada25bb285b4463d",
    "figure_id": "1810.11919v2-Figure4-1",
    "image_file": "1810.11919v2-Figure4-1.png",
    "caption": " Qualitative comparison of three methods. In most cases, our method outperforms baseline methods qualitatively. The rightmost column shows a failure case using our method.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method generated the most realistic images?",
    "answer": "Our method.",
    "rationale": "The figure shows that the images generated by our method are the most realistic, as they are the most similar to the original images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.11919v2",
    "pdf_url": null
  },
  {
    "instance_id": "9360f3bd7af94277bc8d2fc971b83ab8",
    "figure_id": "2109.14707v2-Figure3-1",
    "image_file": "2109.14707v2-Figure3-1.png",
    "caption": " The fraction of xR, xB , and xO over epochs on MNIST.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What happens to the fraction of xR as the number of epochs increases?",
    "answer": " The fraction of xR increases and then plateaus.",
    "rationale": " The red line in the plot shows the fraction of xR over epochs. It can be seen that the fraction of xR increases rapidly in the first few epochs and then levels off at around 0.8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.14707v2",
    "pdf_url": null
  },
  {
    "instance_id": "16367b355b6c460780b8da02452ccb81",
    "figure_id": "2301.10625v3-Figure10-1",
    "image_file": "2301.10625v3-Figure10-1.png",
    "caption": " CIFAR-100 ST",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which query method performs the best?",
    "answer": "BALD",
    "rationale": "The figure shows that BALD has the highest accuracy for all three datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.10625v3",
    "pdf_url": null
  },
  {
    "instance_id": "c21e45826ec64657876674b1b47204f4",
    "figure_id": "2110.13502v1-Figure5-1",
    "image_file": "2110.13502v1-Figure5-1.png",
    "caption": " Separation performance in function of non-Gaussianity Separation performance of algorithms for sub-Gaussian α < 1 and super-Gaussian α > 1 components",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms has the best separation performance for sub-Gaussian components?",
    "answer": "ShiICA-ML",
    "rationale": "The figure shows that ShiICA-ML has the lowest Amari distance for sub-Gaussian components, which indicates that it has the best separation performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.13502v1",
    "pdf_url": null
  },
  {
    "instance_id": "6225b1ac8394448e9b84be698bf93b1e",
    "figure_id": "1812.10607v1-Figure5-1",
    "image_file": "1812.10607v1-Figure5-1.png",
    "caption": " Performance analysis from different perspectives: (A) Generalization: by observed nodes. (B) Compression: by embedding size. (C) Large Game: by game size. (D) Architecture: by attention or not.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which architecture performs best in terms of generalization?",
    "answer": "LSTM with Attention.",
    "rationale": "In panel (D), the LSTM with Attention line (red) is the lowest of all the lines, indicating that it has the lowest error rate and therefore the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.10607v1",
    "pdf_url": null
  },
  {
    "instance_id": "12373e6efad840b8aedd4e360151a2ef",
    "figure_id": "2002.07686v3-Figure4-1",
    "image_file": "2002.07686v3-Figure4-1.png",
    "caption": " The network has been optimized (either by using LAPQ method as our PTQ method or by training using DoReFa as our QAT method) for step size ∆̃. Still, the quantizer uses a slightly different step size ∆. Small changes in optimal step size ∆̃ of the weights tensors cause severe accuracy degradation in the quantized model. KURE significantly enhances the model robustness by promoting solutions that are more robust to uncertainties in the quantizer design. (a) and (b) show models quantized using PTQ method. (c) and (d) show models quantized with QAT method. @ (W,A) indicates the bit-width the model was quantized to.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which quantization method is more robust to uncertainties in the quantizer design?",
    "answer": "KURE.",
    "rationale": "The figure shows that the accuracy of models quantized with KURE is less sensitive to changes in the quantizer step size than the accuracy of models quantized without KURE. This is evident in both the PTQ and QAT methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.07686v3",
    "pdf_url": null
  },
  {
    "instance_id": "83e7a3c911d54329b060162a4561e901",
    "figure_id": "1910.09652v4-Figure7-1",
    "image_file": "1910.09652v4-Figure7-1.png",
    "caption": " KWNG vs Diagonal conditioning in the ill-conditioned case on Cifar10. In red and blue, the euclidean gradient is preconditioned using a diagonal matrixD either given byDi=‖T.,i‖ orDi=‖T̃.,i‖, where T and T̃ are defined in Propositions 5 and 6. The rest of the traces are obtained using the stable version of KWNG in Proposition 6 with different choices for the damping term D=I , D=‖T.,i‖ and ‖T̃.,i‖. All use a gaussian kernel except the yellow traces which uses a rational quadratic kernel.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which preconditioning method achieves the highest training accuracy?",
    "answer": "KWNG with D = ‖T̃.,i‖ (rq-kernel)",
    "rationale": "The training accuracy is shown in the left panel of the figure. The yellow trace, which corresponds to KWNG with D = ‖T̃.,i‖ (rq-kernel), is the highest of all the traces.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.09652v4",
    "pdf_url": null
  },
  {
    "instance_id": "adbbd2bed9bf490e823246f63d127cdf",
    "figure_id": "2205.10060v3-Figure1-1",
    "image_file": "2205.10060v3-Figure1-1.png",
    "caption": " Evolution of parameters during training. In total, 50 independently trained samples are averaged in each epoch. Dots are placed every 10 epochs to indicate the speed and direction of the convergence. (Left) Evolution of the residual γi − x3 for all x ∈ [−4, 4]. (Center) Evolution of νi w.r.t. residual at three different x-positions. (Right) Evolution of √",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the residual as the training progresses?",
    "answer": "The residual decreases as the training progresses.",
    "rationale": "The left plot shows the evolution of the residual γi − x3 for all x ∈ [−4, 4]. The residual is the difference between the predicted value and the actual value. As the training progresses, the residual decreases, indicating that the model is learning to predict the actual values more accurately.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.10060v3",
    "pdf_url": null
  },
  {
    "instance_id": "f2d41f2a61dd45c89b97f160d7e12ad9",
    "figure_id": "2105.08920v1-Figure2-1",
    "image_file": "2105.08920v1-Figure2-1.png",
    "caption": " Correlation between human judgment difference (x-axis) and metric score difference (y-axis). Top: ROC, Bottom: WP. We only show the situation in the positive x-axis, since it is centrosymmetric with that in the negative x-axis. Human (S)/Metric (S) means the difference of human judgment/metric score is significant (p<0.01, t-test), while (NS) means insignificant difference. r2 is the coefficient of determination for linear regression (red line), and is exactly the square of the Pearson correlation coefficient between the x-axis and y-axis.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric has the highest correlation with human judgment difference for ROC?",
    "answer": "UNION",
    "rationale": "The plot in the top right corner shows the correlation between human judgment difference and UNION score difference for ROC. The r^2 value for this plot is 0.92, which is the highest among all the metrics shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.08920v1",
    "pdf_url": null
  },
  {
    "instance_id": "33aa2f64c46549b6af648f4692e2dd44",
    "figure_id": "2110.14625v1-Figure7-1",
    "image_file": "2110.14625v1-Figure7-1.png",
    "caption": " Anisotropic Gaussian target (d = 1000). Minimum (7a), mean (7b) and median (7c) effective sample size of q 7→ qi per second. Average acceptance rates in 7d and estimates of the eigenvalues of DL in 7e. Condition number of transformed Hessian C>Σ−1C in 7f.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms performs the best in terms of ESS/sec?",
    "answer": "ESJD.",
    "rationale": "Figures 7a, 7b, and 7c show the minimum, mean, and median ESS/sec for each algorithm, respectively. In all three figures, ESJD has the highest ESS/sec values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14625v1",
    "pdf_url": null
  },
  {
    "instance_id": "ce9c4b35d8574138a975ee41cd8846b8",
    "figure_id": "1810.11118v2-Figure4-1",
    "image_file": "1810.11118v2-Figure4-1.png",
    "caption": " Time between consecutive messages in conversations. Jumps are at points when the scale shifts as indicated on the x-axis. The circled upper right point is the sum over all larger values, indicating that messages weeks apart are often in the same conversation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common time between consecutive messages in conversations?",
    "answer": "Less than one hour.",
    "rationale": "The plot shows that the count of messages decreases as the time between messages increases. This means that most messages are sent within a short time of each other.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.11118v2",
    "pdf_url": null
  },
  {
    "instance_id": "6c223ea6da0d41e8816a0c9fded60691",
    "figure_id": "2211.15072v4-Figure5-1",
    "image_file": "2211.15072v4-Figure5-1.png",
    "caption": " DEOO v.s. Accuracy & DPE v.s. Accuracy for Model 2",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms achieves the highest accuracy?",
    "answer": "FairREE-EOO",
    "rationale": "The figure shows that FairREE-EOO has the highest accuracy on the x-axis of both plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.15072v4",
    "pdf_url": null
  },
  {
    "instance_id": "58a84f10a5b54c379189e34849157cb6",
    "figure_id": "1712.01651v1-Figure6-1",
    "image_file": "1712.01651v1-Figure6-1.png",
    "caption": " Comparison of training speed using CNN-based training and dilated FCN-based training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method converges faster, CNN-based or FCN-based?",
    "answer": "The CNN-based training method converges faster.",
    "rationale": "The figure shows the training loss and correct action rate for both methods over time. The CNN-based training method reaches a lower training loss and higher correct action rate in less time than the FCN-based training method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1712.01651v1",
    "pdf_url": null
  },
  {
    "instance_id": "a263fed229c94f6a9ea2936ae0d79a97",
    "figure_id": "2204.07439v3-Figure11-1",
    "image_file": "2204.07439v3-Figure11-1.png",
    "caption": " t-SNE visualization of features extracted before the classifier (fully-connected layer) of each model. Half of the CIFAR-10 test set was used (500 images per class). Best viewed in color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods seems to do the best job of separating the different classes of objects?",
    "answer": "INSTA-Th",
    "rationale": "The t-SNE plots show the distribution of the features extracted by each model. The features are projected onto a 2D plane so that we can visualize them. Ideally, we want the features for each class to be clustered together, and well-separated from the features for other classes. In the figure, we can see that the INSTA-Th method produces the most well-separated clusters of features, indicating that it does the best job of separating the different classes of objects.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.07439v3",
    "pdf_url": null
  },
  {
    "instance_id": "84fb118893434fe49da8966b0084892c",
    "figure_id": "2106.04443v2-Figure2-1",
    "image_file": "2106.04443v2-Figure2-1.png",
    "caption": " Heart disease classification example with m = 6, N = 20, N? = 303 and ∆m = 10−3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods has the lowest out-of-sample cost?",
    "answer": "MDI-DRO.",
    "rationale": "The out-of-sample cost is shown in the leftmost plot, and the MDI-DRO line is the lowest of the three lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04443v2",
    "pdf_url": null
  },
  {
    "instance_id": "bb286438bfb344d696554dac564cc627",
    "figure_id": "2002.11841v1-Figure11-1",
    "image_file": "2002.11841v1-Figure11-1.png",
    "caption": " Heatmap visualization of sub-embedding uncertainty on different types of images from IJB-C dataset, shown on the right of each face image. 16 values are arranged in 4×4 grids (no spatial meaning). Brighter color indicates higher uncertainty.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of image has the highest uncertainty in the sub-embedding?",
    "answer": "Large-pose images",
    "rationale": "The heatmaps for large-pose images have the brightest colors, which indicates higher uncertainty.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11841v1",
    "pdf_url": null
  },
  {
    "instance_id": "4860a4727a3d40c18d65e5d1f266bca2",
    "figure_id": "1812.09877v3-Figure5-1",
    "image_file": "1812.09877v3-Figure5-1.png",
    "caption": " A comparison between shoes generated by our method, BicycleGAN, and MUNIT. AMT users preferred BicycleGAN to both, but preferred ours to MUNIT.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most realistic-looking shoes?",
    "answer": "Ours",
    "rationale": "The figure shows that the shoes generated by our method are the most similar to the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.09877v3",
    "pdf_url": null
  },
  {
    "instance_id": "525608e034694bb4be403a60f56477c6",
    "figure_id": "1905.11828v2-Figure14-1",
    "image_file": "1905.11828v2-Figure14-1.png",
    "caption": " Performance comparison under different domain sizes",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest network load for all domain sizes?",
    "answer": "ATWB.",
    "rationale": "The plot in Figure (a) shows that the network load of ATWB is consistently lower than the other algorithms for all domain sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11828v2",
    "pdf_url": null
  },
  {
    "instance_id": "0ec8535e93e2493799e3113599d652e1",
    "figure_id": "2108.13264v4-FigureA.29-1",
    "image_file": "2108.13264v4-FigureA.29-1.png",
    "caption": "Figure A.29: Average Probability of Improvement on Atari 100k. Each subplot shows the probability of improvement of a given algorithm compared to all other algorithms. The interval estimates are based on stratified bootstrap with independent sampling with 2000 bootstrap re-samples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest probability of improving upon the other algorithms on Atari 100k?",
    "answer": "SimPLe",
    "rationale": "The figure shows the average probability of improvement for each algorithm compared to all other algorithms. The probability of improvement for SimPLe is consistently higher than the other algorithms, as shown by the higher position of the boxplots for SimPLe in each subplot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.13264v4",
    "pdf_url": null
  },
  {
    "instance_id": "b4a4e90a13d64ccfae5940280c6d3af9",
    "figure_id": "1811.04331v1-Figure2-1",
    "image_file": "1811.04331v1-Figure2-1.png",
    "caption": " The IMCI instance used in Theorem 1.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many nodes are there in the graph?",
    "answer": "There are 13 nodes in the graph.",
    "rationale": "The graph has 13 nodes, which are represented by the black dots. The nodes are labeled with letters or numbers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.04331v1",
    "pdf_url": null
  },
  {
    "instance_id": "8317544cdaa2444988ac8012adbc280a",
    "figure_id": "2101.10160v1-Figure2-1",
    "image_file": "2101.10160v1-Figure2-1.png",
    "caption": " Raw measure scores on synthetic data with different relationships.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the following measures performs the best on Data A when the number of variables is 8? ",
    "answer": " C-KDM ",
    "rationale": "  The figure shows that the C-KDM line is the highest of all the lines when the number of variables is 8. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.10160v1",
    "pdf_url": null
  },
  {
    "instance_id": "56da2f4346814e35a73b86da707b9666",
    "figure_id": "2302.14372v2-Figure12-1",
    "image_file": "2302.14372v2-Figure12-1.png",
    "caption": " The absolute final performance in continuous action space environments. This table reports the score before normalization. The number in bracket is the standard error. The bold numbers are the best performance in the same setting. Performance was averaged over 10 random seeds, except that CQL had 5 seeds.",
    "figure_type": "table.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment-algorithm combination has the highest average score?",
    "answer": "Ant-expert.",
    "rationale": "The table shows the absolute final performance in continuous action space environments for different environment-algorithm combinations. The average score for Ant-expert is 5077.97, which is the highest average score in the table.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.14372v2",
    "pdf_url": null
  },
  {
    "instance_id": "b98594bcaae04eada2cade88b28189a7",
    "figure_id": "2102.06845v2-Figure1-1",
    "image_file": "2102.06845v2-Figure1-1.png",
    "caption": " Comparison of Linear TV and Log TV penalties under homogeneous block-sparsity (see Fig. 2(a)): (a) NMSE and (b) F1-Score.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three algorithms performs best in terms of NMSE and F1-Score?",
    "answer": "TV-SBL (Log TV)",
    "rationale": "In both (a) and (b), the line for TV-SBL (Log TV) is consistently higher than the other two lines, indicating that it achieves the lowest NMSE and highest F1-Score across the range of SNR values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.06845v2",
    "pdf_url": null
  },
  {
    "instance_id": "0ab9b97305694d64853f2bd83d4e3a63",
    "figure_id": "2004.03607v2-Figure6-1",
    "image_file": "2004.03607v2-Figure6-1.png",
    "caption": " Distribution of ratings for three models: TF-IDF retrieval, GPT3, and T5, along with ratings for the second-best rated Reddit advice. Though deep generators like GPT3 and T5 are often preferred over the retrieval baseline, they also often write advice that would never be helpful (33% GPT3, 13% T5), and that is racist, sexist, or otherwise dangerous (10% GPT3, 3% T5).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model generated the highest percentage of advice that was rated as \"Dangerous\"?",
    "answer": "GPT3-175B",
    "rationale": "The bar chart shows that 10% of the advice generated by GPT3-175B was rated as \"Dangerous\", while only 3% of the advice generated by T5-11B and none of the advice generated by TF-IDF retrieval or the second-best rated Reddit advice was rated as \"Dangerous\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.03607v2",
    "pdf_url": null
  },
  {
    "instance_id": "43e3a9381f2a4e769c1d02f085d589de",
    "figure_id": "2005.04625v2-Figure5-1",
    "image_file": "2005.04625v2-Figure5-1.png",
    "caption": " Performance by various agents on navigation tasks in different lengths. See texts for details.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which agent performed best on the R4R to R4R task according to the CLS metric?",
    "answer": "The LSTM agent.",
    "rationale": "The table shows the performance of different agents on different tasks according to different metrics. The CLS metric for the R4R to R4R task is 44.0 for the LSTM agent, which is higher than the CLS metric for the NULL agent (43.1).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.04625v2",
    "pdf_url": null
  },
  {
    "instance_id": "9b425f655f1341a3be63fc4b7b6aea89",
    "figure_id": "2106.07175v2-Figure3-1",
    "image_file": "2106.07175v2-Figure3-1.png",
    "caption": " Results for E1: (Left) Success Ratio with standard error for all models (top row = GPS); (Center) Success Ratio vs. time taken; (Right) Visualization of attention scores for N-PEPS+U",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest success ratio for E1?",
    "answer": "N-PEPS+U",
    "rationale": "The success ratio for each model is listed in the table on the left side of the figure. N-PEPS+U has the highest success ratio of 87.07 ± 0.28.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07175v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd500a8a054049cea425cad7df1ac955",
    "figure_id": "2003.13630v3-Figure5-1",
    "image_file": "2003.13630v3-Figure5-1.png",
    "caption": " TResNet Vs EfficientNet models training speed comparison. Y label is the accuracy[%]",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest accuracy with the fastest training speed?",
    "answer": "EfficientNet B5@456.",
    "rationale": "The figure shows the accuracy and training speed of different EfficientNet and TResNet models. The EfficientNet B5@456 model has the highest accuracy of all the models shown, and it also has a relatively fast training speed.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.13630v3",
    "pdf_url": null
  },
  {
    "instance_id": "f6d3cae72f744a76afbef07e57e93195",
    "figure_id": "1808.08266v2-Figure5-1",
    "image_file": "1808.08266v2-Figure5-1.png",
    "caption": " Statistic of the IKEA dataset",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language pair has the longest average sentence length?",
    "answer": "EN-FR",
    "rationale": "The table shows that the average sentence length for EN-FR is 82.9, which is higher than the average sentence lengths for the other language pairs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.08266v2",
    "pdf_url": null
  },
  {
    "instance_id": "28487967832c45eea7b93d8029806799",
    "figure_id": "2211.00869v1-Figure3-1",
    "image_file": "2211.00869v1-Figure3-1.png",
    "caption": " The distribution of topics in Title2Event, all non top-10 topics are aggregated as \"Other Topics\".",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which topic is the most popular among Chinese social media users?",
    "answer": "Society.",
    "rationale": "The pie chart shows that Society accounts for 29% of the topics discussed on Chinese social media, which is the largest proportion of any topic.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.00869v1",
    "pdf_url": null
  },
  {
    "instance_id": "389486bd5baa47e085e8b35bbbbb9664",
    "figure_id": "2103.14645v1-Figure4-1",
    "image_file": "2103.14645v1-Figure4-1.png",
    "caption": " Visualization of sparsity loss and visibility culling. We render cross-sections of the Hotdog scene to inspect the effect of our sparsity loss Ls and visibility culling. Our full method (b) represents the scene by only allocating content around visible scene surfaces. Removing either the sparsity loss alone (c) or both the sparsity loss and visibility culling (d) results in a much less compact representation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four images shows the most compact representation of the scene?",
    "answer": "Image (b)",
    "rationale": "Image (b) is a cross-section of the scene rendered by the full method, which includes both the sparsity loss and visibility culling. This method allocates content only around visible scene surfaces, resulting in a more compact representation. In contrast, images (c) and (d) show the scene rendered without the sparsity loss or visibility culling, respectively. These methods result in a much less compact representation, as they allocate content even in areas that are not visible.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.14645v1",
    "pdf_url": null
  },
  {
    "instance_id": "07ffcdb9d9f94fe0bb1056bb06d1640c",
    "figure_id": "2205.10920v4-Figure11-1",
    "image_file": "2205.10920v4-Figure11-1.png",
    "caption": " The learning curves (train CNN on CIFAR10) for strong baselines (including FL, personalized FL and test-time adaptation methods) on mixture of test and Dir(0.1) non-i.i.d local distributions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods tested is the most accurate on the mixture of test and Dir(0.1) non-i.i.d local distributions?",
    "answer": "FedTHE+ (Ours)",
    "rationale": "The figure shows the accuracy of different methods on the mixture of test and Dir(0.1) non-i.i.d local distributions. The FedTHE+ (Ours) method has the highest accuracy, as shown by the red line in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.10920v4",
    "pdf_url": null
  },
  {
    "instance_id": "943ee9dd739144a4a03f6129d792d445",
    "figure_id": "2011.06733v4-Figure10-1",
    "image_file": "2011.06733v4-Figure10-1.png",
    "caption": " Percentage of images explained by different number of patches: comparing ResNet and VGGNet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which explanation technique explains the most images with the least number of patches?",
    "answer": "Combinatorial Search",
    "rationale": "The Combinatorial Search curve reaches the highest percentage of images explained with the fewest number of patches.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.06733v4",
    "pdf_url": null
  },
  {
    "instance_id": "afd1b92c0de44e74a13e75196a2a8760",
    "figure_id": "2012.10412v3-Figure6-1",
    "image_file": "2012.10412v3-Figure6-1.png",
    "caption": " Point cloud completion results delivered by the PC module. Blue and red point clouds represent the data before and after this phase, respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the PC module do?",
    "answer": "The PC module completes point clouds.",
    "rationale": "The figure shows point clouds before and after the PC module has been applied. The blue points represent the data before the PC module, and the red points represent the data after the PC module. The PC module fills in the missing data in the point cloud, making it more complete.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.10412v3",
    "pdf_url": null
  },
  {
    "instance_id": "5d6ed0db1d1f4b439c9f52d7a13685ed",
    "figure_id": "1904.02580v2-Figure2-1",
    "image_file": "1904.02580v2-Figure2-1.png",
    "caption": " Illustration of one step of the online cvxMF algorithm with multiple-representative regions.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the representative regions and bases when a new data point is assigned to a cluster?",
    "answer": "The representative regions and bases are updated.",
    "rationale": "The figure shows that when a new data point is assigned to a cluster, the representative region for that cluster is updated to include the new data point. Additionally, the bases are updated to reflect the new data point. This can be seen in the third panel of the figure, where the representative region for cluster 3 has been updated to include the new data point (shown in red), and the bases have been updated accordingly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.02580v2",
    "pdf_url": null
  },
  {
    "instance_id": "a312d9a4530c4fa58ac8910cd77cab71",
    "figure_id": "1911.04910v3-Figure1-1",
    "image_file": "1911.04910v3-Figure1-1.png",
    "caption": " Snapshot of knowledge graph in FB15k-237. Entities are represented as golden blocks.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between Sergei Rachmaninoff and Franz Liszt?",
    "answer": "Sergei Rachmaninoff was influenced by Franz Liszt.",
    "rationale": "The knowledge graph shows a directed edge from \"Sergei Rachmaninoff\" to \"Franz Liszt\" labeled \"influenced_by\". This indicates that Rachmaninoff was influenced by Liszt.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.04910v3",
    "pdf_url": null
  },
  {
    "instance_id": "037de44580714fc38c50ddb1dc772d7c",
    "figure_id": "2302.03453v2-Figure8-1",
    "image_file": "2302.03453v2-Figure8-1.png",
    "caption": " Visualizations of offset maps in OSRT. Reference and deformed points are depicted in green and red, respectively. The deformable kernel is sparse in the polar area.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the deformable kernel is sparse in the polar area?",
    "answer": "The reference points.",
    "rationale": "The reference points are shown in green in the figure. They are sparse in the polar area, meaning that there are fewer reference points in this area than in other areas. This can be seen in the figure by comparing the density of green points in the polar area to the density of green points in other areas.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.03453v2",
    "pdf_url": null
  },
  {
    "instance_id": "1b9e4810a5024f55ba42fff9b2be35d9",
    "figure_id": "1912.07768v1-Figure6-1",
    "image_file": "1912.07768v1-Figure6-1.png",
    "caption": " GTN samples w/o curriculum.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of not using a curriculum on the generated samples from a GTN?",
    "answer": "The samples are noisy and difficult to read.",
    "rationale": "The figure shows that the samples generated by a GTN without a curriculum are much more noisy and difficult to read than those generated with a curriculum. This is because the curriculum helps the GTN to learn a more structured representation of the data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.07768v1",
    "pdf_url": null
  },
  {
    "instance_id": "8417f038b4904f3eacbd37b99ec5c949",
    "figure_id": "2010.00467v3-Figure2-1",
    "image_file": "2010.00467v3-Figure2-1.png",
    "caption": " Clean accuracy vs. PGD-10 accuracy for different model architectures. The circle sizes are proportional to the number of parameters that specified in Table 12.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture has the highest clean accuracy?",
    "answer": "DenseNet-201.",
    "rationale": "The figure shows the clean accuracy on the x-axis and the PGD-10 accuracy on the y-axis. The point corresponding to DenseNet-201 is the furthest to the right on the x-axis, indicating that it has the highest clean accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.00467v3",
    "pdf_url": null
  },
  {
    "instance_id": "d4e40e3dbe034f69b72b1f7a9b61cad7",
    "figure_id": "1905.13672v2-Figure5-1",
    "image_file": "1905.13672v2-Figure5-1.png",
    "caption": " Baseline Comparison of Forecasting Accuracy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which baseline model has the best performance for intra-domain IIBH?",
    "answer": "TrSVM",
    "rationale": "The figure shows the forecasting accuracy of different baseline models for intra-domain IIBH. The TrSVM model has the highest forecasting accuracy, as indicated by the bar with the highest value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.13672v2",
    "pdf_url": null
  },
  {
    "instance_id": "42dc1919a86d4c5ca8502228bf033d95",
    "figure_id": "1904.12918v1-Figure9-1",
    "image_file": "1904.12918v1-Figure9-1.png",
    "caption": " Empirical Bayes concentrates exploration on the set of the best arms. Displayed is the probability of sampling the top 6 arms as a function of time averaged over 50 simulations. Compared are Thompson sampling using a posterior constructed from the MLEs and from the empirical Bayes estimator.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, MLE or EB, leads to a higher chance of pulling the top arms after 2,000 iterations?",
    "answer": "EB",
    "rationale": "After 2,000 iterations, the lines representing EB are higher than the lines representing MLE for all top arm numbers (1-5). This indicates that EB has a higher chance of pulling the top arms than MLE after 2,000 iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.12918v1",
    "pdf_url": null
  },
  {
    "instance_id": "f6d141981e3b428587095f45c6164ae1",
    "figure_id": "1812.00020v2-Figure5-1",
    "image_file": "1812.00020v2-Figure5-1.png",
    "caption": " At the singularity of the cube, (a)-(c) provides three different ways of unfolding the local neighborhood. Such ambiguity is removed around the singularity by our texture coordinate definition using the shortest path. For the purple point, (a) is a valid neighborhood, while the blue points in (b) and orange points in (c) are unfolded along the paths which are not the shortest. Similarly, the ambiguity of the gap location is removed.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three ways of unfolding the local neighborhood around the singularity of the cube is valid for the purple point?",
    "answer": "(a) Cut the green line.",
    "rationale": "The figure shows that the purple point is located on the green line. When the green line is cut, the purple point is unfolded along the shortest path.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.00020v2",
    "pdf_url": null
  },
  {
    "instance_id": "8ebadb97afb54d3ead575adca992393d",
    "figure_id": "2105.15075v2-Figure12-1",
    "image_file": "2105.15075v2-Figure12-1.png",
    "caption": " Top-1 accuracy v.s. GFLOPs on CIFAR-100. DVT is implemented on top of T2T-ViT12/14.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the accuracy of DVT compare to that of T2T-ViT?",
    "answer": "DVT is more accurate than T2T-ViT.",
    "rationale": "The figure shows that the accuracy of DVT is higher than that of T2T-ViT for all budget values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.15075v2",
    "pdf_url": null
  },
  {
    "instance_id": "9c9028c1bf414fc794cf721d6cee4df3",
    "figure_id": "1908.02738v2-Figure6-1",
    "image_file": "1908.02738v2-Figure6-1.png",
    "caption": " Quantitative measures. Top: Centrality and average deformation norm for templates generated by our model and the baselines on the D-class variant of MNIST. We find that our models yield more central templates. Bottom: Both MSE and Jacobians determinants measures indicate good deformations for all models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model yields the most central templates according to the Centrality metric?",
    "answer": "Ours.",
    "rationale": "The boxplot for the Centrality metric shows that the box for \"ours\" is higher than the boxes for the two baselines, indicating that the median value for \"ours\" is higher. This means that our model produces templates that are more central than the baselines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.02738v2",
    "pdf_url": null
  },
  {
    "instance_id": "300c8faeb0154bafbede8034c3d0bc5a",
    "figure_id": "2205.03512v1-Figure1-1",
    "image_file": "2205.03512v1-Figure1-1.png",
    "caption": " An example of CORWA labels displayed using the BRAT interface (Stenetorp et al., 2012).",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two main approaches to automatic related work generation?",
    "answer": "Extractive and abstractive.",
    "rationale": "The figure shows a text passage with annotations that indicate the two main approaches to automatic related work generation. The extractive approach involves extracting relevant sentences from existing documents, while the abstractive approach involves generating new sentences that summarize the relevant information.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.03512v1",
    "pdf_url": null
  },
  {
    "instance_id": "4b5105b744b64baea06ffe5300b313b8",
    "figure_id": "2103.00131v1-Figure3-1",
    "image_file": "2103.00131v1-Figure3-1.png",
    "caption": " The SER performance comparison for a 64-QAM, M = 64,K=16.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best at high SNR?",
    "answer": "ADMM-PSNet, L=30",
    "rationale": "The figure shows the SER performance of different methods as a function of SNR. At high SNR, the ADMM-PSNet, L=30 method has the lowest SER, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.00131v1",
    "pdf_url": null
  },
  {
    "instance_id": "1bf700de524f4950a9a00542559a7a6e",
    "figure_id": "2206.00484v2-Figure15-1",
    "image_file": "2206.00484v2-Figure15-1.png",
    "caption": " DEP-MPO achieves the most widespread and symmetric gait. Left: The relative sagital foot position w.r.t. the torso visualizes the leg extension during locomotion. DEP-MPO creates a symmetric gait with ≈ 1m step length. For MPO the step length is much shorter. TD4 has a completely shifted gait, the left foot is often in front of the right foot. Right: Foot contact pattern for all gaits. The shaded areas mark the time during which the respective foot (LF: left foot or RF: right foot) is in contact with the ground. Visualized are the last 5 seconds of an evaluation episode to ensure a converged pattern.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which gait is the most symmetric and has the longest step length?",
    "answer": "DEP-MPO",
    "rationale": "The left side of the figure shows the relative sagital foot position for each gait. DEP-MPO shows a clear, symmetric gait pattern with the left and right foot positions mirroring each other. The distance between peaks in the DEP-MPO plot indicates a longer step length compared to the other gaits.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.00484v2",
    "pdf_url": null
  },
  {
    "instance_id": "68e62210c3804fe882e483afc94e6e91",
    "figure_id": "1811.09126v2-Figure1-1",
    "image_file": "1811.09126v2-Figure1-1.png",
    "caption": " Overview of bit sharing method CSE and register sharing method vHLL. Virtual CSE/vHLL sketches of users may contain “noisy” bits/registers (e.g., the bit and register in red and bold in the figure).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, CSE or vHLL, is more likely to introduce noise into the reconstructed data?",
    "answer": "vHLL",
    "rationale": "The figure shows that vHLL uses a max operation to combine the sketches of two users, which can introduce noise into the reconstructed data. CSE, on the other hand, uses a bitwise OR operation, which is less likely to introduce noise.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.09126v2",
    "pdf_url": null
  },
  {
    "instance_id": "f02ed3f8ce0c45b1b9d6d457c5cb3cba",
    "figure_id": "2303.13199v2-Figure2-1",
    "image_file": "2303.13199v2-Figure2-1.png",
    "caption": " Scatter plot of the accuracy differences between FSAFiLM and NA against the minimum cosine distance between a dataset and miniImagenet dataset evaluated using the NA method. We consider the offline setting with 50 shots. A pre-trained EfficientNet-B0 on ImageNet-1k is used as a backbone.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest accuracy difference between FSAFiLM and NA?",
    "answer": "Caltech101",
    "rationale": "The figure shows the accuracy difference between FSAFiLM and NA for each dataset. The dataset with the highest accuracy difference is Caltech101, which is represented by the red diamond at the top of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.13199v2",
    "pdf_url": null
  },
  {
    "instance_id": "de67a9370f1d4b05978774ea26e23c94",
    "figure_id": "2009.07698v5-Figure11-1",
    "image_file": "2009.07698v5-Figure11-1.png",
    "caption": " Paul DeJong, right, after his home run in the second inning of the Cardinals’ win over the Mets on Sunday.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which team is Paul DeJong playing for?",
    "answer": "The St. Louis Cardinals.",
    "rationale": "DeJong is wearing a Cardinals uniform, and the caption states that he hit a home run for the Cardinals.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.07698v5",
    "pdf_url": null
  },
  {
    "instance_id": "2332a159f36f440aaa27812dc8843583",
    "figure_id": "2101.00154v2-Figure5-1",
    "image_file": "2101.00154v2-Figure5-1.png",
    "caption": " Pattern distribution of ATOMIC heads and eventualities in ASER.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pattern is most frequent in both ATOMIC and ASER?",
    "answer": "s-v-O",
    "rationale": "The figure shows the proportion of each pattern in ATOMIC and ASER. The bars for s-v-O are the tallest for both ATOMIC and ASER, indicating that this pattern is the most frequent.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.00154v2",
    "pdf_url": null
  },
  {
    "instance_id": "8b3ab920868144b1a36416fb31ff31fd",
    "figure_id": "2010.12606v3-Figure20-1",
    "image_file": "2010.12606v3-Figure20-1.png",
    "caption": " Performance of experts split by different levels of expertise: The first (second) row shows the data of Experiment I (II) split up by different levels of familiarity with CNNs and feature visualizations. The third row shows the data of Experiment I split up by different backgrounds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of reference image led to the highest proportion of correct responses for participants with expertise level 2 in Experiment I?",
    "answer": "Natural images",
    "rationale": "The bar chart for \"Natural\" reference images in Figure (b) is the highest among all bars for participants with expertise level 2 in Experiment I.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12606v3",
    "pdf_url": null
  },
  {
    "instance_id": "1e4a0812990b4f5292fcaefe08458f9a",
    "figure_id": "2109.07383v2-Figure5-1",
    "image_file": "2109.07383v2-Figure5-1.png",
    "caption": " The selected features for different hardware platforms. A higher score means the feature is more important than others.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which component of the transformer architecture contributes the most to the model's performance on the CPU?",
    "answer": "Decoder Attention",
    "rationale": "The bar chart shows the feature scores for different components of the transformer architecture on both CPU and GPU. The Decoder Attention component has the highest score on the CPU, indicating that it is the most important feature for the model's performance on that platform.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.07383v2",
    "pdf_url": null
  },
  {
    "instance_id": "b3009d57c2df4be1badde6db9e5f0df2",
    "figure_id": "2003.05905v2-Figure13-1",
    "image_file": "2003.05905v2-Figure13-1.png",
    "caption": " Additional expression editing results on CFEED. In each triplet, the first column is input facial image, the second column is the image with desired expression and the last column is the synthesized result.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which facial expression is most common in the image?",
    "answer": "Disgust.",
    "rationale": "While there are a variety of expressions in the image, disgust is the most common, appearing in almost every row of the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.05905v2",
    "pdf_url": null
  },
  {
    "instance_id": "9e235fdbff524d228b8f9332284bb7ef",
    "figure_id": "1906.03318v2-Figure1-1",
    "image_file": "1906.03318v2-Figure1-1.png",
    "caption": " Comparison of nonlinear term found in the log-likelihood for binomial, Poisson, and negative-Binomial observation models (solid) with corresponding second-order Chebyshev approximation (dashed).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three observation models has the most linear log-likelihood function?",
    "answer": "The Poisson observation model.",
    "rationale": "The plot shows that the log-likelihood function for the Poisson model is closest to a straight line, while the other two models have more curvature.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.03318v2",
    "pdf_url": null
  },
  {
    "instance_id": "d706746d7886410cbc537e0ca301a79b",
    "figure_id": "2101.09763v2-Figure18-1",
    "image_file": "2101.09763v2-Figure18-1.png",
    "caption": " Mean test performance (Accuracy/F1 score) of the base model on Clothing1M and NoisyNER with increasing |DC| for the base model and varying for the noise model estimation and with Variable Sampling. Grey error bars show the empirical standard deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which label set has the highest F1 score?",
    "answer": "Label set 7",
    "rationale": "The F1 score is a measure of the accuracy of a classification model. The higher the F1 score, the better the model is at correctly classifying data points. In this figure, the F1 score is plotted for each label set. Label set 7 has the highest F1 score, indicating that the model is most accurate for this label set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.09763v2",
    "pdf_url": null
  },
  {
    "instance_id": "3406c04846ca4579b225dd86197a01d7",
    "figure_id": "2212.01026v1-Figure4-1",
    "image_file": "2212.01026v1-Figure4-1.png",
    "caption": " Running time per epoch in seconds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the biggest decrease in running time per epoch when using SFA with k=8 compared to not using SFA?",
    "answer": "WikiICS",
    "rationale": "The figure shows the running time per epoch for different datasets and SFA configurations. For WikiICS, the running time per epoch without SFA is approximately 0.13 seconds, while with SFA and k=8 it is approximately 0.08 seconds. This is a decrease of approximately 0.05 seconds, which is the biggest decrease among the datasets shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.01026v1",
    "pdf_url": null
  },
  {
    "instance_id": "1bc35d0808774d909d750a03566a6f8f",
    "figure_id": "1805.06563v1-Figure3-1",
    "image_file": "1805.06563v1-Figure3-1.png",
    "caption": " Top-5 similar items for a given item. In each row, the given item is at the left and the top-5 similar items are to its right.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which item is most similar to the red alarm clock?",
    "answer": "The white alarm clock.",
    "rationale": "The white alarm clock is the same shape and style as the red alarm clock, and it is also the closest in color to the red alarm clock.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.06563v1",
    "pdf_url": null
  },
  {
    "instance_id": "633d3891ec9a4889b7a8cb9d7f90ca81",
    "figure_id": "2112.00798v7-Figure18-1",
    "image_file": "2112.00798v7-Figure18-1.png",
    "caption": " GOSDT and DL8.5 trees on the FICO dataset with guessed thresholds and guessed lower bounds at depth limit 5. The reference model was trained using 40 decision stumps. λ = 0.0005.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tree has the higher test accuracy?",
    "answer": "DL8.5+th+lb",
    "rationale": "The test accuracy for GOSDT+th+lb is 0.717, while the test accuracy for DL8.5+th+lb is 0.709.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.00798v7",
    "pdf_url": null
  },
  {
    "instance_id": "5bf873c6e73f4687bfcb47618180bf59",
    "figure_id": "2102.04376v1-Figure9-1",
    "image_file": "2102.04376v1-Figure9-1.png",
    "caption": " Average intrinsic reward for different methods trained in MultiRoomN12S10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in terms of average intrinsic reward?",
    "answer": "AGAC",
    "rationale": "The plot shows that the blue line, which represents AGAC, is the highest among all the lines, indicating that AGAC achieved the highest average intrinsic reward.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.04376v1",
    "pdf_url": null
  },
  {
    "instance_id": "280652e657594669abe90c826de4b556",
    "figure_id": "2005.10825v1-Figure2-1",
    "image_file": "2005.10825v1-Figure2-1.png",
    "caption": " Limitations of existing methods. Existing learning-based methods fail to predict plausible colors for multiple object instances such as skiers (top) and vehicles (bottom). The result of Deoldify [1](bottom) also suffers the context confusion (biasing to green color) due to the lack of clear figure-ground separation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most plausible colors for the orange?",
    "answer": "Ours (d)",
    "rationale": "The orange in (d) has the most realistic and natural-looking color, compared to the other methods. The orange in (b) is too green, the orange in (c) is too yellow, and the orange in (a) is black and white.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.10825v1",
    "pdf_url": null
  },
  {
    "instance_id": "92d15122c2e943f3b0ad521e3b12e92d",
    "figure_id": "2004.12485v2-Figure4-1",
    "image_file": "2004.12485v2-Figure4-1.png",
    "caption": " Performance comparison between Random Search - depicted in blue and PGFS - depicted in orange on three HIV-related QSAR-based scores. (a), (b) and (c): box plots of the corresponding QSAR-based scores per step of the iterative 5-step virtual synthesis. The first step (Reaction Step =0) in each box plot shows the scores of the initial R1s. (d), (e) and (f): distributions of the maximum QSAR-based rewards over 5-step iterations without the AD filtering. (g), (h) and (i): distributions of the maximum QSAR-based rewards over 5-step iterations after compounds that do not satisfy AD criteria of the corresponding QSAR model were filtered out from both sets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, Random Search or PGFS, generally produced compounds with higher predicted activity against HIV according to the three QSAR models?",
    "answer": "PGFS",
    "rationale": "The box plots in (a), (b), and (c) show that the median and upper quartiles of the QSAR scores for PGFS are higher than those for Random Search at each reaction step. The histograms in (d), (e), and (f) show that PGFS produced a greater proportion of compounds with higher QSAR scores than Random Search, both before and after AD filtering.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.12485v2",
    "pdf_url": null
  },
  {
    "instance_id": "d4d7b6062eb1432982365d7292a8fb04",
    "figure_id": "1905.12412v3-Figure1-1",
    "image_file": "1905.12412v3-Figure1-1.png",
    "caption": " The algorithmic parameters for SVRG++ and Katyushans are set according to [2] and [1], respectively, and those for Varag are set as in Theorem 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges to the minimum training loss the fastest for the Diabetes dataset?",
    "answer": "SVRG++",
    "rationale": "The figure shows the training loss for each algorithm as a function of the number of gradient steps. For the Diabetes dataset, the blue line (SVRG++) reaches the lowest training loss value first.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12412v3",
    "pdf_url": null
  },
  {
    "instance_id": "aa78467d36d24c2091ad90a21e1c7a4d",
    "figure_id": "2306.00576v1-Figure6-1",
    "image_file": "2306.00576v1-Figure6-1.png",
    "caption": " The visualization presents various instances of joint animal and behavior classification. In the third column, accurate predictions are displayed, while the fourth, fifth, and sixth columns showcase mispredicted examples where either the animal or the behavior does not correspond to the correct prediction.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which prediction shows an example of a correctly predicted animal but an incorrectly predicted behavior?",
    "answer": "Prediction 2.",
    "rationale": "In the second row of Prediction 2, the animal is correctly identified as a dog, but the behavior is incorrectly predicted as \"eat\" instead of \"hunt.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.00576v1",
    "pdf_url": null
  },
  {
    "instance_id": "5056f37555ee413ba765f704fcd2de77",
    "figure_id": "2302.08631v3-Figure2-1",
    "image_file": "2302.08631v3-Figure2-1.png",
    "caption": " Performance comparison between SquareCB.G and SquareCB on synthetic inventory dataset. Left figure: Results under fixed discretized action set. Right figure: Results under adaptive discretization of the action set. Both figures show the superiority of SquareCB.G compared with SquareCB.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better, SquareCB.G or SquareCB?",
    "answer": "SquareCB.G performs better than SquareCB.",
    "rationale": "The figure shows the PV loss for both algorithms, and SquareCB.G has a lower PV loss than SquareCB in both the fixed and adaptive action set scenarios. This indicates that SquareCB.G is more effective at reducing the PV loss.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.08631v3",
    "pdf_url": null
  },
  {
    "instance_id": "68356bbc70f5466bb4d0e05588fae8f1",
    "figure_id": "1912.09666v2-Figure4-1",
    "image_file": "1912.09666v2-Figure4-1.png",
    "caption": " Clipping levels in different layers for models trained individually with different bit-widths (solid lines) or trained with Vanilla AdaBits (dashed line). Note that the clipping level of a layer refers to the clipping level for the output of this layer. The outputs of the last layer are not clipped.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which bit-width has the lowest clipping levels across all layers?",
    "answer": "5 bits",
    "rationale": "The figure shows the clipping levels for different bit-widths. The 5-bit line is consistently lower than the other lines, indicating that it has the lowest clipping levels across all layers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.09666v2",
    "pdf_url": null
  },
  {
    "instance_id": "8ef268548deb4a6fae25f46793afba12",
    "figure_id": "2305.12268v1-Figure2-1",
    "image_file": "2305.12268v1-Figure2-1.png",
    "caption": " Overall pretraining and finetuning procedures for PATTON. We have two pretraining strategies: networkcontextualized masked language modeling (NMLM) and masked node prediction (MNP). Apart from output layers, the same architectures are used in both pretraining and finetuning (in our experiment, we have 12 layers). The same pretrained model parameters are used to initialize models for different downstream tasks. During finetuning, all parameters are updated.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two pretraining strategies used in PATTON?",
    "answer": "NMLM and MNP.",
    "rationale": "The figure shows two pretraining strategies: NMLM and MNP. NMLM is used to predict masked words in a sentence, while MNP is used to predict masked nodes in a graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.12268v1",
    "pdf_url": null
  },
  {
    "instance_id": "5aa3b27cd9e04301989fc644175c64a5",
    "figure_id": "2010.10814v1-Figure14-1",
    "image_file": "2010.10814v1-Figure14-1.png",
    "caption": " Mean normalized score of different methods on 500 level generalization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which regularization method is the most effective at improving generalization performance on the training set?",
    "answer": "L2 regularization.",
    "rationale": "The plot shows that L2 regularization (red line) has the highest mean normalized score on the training set (solid lines) across all timesteps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.10814v1",
    "pdf_url": null
  },
  {
    "instance_id": "d1fb411e8f13446181cdb2e0e140ae01",
    "figure_id": "2009.03300v3-Figure70-1",
    "image_file": "2009.03300v3-Figure70-1.png",
    "caption": " A World Religions example.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which person did the Great Cloud Sutra prophesy the imminent arrival of?",
    "answer": "Maitreya (Milo)",
    "rationale": "The text in the image states that \"The Great Cloud Sutra prophesied the imminent arrival of Maitreya (Milo).\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.03300v3",
    "pdf_url": null
  },
  {
    "instance_id": "833d68b0362d4996ae849132d3279805",
    "figure_id": "1807.04587v2-Figure5-1",
    "image_file": "1807.04587v2-Figure5-1.png",
    "caption": " Distribution of test accuracies achieved under different hyperparameters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method achieved the highest test accuracy on the MNIST dataset with fully-connected networks?",
    "answer": "SDTP alternating.",
    "rationale": "The figure shows the distribution of test accuracies for different training methods and network architectures on the MNIST and CIFAR datasets. For the MNIST dataset with fully-connected networks, the SDTP alternating method has the highest peak in its distribution, indicating that it achieved the highest test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1807.04587v2",
    "pdf_url": null
  },
  {
    "instance_id": "0871b6076dcc4db78da41a4410961cf0",
    "figure_id": "1905.06394v1-Figure1-1",
    "image_file": "1905.06394v1-Figure1-1.png",
    "caption": " Table of upper bounds and lower bounds on the kernel query complexity.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the upper bound on the kernel query complexity of KRR?",
    "answer": "O(nd^eff/ε log(d^eff/ε))",
    "rationale": "The table shows that the upper bound on the kernel query complexity of KRR is O(nd^eff/ε log(d^eff/ε)).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.06394v1",
    "pdf_url": null
  },
  {
    "instance_id": "31c549d4f97d46f78a7b58db1ec47894",
    "figure_id": "2005.13532v1-Figure8-1",
    "image_file": "2005.13532v1-Figure8-1.png",
    "caption": " Bird Sequences: We captured a wide variety of birds at the National Aviary of Pittsburgh. We have no control on the motion of birds, their environment, lighting condition, and dynamism in the background due to human movement in the aviary. Shown here are examples from three such sequences to give a sense of our capture scenario.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three bird sequences is most likely to be affected by human movement in the background?",
    "answer": "The Tropical sequence.",
    "rationale": "The Tropical sequence shows a bird perched on a branch in a greenhouse-like setting. The background of the image is filled with windows and other structures, which suggests that there are likely to be people moving around in the background. The Wetlands and Penguins sequences, on the other hand, have more natural backgrounds that are less likely to be affected by human movement.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.13532v1",
    "pdf_url": null
  },
  {
    "instance_id": "df80afde7c1249fb81a9e5369422c321",
    "figure_id": "2002.00741v1-Figure4-1",
    "image_file": "2002.00741v1-Figure4-1.png",
    "caption": " Attention visualization. The blue (left) bar is the content-based importance score α , the orange (middle) bar is the contextualized temporal influence score βc , the green (right) bar is the combined importance score γ . The figures contains three different sequences selected from the test set of the UserBehavior dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sequence has the highest combined importance score for the first item?",
    "answer": "Sample Sequence C",
    "rationale": "The green bar in Sample Sequence C is the highest for the first item.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.00741v1",
    "pdf_url": null
  },
  {
    "instance_id": "cec5fb947c1849e186829178a79159b4",
    "figure_id": "2101.08596v1-Figure1-1",
    "image_file": "2101.08596v1-Figure1-1.png",
    "caption": " Breakdown of the computation of mel-filterbanks, Time-Domain filterbanks, SincNet, and the proposed LEAF frontend. Orange boxes are fixed, while computations in blue boxes are learnable. Grey boxes represent activation functions.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which frontend uses the sPCEN activation function?",
    "answer": "LEAF",
    "rationale": "The sPCEN activation function is shown in the last blue box of the LEAF frontend processing pipeline.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.08596v1",
    "pdf_url": null
  },
  {
    "instance_id": "6be744dca171499d84ebd50f524b95da",
    "figure_id": "2003.08607v1-Figure5-1",
    "image_file": "2003.08607v1-Figure5-1.png",
    "caption": " Convergence.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method converges faster, the source model or SRDC?",
    "answer": "The source model converges faster than SRDC.",
    "rationale": "The figure shows that the test error of the source model decreases more rapidly than the test error of SRDC.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.08607v1",
    "pdf_url": null
  },
  {
    "instance_id": "126f76c11483462889e27fb45685e689",
    "figure_id": "1808.09315v1-Figure2-1",
    "image_file": "1808.09315v1-Figure2-1.png",
    "caption": " Key phrase hit rate results computed on SST dev set. The RNFs are implemented with LSTMs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of filter has a higher key phrase hit rate for phrases of length 4?",
    "answer": "Linear filters",
    "rationale": "The blue bars represent the key phrase hit rate for linear filters, and the green bars represent the key phrase hit rate for recurrent neural filters. For phrases of length 4, the blue bar is higher than the green bar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.09315v1",
    "pdf_url": null
  },
  {
    "instance_id": "e7a09829688c420897ce92690984600f",
    "figure_id": "2004.10904v2-Figure1-1",
    "image_file": "2004.10904v2-Figure1-1.png",
    "caption": " We present a novel physically-based deep network for image-based reconstruction of transparent objects with a small number of views. (a) An input photograph of a real transparent object captured under unconstrained conditions (1 of 10 images). (b) and (c): The reconstructed shape rendered under the same view with transparent and white diffuse material. (d) The reconstructed shape rendered under a novel view and environment map.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the images shows the reconstructed shape of the transparent object rendered under a novel view and environment map?",
    "answer": "Image (d)",
    "rationale": "The caption states that image (d) shows the reconstructed shape of the transparent object rendered under a novel view and environment map. This is evident from the fact that the object is shown in a different pose and with different lighting than in the other images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.10904v2",
    "pdf_url": null
  },
  {
    "instance_id": "6d025a173c6f4b7abbf263bbd0afd1f8",
    "figure_id": "2205.03559v2-Figure1-1",
    "image_file": "2205.03559v2-Figure1-1.png",
    "caption": " Sequence classification of numbers, or number entity recognition.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which word in the text is likely to be a number?",
    "answer": "COUNT",
    "rationale": "The text states that the Super Bowl is a game to determine the champion of the NFL for the 2016 season. The word \"COUNT\" is likely a number because it is used to describe the number of Super Bowls that have been played.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.03559v2",
    "pdf_url": null
  },
  {
    "instance_id": "f08ecf0d01094e2cb91a845bf5b267e8",
    "figure_id": "2212.03140v1-Figure2-1",
    "image_file": "2212.03140v1-Figure2-1.png",
    "caption": " Overview of our framework: (1) Contrastive Retrieval (2) Source Encoder; (3) Memory Encoder with Hierarchical Group Attention module (we only show three translation memories for brevity); (4) Decoder; (5) Contrastive Learning.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the framework is responsible for incorporating information from previous translations?",
    "answer": "The Memory Encoder.",
    "rationale": "The Memory Encoder takes as input the translation memories, which contain information from previous translations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.03140v1",
    "pdf_url": null
  },
  {
    "instance_id": "c15beeaa21da47ea927ef9f0267cc4c5",
    "figure_id": "2306.05167v1-Figure3-1",
    "image_file": "2306.05167v1-Figure3-1.png",
    "caption": " A comparison between S4 and RNN on the HalfCheetah dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the Medium-replay dataset?",
    "answer": "DS4",
    "rationale": "The figure shows the normalized rewards for three different methods (DS4, RNN, and DT) on three different datasets (Medium, Medium-replay, and Expert). The DS4 method has the highest normalized reward on the Medium-replay dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.05167v1",
    "pdf_url": null
  },
  {
    "instance_id": "320334a742244252b6e2695552fbdfd2",
    "figure_id": "2012.05400v1-Figure7-1",
    "image_file": "2012.05400v1-Figure7-1.png",
    "caption": " The curves of mean self-entropy and the corresponding AP or mAP vary with confidence threshold in four adaptation tasks. It can nearly search the best mAP via SED.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which adaptation task does SFOCD-Mosaic-Defoggy achieve the highest mAP?",
    "answer": "Cityscapes to Foggy Cityscapes.",
    "rationale": "The figure shows that SFOCD-Mosaic-Defoggy achieves the highest mAP in the Cityscapes to Foggy Cityscapes task, as indicated by the peak of the green line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.05400v1",
    "pdf_url": null
  },
  {
    "instance_id": "9449b5df68e141ce8f629eabbc47569f",
    "figure_id": "2302.04693v1-Figure6-1",
    "image_file": "2302.04693v1-Figure6-1.png",
    "caption": " Testing our seek-avoid controllability measure in toy domains using the F1 classification metric. In each domain, we collect transitions using a random policy and measure how effectively we can classify controllability for different values of the threshold τ1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment has the highest F1 classification score for all values of tau?",
    "answer": "Taxi-v3",
    "rationale": "The Taxi-v3 environment has the highest F1 classification score for all values of tau, as can be seen from the plot. The F1 classification score is a measure of how well the controllability measure is able to classify controllable and uncontrollable states.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.04693v1",
    "pdf_url": null
  },
  {
    "instance_id": "ac5854d4a6b044ab8ba725757cda8af9",
    "figure_id": "2104.06400v1-Figure6-1",
    "image_file": "2104.06400v1-Figure6-1.png",
    "caption": " The range of possible expected layers when varying context length, for each of the seven tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task has the widest range of possible expected layers?",
    "answer": "RC",
    "rationale": "The figure shows that the range of possible expected layers for RC is from 0.5 to 4.5, which is the widest range of any of the tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.06400v1",
    "pdf_url": null
  },
  {
    "instance_id": "9bc94dfca48846e2a36825b28f9e455a",
    "figure_id": "2204.00926v1-Figure4-1",
    "image_file": "2204.00926v1-Figure4-1.png",
    "caption": " L2Aug is easily extended to support more actions (i.e., substitute)with improved casual user recommendation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which strategy results in better performance for casual user recommendation, L2Aug with substitute or L2Aug with drop and keep?",
    "answer": "L2Aug with substitute.",
    "rationale": "The plot shows the testing NDCG@5 (%) for both strategies over epochs. L2Aug with substitute (red line) consistently performs better than L2Aug with drop and keep (blue line) throughout the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.00926v1",
    "pdf_url": null
  },
  {
    "instance_id": "e06fea8a8ac94bc1986ddbff01625f70",
    "figure_id": "2010.02317v3-Figure8-1",
    "image_file": "2010.02317v3-Figure8-1.png",
    "caption": " We show the differences in performance (mean number of blue tiles, so lower is better) across humans and agents for the chain, tree, loop compositional conditions and the null condition. Differences are shown as t-values from testing difference in means across different participants or different agent runs. Any non-statistically significant differences are set to 0 (shown as the color white). Note that a negative t-value indicates better performance, since the metric is mean number of blue tiles revealed.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the conditions was the most difficult for humans?",
    "answer": "The loop condition.",
    "rationale": "The t-values for the loop condition are the most positive, which indicates that humans performed worse on this condition than on the other conditions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02317v3",
    "pdf_url": null
  },
  {
    "instance_id": "bfbecaef4a8f457ebfc9a8519bc40c99",
    "figure_id": "2209.05578v1-Figure9-1",
    "image_file": "2209.05578v1-Figure9-1.png",
    "caption": " Comparison of a random subset of images recovered by various gradient inversion attacks carried out using the gradients from FC2 with a batch of 64 Tiny-ImageNet images. Images are not cherry-picked.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method of gradient inversion attack produces the most recognizable images?",
    "answer": "The CP method.",
    "rationale": "The CP method produces images that are closest to the original images. This can be seen by comparing the CP images to the original images in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.05578v1",
    "pdf_url": null
  },
  {
    "instance_id": "4b651a6811f24cbb925d51c95f887a02",
    "figure_id": "2008.07176v1-Figure6-1",
    "image_file": "2008.07176v1-Figure6-1.png",
    "caption": " Total execution time of experiments on datasets with 75% duplicates. SOM means simple object map, ORM object reference map and OJM object join map. RocketRML generates incorrect results running OJM mappings.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest for datasets with 100k records and 5 different types of PredicateObjectMap (POM)?",
    "answer": "SOM-RDFizer*.",
    "rationale": "The figure shows the total execution time of different methods for datasets with different numbers of records and different types of POM. For datasets with 100k records and 5 different types of POM, the bar for SOM-RDFizer* is the shortest, indicating that it is the fastest method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.07176v1",
    "pdf_url": null
  },
  {
    "instance_id": "c8855de0a6ad4b9394a80b299159d4d7",
    "figure_id": "2204.05562v5-Figure7-1",
    "image_file": "2204.05562v5-Figure7-1.png",
    "caption": " Accuracy by levels of homophily and methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best at higher levels of homophily?",
    "answer": "Personalization.",
    "rationale": "The figure shows that the Personalization method has the highest accuracy at higher levels of homophily.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.05562v5",
    "pdf_url": null
  },
  {
    "instance_id": "58f34e9112ed48a79717bcd9e9c87fa7",
    "figure_id": "2305.00843v1-Figure3-1",
    "image_file": "2305.00843v1-Figure3-1.png",
    "caption": " Example instance from the proof of Theorem 11 showing that the PoS is larger than 1.",
    "figure_type": "schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which player is worse off in the unique IBE compared to the social optimum?",
    "answer": "Player 2.",
    "rationale": "In the social optimum, player 2 gets a payoff of y - x + 1. However, in the unique IBE, player 2 gets a payoff of 1. Since y - x + 1 is always greater than 1, player 2 is worse off in the unique IBE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.00843v1",
    "pdf_url": null
  },
  {
    "instance_id": "ddcdfdff6aec4be88f00b083188f7d9f",
    "figure_id": "2006.07364v2-Figure2-1",
    "image_file": "2006.07364v2-Figure2-1.png",
    "caption": " Learning curves of our RFC models and DeepMimic for imitating various agile motions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best for imitating the Ballet1 motion?",
    "answer": "RFC-Explicit (Ours)",
    "rationale": "The learning curve for RFC-Explicit (Ours) is the highest of all the models for the Ballet1 motion. This means that this model is able to learn the motion more quickly and accurately than the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.07364v2",
    "pdf_url": null
  },
  {
    "instance_id": "558780e2b4a2425da567bce322946fe8",
    "figure_id": "2106.02993v1-Figure4-1",
    "image_file": "2106.02993v1-Figure4-1.png",
    "caption": " Physics Consistency Score for Collision Dataset",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better in terms of Physics Consistency Score?",
    "answer": "PIG-GAN",
    "rationale": "The figure shows the distribution of Physics Consistency Scores for both PIG-GAN and PID-GAN. The distribution for PIG-GAN is shifted more to the right, indicating that it has a higher average score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02993v1",
    "pdf_url": null
  },
  {
    "instance_id": "b423a7625f9a4d2f9830f5370ef9e157",
    "figure_id": "2302.05915v1-Figure11-1",
    "image_file": "2302.05915v1-Figure11-1.png",
    "caption": " Feature importance for our explainable models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature is most important for the Logistic Regression model?",
    "answer": "posts_tr",
    "rationale": "The bar for \"posts_tr\" is the longest in the Logistic Regression plot, indicating that it has the highest importance score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.05915v1",
    "pdf_url": null
  },
  {
    "instance_id": "7aa961e1845249c18cff3f0daaa6f99c",
    "figure_id": "1910.13092v1-Figure4-1",
    "image_file": "1910.13092v1-Figure4-1.png",
    "caption": " Prediction accuracy of different machine learning models on MNIST dataset using different algorithms. Mean and standard error over 20 repetitions are shown. (Best seen in color)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best on the MNIST dataset?",
    "answer": "GPUCB-UBO",
    "rationale": "The figure shows the prediction accuracy of different machine learning models on the MNIST dataset using different algorithms. The GPUCB-UBO algorithm achieved the highest prediction accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.13092v1",
    "pdf_url": null
  },
  {
    "instance_id": "95e40f3843b84ce2ba883ce0a8b6b666",
    "figure_id": "2102.12677v3-Figure2-1",
    "image_file": "2102.12677v3-Figure2-1.png",
    "caption": " Stable rank ‖ · ‖2F /‖ · ‖2 (?) of batch gradient matrix of given groups (with p parameters). The setting is ResNet20 on CIFAR-10. The stable rank is small throughout training.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which layer has the most stable gradient during training? ",
    "answer": " The input layer.",
    "rationale": " The plot shows that the stable rank of the input layer is consistently lower than the stable ranks of the other layers. This means that the gradient of the input layer is more stable than the gradients of the other layers. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.12677v3",
    "pdf_url": null
  },
  {
    "instance_id": "251d1d78666e404caa5904ed60b9e370",
    "figure_id": "2204.00871v1-Figure2-1",
    "image_file": "2204.00871v1-Figure2-1.png",
    "caption": " Alignments for de→en (top-row) and en→hi (bottom-row) by NAIVEATT, PRIORATT, and POSTALN. Note that POSTALN is most similar to Gold alignments in the last column.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the alignment methods is most similar to the Gold alignments?",
    "answer": "POSTALN",
    "rationale": "The figure shows that the POSTALN alignments are most similar to the Gold alignments in both de→en and en→hi. This is evident from the fact that the POSTALN alignments have the most dark blue squares, which indicate that the words are aligned.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.00871v1",
    "pdf_url": null
  },
  {
    "instance_id": "f0d1dae837d749d9ba7dc44ce0cc2ad3",
    "figure_id": "2111.13657v3-Figure5-1",
    "image_file": "2111.13657v3-Figure5-1.png",
    "caption": " Model Quality tab in SageMaker Studio showing of Model Monitor Model Quality calculation of the F0.5 score for a binary classification model",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the F0.5 score for the baseline model at 11:30 AM?",
    "answer": "0.8",
    "rationale": "The figure shows the F0.5 score for the baseline and current models over time. The F0.5 score for the baseline model at 11:30 AM is 0.8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.13657v3",
    "pdf_url": null
  },
  {
    "instance_id": "2463a9bcd6654435912baa7a65e45640",
    "figure_id": "2307.07929v1-Figure12-1",
    "image_file": "2307.07929v1-Figure12-1.png",
    "caption": " CORD receipt parsing results on receipt_00058 sample. Each result consists of the visualization of model predictions, and the parsing outputs. (a) IOB tagging visualizes the predicted tags of OCR words. (b) SPADE visualizes the decoded graph, and arrows between words indicate that the words are linked in the same entity. (c) Ours visualizes the predicted anchor words and their bounding boxes. For simplicity of the visualization, entity-linking results are not visualized in here. For the parsing outputs, green/red text means the predicted text matches/does not match ground truth. Strikethrough text means the ground truth text is missed from prediction. Best view in color and zoom-in for details of the visualization. In this example, “CASH” is a subtotal. But it is right below the line-item. Thus, both “IOB Tagging” and “SPADE” missed this detection.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods shown in the figure is the most accurate?",
    "answer": "Ours.",
    "rationale": "The figure shows that \"Ours\" is the only method that correctly identifies all of the key information on the receipt, including the menu item, the price, the subtotal, the tax, the cash paid, and the change due. In contrast, both IOB Tagging and SPADE miss the subtotal information.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.07929v1",
    "pdf_url": null
  },
  {
    "instance_id": "41d069c9243f4608a2686e371a809084",
    "figure_id": "2305.16896v1-Figure8-1",
    "image_file": "2305.16896v1-Figure8-1.png",
    "caption": " An example of variations in answer formats",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many moles of water are produced when 2 moles of NaOH react with 1 mole of Cl2?",
    "answer": "1 mole of H2O is produced.",
    "rationale": "The balanced chemical equation for the reaction is:\n\n2 NaOH + Cl2 → H2O + NaCl + NaClO\n\nThis equation shows that 2 moles of NaOH react with 1 mole of Cl2 to produce 1 mole of H2O.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16896v1",
    "pdf_url": null
  },
  {
    "instance_id": "8cf305a6fc5740b4aaa9bea2633d89c8",
    "figure_id": "2211.15516v2-Figure3-1",
    "image_file": "2211.15516v2-Figure3-1.png",
    "caption": " Comparisons of different metrics. We only plot objects corresponding to the phrases “Two pandas” for a better comparison. (a) and (b) are used for phrase grounding. (a) The ANY-BOX setting treats a prediction as correct if any of the ground truth boxes is matched. (b) MERGED-BOXES combines all objects for one phrase to a big box for evaluation. (c) Our metric CMAP encourages a model to predict all objects and their corresponding phrases.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \n\nWhich metric encourages a model to predict all objects and their corresponding phrases? ",
    "answer": " CMAP",
    "rationale": " \n\nThe figure shows three different metrics for evaluating image captioning models. CMAP encourages a model to predict all objects and their corresponding phrases, as shown in (c). This is evident because the two separate boxes correspond to each panda in the image, with the caption above each box accurately reflecting what is within the box. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.15516v2",
    "pdf_url": null
  },
  {
    "instance_id": "6b9807b4d6f14d948204973dbb090af3",
    "figure_id": "2301.08249v1-Figure5-1",
    "image_file": "2301.08249v1-Figure5-1.png",
    "caption": " Visual comparisons of learned causal graphs",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the following subfigures shows the causal graph learned by the model that does not use any prior information?",
    "answer": " Subfigure (c)",
    "rationale": " The caption of the figure states that it shows visual comparisons of learned causal graphs. Each subfigure is labeled with the model that was used to learn the causal graph. Subfigure (c) is labeled \"w/o Prior\", which indicates that the model did not use any prior information to learn the causal graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.08249v1",
    "pdf_url": null
  },
  {
    "instance_id": "10ccf9f77d4a45869541ca5d3ce77211",
    "figure_id": "1912.04958v2-Figure17-1",
    "image_file": "1912.04958v2-Figure17-1.png",
    "caption": " The mean and standard deviation of the magnitudes of sorted singular values of the Jacobian matrix evaluated at random latent space points w, with largest eigenvalue normalized to 1. In both datasets, path length regularization (Config D) and novel architecture (Config F) exhibit better conditioning; notably, the effect is more pronounced in the Cars dataset that contains much more variability, and where path length regularization has a relatively stronger effect on the PPL metric (Table 1).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which configuration of FFHQ has the best conditioning, according to the plot?",
    "answer": "Config D (path length regularization)",
    "rationale": "The plot shows the mean and standard deviation of the magnitudes of sorted singular values of the Jacobian matrix for different configurations of FFHQ and Cars. Config D has the lowest mean and standard deviation, indicating that it has the best conditioning.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.04958v2",
    "pdf_url": null
  },
  {
    "instance_id": "394cb8f0a73f412cb957e69b24434275",
    "figure_id": "2303.02664v1-Figure7-1",
    "image_file": "2303.02664v1-Figure7-1.png",
    "caption": " Average results for instances with 5 processes and dur(b) = 3. Ellipse centers are averages over instances, the shaded area just covers the results for all 10 instances.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the processes had the highest average probability of success?",
    "answer": "proc1",
    "rationale": "The ellipse for proc1 is centered at the highest point on the y-axis, which represents the probability of success.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.02664v1",
    "pdf_url": null
  },
  {
    "instance_id": "4c580dd9151a41ed91ab1cd7fcb310e5",
    "figure_id": "2306.10191v2-Figure6-1",
    "image_file": "2306.10191v2-Figure6-1.png",
    "caption": " Frequency of dataset class names in the LAION-2B dataset normalized to 1. We find Neural Priming improves accuracy more for datasets that are rare.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset is the most prevalent in LAION-2B?",
    "answer": "SUN397",
    "rationale": "The bar for SUN397 is the tallest, indicating it has the highest relative frequency.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.10191v2",
    "pdf_url": null
  },
  {
    "instance_id": "61c226e36ce54759b279f219a7af5f29",
    "figure_id": "1910.12853v1-Figure6-1",
    "image_file": "1910.12853v1-Figure6-1.png",
    "caption": " Instructions of the subjective evaluation on hotel review.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two possible scores that a review can be given?",
    "answer": "Positive or negative.",
    "rationale": "The instructions explicitly state that the only two possible scores are \"positive\" and \"negative.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.12853v1",
    "pdf_url": null
  },
  {
    "instance_id": "43c75628cde94a72b5e10ba33c4d60b2",
    "figure_id": "1911.05416v2-Figure1-1",
    "image_file": "1911.05416v2-Figure1-1.png",
    "caption": " Clause-Gadget for clause Ci: the valuations of its three agents inside the gadget. Every block in this figure has value 0.24.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the total value of the clause gadget for clause Ci?",
    "answer": "0.72",
    "rationale": "The figure shows that each block in the clause gadget has a value of 0.24. Since there are three blocks, the total value is 0.24 * 3 = 0.72.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.05416v2",
    "pdf_url": null
  },
  {
    "instance_id": "3676ee93710149aea854de82b69d75f9",
    "figure_id": "2106.05933v2-Figure52-1",
    "image_file": "2106.05933v2-Figure52-1.png",
    "caption": " Sparsity over layers for wav2vec-base finetuned for French fr at 30% sparsity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the sparsity of the final layer of the wav2vec-base model finetuned for French fr at 30% sparsity?",
    "answer": "43.376%",
    "rationale": "The figure shows the sparsity of each layer of the wav2vec-base model. The final layer is the 11th layer, and its sparsity is shown as 43.376%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05933v2",
    "pdf_url": null
  },
  {
    "instance_id": "88bc0b0d2be64724a59ac15c8ac34616",
    "figure_id": "2305.04530v1-Figure1-1",
    "image_file": "2305.04530v1-Figure1-1.png",
    "caption": " A case from the PMR (Dong et al., 2022) data set, where the correct option is answer B. The bluecolor words represent the pivotal textual clue to infer the correctness of answers A and B.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the correct answer to the question of what Person 1 will do next, given that they are very tired?",
    "answer": "Person 1 will have a rest on the couch.",
    "rationale": "The figure shows that the correct answer is B, which is \"Person 1 will have a rest on the couch.\" This is because the figure shows that Person 1 is tired, and the most likely thing for a tired person to do is to rest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.04530v1",
    "pdf_url": null
  },
  {
    "instance_id": "ad86d14161e04e1fbb82a23696ea8e76",
    "figure_id": "1908.07831v1-Figure7-1",
    "image_file": "1908.07831v1-Figure7-1.png",
    "caption": " Number of reference paraphrases v.s. percentage of MSCOCO dataset",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of MSCOCO entries have 4 reference paraphrases?",
    "answer": "90.7%",
    "rationale": "The pie chart shows the distribution of the number of reference paraphrases for MSCOCO entries. The green slice represents the entries with 4 reference paraphrases, and the percentage is shown next to it.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.07831v1",
    "pdf_url": null
  },
  {
    "instance_id": "d01d0bd5320147bc8a0d3ea57f43bce7",
    "figure_id": "2002.10585v1-Figure4-1",
    "image_file": "2002.10585v1-Figure4-1.png",
    "caption": " Training curves for the cue-reward association task with fixed, binary four-bit cues (medians and inter-quartile ranges of rewards per episode over 10 runs). “Soft clip” refers to a different clipping operation used in Equation 2; “Hard clip” is the same as used in the present paper, i.e. the simple clipping described in Methods. Note that non-modulated plastic network succeed in solving this task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which learning algorithm performed the best in the cue-reward association task?",
    "answer": "Simple modulation (Soft Clip)",
    "rationale": "The figure shows the training curves for different learning algorithms on the cue-reward association task. The Simple modulation (Soft Clip) algorithm has the highest reward at the end of training, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.10585v1",
    "pdf_url": null
  },
  {
    "instance_id": "b81d224e8002432e904c50a386d374fe",
    "figure_id": "1904.06475v1-Figure1-1",
    "image_file": "1904.06475v1-Figure1-1.png",
    "caption": " T-SNE visualization of the mention embeddings generated by NFETC (left) and CLSC (right) on the BBN dataset. Our model (CLSC) clearly groups mentions of the same type into a compact cluster.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two models, NFETC or CLSC, seems to perform better at grouping mentions of the same type together?",
    "answer": "CLSC.",
    "rationale": "The T-SNE visualization shows that the mention embeddings generated by CLSC are more tightly clustered together than those generated by NFETC. This suggests that CLSC is better at grouping mentions of the same type together.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.06475v1",
    "pdf_url": null
  },
  {
    "instance_id": "a846cbf54e72402a9c26470a73c0a08f",
    "figure_id": "2308.09303v1-Figure7-1",
    "image_file": "2308.09303v1-Figure7-1.png",
    "caption": " This figure represents the task configuration of training data in the best case. We reported the number of samples from each task. Total means the summation of training samples. We observed that training data are impartially distributed among the tasks in the best case.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which task has the highest number of total samples?",
    "answer": " Task 2",
    "rationale": " The green bars in the figure represent the total number of samples for each task. The green bar for Task 2 is the highest, indicating that it has the most samples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.09303v1",
    "pdf_url": null
  },
  {
    "instance_id": "26ba362469b347ae8603c81777b34e2b",
    "figure_id": "2109.08253v2-Figure4-1",
    "image_file": "2109.08253v2-Figure4-1.png",
    "caption": " Bios dataset statistics.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which profession has the highest percentage of female workers?",
    "answer": "Dietitian",
    "rationale": "The bar chart shows that the percentage of female workers in the dietitian profession is 92.87%, which is the highest among all the professions listed.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.08253v2",
    "pdf_url": null
  },
  {
    "instance_id": "c44f14def2aa4f74b120587dcd1b1513",
    "figure_id": "2210.06324v2-Figure5-1",
    "image_file": "2210.06324v2-Figure5-1.png",
    "caption": " Cross-locale performance, Kendall Tau with human ratings.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which languages have the highest correlation in terms of cross-locale performance?",
    "answer": "English (US) and Japanese (JP)",
    "rationale": "The color of the squares in the heatmap represents the correlation between the fine-tuning locale and the test locale. The darker the blue color, the higher the correlation. The square corresponding to English (US) and Japanese (JP) is the darkest blue, indicating the highest correlation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06324v2",
    "pdf_url": null
  },
  {
    "instance_id": "360813c127fd4d8f8e3a33f1c4c528b4",
    "figure_id": "2006.01959v2-Figure3-1",
    "image_file": "2006.01959v2-Figure3-1.png",
    "caption": " Left: P-control trajectories for point mass, reacher-2D and fetch-3D environments. Plots are in the latent space of Fig. 2. We can see that only NewtonianVAE produces a latent space where a P-controller correctly leads the systems from the initial to goal state. Right: Convergence rates of PID control using various latent embeddings for the point mass (left) and reacher-2D (right) systems, over 50 episodes. We use gain parameters Kp = 8, Ki = 2, Kd = 0.5. For contrast, we show Model Predictive Control (MPC, using CEM planning as per [22]).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods produces a latent space where a P-controller correctly leads the systems from the initial to goal state?",
    "answer": "NewtonianVAE",
    "rationale": "The left side of the figure shows the P-control trajectories for point mass, reacher-2D and fetch-3D environments. We can see that only NewtonianVAE produces a latent space where a P-controller correctly leads the systems from the initial to goal state.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.01959v2",
    "pdf_url": null
  },
  {
    "instance_id": "3a6f3a80dfe441fc89a0cb729ac99645",
    "figure_id": "2208.12210v7-Figure6-1",
    "image_file": "2208.12210v7-Figure6-1.png",
    "caption": " Comparison of F1-score for d-RCD and σ-RCD based on isPossibleAncestor (top row) and isPossibleCycle (bottom row) queries. The number of entity types increases from left to right.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better for identifying possible cycles in a knowledge graph with a large number of entities?",
    "answer": "σ-RCD",
    "rationale": "The bottom row of the figure shows that the F1-score for σ-RCD is higher than that of d-RCD for the isPossibleCycle query, regardless of the number of entities.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.12210v7",
    "pdf_url": null
  },
  {
    "instance_id": "ade27314830f4dc3b6d7e782c305ea7e",
    "figure_id": "2005.01218v1-Figure1-1",
    "image_file": "2005.01218v1-Figure1-1.png",
    "caption": " An example question that requires multi-hop reasoning, together with its gold justifications from the QASC dataset. The two parallel evidence chains retrieved by AIR (see section 3) provide imperfect but relevant explanations for the given question.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to iron when it oxidizes?",
    "answer": "It rusts.",
    "rationale": "The image shows a passage about iron oxidation and rusting. The passage states that \"When iron oxidizes, it rusts.\" This means that when iron is exposed to oxygen, it will react with the oxygen to form rust.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.01218v1",
    "pdf_url": null
  },
  {
    "instance_id": "6478a5f4917f4e95af63335da0700504",
    "figure_id": "2210.17517v2-Figure2-1",
    "image_file": "2210.17517v2-Figure2-1.png",
    "caption": " Question n-gram distribution in L ĪLA.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of question is the most common in the L ĪLA dataset?",
    "answer": "\"What\" questions are the most common.",
    "rationale": "The figure shows the distribution of question n-grams in the L ĪLA dataset. The largest segment of the circle is labeled \"What\", indicating that questions beginning with this word are the most frequent.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.17517v2",
    "pdf_url": null
  },
  {
    "instance_id": "ec4b5c14358944cfb6008f044b659516",
    "figure_id": "2112.05195v2-Figure3-1",
    "image_file": "2112.05195v2-Figure3-1.png",
    "caption": " R@k of predicting persistent/emerging diseases for diagnosis prediction on the MIMIC-III dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed best for predicting persistent diseases at R@40?",
    "answer": "CGL",
    "rationale": "The figure shows the R@k of different methods for predicting persistent and emerging diseases. At R@40, CGL has the highest R@k for predicting persistent diseases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.05195v2",
    "pdf_url": null
  },
  {
    "instance_id": "db64a394e55c4b7ab35a36eee006a16a",
    "figure_id": "1908.10831v5-Figure2-1",
    "image_file": "1908.10831v5-Figure2-1.png",
    "caption": " Comparison of testing AUC on Cat&Dog, CIFAR10, CIFAR100 and STL10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performed the best on the Cat&Dog dataset?",
    "answer": "PPD-SG",
    "rationale": "The figure shows the testing AUC for different optimizers on the Cat&Dog dataset. The PPD-SG optimizer has the highest AUC, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.10831v5",
    "pdf_url": null
  },
  {
    "instance_id": "49d9626802ba47f5a6806a7de0c9cd54",
    "figure_id": "2111.00648v1-Figure4-1",
    "image_file": "2111.00648v1-Figure4-1.png",
    "caption": " Evaluation on the Kitti dataset for 3D scene flow. Black numbers and crosses (×) correspond to results on scene pairs that are sampled with 8,192 points per frame; red numbers and plus signs (+) correspond to scene pairs that are sampled with 30,000 points per frame.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest accuracy on the Kitti dataset for 3D scene flow, when using 30,000 points per frame?",
    "answer": "D-RoboOT (spline)",
    "rationale": "The table shows the results of different methods on the Kitti dataset for 3D scene flow. The red numbers and plus signs (+) correspond to results on scene pairs that are sampled with 30,000 points per frame. The D-RoboOT (spline) method has the highest accuracy (99.19%) for this setting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.00648v1",
    "pdf_url": null
  },
  {
    "instance_id": "b7d91bbf923d4e5f892159b026fffef0",
    "figure_id": "2007.14864v1-Figure2-1",
    "image_file": "2007.14864v1-Figure2-1.png",
    "caption": " Result of query in Fig. 1a and its potential matches.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between Carey and Stonebreaker?",
    "answer": "Carey was advised by Stonebreaker.",
    "rationale": "The figure shows a directed edge labeled \"hadAdvisor\" from Carey to Stonebreaker. This indicates that Stonebreaker was Carey's advisor.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.14864v1",
    "pdf_url": null
  },
  {
    "instance_id": "463443a4d5ca41f5a809d185114a7318",
    "figure_id": "1904.01763v3-Figure1-1",
    "image_file": "1904.01763v3-Figure1-1.png",
    "caption": " Empirical regret performances of the BaSE policy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the grid policies achieves the lowest average regret in most scenarios?",
    "answer": "The Minimax Grid policy.",
    "rationale": "In subfigures (a), (b), and (c), the Minimax Grid policy (blue line) consistently achieves the lowest average regret compared to the other grid policies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.01763v3",
    "pdf_url": null
  },
  {
    "instance_id": "815eb96ebaaf47eeb7b30315a5a707bd",
    "figure_id": "1903.05246v1-Figure5-1",
    "image_file": "1903.05246v1-Figure5-1.png",
    "caption": " Minimizing PX gives us refined information about the connectivity structure of different sets sharing metadata attributes in Facebook 100 datasets. Plotting F1 detection scores against the minimum of PX shows especially clear trends for the gender and residence metadata attributes. Plots for the graduation year attribute highlight an anomaly in the connectivity patterns of the 2009 graduating year classes. We explore this in further depth in the main text.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metadata attribute shows the clearest trend in the connectivity structure of different sets sharing metadata attributes in Facebook 100 datasets?",
    "answer": "Gender and residence",
    "rationale": "The plots for F1 detection scores against the minimum of PX show especially clear trends for the gender and residence metadata attributes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.05246v1",
    "pdf_url": null
  },
  {
    "instance_id": "c16598ce574a498f89d8c23fdb3e1643",
    "figure_id": "2011.14244v1-Figure5-1",
    "image_file": "2011.14244v1-Figure5-1.png",
    "caption": " Analysis of state ngrams. State ngrams correlate to sentence meaning. In cases (A, B, D, E), semantically similar sentence segments are clustered to the same state ngrams: (A) “location” (B) “rating” (D) “location” (E) “food“ and “price”. Yet there are also cases where state ngrams correspond to sentence segments with different meaning: (C1) “location” v.s. (C2) “comments”; (F1) “price” v.s. (F2) “price” and “comments”.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between state ngrams and sentence meaning?",
    "answer": "State ngrams correlate to sentence meaning.",
    "rationale": "The figure shows that semantically similar sentence segments are often clustered to the same state ngrams. For example, the state ngram \"location\" corresponds to the sentence segments \"located near\", \"restaurant near\", and \"restaurant located near\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.14244v1",
    "pdf_url": null
  },
  {
    "instance_id": "74be3892af314fc9a9df4998dca2a052",
    "figure_id": "2007.13544v2-Figure3-1",
    "image_file": "2007.13544v2-Figure3-1.png",
    "caption": " Illustration of Lemma 2. In this simple example, the subgame begins with some probability β(heads) of a coin being heads-up, which player 1 observes. Player 2 then guesses if the coin is heads or tails, and wins if he guesses correctly. The payoffs for Player 2’s pure strategies are shown as the lines marked πheads",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the payoff to Player 2 if he guesses heads and the coin is actually heads?",
    "answer": "1",
    "rationale": "The payoff to Player 2 for guessing heads is shown by the orange dashed line labeled π2heads. This line intersects the y-axis at 1, which means that the payoff to Player 2 is 1 if he guesses heads and the coin is actually heads.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.13544v2",
    "pdf_url": null
  },
  {
    "instance_id": "e5c03a5d757e4a5aa7c5cafd98e3225f",
    "figure_id": "2303.08308v1-Figure3-1",
    "image_file": "2303.08308v1-Figure3-1.png",
    "caption": " (a) We simplify space search into model search process; (b) Illustration of our hyperspace. A sampled search space is encoded by a sequential elastic stages. Contents in blue are searched: an elastic stage can search its block type and channel number list.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different block types are there in Elastic Stage 4?",
    "answer": "There is only one block type in Elastic Stage 4.",
    "rationale": "In Elastic Stage 4, there is only one block listed in the table below the stage, which is \"Residual bottleneck+SE\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.08308v1",
    "pdf_url": null
  },
  {
    "instance_id": "ff4d2707a448408f90a41b2abc495a35",
    "figure_id": "1902.04783v4-Figure2-1",
    "image_file": "1902.04783v4-Figure2-1.png",
    "caption": " The user interface eliciting structured explanations from participants. All benefit metrics are computed and displayed to reduce the cognitive burden of evaluating our fairness notions.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is more discriminatory according to the user's explanation?",
    "answer": "Algorithm #1",
    "rationale": "The user has selected Algorithm #1 and explained that it is more discriminatory because it results in less equal number of correct predictions across racial groups.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.04783v4",
    "pdf_url": null
  },
  {
    "instance_id": "f08614bce7524b0f9a4110c5f4cb73a2",
    "figure_id": "2203.16639v1-Figure8-1",
    "image_file": "2203.16639v1-Figure8-1.png",
    "caption": " Fast concept learning performance under different percentage of related concepts in the supplemental sentence.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best when the percentage of related concepts in the supplementary sentence was low?",
    "answer": "MAC",
    "rationale": "The MAC line is the highest at the beginning of the graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.16639v1",
    "pdf_url": null
  },
  {
    "instance_id": "f57bf9c824624ad0af8a20aafcae9ad1",
    "figure_id": "2206.02608v1-Figure2-1",
    "image_file": "2206.02608v1-Figure2-1.png",
    "caption": " For selected models, the average F1-score (yaxis) for how well a character (x-axis) can be classified on our main probing task. The control (random embeddings) appears in red, the syntax baseline in green. The other 4 models are shown in grayscale, with the largest and most recent model (GPT-J) in the darkest color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the main probing task?",
    "answer": "GPT-J",
    "rationale": "The figure shows the F1-score for each model, with the highest score indicating the best performance. GPT-J has the highest score, as shown by the black line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.02608v1",
    "pdf_url": null
  },
  {
    "instance_id": "0b4bef7ce1224787a1d02e9eef91c640",
    "figure_id": "2010.13773v1-Figure1-1",
    "image_file": "2010.13773v1-Figure1-1.png",
    "caption": " Adversarial samples and their corresponding perturbations generated by different sparse adversarial attack methods. Our method achieves the best performance of sparsity and invisibility.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods produces the most sparse perturbations?",
    "answer": "Our method.",
    "rationale": "The figure shows that our method produces the fewest white pixels in the perturbation image, which indicates that it is the most sparse.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.13773v1",
    "pdf_url": null
  },
  {
    "instance_id": "ad1dfd44a101493587942845067e141e",
    "figure_id": "2205.01898v1-Figure3-1",
    "image_file": "2205.01898v1-Figure3-1.png",
    "caption": " Relation annotation distribution by MTurkers for compared models trained on ROCStories.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model generated the most relations that were classified as \"vague\" by MTurkers?",
    "answer": "The RL model.",
    "rationale": "The figure shows that the RL model has the highest proportion of \"vague\" relations, as indicated by the blue bars.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.01898v1",
    "pdf_url": null
  },
  {
    "instance_id": "09fb27a6fd4e4fa58d5e570aab843d61",
    "figure_id": "2010.12645v2-Figure5-1",
    "image_file": "2010.12645v2-Figure5-1.png",
    "caption": " Example NS-MDP.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the probability of transitioning from state S1 to state S2 in the NS-MDP shown in the figure?",
    "answer": "0.1",
    "rationale": "The figure shows a transition from state S1 to state S2 with a probability of 0.1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12645v2",
    "pdf_url": null
  },
  {
    "instance_id": "b940d3792c6e42d5898824cd4dc81ad0",
    "figure_id": "2004.12331v1-Figure4-1",
    "image_file": "2004.12331v1-Figure4-1.png",
    "caption": " The comparison of average topic coherence vs. different topic proportion on three datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which topic modeling technique has the highest average topic coherence on the Grolier dataset when the topic proportion is 50%?",
    "answer": "Gaussian-BAT",
    "rationale": "The figure shows that the average topic coherence for Gaussian-BAT is higher than for the other topic modeling techniques at a topic proportion of 50% on the Grolier dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.12331v1",
    "pdf_url": null
  },
  {
    "instance_id": "3ffad386505541b9a60ff77e46b32297",
    "figure_id": "2102.05188v2-Figure2-1",
    "image_file": "2102.05188v2-Figure2-1.png",
    "caption": " Using CaPC to improve model performance. Dashed lines represent mean accuracy. With homogeneous models, we observe a mean increase of 4.09 and of 1.92 percentage points on CIFAR10 and SVHN, respectively, and an increase of 2.64 with heterogeneous models; each party still sees improvements despite differing model architectures (see Figure 7 in Appendix F).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture benefited the most from using CaPC in the homogeneous setting?",
    "answer": "CIFAR10.",
    "rationale": "The figure shows that the mean accuracy for CIFAR10 increased by 4.09 percentage points after using CaPC, while the mean accuracy for SVHN increased by only 1.92 percentage points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.05188v2",
    "pdf_url": null
  },
  {
    "instance_id": "81a44e756f054823afeec7182b917463",
    "figure_id": "2210.14026v1-Figure5-1",
    "image_file": "2210.14026v1-Figure5-1.png",
    "caption": " Average communication and epoch times for increasing numbers of clients.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization algorithm has the lowest communication time for 16 clients?",
    "answer": "SWIFT (2-SGD)",
    "rationale": "The plot in Figure (a) shows the average communication time for different optimization algorithms. The SWIFT (2-SGD) line is the lowest at 16 clients.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.14026v1",
    "pdf_url": null
  },
  {
    "instance_id": "0c1d1814e51942ddb5c74e911c4291bf",
    "figure_id": "2205.14393v1-Figure5-1",
    "image_file": "2205.14393v1-Figure5-1.png",
    "caption": " Visualization on relation attentions to different mentions of a given entity.",
    "figure_type": "** Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which mentions have the highest attention weights for Relation 1?",
    "answer": " Mentions 1 and 2.",
    "rationale": " The heatmap shows the attention weights for each relation-mention pair. The color of each cell indicates the weight, with red being the highest and blue being the lowest. For Relation 1, the cells corresponding to Mentions 1 and 2 are the darkest red, indicating that these mentions have the highest attention weights.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.14393v1",
    "pdf_url": null
  },
  {
    "instance_id": "ac633edd630f424c99f6a8e2ff8e50e0",
    "figure_id": "2012.14558v2-Figure3-1",
    "image_file": "2012.14558v2-Figure3-1.png",
    "caption": " Training loss and test accuracy on CIFAR-10 and CIFAR-100 datasets for VGG-16",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performs the best on the CIFAR-100 dataset?",
    "answer": "SC-PDA",
    "rationale": "The test accuracy for SC-PDA is the highest on the CIFAR-100 dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.14558v2",
    "pdf_url": null
  },
  {
    "instance_id": "ef88d58ca0dd4603bbafccd2f9a1aa25",
    "figure_id": "2108.10573v2-Figure4-1",
    "image_file": "2108.10573v2-Figure4-1.png",
    "caption": " Learning Staircase and Parity functions with UnBiased Rademacher data.",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which function is easier to learn, parity or staircase?",
    "answer": "Staircase.",
    "rationale": "The loss for the staircase function decreases to almost zero within 20,000 iterations, while the loss for the parity function remains relatively high even after 100,000 iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.10573v2",
    "pdf_url": null
  },
  {
    "instance_id": "7059c02f3de647b093b85c182193d18d",
    "figure_id": "2003.13328v1-Figure2-1",
    "image_file": "2003.13328v1-Figure2-1.png",
    "caption": " Schematic illustration of the Strip Pooling (SP) module.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two types of strip pooling used in the Strip Pooling module?",
    "answer": "Horizontal and vertical strip pooling.",
    "rationale": "The figure shows two branches of strip pooling, one with a 1 x W kernel and the other with an H x 1 kernel. The 1 x W kernel performs horizontal strip pooling, while the H x 1 kernel performs vertical strip pooling.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.13328v1",
    "pdf_url": null
  },
  {
    "instance_id": "ead85cfd0bb24a708cc592bb7fabe543",
    "figure_id": "2301.08556v1-Figure5-1",
    "image_file": "2301.08556v1-Figure5-1.png",
    "caption": " Example tasks in the simulated 6-DoF grasping benchmark, first introduced in [58]. There are ∼ 1,500 ShapeNet objects in training demonstrations, and held out YCB objects for evaluation. A camera mounted to the Franka Panda robot’s arm provides observations, and the policy controls the 6-DoF endeffector pose.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the task that the robot is performing in the image?",
    "answer": "The robot is performing a grasping task.",
    "rationale": "The image shows a robot arm with a gripper that is grasping a can of soup and a bottle of mustard. This is a common task for robots in industrial and domestic settings.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.08556v1",
    "pdf_url": null
  },
  {
    "instance_id": "a128f1c50b0741dfb4cac035202f567f",
    "figure_id": "2207.12654v2-Figure1-1",
    "image_file": "2207.12654v2-Figure1-1.png",
    "caption": " Comparison of VoxelNet representations learned from PointContrast [38] and our ProposalContrast.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pre-training method produces more complete object representations?",
    "answer": "Proposal-level pre-training.",
    "rationale": "The figure shows that proposal-level pre-training produces more complete object representations than point-level pre-training. This is evident in the dotted circles, which show that proposal-level pre-training is able to capture more of the object's features.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.12654v2",
    "pdf_url": null
  },
  {
    "instance_id": "0eb485609fb7496980659268fc6a902b",
    "figure_id": "2211.07830v2-Figure5-1",
    "image_file": "2211.07830v2-Figure5-1.png",
    "caption": " Results of ablating the surface form of the labels for structured prompting.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which POS tag has the largest difference in accuracy between the original and proxy methods?",
    "answer": "DET (determiner)",
    "rationale": "The figure shows that the accuracy for DET is around 90% for the original method and around 40% for the proxy method, which is a difference of 50%. This is the largest difference in accuracy between the two methods for any POS tag.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.07830v2",
    "pdf_url": null
  },
  {
    "instance_id": "79e9dc5177d1484d86a6c01437e819fb",
    "figure_id": "2210.10300v1-Figure1-1",
    "image_file": "2210.10300v1-Figure1-1.png",
    "caption": " Example of an intricate VideoQA problem. The semantic elements of the video, which consists of characters, their actions, and the relationship between the characters, is continually changing along with temporal axis. Therefore, it is hard to answer the questions requiring understanding the complex semantic structure.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What was the woman doing before she walked in front of the laptop?",
    "answer": "The woman was laughing.",
    "rationale": "The photograph shows the woman laughing in the first frame, and then walking in front of the laptop in the next frame.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.10300v1",
    "pdf_url": null
  },
  {
    "instance_id": "477896996920425fa0a3a949785a33ec",
    "figure_id": "2007.06925v1-Figure5-1",
    "image_file": "2007.06925v1-Figure5-1.png",
    "caption": " Visualization of HOI detections. The first and second rows show results compared with baseline on V-COCO. Each subplot displays one detected 〈 human, verb, object 〉 triplet for easy observation. Texts below indicate the 〈 verb, object 〉 tuple and two numbers in turn represent scores predicted by baseline and our approach. The last row shows multiple people take interactions with various objects concurrently detected by our method.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activity is most likely to be performed by a person sitting on a bed?",
    "answer": "lay",
    "rationale": "The image shows a person lying on a bed, which is a common position for someone who is sleeping or resting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.06925v1",
    "pdf_url": null
  },
  {
    "instance_id": "cb9c78ba860a438bb60b81a0d33f557b",
    "figure_id": "2305.03462v2-Figure6-1",
    "image_file": "2305.03462v2-Figure6-1.png",
    "caption": " Ablation study of the regularization weights and prior distribution discrepancy weight.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which regularization method appears to be the most effective at improving the PSNR of the reconstructed image when projecting from 3D space to a 2D plane?",
    "answer": "InfoReg",
    "rationale": "The plots in the top left and bottom right corners show that InfoReg consistently achieves the highest PSNR across all regularization weights, compared to Cycle Loss and StruReg.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.03462v2",
    "pdf_url": null
  },
  {
    "instance_id": "639bfaed0667450fab246221c66a9384",
    "figure_id": "1904.05916v2-Figure10-1",
    "image_file": "1904.05916v2-Figure10-1.png",
    "caption": " Error as a function of deer or coyote simulated images: 100K simulated images. Error is calculated as in Fig. 4 in the main paper. Trans+ deer and coyote performance are highlighted.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the cis+ deer test set?",
    "answer": "The CCT model.",
    "rationale": "The figure shows the error of different models on different test sets. The CCT model has the lowest error on the cis+ deer test set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.05916v2",
    "pdf_url": null
  },
  {
    "instance_id": "5331639c21734994b1f730765a30031b",
    "figure_id": "2003.08757v2-Figure7-1",
    "image_file": "2003.08757v2-Figure7-1.png",
    "caption": " The original and adversarial images crafted by PGD-16, AdvPatch and our AdvCam attack.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the images are adversarial examples?",
    "answer": "Images (b), (c), and (d) are adversarial examples.",
    "rationale": "The caption states that the images were crafted by PGD-16, AdvPatch, and AdvCam, which are all methods for generating adversarial examples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.08757v2",
    "pdf_url": null
  },
  {
    "instance_id": "e4c2ffdb2f764a06ba16771c4ccc1986",
    "figure_id": "2112.06482v4-Figure1-1",
    "image_file": "2112.06482v4-Figure1-1.png",
    "caption": " The architecture of ITA. ITA aligns an image into object tags, image captions and texts from OCR. ITA takes them as visual contexts and then feeds them together with the input texts into the transformer-based embeddings. In the cross-view alignment module, ITA minimizes the distance between the output distribution of cross-modal inputs and textual inputs.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the different types of visual contexts that ITA uses?",
    "answer": "Object tags, image captions, and OCR text.",
    "rationale": "The figure shows that the visual contexts are fed into the transformer-based embeddings along with the input texts. The figure also shows that the visual contexts are extracted from the image using object detection, image captioning, and OCR.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.06482v4",
    "pdf_url": null
  },
  {
    "instance_id": "f183d2aed0114f65a51d33724c7d0795",
    "figure_id": "2102.04897v2-Figure5-1",
    "image_file": "2102.04897v2-Figure5-1.png",
    "caption": " Learning curves of different question networks in six Atari games. x-axis denotes the number of frames and y-axis denotes the episode returns. Each curve is averaged over 5 independent runs with different random seeds. Shaded area shows the standard error.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which question network performs the best on the Breakout game?",
    "answer": "A2C",
    "rationale": "The A2C curve is the highest among all the curves in the Breakout subplot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.04897v2",
    "pdf_url": null
  },
  {
    "instance_id": "def4f06d4c064d56bb0a0e649461db35",
    "figure_id": "1911.08718v2-Figure6-1",
    "image_file": "1911.08718v2-Figure6-1.png",
    "caption": " Comparison the shadow detection results with state-of-the-art methods on the SBU dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate shadow detection results?",
    "answer": "Ours+",
    "rationale": "The figure shows the shadow detection results of different methods on two images. The Ours+ method produces the most accurate results, as its shadow masks are closest to the target masks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.08718v2",
    "pdf_url": null
  },
  {
    "instance_id": "bb8f80f40b794dbd817bdff8eb8ec5e6",
    "figure_id": "2209.09510v1-Figure7-1",
    "image_file": "2209.09510v1-Figure7-1.png",
    "caption": " Histograms of the number of iterations on the AIM@SHAPE dataset. The horizontal axis is the number of iterations that iPSR takes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which initialization method leads to faster convergence of the iPSR algorithm?",
    "answer": "Visibility initialization.",
    "rationale": "The histogram for visibility initialization shows that the algorithm converges in fewer iterations than the histogram for random initialization. This is because visibility initialization provides the algorithm with a better starting point, which allows it to converge more quickly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.09510v1",
    "pdf_url": null
  },
  {
    "instance_id": "59830f1db0fb4cf8aa2cb40d6c9a4ab7",
    "figure_id": "2204.12436v2-Figure1-1",
    "image_file": "2204.12436v2-Figure1-1.png",
    "caption": " Overview of results. An arrow from an axiom X to another axiom Y indicates that X implies Y . The red lines between axioms represent impossibility theorems. Note that Theorems 2 and 3 additionally require anonymity and neutrality. Axioms labeled with ML are satisfied by maximal lotteries, and axioms labeled with RD are satisfied by the uniform random dictatorship.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between Condorcet-consistency and SD-efficiency?",
    "answer": "Condorcet-consistency implies SD-efficiency.",
    "rationale": "There is an arrow pointing from Condorcet-consistency to SD-efficiency in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.12436v2",
    "pdf_url": null
  },
  {
    "instance_id": "bb388f8765c0427bb49adbf92d2b6200",
    "figure_id": "1903.04019v2-Figure5-1",
    "image_file": "1903.04019v2-Figure5-1.png",
    "caption": " Comparisons against the state-of-the-arts. Given different inputs and the referenced groundtruth, we show the completion results of three methods, with the corresponding point cloud error maps below, and zoom-in areas beside.",
    "figure_type": "** Photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method produces the most accurate results based on the point cloud error maps?",
    "answer": " Our method.",
    "rationale": " The point cloud error maps show the difference between the predicted point cloud and the ground truth point cloud. The error maps for our method have the least amount of red, indicating that our method produces the most accurate results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.04019v2",
    "pdf_url": null
  },
  {
    "instance_id": "c766aa70fdd142abb3167e60773bf86a",
    "figure_id": "2006.10916v3-Figure12-1",
    "image_file": "2006.10916v3-Figure12-1.png",
    "caption": " Diagram for the final network flow graph.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which node has the highest outdegree?",
    "answer": "Node 2.",
    "rationale": "Node 2 has 4 outgoing edges, which is more than any other node in the graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.10916v3",
    "pdf_url": null
  },
  {
    "instance_id": "2d998546b79a4c37948716a0201f92e1",
    "figure_id": "1907.04312v2-Figure6-1",
    "image_file": "1907.04312v2-Figure6-1.png",
    "caption": " We extract the PONO statistics from VGG-19, ResNet-152, and Dense-161 at layers right before downsampling (max-pooling or strided convolution).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models, VGG-19, ResNet-152, or DenseNet-161, preserves the most spatial information in the earlier layers?",
    "answer": "VGG-19.",
    "rationale": "The figure shows the feature maps extracted from the three models at different layers. The feature maps from VGG-19 in the earlier layers (Conv1_2, Conv2_2, Conv3_4, Conv4_4) are more detailed and preserve more spatial information than the feature maps from ResNet-152 and DenseNet-161 at the corresponding layers. This is because VGG-19 has a simpler architecture with fewer downsampling operations than the other two models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.04312v2",
    "pdf_url": null
  },
  {
    "instance_id": "c024c3d29a5c4d7ea6a77c1b016417c4",
    "figure_id": "2006.08545v1-Figure7-1",
    "image_file": "2006.08545v1-Figure7-1.png",
    "caption": " Negative training. The histograms of log-likelihood for RealNVP when in training likelihood is maximized on one dataset and minimized on another dataset: (a) maximized on CIFAR, minimized on SVHN; (b) maximized on SVHN, minimized on CIFAR; (c) maximized on CIFAR, minimized on CelebA; (d) maximized on CelebA, minimized on CIFAR. (e) maximized on FashionMNIST, minimized on MNIST; (f) maximized on MNIST, minimized on FashionMNIST;",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following datasets has the highest log-likelihood when trained on CIFAR and minimized on SVHN?",
    "answer": "SVHN",
    "rationale": "In subfigure (a), the log-likelihood for SVHN is higher than the log-likelihood for CIFAR when trained on CIFAR and minimized on SVHN.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.08545v1",
    "pdf_url": null
  },
  {
    "instance_id": "d252fe3e395f487283a6bef560ad9d38",
    "figure_id": "2209.13885v2-Figure5-1",
    "image_file": "2209.13885v2-Figure5-1.png",
    "caption": " Human evaluation on explanation quality.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model received the most votes for being informative?",
    "answer": "UCEpic",
    "rationale": "The figure shows that UCEpic received the most votes for informativeness, as indicated by the height of the blue bar in the \"Informativeness\" section.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.13885v2",
    "pdf_url": null
  },
  {
    "instance_id": "ff6a8824bf904e1a9b6b5089c403ec8d",
    "figure_id": "2305.08566v4-Figure4-1",
    "image_file": "2305.08566v4-Figure4-1.png",
    "caption": " System-level evaluation in Text Summarization (TextSumm). Left: Kolmogorov-Smirnov (KS) score on discerning the performance difference between two independent NLG systems – Higher is better [0, 1]. Right: Preference similarity between human and automatic metric – Higher is better [0, 1].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which automatic metric is most similar to human preference in identifying system-level performance difference?",
    "answer": "Single-CTRLEval",
    "rationale": "The figure shows the Kolmogorov-Smirnov (KS) score and preference similarity for different automatic metrics. The KS score measures how well an automatic metric can distinguish between the performance of two different NLG systems, while the preference similarity measures how well an automatic metric's preferences match human preferences. The figure shows that Single-CTRLEval has the highest KS score and preference similarity, indicating that it is most similar to human preference in identifying system-level performance difference.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.08566v4",
    "pdf_url": null
  },
  {
    "instance_id": "acebe66e0e6840a19fb20e6805422749",
    "figure_id": "1711.07838v1-Figure3-1",
    "image_file": "1711.07838v1-Figure3-1.png",
    "caption": " Visualization of Cit-DBLP dataset. Each point represents one paper. Different colors correspond to different publication divisions. Red: “Information Science”, blue: “ACM Transactions on Graphics”, green: “Human-Computer Interaction”.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm produced the most well-separated clusters?",
    "answer": "DeepWalk.",
    "rationale": "The figure shows the visualization of the Cit-DBLP dataset using different algorithms. Each point represents one paper, and different colors correspond to different publication divisions. The DeepWalk algorithm produced the most well-separated clusters, as the points of each color are grouped together more tightly than in the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.07838v1",
    "pdf_url": null
  },
  {
    "instance_id": "d25b048e5c1a40e0bfaf4bbde2133416",
    "figure_id": "1909.11515v2-Figure2-1",
    "image_file": "1909.11515v2-Figure2-1.png",
    "caption": " The results are averaged on 100 randomly test clean samples of CIFAR-10. The adversarial attack is untargeted PGD-10. Note that the ∆Gy calculated here is the minus value of it in Eq. (12) and Eq. (15).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which input type results in a larger value of ΔG_y when λ is close to 1?",
    "answer": "Adversarial inputs",
    "rationale": "The plots in the right column show that the value of ΔG_y is larger for adversarial inputs (orange line) than for clean inputs (green line) when λ is close to 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.11515v2",
    "pdf_url": null
  },
  {
    "instance_id": "cbdb39435c2741ddbdb85b504a3915c4",
    "figure_id": "2210.12400v1-Figure2-1",
    "image_file": "2210.12400v1-Figure2-1.png",
    "caption": " The distribution of informativeness scores across the different systems. Brighter means better.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which system produced the most informative questions?",
    "answer": "SQuAD.",
    "rationale": "The figure shows that SQuAD produced the highest proportion of informative questions (lightest gray bars) compared to the other systems.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12400v1",
    "pdf_url": null
  },
  {
    "instance_id": "80cfeb7734cd46f080265aa109ed9198",
    "figure_id": "2007.05084v1-Figure11-1",
    "image_file": "2007.05084v1-Figure11-1.png",
    "caption": " Edge vs Normal Case on More Sentences for Task-5",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the performance of the target task compare to the main task when both tasks use 100% adversarial examples and 0% honest examples?",
    "answer": "The target task performs worse than the main task.",
    "rationale": "In the figure, the blue line represents the target task and the green line represents the main task. When both tasks use 100% adversarial examples and 0% honest examples, the blue line is lower than the green line. This indicates that the target task has a lower test accuracy than the main task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.05084v1",
    "pdf_url": null
  },
  {
    "instance_id": "0ee2530d33ab496c8f0bbc327c58ebe7",
    "figure_id": "1904.09412v1-Figure3-1",
    "image_file": "1904.09412v1-Figure3-1.png",
    "caption": " Comparision with DrNet and MCNet on KTH Actions dataset.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of PSNR?",
    "answer": "CubicLSTM",
    "rationale": "The PSNR values for each model are shown in the table. CubicLSTM has the highest PSNR value of 28.2, which indicates that it performs the best in terms of PSNR.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.09412v1",
    "pdf_url": null
  },
  {
    "instance_id": "6a861d26890f4f008a986a5993372ae5",
    "figure_id": "2008.09159v4-Figure9-1",
    "image_file": "2008.09159v4-Figure9-1.png",
    "caption": " The percentage of policies, after deduplication, referencing self-regulation bodies over time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which self-regulation body experienced the largest increase in the percentage of policies referencing it between 2014 and 2019?",
    "answer": "TrustArc",
    "rationale": "The figure shows that the percentage of policies referencing TrustArc increased from around 2% in 2014 to around 18% in 2019, which is the largest increase among the self-regulation bodies shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.09159v4",
    "pdf_url": null
  },
  {
    "instance_id": "daa5815e857a46e2af68aaf41fab6ad3",
    "figure_id": "2102.11793v1-Figure8-1",
    "image_file": "2102.11793v1-Figure8-1.png",
    "caption": " We vary the predicate selectivity and evaluate the query execution time on the Windows System Log dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which predicate selectivity results in the shortest query execution time?",
    "answer": "0.01",
    "rationale": "The green bars in the figure represent the query execution time for a predicate selectivity of 0.01. These bars are consistently shorter than the bars for the other two predicate selectivities, indicating that a predicate selectivity of 0.01 results in the shortest query execution time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.11793v1",
    "pdf_url": null
  },
  {
    "instance_id": "f3f20dfe5ab649aca39727a940127db2",
    "figure_id": "2304.05939v1-Figure6-1",
    "image_file": "2304.05939v1-Figure6-1.png",
    "caption": " Here, we present the absolute value of the Fourier transform in log scale for the reconstructions and samples in the top and bottom row, respectively. To obtain the shown images we average the values over 128 individual images from the CelebA64 dataset. To the right the corresponding image for the data distribution is shown. In addition, we report the mean difference between each shown image with respect to the data image. Both qualitatively and quantitatively our approach can better replicate the frequency space of the data distribution.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the methods best replicates the frequency space of the data distribution?",
    "answer": " Ours",
    "rationale": " The figure shows the absolute value of the Fourier transform in log scale for the reconstructions and samples of different methods. The mean difference between each shown image and the data image is also reported. The method labeled \"Ours\" has the lowest mean difference, indicating that it best replicates the frequency space of the data distribution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.05939v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee34ca0ad0c24196a3fec3cc6a10d91e",
    "figure_id": "1909.07521v2-Figure4-1",
    "image_file": "1909.07521v2-Figure4-1.png",
    "caption": " Generating ENTAILMENT for monotonicity fragments starting from the premise (top). Each node in the tree shows an entailment generated by one substitution (in blue). Substitutions are based on a hand-coded knowledge base with information such as: all ≤ some/a, poodle ≤ dog ≤ mammal, and black mammal ≤ mammal. CONTRADICTION examples are generated for each inference using simple rules such as “replace some/many/every in subjects by no”. NEUTRALs are generated in a reverse manner as the entailments.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following statements is an entailment of the premise \"All black mammals saw exactly 5 stallions who danced\"?",
    "answer": "Some mammal saw exactly 5 stallions who danced.",
    "rationale": "The figure shows a tree of entailments generated from the premise. The statement \"Some mammal saw exactly 5 stallions who danced\" is a child node of the premise, indicating that it is an entailment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.07521v2",
    "pdf_url": null
  },
  {
    "instance_id": "d9ca2de3d03444a681547bb302a6f4f8",
    "figure_id": "2111.00210v2-Figure7-1",
    "image_file": "2111.00210v2-Figure7-1.png",
    "caption": " Evaluation curves of EfficientZero on Atari 100k benchmark for individual games. The average of the total rewards among 32 evaluation seeds for 3 runs is show on the y-axis and the number of total training steps is 120,000, shown on the x-axis.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which game has the highest average reward at the end of training?",
    "answer": "Breakout",
    "rationale": "The plot for Breakout shows the highest average reward at the end of training, which is around 45000.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.00210v2",
    "pdf_url": null
  },
  {
    "instance_id": "25ac70b3c29244479aab00c64db42ce7",
    "figure_id": "2209.15283v1-Figure11-1",
    "image_file": "2209.15283v1-Figure11-1.png",
    "caption": " Optimization behaviour of randomly, RF and GBDT initialized MLP and SAINT evaluated over a 5 times repeated (statisfied) 5-fold of each data set, according to Protocol P2. The lines and shaded areas report the mean and standard deviation. *evaluation on a single 5-fold cross validation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm generally performs the best across all datasets?",
    "answer": "SAINT",
    "rationale": "The SAINT lines (dark green) are generally lower than the other lines, indicating that it achieves the lowest cross-entropy loss.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.15283v1",
    "pdf_url": null
  },
  {
    "instance_id": "58e2f6b01e7349588193b50388b6c2e8",
    "figure_id": "2106.06770v2-Figure16-1",
    "image_file": "2106.06770v2-Figure16-1.png",
    "caption": " Alignment of the eigenfunctions of the NTK of a randomly initialized LeNet with the NTK computed after training to predict different initialization eigenfunctions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which eigenfunction of the NTK of a randomly initialized LeNet is most aligned with the NTK computed after training to predict different initialization eigenfunctions?",
    "answer": "The first eigenfunction.",
    "rationale": "The figure shows that the alignment between the eigenfunctions of the NTK of a randomly initialized LeNet and the NTK computed after training to predict different initialization eigenfunctions is highest for the first eigenfunction. This is because the first eigenfunction is the most important one for the network's performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.06770v2",
    "pdf_url": null
  },
  {
    "instance_id": "b37c16033e11471a9d32bc8f087ce108",
    "figure_id": "1911.02357v2-Figure6-1",
    "image_file": "1911.02357v2-Figure6-1.png",
    "caption": " Qualitative results of our method on selected textures (left) and objects (right) of the MVTec Anomaly Detection dataset. Our algorithm performs robustly across various defect categories, such as color defects, contaminations, and structural anomalies. Top row: Input images containing defects. Center row: Ground truth regions of defects in red. Bottom row: Anomaly scores for each image pixel predicted by our algorithm.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of anomaly is most common in the images?",
    "answer": "Structural anomalies.",
    "rationale": "The figure shows that the most common type of anomaly is structural, such as cracks, holes, and scratches. These anomalies are visible in the images as deviations from the expected structure of the object or texture.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.02357v2",
    "pdf_url": null
  },
  {
    "instance_id": "a41d94dd3d154c2780658058b42fcb85",
    "figure_id": "1904.04092v1-Figure4-1",
    "image_file": "1904.04092v1-Figure4-1.png",
    "caption": " The confusion matrix on the evaluation of aPaY dataset.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class has the highest accuracy?",
    "answer": "Train",
    "rationale": "The diagonal of the confusion matrix shows the number of correctly classified instances for each class. The highest value on the diagonal is for the \"train\" class.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.04092v1",
    "pdf_url": null
  },
  {
    "instance_id": "6a9b40efa3df4bbb88ff1fd428cb4275",
    "figure_id": "2006.16923v2-Figure16-1",
    "image_file": "2006.16923v2-Figure16-1.png",
    "caption": " On accuracy variations and human delta",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has a larger variance in accuracy change for the Top 25 Human-delta group?",
    "answer": "ResNet-50 Top-5.",
    "rationale": "The box plot for ResNet-50 Top-5 is taller than the box plot for NasNet-Mobile Top-5, indicating a larger range of values and therefore a larger variance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.16923v2",
    "pdf_url": null
  },
  {
    "instance_id": "199a30f3fdc54689b131f86f071ae389",
    "figure_id": "1911.04112v3-Figure5-1",
    "image_file": "1911.04112v3-Figure5-1.png",
    "caption": " The decision tree of a Dec-POMDP example with two agents and h = 2.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the probability of reaching the state \nΔ(σ0)(s0) T (s0,  σ0,  s1) Ω(s1)?",
    "answer": "0.25",
    "rationale": "The probability of reaching the state Δ(σ0)(s0) T (s0,  σ0,  s1) Ω(s1) \nis the product of the probabilities of all the branches leading to it. The first branch \nhas a probability of 0.5, the second branch has a probability of 0.5, and the third branch \nhas a probability of 1. Therefore, the probability of reaching the state Δ(σ0)(s0) T (s0,  σ0,  s1) Ω(s1) \nis 0.5 × 0.5 × 1 = 0.25.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.04112v3",
    "pdf_url": null
  },
  {
    "instance_id": "63f2b24e872745e2b89c9080272e425c",
    "figure_id": "1901.06709v1-Figure3-1",
    "image_file": "1901.06709v1-Figure3-1.png",
    "caption": " Illustration of the hard instance used in the proof of Proposition 3.1. The white point indicates the position of all the voters, and black points correspond to the candidates. The length of each acceptability radius is 1—as it is the same for all the voters, the instance is globally consistent.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the distance between the two candidates in the image?",
    "answer": "The distance between the two candidates is 1.",
    "rationale": "The caption states that the length of each acceptability radius is 1. In the image, the two candidates are shown to be exactly one acceptability radius apart.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.06709v1",
    "pdf_url": null
  },
  {
    "instance_id": "3147f584e1264bd7b4ac7e87a49e7d40",
    "figure_id": "2006.07507v3-Figure4-1",
    "image_file": "2006.07507v3-Figure4-1.png",
    "caption": " Difference between normalized competitor and CODE test losses using a tuned learning rate on 17 regression (1) and 21 classification (2) datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms generally performs the best across the regression datasets?",
    "answer": "Coin",
    "rationale": "The figure shows that Coin generally has the smallest difference between competitor and CODE test losses across the regression datasets. This means that Coin is generally performing better than the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.07507v3",
    "pdf_url": null
  },
  {
    "instance_id": "ce94e69801e54c01929c38d377241d99",
    "figure_id": "2005.01672v1-Figure1-1",
    "image_file": "2005.01672v1-Figure1-1.png",
    "caption": " PPL for each explanation method on TRANSFORMER over the IWSLT De⇒En dataset with different k value.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which explanation method has the highest PPL for all values of k?",
    "answer": "Ngrad",
    "rationale": "The figure shows that the Ngrad line is always above the other lines, indicating that it has the highest PPL for all values of k.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.01672v1",
    "pdf_url": null
  },
  {
    "instance_id": "5a8b369742a240c1b2b59adca61feca1",
    "figure_id": "1907.11584v1-Figure4-1",
    "image_file": "1907.11584v1-Figure4-1.png",
    "caption": " Test error of different S3VM solvers v.s. iterations on the eight benchmark data sets.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the eight benchmark datasets shows the fastest convergence of the TSGS3VM solver?",
    "answer": " The Higgs dataset.",
    "rationale": " The figure shows the test error of the TSGS3VM solver as a function of the number of iterations for each of the eight benchmark datasets. The Higgs dataset shows the fastest convergence, as the test error decreases to a very low value within the first 50 iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.11584v1",
    "pdf_url": null
  },
  {
    "instance_id": "eeb1beeeba4246d8a1efe3c64ac6f0a9",
    "figure_id": "2212.10711v1-Figure6-1",
    "image_file": "2212.10711v1-Figure6-1.png",
    "caption": " Features such as name origin can influence how models behave under task ambiguity. Effect of task ambiguity on text-davinci-002 when generating an AWS bucket command given an ambiguous natural language string. Graph shows distribution of generated regions: apnortheast-01 (the region in the prompts), another region, or invalid command.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which type of test names is most likely to lead to the model generating an invalid command when given an ambiguous natural language string?",
    "answer": "Greek test names.",
    "rationale": "The figure shows that for Greek test names, the model is most likely to generate an invalid command (purple bar) compared to other regions or the correct region.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10711v1",
    "pdf_url": null
  },
  {
    "instance_id": "88f941dbda9545148224d853417014aa",
    "figure_id": "2307.05356v1-Figure1-1",
    "image_file": "2307.05356v1-Figure1-1.png",
    "caption": " The VisText dataset consists of 12,441 charts represented as a rasterized image, data table, and scene graph. Before model training, each data table and scene graph is processed from its original form (shown) to a minimized and linearized text representation. Each chart is accompanied by a generated L1 caption describing the aspects of the chart’s construction (e.g., chart type and axis labels) and a crowdsourced L2/L3 caption describing summary statistics and interesting trends (Lundgard and Satyanarayan, 2022).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the chart, which month had the highest increase in the cumulative number of patients diagnosed with COVID-19 in Japan?",
    "answer": "December 2020",
    "rationale": "The chart shows a steep increase in the cumulative number of patients diagnosed with COVID-19 in Japan between November and December 2020. This indicates that the highest increase occurred in December 2020.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.05356v1",
    "pdf_url": null
  },
  {
    "instance_id": "5357f2118f374c538c7a75263e634ad9",
    "figure_id": "2305.17442v3-Figure3-1",
    "image_file": "2305.17442v3-Figure3-1.png",
    "caption": " The impact of the number of clean validation samples on performance. We plot average performance and standard deviation over 5 runs varying the size of the clean validation data. Whenever a small proportion of validation data is provided, most WSL techniques generalize better than the weak label baseline (grey dashed line). Performance improves with additional validation samples, but this tendency usually levels out with a moderate number of validation samples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which weakly supervised learning (WSL) technique performs best on the AGNews dataset when all validation samples are used?",
    "answer": "FT_w",
    "rationale": "The figure shows the performance of different WSL techniques on the AGNews dataset. When all validation samples are used, the FT_w technique has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.17442v3",
    "pdf_url": null
  },
  {
    "instance_id": "27bcb24e380a40baa1a392160aea8337",
    "figure_id": "2002.06440v1-Figure3-1",
    "image_file": "2002.06440v1-Figure3-1.png",
    "caption": " The effect of number of local training epochs on various methods.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest test accuracy?",
    "answer": "FedProx.",
    "rationale": "The figure shows the test accuracy of three methods, FedAvg, FedProx, and FedMA, as a function of the number of local training epochs. FedProx achieves the highest test accuracy for all values of local epochs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.06440v1",
    "pdf_url": null
  },
  {
    "instance_id": "79594da0d43d4dfa8a151559df3fb872",
    "figure_id": "2008.06312v1-Figure3-1",
    "image_file": "2008.06312v1-Figure3-1.png",
    "caption": " Histograms of indegree for the six social networks. In each figure, The ’Mean’ and ’C’ represent the average degree and clustering coefficient.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of social network has the highest average degree?",
    "answer": "Friendship.",
    "rationale": "The average degree is shown in the top right corner of each plot. The plot for Friendship has the highest average degree of 6.72.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.06312v1",
    "pdf_url": null
  },
  {
    "instance_id": "68e88300262a4c6a9e7cb0db99e798aa",
    "figure_id": "1807.03146v2-Figure7-1",
    "image_file": "1807.03146v2-Figure7-1.png",
    "caption": " An ablation study for the losses. a) Our baseline model. b) and c) use twice the noise (0.2) and no noise respectively in the pose estimation loss. d) removes the pose estimation loss. e) removes the silhouette loss.",
    "figure_type": "** schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the following images shows the baseline model?",
    "answer": " a)",
    "rationale": " The caption states that a) is the baseline model. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1807.03146v2",
    "pdf_url": null
  },
  {
    "instance_id": "dbc5f6d04591476289d4d3012b9783b2",
    "figure_id": "2104.04244v1-Figure8-1",
    "image_file": "2104.04244v1-Figure8-1.png",
    "caption": " The classification error of the minimum norm interpolator and ridge estimator for the ALLAML dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better in the presence of label noise?",
    "answer": "The Ridge estimator.",
    "rationale": "The Ridge estimator has a lower classification error than the minimum norm interpolator in the presence of label noise, as shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.04244v1",
    "pdf_url": null
  },
  {
    "instance_id": "f35770ba278e433492dffc5eea800145",
    "figure_id": "1903.10979v4-Figure4-1",
    "image_file": "1903.10979v4-Figure4-1.png",
    "caption": " The s arched archit cture pattern comparison in the small (20 blocks) search spac . From top to bottom, they are ClsNASNet, DetNAS (COCO-FPN) and DetNAS (COCO-RetinaNet).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three architectures has the most complex structure?",
    "answer": "DetNAS (COCO-RetinaNet)",
    "rationale": "The figure shows that DetNAS (COCO-RetinaNet) has the most layers and connections, which indicates a more complex structure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.10979v4",
    "pdf_url": null
  },
  {
    "instance_id": "445a652240904c62aea032cb2f3b847b",
    "figure_id": "2208.09218v2-Figure2-1",
    "image_file": "2208.09218v2-Figure2-1.png",
    "caption": " Example of disturbed images",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the different types of image disturbances shown in the figure?",
    "answer": "Gaussian blur, Gaussian noise, and color jitter.",
    "rationale": "The figure shows an original image of a dog and three types of disturbed versions of the image. The disturbed versions are labeled as Gaussian blur, Gaussian noise, and color jitter.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.09218v2",
    "pdf_url": null
  },
  {
    "instance_id": "6d48e0108c9b4cd18dac1b65e1e3f3f3",
    "figure_id": "2002.12591v1-Figure2-1",
    "image_file": "2002.12591v1-Figure2-1.png",
    "caption": " Ablation study results on Natural Questions Open.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture performs best on Natural Questions Open?",
    "answer": "DC-BERT.",
    "rationale": "The figure shows that DC-BERT has the highest P@N for all values of N, which means it is the most accurate model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.12591v1",
    "pdf_url": null
  },
  {
    "instance_id": "2c95d510d7044f47a9ab1b06d5b61a48",
    "figure_id": "2212.06225v1-Figure7-1",
    "image_file": "2212.06225v1-Figure7-1.png",
    "caption": " The probability distribution of query sequences for each of the identified k = 4 intents. The sequences are grouped together intent-wise. This graph illustrates the distinguishable definitions of intents through their probability distributions. Since trend on Income dataset is similar, we omit the details.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which intent is the most common for the Flights dataset?",
    "answer": "Intent 1.",
    "rationale": "The probability distribution for Intent 1 is the highest for the Flights dataset, as shown in Figure (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.06225v1",
    "pdf_url": null
  },
  {
    "instance_id": "f61f905a91624200aa50c907b73c1f20",
    "figure_id": "2011.14381v1-Figure7-1",
    "image_file": "2011.14381v1-Figure7-1.png",
    "caption": " Comparison of VAE and SCALOR representations. (a) shows MIG scores of VAE and SCALOR representations on data obtained from running a random policy in the Visual Rearrange environment with 3 objects (with whisker showing the standard deviation over 5 runs), (b) shows the mutual information matrix for SCALOR representations on the same data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which representation, VAE or SCALOR, has higher mutual information between its features and the ground truth coordinates?",
    "answer": "SCALOR.",
    "rationale": "This can be seen in two ways: \n1) Panel (a) shows that the MIG score of SCALOR is higher than that of VAE. MIG score measures the mutual information between the representation and the ground truth coordinates. \n2) Panel (b) shows the mutual information matrix for SCALOR representations. The darker blue colors indicate higher mutual information. We can see that there is high mutual information between some of the SCALOR features and the ground truth coordinates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.14381v1",
    "pdf_url": null
  },
  {
    "instance_id": "e9bd9dac93d049398d05eb5a96280e72",
    "figure_id": "2009.09768v1-Figure1-1",
    "image_file": "2009.09768v1-Figure1-1.png",
    "caption": " (a) L is latent unobserved variable. (b) CPT for P (X |A,L). (c) Decision tree with P (X |A,L) given in the leaf nodes. (d) corresponding labeled DAG (LDAG). (e) LDAG with an intervention node added for X .",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the figures shows a decision tree with the conditional probability distribution of X given A and L?",
    "answer": "Figure (c)",
    "rationale": "Figure (c) shows a decision tree with the conditional probability distribution of X given A and L in the leaf nodes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.09768v1",
    "pdf_url": null
  },
  {
    "instance_id": "3225e50930e24a3f98de65ab42f9ce4a",
    "figure_id": "1803.06966v2-Figure2-1",
    "image_file": "1803.06966v2-Figure2-1.png",
    "caption": " A DAFSA representation for a portion of the component sequence search space C that includes math functions in C and Clojure, and an example path/translation (in bold): 2C numeric math ceil arg.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which function is called by the math function in C?",
    "answer": "ceil",
    "rationale": "The figure shows that the math function in C has an outgoing edge labeled \"ceil\". This means that the math function in C calls the ceil function.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1803.06966v2",
    "pdf_url": null
  },
  {
    "instance_id": "e609811c10b44a9383d53c7743e2f4e1",
    "figure_id": "1907.12189v2-Figure5-1",
    "image_file": "1907.12189v2-Figure5-1.png",
    "caption": " Gt",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the degree of the node $r_t$?",
    "answer": "6",
    "rationale": "The degree of a node is the number of edges incident to it. In the figure, we can see that there are 6 edges incident to the node $r_t$.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.12189v2",
    "pdf_url": null
  },
  {
    "instance_id": "9ed2adcc5dff4465852da7e95dc18ac9",
    "figure_id": "1912.12016v1-Figure5-1",
    "image_file": "1912.12016v1-Figure5-1.png",
    "caption": " (a) Backing distribution from Dataset. (b) Termination value learned.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which option has the higher termination probability?",
    "answer": "Option 1",
    "rationale": "The plot in (b) shows the termination probability for Option 1 and Option 2. Option 1 is represented by the blue line, and Option 2 is represented by the green line. The blue line is consistently above the green line, indicating that Option 1 has a higher termination probability for all percentages of durations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.12016v1",
    "pdf_url": null
  },
  {
    "instance_id": "9088d14732fb4db5b29631b913fd082c",
    "figure_id": "2103.01786v1-Figure5-1",
    "image_file": "2103.01786v1-Figure5-1.png",
    "caption": " Reconstructed frames on real data, Domino, Water Balloon, and UCF. The test time on the challenging UCF is: 0.51s (MetaSCI), 300.84s (GAP-TV), 15045s (DeSCI), 12.52s (PnP-FFDNet).",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which algorithm is the fastest for reconstructing frames on real data?",
    "answer": " MetaSCI",
    "rationale": " The caption states that the test time on the challenging UCF dataset is: 0.51s (MetaSCI), 300.84s (GAP-TV), 15045s (DeSCI), 12.52s (PnP-FFDNet). This indicates that MetaSCI has the shortest test time, making it the fastest algorithm.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.01786v1",
    "pdf_url": null
  },
  {
    "instance_id": "c5ffa34abebd4021a202d241348afdfc",
    "figure_id": "2003.14013v1-Figure4-1",
    "image_file": "2003.14013v1-Figure4-1.png",
    "caption": " The non-local attention module. The green, blue, and orange modules represent the spatial, channel, and temporal attention respectively.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which module represents the temporal attention?",
    "answer": "The orange module.",
    "rationale": "The caption states that the green, blue, and orange modules represent the spatial, channel, and temporal attention respectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.14013v1",
    "pdf_url": null
  },
  {
    "instance_id": "aee5d17163164a1cb91e82e81a6a35f2",
    "figure_id": "2302.11636v1-Figure17-1",
    "image_file": "2302.11636v1-Figure17-1.png",
    "caption": " Comparison on the training loss landscape (Contour) on GDELT-ne Dataset.",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which model has the most complex loss landscape?",
    "answer": " TGN and JODIE.",
    "rationale": " The loss landscapes of TGN and JODIE are more complex than the other models, as they have more local minima and saddle points. This can be seen in the contour plots, which show a greater number of peaks and valleys for these models.\n\n**Figure type:** Plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.11636v1",
    "pdf_url": null
  },
  {
    "instance_id": "0c6c2226c2ec475d842892e436b50ac4",
    "figure_id": "1707.09627v5-Figure12-1",
    "image_file": "1707.09627v5-Figure12-1.png",
    "caption": " Pairs of images either close together or far apart in different features spaces. The symbol ↔ points to the compared images. Features of the program capture abstract notions like symmetry and repetition. Distance metric over images is Llearned(·|·) (see Section 2.1). The off-diagonal entries highlight the difference between these metrics: similarity of programs captures high-level features like repetition and symmetry, whereas similarity of images correspondends to similar drawing commands being in similar places.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following pairs of images are closest in program space?",
    "answer": "The pair of images in the top left corner.",
    "rationale": "The figure shows that the pair of images in the top left corner are close in program space, meaning that they share similar abstract features like symmetry and repetition. This is because the programs that generated these images are similar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1707.09627v5",
    "pdf_url": null
  },
  {
    "instance_id": "6907e80c7296435da883876e8039db79",
    "figure_id": "2012.14905v4-Figure12-1",
    "image_file": "2012.14905v4-Figure12-1.png",
    "caption": " Meta testing learning curves. All 6 meta test tasks are unseen. VSML RNN has been meta trained on MNIST, Fashion MNIST, EMNIST, KMNIST, and Random, excluding the respective dataset that is being meta tested on. Standard deviations are over 32 seeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best on the Kuzushiji MNIST dataset?",
    "answer": "VSMLRNN",
    "rationale": "The figure shows that the VSMLRNN algorithm has the highest cumulative accuracy on the Kuzushiji MNIST dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.14905v4",
    "pdf_url": null
  },
  {
    "instance_id": "b634742f68064d18a87ec59836e9eb16",
    "figure_id": "2103.17230v1-Figure6-1",
    "image_file": "2103.17230v1-Figure6-1.png",
    "caption": " Illustration of accuracy changes as tasks are being learned in (a) CIFAR10-Disjoint-Offline, (b) CIFAR10-Blurry10-Offline, (c) CIFAR10-Blurry30-Offline settings.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest accuracy in the CIFAR10-Disjoint-Offline setting?",
    "answer": "BiC.",
    "rationale": "In Figure (a), the BiC line is the highest line in the plot. This indicates that BiC has the highest accuracy in the CIFAR10-Disjoint-Offline setting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.17230v1",
    "pdf_url": null
  },
  {
    "instance_id": "4fae1c18391a46779a0e98b30f8ba414",
    "figure_id": "1903.10245v4-Figure3-1",
    "image_file": "1903.10245v4-Figure3-1.png",
    "caption": " Results of how each model performs on EMNLP dataset when we gradually reduce the ratio of sampled training data from 100% to 10% (from right to left).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the EMNLP dataset when the ratio of sampled training data is 40%?",
    "answer": "AKGACM",
    "rationale": "The plot shows that the AKGACM model has the highest score for both BLEU-4 and ROUGE-L metrics when the ratio of sampled training data is 40%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.10245v4",
    "pdf_url": null
  },
  {
    "instance_id": "bb1accc306e04b7187c7e85fb3c61555",
    "figure_id": "2210.14424v1-Figure4-1",
    "image_file": "2210.14424v1-Figure4-1.png",
    "caption": " Variation of mean citations (per paper) across age of publication for the top-10 publishing countries.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which country has the highest average citation per paper for papers that are 10 years old?",
    "answer": "United States",
    "rationale": "The plot shows the average citation per paper for each country as a function of the age of the paper. The United States line is the highest at the 10 year mark.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.14424v1",
    "pdf_url": null
  },
  {
    "instance_id": "af1e763eb8aa4614b48b6ae4cd1cb02a",
    "figure_id": "2109.11635v1-Figure3-1",
    "image_file": "2109.11635v1-Figure3-1.png",
    "caption": " Mean ∆LogLik as a function of the exponent k for the sentence-level predictor (Eq. (3)) of reading time and linguistic acceptability. Shaded region connects standard error estimates from each point. We observe that often, our predictor with k > 1 explains the data at least as well as k = 1. Baseline models against which ∆LogLik is computed are specified in §4.2. For reading times, the augmented models additionally contain fixed effects and per-subject random effects slopes for the UID operationalization; for acceptability judgments, only a fixed effect for the UID operationalization is added.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the CoLA dataset?",
    "answer": "TransXL",
    "rationale": "The figure shows that the TransXL model has the highest ∆LogLik values for the CoLA dataset, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.11635v1",
    "pdf_url": null
  },
  {
    "instance_id": "20af0bb51b7841099d216af45195553a",
    "figure_id": "2009.10623v5-Figure9-1",
    "image_file": "2009.10623v5-Figure9-1.png",
    "caption": " Percentage of points with certificate above different radii, and average certified radius (ACR) for on the ImageNet dataset, including other SOA methods. Randomized smoothing with meta-tailoring are very competitive with other SOA methods, including having the biggest ACR for σ = 1.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the ImageNet dataset for σ = 1.0?",
    "answer": "Meta-tailored RS.",
    "rationale": "The table shows the percentage of points with certificate above different radii, and the average certified radius (ACR) for each method on the ImageNet dataset. For σ = 1.0, Meta-tailored RS has the highest ACR (0.494).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.10623v5",
    "pdf_url": null
  },
  {
    "instance_id": "eb6b24d03f2540129fa443dc6708f4fb",
    "figure_id": "2010.11876v1-Figure4-1",
    "image_file": "2010.11876v1-Figure4-1.png",
    "caption": " Policy evaluation errors (γ = 0.999) on environment models trained by BC and GAIL.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better on the Walker2d-v2 environment?",
    "answer": "MB-GAIL",
    "rationale": "The figure shows that the policy evaluation error for MB-GAIL is lower than that of MB-BC on the Walker2d-v2 environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.11876v1",
    "pdf_url": null
  },
  {
    "instance_id": "3a26fc42bb08465a99954f490205d415",
    "figure_id": "2104.09937v3-Figure4-1",
    "image_file": "2104.09937v3-Figure4-1.png",
    "caption": " Performance on CDSPRITES-N, with N ∈ [5, 50]",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method performs the best on the test set when the number of domains is 50?",
    "answer": " ERM",
    "rationale": " The figure shows the accuracy of three different methods on the train and test sets. The ERM method has the highest accuracy on the test set when the number of domains is 50.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.09937v3",
    "pdf_url": null
  },
  {
    "instance_id": "2013db26b5f34001ae699fc2faa26ddc",
    "figure_id": "2010.12663v2-Figure2-1",
    "image_file": "2010.12663v2-Figure2-1.png",
    "caption": " Results for Transformer in the variable misuse task: joint bug localization and repair accuracy, mean value ± standard deviation (over 3 runs). Models with the proposed OOV anonymization significantly outperform the standard model (all OOV identifiers are replaced with an UNK token). The numerical data for the plots is given in Table 2 in Appendix.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which anonymization method performs the best for the Python150k dataset?",
    "answer": "OOV anonymization (ordered)",
    "rationale": "The figure shows that the OOV anonymization (ordered) method achieves the highest joint accuracy for the Python150k dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12663v2",
    "pdf_url": null
  },
  {
    "instance_id": "2dc1093a6bc54be488a500976bfc005f",
    "figure_id": "1910.13857v1-Figure3-1",
    "image_file": "1910.13857v1-Figure3-1.png",
    "caption": " Convergence behavior with respect to training data and resulting test accuracies for binary classification task on breast-cancer dataset from LIBSVM [Chang and Lin, 2011]",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer has the fastest convergence rate and the highest test accuracy?",
    "answer": "AXGD",
    "rationale": "The figure shows the convergence rate and test accuracy of five different optimizers. AXGD has the fastest convergence rate and the highest test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.13857v1",
    "pdf_url": null
  },
  {
    "instance_id": "e434196068084889a10a3e90068851d9",
    "figure_id": "1812.07003v3-Figure3-1",
    "image_file": "1812.07003v3-Figure3-1.png",
    "caption": " Qualitative comparison of 3D object detection and instance segmentation on ScanNetV2 [5] (full scans above; close-ups below). Our joint color-geometry feature learning combined with our fully-convolutional approach to inference on full test scans at once enables more accurate and semantically coherent predictions. Note that different colors represent different instances, and the same instances in the ground truth and predictions are not necessarily the same color.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate and semantically coherent predictions?",
    "answer": "Ours: 3D-SIS.",
    "rationale": "The caption states that \"Our joint color-geometry feature learning combined with our fully-convolutional approach to inference on full test scans at once enables more accurate and semantically coherent predictions.\" This can be seen in the figure by comparing the \"Ours: 3D-SIS\" column to the other columns. The \"Ours: 3D-SIS\" column more closely resembles the ground truth than the other columns.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.07003v3",
    "pdf_url": null
  },
  {
    "instance_id": "9b44d046c11e464fb5b6ca541f50c6a9",
    "figure_id": "2008.01334v2-Figure2-1",
    "image_file": "2008.01334v2-Figure2-1.png",
    "caption": " Video Retrieval performance comparison on ISVR task of FIVR [30] in terms of mAP, inference time, and computational cost of the model (ISVR is the most complete and hard task of FIVR). The proposed approach achieves the best trade-off between performance and efficiency with both video-level and frame-level features against state-of-the-art methods. (Best viewed in color)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the best performance in terms of mAP on the ISVR task of FIVR?",
    "answer": "TCAc (ours)",
    "rationale": "The figure shows the mAP on the y-axis and the inference time on the x-axis. The TCAc (ours) model is located at the top left of the figure, which indicates that it has the highest mAP and the lowest inference time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.01334v2",
    "pdf_url": null
  },
  {
    "instance_id": "1427e0df0cd64334a9d13c5bfb06c6b2",
    "figure_id": "2012.04373v2-Figure4-1",
    "image_file": "2012.04373v2-Figure4-1.png",
    "caption": " Vocabulary overlaps of the unlabeled domainrelated corpora between domains (%). Reuters denotes the Reuters News domain, “Science” denotes the natural science domain, “Litera.” denotes the literature domain, and “AI” denotes the artificial intelligence domain.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two domains have the least overlap in vocabulary?",
    "answer": "AI and Music.",
    "rationale": "The figure shows a heatmap of the vocabulary overlap percentages between different domains. The darker the color, the higher the overlap. The lightest color in the figure is between AI and Music, which corresponds to an overlap of 22.1%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.04373v2",
    "pdf_url": null
  },
  {
    "instance_id": "030017aab8b54091aa7c8a67568f4841",
    "figure_id": "2204.12648v1-Figure1-1",
    "image_file": "2204.12648v1-Figure1-1.png",
    "caption": " An example of an Azure CLI command for creating an Ubuntu virtual machine on Azure. All Azure CLI commands begin with ‘az’ followed by a command name. A commandmay be followed by a set of parameters and corresponding values, where parameters begin with ‘--’.",
    "figure_type": "Other (command line code)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the name of the resource group where the virtual machine will be created?",
    "answer": "MyResourceGroup",
    "rationale": "The command specifies the resource group using the `--resource-group` parameter. The value following this parameter is `MyResourceGroup`.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.12648v1",
    "pdf_url": null
  },
  {
    "instance_id": "fb3ea88b7241489ba4c6341f82611be4",
    "figure_id": "1711.00513v3-Figure4-1",
    "image_file": "1711.00513v3-Figure4-1.png",
    "caption": " The baseline model and the two contextual strategies tested (single and multi-encoder).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models depicted in the figure utilizes a multi-encoder approach?",
    "answer": "Model (c) utilizes a multi-encoder approach.",
    "rationale": "The figure shows three models: (a) S2S with attention (baseline), (b) Concatenate input, and (c) Multi-source S2S with attention. Model (c) is the only one that has two separate encoders, one for the current sentence and one for the previous sentence. This indicates that it uses a multi-encoder approach.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.00513v3",
    "pdf_url": null
  },
  {
    "instance_id": "bd2a0aa7c54c45b58e48aa72911f395c",
    "figure_id": "2105.03193v1-Figure4-1",
    "image_file": "2105.03193v1-Figure4-1.png",
    "caption": " Iterative pruning on CIFAR-10 dataset using `1-norm filters pruning Li et al. (2016) ((a) and (b)) and on ImageNet using magnitude-based weights pruning (Han et al., 2015) ((c)).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pruning method has the highest accuracy at a sparsity of 0.8 on the ImageNet dataset?",
    "answer": "CLR",
    "rationale": "The figure shows that the CLR method has the highest accuracy at a sparsity of 0.8 on the ImageNet dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.03193v1",
    "pdf_url": null
  },
  {
    "instance_id": "1a9a44867a644c028193f5c9a7632bb3",
    "figure_id": "2110.05571v1-Figure4-1",
    "image_file": "2110.05571v1-Figure4-1.png",
    "caption": " Average attention weights from the last encoder layer.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the three models, Conformor, Uni-SRU++, or Bi-SRU++, exhibits the most localized attention? ",
    "answer": " Conformor",
    "rationale": " The attention weights in (a) are more concentrated along the diagonal, indicating that the model is attending to nearby tokens in the sequence. In contrast, the attention weights in (b) and (c) are more spread out, suggesting that the models are attending to a wider range of tokens.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.05571v1",
    "pdf_url": null
  },
  {
    "instance_id": "89b4f979327a436aac2834bc7375d4c7",
    "figure_id": "2109.06096v3-Figure10-1",
    "image_file": "2109.06096v3-Figure10-1.png",
    "caption": " The accuracy on wh vs that with gap during training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best at lower perplexity values?",
    "answer": "Gpt2 performs the best at lower perplexity values.",
    "rationale": "The plot shows the accuracy of different models as a function of perplexity. Gpt2 has the highest accuracy at lower perplexity values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.06096v3",
    "pdf_url": null
  },
  {
    "instance_id": "f2973ff553214751bb388058f88e4be8",
    "figure_id": "2205.09852v2-Figure2-1",
    "image_file": "2205.09852v2-Figure2-1.png",
    "caption": " Visualization of the action distribution in the 3-dimensional action space on MIMIC-III dataset. The horizontal axis denotes the discritized actions and the vertical axis denotes the distribution of corresponding actions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which action is most commonly taken by clinicians according to the figure?",
    "answer": "Action 1",
    "rationale": "The figure shows the distribution of actions taken by clinicians and the data-driven approach (DAC). The blue bars represent the actions taken by clinicians, and the red bars represent the actions taken by the DAC. The height of the bars indicates the frequency of each action. In all three subfigures, the blue bar for action 1 is the highest, indicating that action 1 is the most commonly taken action by clinicians.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.09852v2",
    "pdf_url": null
  },
  {
    "instance_id": "f6fb020b739a46d7b2d600721c01329b",
    "figure_id": "2202.00217v1-Figure6-1",
    "image_file": "2202.00217v1-Figure6-1.png",
    "caption": " EM results on zero-shot and few-shot learning.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which event attribute is the most difficult for the model to learn in the zero-shot setting?",
    "answer": "Location",
    "rationale": "The figure shows that the EM for location is the lowest in the zero-shot setting (i.e., when the number of training examples is 0).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.00217v1",
    "pdf_url": null
  },
  {
    "instance_id": "394a880ac4e34003befdef60c624ce7e",
    "figure_id": "2211.16158v2-Figure4-1",
    "image_file": "2211.16158v2-Figure4-1.png",
    "caption": " OMS results for optimal OOD monitors. Distribution of the OMS recall and precision obtained by the optimal OOD detector m∗ across the 27 OOD datasets and two neural networks tested in our experiments.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of attack has the highest recall and precision?",
    "answer": "Adversarial attacks.",
    "rationale": "The box plots for adversarial attacks are the highest for both recall and precision, indicating that the OMS recall and precision are highest for this type of attack.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.16158v2",
    "pdf_url": null
  },
  {
    "instance_id": "ac9585d452754e6d863d9e20d8ccce4b",
    "figure_id": "1811.05652v1-Figure6-1",
    "image_file": "1811.05652v1-Figure6-1.png",
    "caption": " Oracle calls comparison (lower is better)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the best performance for the StackExchange dataset?",
    "answer": "BasicStreaming",
    "rationale": "The figure shows the number of oracle calls for each algorithm on the StackExchange dataset. BasicStreaming has the lowest number of oracle calls, which indicates that it has the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.05652v1",
    "pdf_url": null
  },
  {
    "instance_id": "067f42dce5ef49468c5571b45d99c090",
    "figure_id": "2210.06361v3-Figure6-1",
    "image_file": "2210.06361v3-Figure6-1.png",
    "caption": " Visual comparisons of some latest research algorithms and our proposed MFFN in some typical images. We can find that the prediction results of MFFN have clearer boundary and region shape.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm produces the most accurate segmentation results for the coral image?",
    "answer": "MFFN",
    "rationale": "The MFFN algorithm produces the segmentation result that most closely matches the ground truth segmentation. This can be seen by comparing the \"Ground Truth\" row with the \"MFFN (Ours)\" row.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06361v3",
    "pdf_url": null
  },
  {
    "instance_id": "82f2301bf7dd4b44896e59600ac57e32",
    "figure_id": "2308.06480v1-Figure6-1",
    "image_file": "2308.06480v1-Figure6-1.png",
    "caption": " Results with different number of contexts.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best when the number of contexts is 3?",
    "answer": "IR",
    "rationale": "The figure shows that IR has the highest MRR and HIT@10 values when the number of contexts is 3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.06480v1",
    "pdf_url": null
  },
  {
    "instance_id": "353ac63476ff43afbc01baf430e8b377",
    "figure_id": "2208.02108v3-Figure6-1",
    "image_file": "2208.02108v3-Figure6-1.png",
    "caption": " Transformed distributions of multiple entities.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which entity has the highest density of values around 0?",
    "answer": "P301",
    "rationale": "The P301 distribution has the highest peak at 0, indicating that it has the highest density of values around 0.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.02108v3",
    "pdf_url": null
  },
  {
    "instance_id": "17f8dcd9bf99441d82b3cfc0b71a82f1",
    "figure_id": "2211.14563v2-Figure2-1",
    "image_file": "2211.14563v2-Figure2-1.png",
    "caption": " Numbers of mentions as part of the coreference chain for pronouns them, he, it, who, she in CIN .",
    "figure_type": "\"plot\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pronoun is most frequently used to refer to a group of people?",
    "answer": "\"They\"",
    "rationale": "The figure shows that \"they\" is mentioned more than 20 times, which is more than any other pronoun in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.14563v2",
    "pdf_url": null
  },
  {
    "instance_id": "36be59494e60468fb7b50a5347c2ce75",
    "figure_id": "2107.03250v2-Figure1-1",
    "image_file": "2107.03250v2-Figure1-1.png",
    "caption": " Intrinsic robustness estimates for classification tasks on CIFAR-10 under (a) `∞ perturbations with ε = 8/255 and (b) `2 perturbations with ε = 0.5. Orange dots are intrinsic robustness estimates using the method in Prescott et al. (2021), which does not consider labels; green dots show the results using our methods that incorporate label uncertainty; blue dots are results achieved by the state-of-the-art adversarially-trained models in RobustBench (Croce et al., 2020). Three fundamental causes behind the adversarial vulnerability can be summarized as imperfect risk (red region), concentration of measure (orange region) and existence of uncertain inputs (green region).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of perturbation leads to higher intrinsic robustness estimates?",
    "answer": "l∞ perturbations.",
    "rationale": "The intrinsic robustness estimates are higher for l∞ perturbations than for l2 perturbations. This can be seen by comparing the y-axis values for the different types of perturbations in the two subplots. The blue dots, which represent the RobustBench models, are consistently higher in the left subplot than in the right subplot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.03250v2",
    "pdf_url": null
  },
  {
    "instance_id": "025d1c2ddc9c409ab5fa240915645c0b",
    "figure_id": "2106.09686v3-Figure18-1",
    "image_file": "2106.09686v3-Figure18-1.png",
    "caption": " Pareto frontier estimates in meta-LOOCV Setting II (full meta-training error matrix, a 816MB memory cap), Setting III (uniformly sample 20% meta-training measurements, no meta-test memory cap), Setting V (non-uniformly sample 20% meta-training measurements, no meta-test memory cap), and Setting VI (non-uniformly sample 20% meta-training measurements, an 816MB meta-test memory cap). Each error bar is the standard error across datasets. ED-MF is among the best in every setting and under both metrics.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms tested performs the best in terms of convergence across all settings?",
    "answer": "ED-MF (PEPPP)",
    "rationale": "The plots show the convergence of different algorithms in different settings. In each plot, the ED-MF (PEPPP) algorithm is consistently among the algorithms with the lowest convergence values, indicating that it performs the best in terms of convergence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.09686v3",
    "pdf_url": null
  },
  {
    "instance_id": "e70371f1bb7d467f9f5c5f51b40cabb4",
    "figure_id": "2012.03137v1-Figure6-1",
    "image_file": "2012.03137v1-Figure6-1.png",
    "caption": " Testing loss over real-world datasets.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the datasets in the table had the best performance with the ACNet model?",
    "answer": "GOOG-FB",
    "rationale": "The table shows the testing loss for each dataset with both the best parametric model and the ACNet model. The lower the testing loss, the better the performance. For the ACNet model, the GOOG-FB dataset has the lowest testing loss of -0.9558.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.03137v1",
    "pdf_url": null
  },
  {
    "instance_id": "3e7d2a20b1e549da87d0fe2236e0d538",
    "figure_id": "1910.10006v1-Figure1-1",
    "image_file": "1910.10006v1-Figure1-1.png",
    "caption": " Two measurements M with the same five rotated copies of a target image, but different noise levels: SNR = 10 (left), and SNR = 0.1 (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image has a higher signal-to-noise ratio (SNR)?",
    "answer": "The image on the left has a higher SNR.",
    "rationale": "The SNR is a measure of how strong the signal is compared to the noise. The higher the SNR, the easier it is to see the signal. In this case, the image on the left has a much higher SNR because the signal (the five rotated copies of the target image) is much stronger than the noise. In the image on the right, the signal is much weaker than the noise, making it difficult to see the five rotated copies of the target image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.10006v1",
    "pdf_url": null
  },
  {
    "instance_id": "126e4ecd39ef4647b72178e23c16e9ca",
    "figure_id": "2010.12883v1-Figure5-1",
    "image_file": "2010.12883v1-Figure5-1.png",
    "caption": " Graphs of averaged KL divergence vs. λ achieved by EUBO, rKL, and q(θ|D) (i.e., baseline labeled as full) overDr andDe for the banknote authentication dataset with the approximate posterior beliefs of model parameters represented by (a-b) multivariate Gaussians and (c-d) normalizing flows.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method consistently achieves the lowest KL divergence for all values of λ?",
    "answer": "rKL",
    "rationale": "The figure shows that the rKL curve (red line with triangles) is consistently below the other two curves (EUBO and full) for all values of λ. This means that rKL achieves the lowest KL divergence for all values of λ.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12883v1",
    "pdf_url": null
  },
  {
    "instance_id": "25d7cce526084bcf862554d8c4206078",
    "figure_id": "1905.07512v3-Figure7-1",
    "image_file": "1905.07512v3-Figure7-1.png",
    "caption": " IndoorEnv Task2Task performance as a function of target training episodes. SplitNet Transfer and E2E Transfer are first trained on IndoorEnv Point-Nav, but SplitNet only updates the policy layers whereas E2E updates the entire network. E2E from scratch is randomly initialized a episode 0. The Blind method only receives its previous action as input and is randomly initialized. Oracle agents perform at 33.5 and 19.5 respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of geodesic distance from the start location?",
    "answer": "SplitNet Transfer",
    "rationale": "The SplitNet Transfer curve is the lowest of all the curves in the figure, indicating that it has the smallest geodesic distance from the start location.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.07512v3",
    "pdf_url": null
  },
  {
    "instance_id": "35b5e1e281c6412185251a27b6ce5be7",
    "figure_id": "1901.01347v2-Figure5-1",
    "image_file": "1901.01347v2-Figure5-1.png",
    "caption": " Memory operations on max task in DNC (a), DNC+UW (b) and DNC+CUW(c). Each row is a timestep and each column is a memory slot.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the most active memory slots?",
    "answer": "DNC+CUW",
    "rationale": "The DNC+CUW model has the most active memory slots, as can be seen by the number of non-black squares in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.01347v2",
    "pdf_url": null
  },
  {
    "instance_id": "bbcc28a0667e4262a8068a5861d3b6b4",
    "figure_id": "2106.01011v1-Figure2-1",
    "image_file": "2106.01011v1-Figure2-1.png",
    "caption": " Error distribution on recorded data from the Pyramic dataset",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better in terms of error distribution on recorded data from the Pyramic dataset?",
    "answer": "MUSIC (Grid 100 + 30 iter.)",
    "rationale": "The figure shows the error distribution of two algorithms, SRP-PHAT and MUSIC, on the Pyramic dataset. The error distribution of MUSIC (Grid 100 + 30 iter.) is more concentrated around 0 degrees, which indicates that it has a lower error rate than the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01011v1",
    "pdf_url": null
  },
  {
    "instance_id": "947c1c8bccf840338a8ffdfc848636de",
    "figure_id": "2110.14001v2-Figure3-1",
    "image_file": "2110.14001v2-Figure3-1.png",
    "caption": " RMSE of estimating the survival function S0(t|x) (top) and the treatment effect HTEsurv(t|x) (bottom) for different time steps across synthetic settings. Averaged across 5 runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in terms of RMSE for estimating the survival function S0(t|x)?",
    "answer": "CSA",
    "rationale": "The figure shows that CSA has the lowest RMSE for estimating the survival function S0(t|x) across all time steps and synthetic settings.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14001v2",
    "pdf_url": null
  },
  {
    "instance_id": "2dbf3a7a75b843a5b01186d170a5d7c8",
    "figure_id": "1911.09291v1-Figure1-1",
    "image_file": "1911.09291v1-Figure1-1.png",
    "caption": " A grid MDP (bottom) with a copy of itself (top). The goal of an agent is to find the shortest path to the green cells. At each iteration, the agent has a 50% chance of jumping to the other level.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the agent in the grid MDP know which level it is on?",
    "answer": "The agent does not know which level it is on.",
    "rationale": "The figure shows that the two levels are identical, and the agent has a 50% chance of jumping to the other level at each iteration. This means that the agent cannot distinguish between the two levels and must make decisions based on its current state and the possible actions it can take.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.09291v1",
    "pdf_url": null
  },
  {
    "instance_id": "5dec51f5055c4bfa94a8b97164ca043e",
    "figure_id": "2112.05785v1-Figure1-1",
    "image_file": "2112.05785v1-Figure1-1.png",
    "caption": " (a) The underlying TKG for the question “Which movie won the Best Picture after The Godfather?”; Answer: ‘The Sting’. (b) The context-aware question representation (dashed arrows) scores higher (numbers in parentheses) movie entities. (c) The representation is grounded to the entity ‘Best Picture’ and gives higher scores to movies that won the Best Picture. (d) The representation is grounded to the correct time scope (after year 1972) and gives higher scores to the movies that won the Best Picture after 1972, i.e., ‘The Sting’.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which movie won the Best Picture before \"The Godfather\"?",
    "answer": "\"The Sting\"",
    "rationale": "The underlying TKG shows that \"The Godfather\" won the Best Picture in 1972, and \"The Sting\" won the Best Picture in 1973.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.05785v1",
    "pdf_url": null
  },
  {
    "instance_id": "b9ae3599f6084f218c9a9164d6570435",
    "figure_id": "2101.03778v2-Figure2-1",
    "image_file": "2101.03778v2-Figure2-1.png",
    "caption": " Comparison of models of different sizes on ROSTD and CLINC150. Maha stands for Mahalanobis distance",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the CLINC150 dataset?",
    "answer": "RoBERTa large with Mahalanobis distance.",
    "rationale": "The figure shows the performance of different models on the CLINC150 dataset. The RoBERTa large with Mahalanobis distance model has the highest accuracy on this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.03778v2",
    "pdf_url": null
  },
  {
    "instance_id": "8a11f25f989e4928acfcc9edd8e0e8b9",
    "figure_id": "2104.07831v1-Figure8-1",
    "image_file": "2104.07831v1-Figure8-1.png",
    "caption": " Univariate box plots for All candidates, Max-PMI responses and Fused-PCMI responses. (a) is w.r.t. pcmik and (b) w.r.t pcmih. Pink horizontal lines indicate 75% quartile for All candidates. MaxPMI responses (orange) have high pcmik (median above pink line), but low pcmih. Fused-PCMI responses (green) show balanced yet high pcmih and pcmik (medians cross pink lines).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods produces responses with the highest average pcmik?",
    "answer": "The Max. PMI method.",
    "rationale": "The box plot for the Max. PMI method in panel (a) has the highest median value, which indicates that the average pcmik for responses generated by this method is higher than for the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.07831v1",
    "pdf_url": null
  },
  {
    "instance_id": "d89c7b37bc9e4218b6317511ce13ff3b",
    "figure_id": "2307.09775v1-Figure5-1",
    "image_file": "2307.09775v1-Figure5-1.png",
    "caption": " Case study with t-SNE transformed embeddings derived from different baselines with our DisCover framework, where colored nodes represent the different songs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following baselines produces the most distinct clusters of songs?",
    "answer": "PICKINet with DisCover",
    "rationale": "The figure shows that PICKINet with DisCover produces the most distinct clusters of songs, as the different colors are more clearly separated into different groups. This suggests that PICKINet with DisCover is better at identifying different types of songs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.09775v1",
    "pdf_url": null
  },
  {
    "instance_id": "7ec55a604ade4e60bc60d1e0493afbdf",
    "figure_id": "2112.03497v2-Figure17-1",
    "image_file": "2112.03497v2-Figure17-1.png",
    "caption": " SQuAD Geographic Distributions.",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which country has the most entities in the SQuAD dataset?",
    "answer": "The United States.",
    "rationale": "The map shows that the US has the largest circle, which indicates that it has the most entities in the dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.03497v2",
    "pdf_url": null
  },
  {
    "instance_id": "b05a759fd02241e3b5c5d1b5d385372a",
    "figure_id": "2106.04165v1-Figure4-1",
    "image_file": "2106.04165v1-Figure4-1.png",
    "caption": " [Left] Reconstruction of system trajectories through NHA vector field decoders Fz and corresponding modes z encoded by E for Reno TCP. Although the encoder is initialized with more modes (10) than there are in the underlying system (3), mode clustering is sparse and accurate. [Right] Flow reconstruction test MSE for different classes of decoders. NHA decoders (10 modes) can reconstruct the flows as well as other NODE baselines, with the added benefit of being able to recover mode labels during training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best at reconstructing the system trajectories?",
    "answer": "NHA",
    "rationale": "The figure shows the test MSE for different models. NHA has the lowest test MSE, which indicates that it performs the best at reconstructing the system trajectories.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04165v1",
    "pdf_url": null
  },
  {
    "instance_id": "b31c0a3e0070487f894c001c02e32b42",
    "figure_id": "2203.15266v1-Figure14-1",
    "image_file": "2203.15266v1-Figure14-1.png",
    "caption": " Performance on Tiny-DOTA. Top: Both Faster R-CNN and RoI Transformer [10] (solid lines) are improved by adding C3Det (dashed lines). Bottom: Our method against Faster R-CNN baselines measured with COCO AP@[.50:.05:.95].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest COCO AP at 10 clicks?",
    "answer": "Ours",
    "rationale": "The bottom plot shows the COCO AP of different models as a function of the number of clicks. The red line, which represents our model, is the highest at 10 clicks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.15266v1",
    "pdf_url": null
  },
  {
    "instance_id": "c36c85fb7b4f43148aefc325578ea285",
    "figure_id": "2205.01523v1-Figure4-1",
    "image_file": "2205.01523v1-Figure4-1.png",
    "caption": " Heatmaps of two-stage transfer learning.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the highest accuracy achieved by BERT_LARGE on the SM-A task when transferring from the ARCT task?",
    "answer": "89.4%",
    "rationale": "The heatmap for BERT_LARGE shows that the highest accuracy for the SM-A task when transferring from the ARCT task is 89.4%. This is indicated by the darkest orange color in the corresponding cell of the heatmap.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.01523v1",
    "pdf_url": null
  },
  {
    "instance_id": "4679f3240f8543bd905aa8ff3a004519",
    "figure_id": "2105.08195v2-Figure6-1",
    "image_file": "2105.08195v2-Figure6-1.png",
    "caption": " Acquisition optimization wall time under a sequential greedy approximation using LBFGS-B for three and four objectives. CBD enables scaling to much larger batch sizes q than using the inclusion-exclusion principle (IEP) and avoids running out-of-memory (OOM) on a GPU. Independent GPs are used for each outcome and are initialized with 20 points from the Pareto frontier of the 6-dimensional DTLZ2 problem [13] with 3 objectives (left) and 4 objectives (right). Wall times were measured on a Tesla V100 SXM2 GPU (16GB GPU RAM) and a Intel Xeon Gold 6138 CPU @ 2GHz CPU (251GB RAM).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more efficient for acquisition optimization, CBD or IEP?",
    "answer": "CBD is more efficient than IEP.",
    "rationale": "The figure shows that CBD (both CPU and GPU) requires less wall time than IEP (both CPU and GPU) for the same batch size q. This means that CBD is able to optimize the acquisition function more quickly than IEP. Additionally, IEP runs out of memory (OOM) on a GPU for larger batch sizes, while CBD does not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.08195v2",
    "pdf_url": null
  },
  {
    "instance_id": "36c92a304f5642eb8209f80b096caf82",
    "figure_id": "1905.03277v2-Figure10-1",
    "image_file": "1905.03277v2-Figure10-1.png",
    "caption": " Statistical robustness model: The relationship between color difference 𝑑 and local standard deviation 𝜎 dictates how we merge a given frame with respect to the base frame.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which of the following operations is most likely to introduce color differences in the merged frame?",
    "answer": "Aliasing",
    "rationale": "The figure shows that the color difference d increases as the local standard deviation σ increases. This means that operations that increase the local standard deviation, such as aliasing, are more likely to introduce color differences in the merged frame.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.03277v2",
    "pdf_url": null
  },
  {
    "instance_id": "d7f0a37c4a6a4f2392eda1bee8dd0c5a",
    "figure_id": "2202.02363v3-Figure8-1",
    "image_file": "2202.02363v3-Figure8-1.png",
    "caption": " Reward profiles of MAML, RL2 and MetODS over 5 consecutive episodes with randomly varying rewarded direction. In Cheetah, MetODS slightly overperforms RL2 and in Ant, MetODS is the only approach able to reach performance comparable to first episode in subsequent episodes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best in the Ant environment?",
    "answer": "MetODS.",
    "rationale": "The figure shows that MetODS is the only algorithm that is able to reach performance comparable to the first episode in subsequent episodes in the Ant environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.02363v3",
    "pdf_url": null
  },
  {
    "instance_id": "e63ca38add5f4fccaa2fe537ff225af0",
    "figure_id": "1902.03228v1-Figure4-1",
    "image_file": "1902.03228v1-Figure4-1.png",
    "caption": " Comparison of non-convex optimization algorithms for the task of visual object localization on PASCAL VOC 2007 for λ = 1/n. Plots for all other classes are in Appendix E.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization algorithm achieves the highest localization accuracy for the bird class?",
    "answer": "PL-Casimir-SVRG",
    "rationale": "The figure shows the localization accuracy for different optimization algorithms on the bird class. The PL-Casimir-SVRG algorithm achieves the highest accuracy of about 0.48.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.03228v1",
    "pdf_url": null
  },
  {
    "instance_id": "8f2536b9c49c453f9416965e3cf48687",
    "figure_id": "1812.00740v2-Figure15-1",
    "image_file": "1812.00740v2-Figure15-1.png",
    "caption": " Experiments with multilayer-perceptrons on FONTS, EMNIST and F-MNIST. We plot on-manifold (left) or regular success rate (right) against test error. Onmanifold robustness is strongly related to generalization, while regular robustness seems mostly independent of generalization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method performs the best in terms of success rate on FONTS dataset?",
    "answer": "On-Learned-Manifold Adversarial Training",
    "rationale": "The On-Learned-Manifold Adversarial Training line (green) is the highest in the top left plot, which shows the success rate on the FONTS dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.00740v2",
    "pdf_url": null
  },
  {
    "instance_id": "924333c4cfb54cc1a400163f8aa2133c",
    "figure_id": "2006.09107v2-Figure2-1",
    "image_file": "2006.09107v2-Figure2-1.png",
    "caption": " The lower-dimensional encoding i of the environment context I is observed. Conditioned on i and the latent variables c, sampled from the prior over c, we get a distribution over possible robot trajectories x and user labels y.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which variables are observed in this model?",
    "answer": "The observed variables are $i$, $x$, and $y$.",
    "rationale": "The figure shows that $i$, $x$, and $y$ are shaded in gray, which indicates that they are observed. The other variables, $c_s$, $c_e$, and $c_u$, are not shaded, which indicates that they are latent variables.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.09107v2",
    "pdf_url": null
  },
  {
    "instance_id": "2e6db5adf7054653851ce4b6c49db81a",
    "figure_id": "2301.10910v2-Figure7-1",
    "image_file": "2301.10910v2-Figure7-1.png",
    "caption": " Throughput with respect to λ for finite queues (bars represent standard derivations)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest throughput for all values of λ?",
    "answer": "FCFS",
    "rationale": "The figure shows the throughput of different algorithms with respect to λ. The FCFS algorithm has the highest throughput for all values of λ.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.10910v2",
    "pdf_url": null
  },
  {
    "instance_id": "df34cdf4db8649b9b21103b38498396a",
    "figure_id": "1904.03175v2-Figure1-1",
    "image_file": "1904.03175v2-Figure1-1.png",
    "caption": " First row of each sequence: images captured in a industrial or wildlife monitoring system. Second row: results of our proposed method to detect foreground objects.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which row in the figure shows the results of the proposed method to detect foreground objects?",
    "answer": "Second row.",
    "rationale": "The caption explicitly states that the second row of each sequence shows the results of the proposed method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03175v2",
    "pdf_url": null
  },
  {
    "instance_id": "2aef0f16f107478fb6b86ef90eec2a6b",
    "figure_id": "2206.02663v1-Figure5-1",
    "image_file": "2206.02663v1-Figure5-1.png",
    "caption": " Target weight and scalability analysis.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the target weight of 0.8 the fastest?",
    "answer": "TransBO.",
    "rationale": "The plot in (a) shows that TransBO reaches the target weight of 0.8 in the fewest number of trials.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.02663v1",
    "pdf_url": null
  },
  {
    "instance_id": "bf0a00512f1546e8aaa0f88d74f1bc78",
    "figure_id": "1910.06222v2-Figure6-1",
    "image_file": "1910.06222v2-Figure6-1.png",
    "caption": " Evaluation of Î([X1, X2]; [Y1, Y2])/Î(X,Y ), where the ideal value is 2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the MNIST dataset?",
    "answer": "GM (VAE)",
    "rationale": "The figure shows that the GM (VAE) method has the highest ratio for all numbers of rows used on the MNIST dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.06222v2",
    "pdf_url": null
  },
  {
    "instance_id": "f7bd0b8e92a64c95bab8621103c45ac8",
    "figure_id": "1908.03675v1-Figure7-1",
    "image_file": "1908.03675v1-Figure7-1.png",
    "caption": " Qualitative results in MS-COCO [13]. In the first two examples, UFO Search retrieves objects from the only object type in MS-COCO (surfer and racket, respectively) that is really compatible with the context. The bottom two examples show that our method can retrieve compatible objects from differing categories when numerous object types are appropriate for the scene. For example, our method retrieves truck, jeep and cattle for a hole on a road (second to bottom example) and retrieves bag, book and laptop for a hole on a cluttered table (bottom example).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four retrieval methods seems to be the most successful in retrieving compatible objects for the scene?",
    "answer": "UFO Search.",
    "rationale": "The UFO Search method retrieves objects that are most compatible with the context of the scene, as shown in the examples in the figure. For example, in the first example, UFO Search retrieves surfers, which are the only objects in MS-COCO that are compatible with the context of a beach scene. In the second example, UFO Search retrieves rackets, which are the only objects in MS-COCO that are compatible with the context of a tennis court scene. In the third example, UFO Search retrieves trucks, jeeps, and cattle, which are all compatible with the context of a road scene. In the fourth example, UFO Search retrieves bags, books, and laptops, which are all compatible with the context of a cluttered table scene.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.03675v1",
    "pdf_url": null
  },
  {
    "instance_id": "82333eed086442318f7ca43c421a718e",
    "figure_id": "1809.04497v3-Figure2-1",
    "image_file": "1809.04497v3-Figure2-1.png",
    "caption": " Disentanglement Metric plotted against Reconstruction error for CHyVAE, β-VAE, and FactorVAE on 2DShapes dataset averaged over 10 random restarts.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best disentanglement and reconstruction error trade-off?",
    "answer": "FactorVAE.",
    "rationale": "The figure shows that FactorVAE has the lowest reconstruction error for a given disentanglement metric.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.04497v3",
    "pdf_url": null
  },
  {
    "instance_id": "c92677ab3bae4ac28020ee5875ab2553",
    "figure_id": "2108.07797v1-Figure4-1",
    "image_file": "2108.07797v1-Figure4-1.png",
    "caption": " The distribution of the differences of scores in the training set under different partition strategy. (a) Uniform partition. We can observe a large variation of frequency among different groups. (b) The proposed grouping strategy in Equation (5). The training pairs belonging to each group are balanced.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which partition strategy results in a more balanced distribution of frequency among different groups?",
    "answer": "The proposed grouping strategy in Equation (5).",
    "rationale": "The figure shows that the proposed grouping strategy results in a more uniform distribution of frequency among different groups than the uniform partition strategy. This is evident from the fact that the bars in (b) are more evenly distributed than the bars in (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.07797v1",
    "pdf_url": null
  },
  {
    "instance_id": "3b71331cb1f24ffb803b505f1c06ebdc",
    "figure_id": "2201.10474v2-Figure2-1",
    "image_file": "2201.10474v2-Figure2-1.png",
    "caption": " Scatter plots displaying correlations of select demographic features of a school’s ZIP code or county with its average P (high quality).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four demographic features has the strongest negative correlation with the average probability of a school having high quality?",
    "answer": "2016 GOP Vote",
    "rationale": "The scatter plots show the correlation between the average probability of a school having high quality and four different demographic features. The correlation coefficient (r) is a measure of the strength and direction of the linear relationship between two variables. The closer the correlation coefficient is to -1, the stronger the negative correlation. In this case, the scatter plot for 2016 GOP Vote has the correlation coefficient closest to -1, indicating the strongest negative correlation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.10474v2",
    "pdf_url": null
  },
  {
    "instance_id": "13f84a79d9dc4741a85481fed3db5f6c",
    "figure_id": "1811.09745v1-Figure13-1",
    "image_file": "1811.09745v1-Figure13-1.png",
    "caption": " Comparison only at bifurcation point.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods is the most accurate in identifying bifurcation points at a fall-out ratio of 2%?",
    "answer": "NMS.",
    "rationale": "The figure shows the ROC curves for four different methods at three different levels of noise (std = 5, 10, 15). The ROC curve plots the recall ratio (the proportion of true positives correctly identified) against the fall-out ratio (the proportion of false positives incorrectly identified). At a fall-out ratio of 2%, the NMS method has the highest recall ratio for all three levels of noise. This indicates that NMS is the most accurate method at identifying bifurcation points at this level of false positives.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.09745v1",
    "pdf_url": null
  },
  {
    "instance_id": "0c8b07f4b0134bc094cdd5c170c9aa80",
    "figure_id": "2008.09020v1-Figure5-1",
    "image_file": "2008.09020v1-Figure5-1.png",
    "caption": " Training accuracy of (a) NCI1 and (b) REDDITM datasets for different number of hidden and MLP layers. 3.3.4 Hyper-parameter sensitivity. We also study several hyperparameters in graph isomorphism network such as hidden dimension, learning rate, the number of hidden layers, and the number of layers in MLP. Notably, we use the default value for other parameters in the model. The parameter sensitivity of the embedding dimension has been studied in supervised learning [5, 11]. So, we also conduct experiments varying the size of the hidden dimension in GIN for the REDDITM dataset. For this experimental setup, we get a test accuracy of 39.1%, 39%, 39%, and 38.4% for 16, 32, 64, and 128-dimensional embedding, respectively. Thus, we do not see any significant difference for other datasets as well but in general, accuracy may drop for very higher-dimensional representation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the NCI1 dataset?",
    "answer": "L5_MLP4",
    "rationale": "The figure shows the training accuracy of different models on the NCI1 dataset. The L5_MLP4 model has the highest training accuracy, which suggests that it performs the best on this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.09020v1",
    "pdf_url": null
  },
  {
    "instance_id": "518fd99c28fa4961a73336d0d454612a",
    "figure_id": "1904.06827v1-Figure6-1",
    "image_file": "1904.06827v1-Figure6-1.png",
    "caption": " Our proposed encoder architecture takes as input a sequence of length T of point cloud data, each containing N points, and outputs a single vector.",
    "figure_type": "** schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the output of the encoder architecture shown in the figure? ",
    "answer": " A single vector. ",
    "rationale": " The figure shows a schematic of the encoder architecture, which takes as input a sequence of length T of point cloud data, each containing N points, and outputs a single vector. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.06827v1",
    "pdf_url": null
  },
  {
    "instance_id": "1184421c62224ce0b55a7d19f091788c",
    "figure_id": "2005.01901v1-Figure3-1",
    "image_file": "2005.01901v1-Figure3-1.png",
    "caption": " Screenshot of Best-Worst Scaling Task.",
    "figure_type": "Screenshot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three criteria used to evaluate the summaries?",
    "answer": "Informativeness, Coherence, and No Redundancy.",
    "rationale": "The screenshot shows a Best-Worst Scaling Task, where participants are asked to rate the summaries according to three criteria: Informativeness, Coherence, and No Redundancy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.01901v1",
    "pdf_url": null
  },
  {
    "instance_id": "a57d231d7ffe4eae8cd987b596859f35",
    "figure_id": "2011.12328v1-Figure31-1",
    "image_file": "2011.12328v1-Figure31-1.png",
    "caption": " Mean accuracy of individual tasks after training for the top 5 performing approaches on mixed vision tasks",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach performs the best on the Cifar100 task?",
    "answer": "HAT",
    "rationale": "The figure shows the performance of different approaches on various vision tasks. The line for HAT is the highest at the Cifar100 task, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.12328v1",
    "pdf_url": null
  },
  {
    "instance_id": "cb7c7c124b754d14afae16bbcdd39756",
    "figure_id": "1909.00015v2-Figure9-1",
    "image_file": "1909.00015v2-Figure9-1.png",
    "caption": " Interrogation-detecting heads in the three models. The top sentence is interrogative while the bottom one is declarative but includes the interrogative word “what”. In the top example, these interrogation heads assign a high probability to the question mark in the time step of the interrogative word (with ≥ 97.0% probability), while in the bottom example since there is no question mark, the same head does not assign a high probability to the last token in the sentence during the interrogative word time step. Surprisingly, this head prefers a low α = 1.05, as can be seen from the dense weights. This allows the head to identify the noun phrase “Armani Polo” better.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is better at identifying the noun phrase \"Armani Polo\" in the sentence \"however, what is Armani Polo?\"",
    "answer": "The α-entmax model.",
    "rationale": "The figure shows that the α-entmax model assigns a higher probability to the noun phrase \"Armani Polo\" than the other two models. This is because the α-entmax model has a lower α value, which allows it to focus on a smaller number of words.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.00015v2",
    "pdf_url": null
  },
  {
    "instance_id": "0f099d2eb0ab42b68a9b9e614f845da8",
    "figure_id": "1904.00152v2-Figure8-1",
    "image_file": "1904.00152v2-Figure8-1.png",
    "caption": " AUC and AP scores for different choices of d. The datasets are the same as those in Section 4.2, where the outlier ratio is c = 0.5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest AUC score for d = 50?",
    "answer": "Caltech 101",
    "rationale": "The figure shows that the AUC score for Caltech 101 is slightly above 0.8 for d = 50, while the AUC scores for the other datasets are all below 0.8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.00152v2",
    "pdf_url": null
  },
  {
    "instance_id": "21ac4c15a20e4e58b4800373e994f8f6",
    "figure_id": "2311.17053v1-Figure2-1",
    "image_file": "2311.17053v1-Figure2-1.png",
    "caption": " The DiffuseBot framework consists of three modules: (i) robotizing, which converts diffusion samples into physically simulatable soft robot designs (ii) embedding optimization, which iteratively generate new robots to be evaluated for training the conditional embedding (iii) diffusion as co-design, which guides the sampling process with co-design gradients from differentiable simulation. Arrow (A): evaluation of robots to guide data distribution. (B): differentiable physics as feedback.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which module of the DiffuseBot framework is responsible for converting diffusion samples into physically simulatable soft robot designs?",
    "answer": " The robotizing module.",
    "rationale": " The figure shows that the robotizing module takes diffusion samples as input and outputs soft robot designs with solid geometry, body stiffness, and actuators. This suggests that this module is responsible for converting the diffusion samples into physically simulatable designs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2311.17053v1",
    "pdf_url": null
  },
  {
    "instance_id": "f5bc4877ee2e49c787e08d78820809c5",
    "figure_id": "2202.02790v1-Figure14-1",
    "image_file": "2202.02790v1-Figure14-1.png",
    "caption": " The Cliff environment RNs optimized with the reward threshold objective (similar to the results reported in Section 6).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the highest cumulative reward in the Cliff Walking environment?",
    "answer": "SARSA + exec. pot. RN",
    "rationale": "The figure shows the cumulative reward for different methods in the Cliff Walking environment. The SARSA + exec. pot. RN method has the highest cumulative reward.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.02790v1",
    "pdf_url": null
  },
  {
    "instance_id": "2fedad7cd8964801b63dea7a08581c4c",
    "figure_id": "2102.09635v3-Figure1-1",
    "image_file": "2102.09635v3-Figure1-1.png",
    "caption": " An example showing four users 𝑢1 . . . 𝑢4 with their ideological positions.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the ideological position of user 𝑢1?",
    "answer": "User 𝑢1 has an ideological position of -1.",
    "rationale": "The figure shows a number line with the ideological positions of four users marked on it. User 𝑢1 is marked with a blue dot at -1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.09635v3",
    "pdf_url": null
  },
  {
    "instance_id": "63e00cd567ba41b5906554ad5f0b63c0",
    "figure_id": "2006.11947v1-Figure5-1",
    "image_file": "2006.11947v1-Figure5-1.png",
    "caption": " Comparison against baseline. For each dataset and for each parameter setting, we run all the algorithms 100 times using the same randomly chosen seed vertex.We compare themedian relative error in estimation vs the percentage of queries made. The median error of RWS does not drop below 10% for any of the datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best on the orkut dataset?",
    "answer": "SRW1",
    "rationale": "The figure shows that SRW1 has the lowest median relative error for all percentages of queries made on the orkut dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.11947v1",
    "pdf_url": null
  },
  {
    "instance_id": "437598cb462b43f2a5043e999814afa3",
    "figure_id": "2006.04635v4-Figure10-1",
    "image_file": "2006.04635v4-Figure10-1.png",
    "caption": " Long-term CCEDist reached by BRPI(2, 16) with with πtb = πt−1 in Blotto(3, 10, 3) (left), Blotto(4, 8, 3) (middle), and Blotto(5, 6, 3) (right).",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \n\nWhich of the Blotto games resulted in the highest CCEDist for BRPI with the given parameters? ",
    "answer": " Blotto5",
    "rationale": " The figure shows that the highest CCEDist achieved by BRPI with πtb = πt−1 and B = 2, C = 16 is in Blotto5, where it reaches a value of approximately 2. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04635v4",
    "pdf_url": null
  },
  {
    "instance_id": "c47dfd45dbe34b8e8f43ba2a6f91ed3b",
    "figure_id": "2205.02355v2-Figure2-1",
    "image_file": "2205.02355v2-Figure2-1.png",
    "caption": " Effect of the parameters of the retrieval process.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which parameter has a greater impact on the F1 score, λ or k?",
    "answer": "λ",
    "rationale": "The F1 score changes more dramatically with changes in λ than with changes in k. For example, when λ increases from 0.2 to 1.0, the F1 score decreases by approximately 2.5%. In contrast, when k increases from 8 to 512, the F1 score increases by less than 0.5%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.02355v2",
    "pdf_url": null
  },
  {
    "instance_id": "83cbe57fd7454dba9d1df25ae4121c75",
    "figure_id": "2007.04589v6-Figure2-1",
    "image_file": "2007.04589v6-Figure2-1.png",
    "caption": " Accuracy of a classifier when trained on the onevs-all CIFAR-10 classification task. Regularized with the InfoMax objective by minimizing (4), the classifier successfully predicts classes trained from previous iterations even when the underlying class distribution changes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved higher accuracy after 10,000 iterations?",
    "answer": "Infomax",
    "rationale": "The plot shows that the blue line (Infomax) is above the red line (Original) after 10,000 iterations. This means that the Infomax method achieved higher accuracy than the Original method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.04589v6",
    "pdf_url": null
  },
  {
    "instance_id": "d4c13ab29adc48599c974b6293746b29",
    "figure_id": "2110.03372v2-Figure2-1",
    "image_file": "2110.03372v2-Figure2-1.png",
    "caption": " Top score (y-axis) curves of different methods on 3 sequence design problems (left: UTR, middle: AMP, right: Fluo) with regard to the number of rounds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best on the UTR sequence design problem?",
    "answer": "IPS-A and IPS-B",
    "rationale": "The figure shows the top-10 score curves for different methods on the UTR sequence design problem. IPS-A and IPS-B have the highest top-10 score curves, indicating that they performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.03372v2",
    "pdf_url": null
  },
  {
    "instance_id": "afd959815d7d4c748b795c1074f37b6c",
    "figure_id": "1901.09791v1-Figure5-1",
    "image_file": "1901.09791v1-Figure5-1.png",
    "caption": " PUT-STV early discovery. m = 30.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest at discovering 50% of the winners?",
    "answer": "LPML",
    "rationale": "The plot shows that the LPML line reaches 50% of the winners discovered at the lowest time point.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.09791v1",
    "pdf_url": null
  },
  {
    "instance_id": "0d86b972b5aa48238905512db3428969",
    "figure_id": "2308.11662v2-Figure5-1",
    "image_file": "2308.11662v2-Figure5-1.png",
    "caption": " Amount of multiple answer groundings per visual question for four vision skills, overall and per dataset source (VQAv2 and VizWiz-VQA).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest percentage of multiple answer groundings for the All-Multiple category?",
    "answer": "OBJ",
    "rationale": "The figure shows that for the All-Multiple category, OBJ has the highest percentage of multiple answer groundings at 22%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.11662v2",
    "pdf_url": null
  },
  {
    "instance_id": "00893776b23b42da987ea4ab8271a757",
    "figure_id": "2112.06174v1-Figure4-1",
    "image_file": "2112.06174v1-Figure4-1.png",
    "caption": " Visual comparison with state-of-the-arts for arbitrary SR results. The input is a 48 × 48 patch from an image in SCID [12] test set. All the three models are trained with continuous scales in the range ×1 ∼ ×4 and are tested for ×10 magnification. Note that the LR input is generated with ×4 downsampling, and there is no groud truth for its ×10 magnification.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models produces the sharpest image?",
    "answer": "ITSRN(480px)",
    "rationale": "The ITSRN(480px) image is the sharpest and has the most detail. The MetaSR and LIIF images are also sharp, but they are not as detailed as the ITSRN image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.06174v1",
    "pdf_url": null
  },
  {
    "instance_id": "aa8b1cca41f9477cbbb90f87b1110997",
    "figure_id": "2301.04110v1-Figure3-1",
    "image_file": "2301.04110v1-Figure3-1.png",
    "caption": " Impact of case size on gains in EM (y-axis) of adapted models (x-axis). With 20 cases, StructCBR still outperforms GTM with 30 case examples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best when the number of cases is 20?",
    "answer": "StructCBR",
    "rationale": "The figure shows that StructCBR has the highest EM (y-axis) when the number of cases is 20 (blue bars).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.04110v1",
    "pdf_url": null
  },
  {
    "instance_id": "5a121541e21b4c1bb9d84d58f27f2833",
    "figure_id": "1903.04959v1-Figure6-1",
    "image_file": "1903.04959v1-Figure6-1.png",
    "caption": " Win rates for Deep MAPQN, Deep MAHHQN and PDQN in 3v3 mode of Ghost Story",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the highest win rate in the 3v3 mode of Ghost Story?",
    "answer": "Deep MAHHQN",
    "rationale": "The plot shows the win rates for Deep MAPQN, Deep MAHHQN, and PDQN in the 3v3 mode of Ghost Story. Deep MAHHQN achieves the highest win rate at around 0.8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.04959v1",
    "pdf_url": null
  },
  {
    "instance_id": "e8bd3a76e2f048d1ad11d66ee242cbe3",
    "figure_id": "2103.06257v2-Figure3-1",
    "image_file": "2103.06257v2-Figure3-1.png",
    "caption": " MaxEnt RL is competitive with prior robust RL methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the Walker2d-v2 environment?",
    "answer": "MaxEnt RL (α = 0.1)",
    "rationale": "The figure shows the reward achieved by different algorithms on different environments. For the Walker2d-v2 environment, the MaxEnt RL (α = 0.1) curve is the highest, indicating that it achieves the highest reward.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.06257v2",
    "pdf_url": null
  },
  {
    "instance_id": "cb455f9d13ef4999b9e03a7c706a718b",
    "figure_id": "2102.10462v1-Figure9-1",
    "image_file": "2102.10462v1-Figure9-1.png",
    "caption": " Layer-wise precision comparison of the quantization schemes achieved under different regularization strengths with 3-bit activation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which quantization scheme has the highest precision in the `layer1.1.conv1` layer?",
    "answer": "2e-3",
    "rationale": "The plot shows the precision of different quantization schemes for different layers. The line corresponding to the 2e-3 quantization scheme is the highest at the `layer1.1.conv1` layer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.10462v1",
    "pdf_url": null
  },
  {
    "instance_id": "7b61e1713806429a852ec98da4a0a1d4",
    "figure_id": "2007.00232v2-Figure9-1",
    "image_file": "2007.00232v2-Figure9-1.png",
    "caption": " Logistic regression in the homogeneous case (mini-batch gradient)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges fastest?",
    "answer": "LEAD (2 bits)",
    "rationale": "The figure shows the loss function of different algorithms over epochs (left) and over bits transmitted (right). LEAD (2 bits) reaches a lower loss value in fewer epochs and with fewer bits transmitted than any other algorithm.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.00232v2",
    "pdf_url": null
  },
  {
    "instance_id": "dc4864ccfc2747c1a58eaff58d7e559b",
    "figure_id": "1906.02085v2-Figure4-1",
    "image_file": "1906.02085v2-Figure4-1.png",
    "caption": " Alignment and community detection performance for distorted stochastic block model graphs as a function of the edge removal probability. The first three plots show different error measures (closer to 0 the better); the last one shows the community detection performance in terms of Normalized Mutual Information (NMI closer to 1 the better).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of community detection?",
    "answer": "L2",
    "rationale": "The NMI plot shows that the L2 method has the highest NMI values for all edge removal probabilities, which means that it is the best at detecting communities in the distorted stochastic block model graphs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02085v2",
    "pdf_url": null
  },
  {
    "instance_id": "650604eb12624e32a400f0de0dc632e4",
    "figure_id": "2305.01812v1-Figure2-1",
    "image_file": "2305.01812v1-Figure2-1.png",
    "caption": " Summarizing performance evaluation methodology of post-abstention. Given a selective prediction system with coverage covth and risk riskth at abstention threshold th, let the new coverage and risk after applying a post-abstention method be cov′th and risk′th respectively. From the risk-coverage curve of the given system, we calculate its risk at coverage cov′th and compare it with risk′th (diff). For the method to have a positive impact, risk′th should be lower than the risk of the given system at coverage cov′th.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the plot, what happens to the risk of the prediction system when a post-abstention method is applied?",
    "answer": "The risk decreases.",
    "rationale": "The risk-coverage curve shows that the risk of the prediction system increases as the coverage increases. When a post-abstention method is applied, the coverage increases, but the risk decreases. This can be seen in the plot where the point representing the system after post-abstention is below the point representing the system before post-abstention.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.01812v1",
    "pdf_url": null
  },
  {
    "instance_id": "043e9767da574214bf093dbbba802cb2",
    "figure_id": "2008.05363v1-Figure3-1",
    "image_file": "2008.05363v1-Figure3-1.png",
    "caption": " Screenshot of the label-selection UI of the FiRA annotation tool. The numbers indicate keyboard shortcuts.",
    "figure_type": "screenshot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the keyboard shortcut for selecting the \"Wrong\" label?",
    "answer": "1",
    "rationale": "The figure shows a screenshot of the label-selection UI of the FiRA annotation tool. The number 1 is shown next to the \"Wrong\" label, indicating that it is the keyboard shortcut for selecting that label.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.05363v1",
    "pdf_url": null
  },
  {
    "instance_id": "d85165e848a34f909b4e714b09a85164",
    "figure_id": "2302.05328v3-Figure4-1",
    "image_file": "2302.05328v3-Figure4-1.png",
    "caption": " 3D Visualizations of item representations learned by MF backbone model on Yelp2018. Subfigures (a-d) showcase the preference and property representations of the identical head user as red and blue stars, respectively. In each subfigure, representations of the head user’s all historical items are projected on the unit sphere. The brightness of the color indicates the popularity degree, while red and blue dots denote preference and popularity representations of items, respectively. More visualization results can be found in Appendix A.4.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four loss functions shown in the figure leads to the most compact item representations?",
    "answer": "InvCF.",
    "rationale": "The figure shows that the InvCF loss function leads to the most compact item representations, as the points are more tightly clustered together than in the other three cases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.05328v3",
    "pdf_url": null
  },
  {
    "instance_id": "e182fc1095fb45939a8860ee9ddc2208",
    "figure_id": "2111.04276v1-Figure2-1",
    "image_file": "2111.04276v1-Figure2-1.png",
    "caption": " Volume Subdivision: Each surface tet.(blue) is divided into 8 tet.(red) by adding midpoints.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many new tetrahedra are created by subdividing a single surface tetrahedron?",
    "answer": "8",
    "rationale": "The figure shows a surface tetrahedron (blue) being divided into 8 smaller tetrahedra (red) by adding midpoints.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.04276v1",
    "pdf_url": null
  },
  {
    "instance_id": "07447df28a244530826c6fca88a4a5c8",
    "figure_id": "1904.05160v2-Figure15-1",
    "image_file": "1904.05160v2-Figure15-1.png",
    "caption": " Examples of the infused visual concepts from memory feature in MS1M-LT.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What visual concepts are infused into the average images in the figure?",
    "answer": "High cheekbones, dark skin color, and narrow eyes.",
    "rationale": "The figure shows three sets of average images, each with a different visual concept infused. The first set shows an average image with high cheekbones, the second set shows an average image with dark skin color, and the third set shows an average image with narrow eyes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.05160v2",
    "pdf_url": null
  },
  {
    "instance_id": "a3c8d17f3d7a4129b298207a6937de40",
    "figure_id": "1912.07213v2-Figure7-1",
    "image_file": "1912.07213v2-Figure7-1.png",
    "caption": " Two types of cascaded structures for performing joint VFI-SR",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How are the HR-LFR images in (a) and (b) related?",
    "answer": "The HR-LFR images in (a) and (b) are identical.",
    "rationale": "The HR-LFR images in (a) and (b) are both generated by upsampling the LR-LFR images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.07213v2",
    "pdf_url": null
  },
  {
    "instance_id": "15561a7788d9470ca506f44a6da0b37e",
    "figure_id": "2205.10186v3-Figure3-1",
    "image_file": "2205.10186v3-Figure3-1.png",
    "caption": " Visualization of the simulators (excl. Ishigami and Hartmann due to the dimensionality).",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which simulator is the most complex?",
    "answer": "Branin.",
    "rationale": "The Branin simulator has the most complex shape, with multiple peaks and valleys. This suggests that it is the most difficult simulator to optimize.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.10186v3",
    "pdf_url": null
  },
  {
    "instance_id": "fb7a458acc194d178eb0e05f7c285602",
    "figure_id": "2302.04308v2-Figure13-1",
    "image_file": "2302.04308v2-Figure13-1.png",
    "caption": " Qualitative comparisons with SOTA. Column 1: four MRI modalities. Column 2-4: segmentation maps predicted by three methods for different combinations of modalities. Column 5: Ground truth.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best at segmenting the tumor in the T1+T1c+T2+Flair modality?",
    "answer": "Ours.",
    "rationale": "The figure shows the segmentation predictions for different methods and modalities. The \"Ours\" column has the highest WT and ET scores for the T1+T1c+T2+Flair modality, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.04308v2",
    "pdf_url": null
  },
  {
    "instance_id": "29dbf907e77943c989476a19c7fd8197",
    "figure_id": "2011.12328v1-Figure23-1",
    "image_file": "2011.12328v1-Figure23-1.png",
    "caption": " Mean accuracy of individual tasks after training for the top 5 performing approaches on Split-MNIST",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach performed the best on task 1?",
    "answer": "GVCL-F",
    "rationale": "The figure shows the performance of the top 5 approaches on each task, and GVCL-F has the highest accuracy on task 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.12328v1",
    "pdf_url": null
  },
  {
    "instance_id": "e7555aa07b9c465fb91f96dd4221aabd",
    "figure_id": "1907.00098v3-Figure13-1",
    "image_file": "1907.00098v3-Figure13-1.png",
    "caption": " Examples of unsafe and safe perturbations on the optical flows of a FloorGymnastics video. Top row: five sampled frames from 0 s to 4 s. 2nd row: optical flows of the frames from 0 s to 5 s. 3rd row: unsafe perturbations on the flows corresponding to the upper bound. Bottom row: safe perturbations on the flows corresponding to the lower bound.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the perturbations is more likely to cause the gymnast to fall?",
    "answer": "The unsafe perturbation.",
    "rationale": "The unsafe perturbation shows a large change in the optical flow, which could cause the gymnast to lose her balance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.00098v3",
    "pdf_url": null
  },
  {
    "instance_id": "a9040ae6089944eaad945cc7437947ef",
    "figure_id": "1811.10786v2-Figure8-1",
    "image_file": "1811.10786v2-Figure8-1.png",
    "caption": " Experimental results on synthetic dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which clustering algorithm performs the best on the synthetic dataset with increasing noise percentage?",
    "answer": "AdaWave",
    "rationale": "The figure shows the adjusted mutual information for different clustering algorithms as a function of the noise percentage. AdaWave has the highest adjusted mutual information for all noise percentages, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.10786v2",
    "pdf_url": null
  },
  {
    "instance_id": "0562385d82554d6f8f8085ce51e8991d",
    "figure_id": "1810.03264v1-Figure7-1",
    "image_file": "1810.03264v1-Figure7-1.png",
    "caption": " The number of batches to reach 92% test accuracy using DNNs with varying numbers of hidden layers under 1 worker. We consider several variants of SGD algorithms (a)-(e). Note that with depth 0 the model reduces to MLR, which is convex. The numbers are averaged over 5 randomized runs. We omit the result whenever convergence is not achieved within the experiment horizon (77824 batches), such as SGD with momentum at depth 6 and s = 32.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer is the most efficient in terms of reaching 92% test accuracy with the fewest batches?",
    "answer": "Adam.",
    "rationale": "Figure (c) shows that Adam requires the least number of batches to reach 92% test accuracy for all depths and step sizes considered.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.03264v1",
    "pdf_url": null
  },
  {
    "instance_id": "33949ddce33f447c9e64f02f6d940cbe",
    "figure_id": "2110.04410v1-Figure2-1",
    "image_file": "2110.04410v1-Figure2-1.png",
    "caption": " DET curve for VoxCeleb1 cleaned trial comparing with previous studies",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods has the lowest miss probability for a fixed false alarm probability?",
    "answer": "TitaNet",
    "rationale": "The DET curve shows the relationship between the miss probability and the false alarm probability. The lower the curve, the lower the miss probability for a fixed false alarm probability. In this case, the TitaNet curve is consistently below the other two curves, indicating that it has the lowest miss probability for a fixed false alarm probability.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.04410v1",
    "pdf_url": null
  },
  {
    "instance_id": "4eabd146c26e41f1bd815a7e6ce03c49",
    "figure_id": "2303.15012v1-Figure20-1",
    "image_file": "2303.15012v1-Figure20-1.png",
    "caption": " Example of 3D-aware I2I translation of wild into cat and dog on AFHQ 2562.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two animal types that are being translated into from the wild animal?",
    "answer": "Cat and dog.",
    "rationale": "The figure shows that the wild animal (lion) is being translated into a cat and a dog.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.15012v1",
    "pdf_url": null
  },
  {
    "instance_id": "a7f0c89f9931473b8e98ba7566a3b854",
    "figure_id": "2211.01618v1-Figure8-1",
    "image_file": "2211.01618v1-Figure8-1.png",
    "caption": " Result of denoising for comparison. We have shown an example of denoising performance on image taken from the ELCAP Public Lung Image Database. The display window is [−175, 240] HU. Readers are requested to zoom in for better view.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the denoising methods performed the best?",
    "answer": "The proposed method performed the best.",
    "rationale": "The proposed method produced the image with the least amount of noise and the most detail. This can be seen in the area of the liver tumor, which is clearly visible in the proposed method's image but is obscured by noise in the other images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.01618v1",
    "pdf_url": null
  },
  {
    "instance_id": "65bec3dedf9048b19df19062fd67e7d1",
    "figure_id": "2210.05177v2-Figure4-1",
    "image_file": "2210.05177v2-Figure4-1.png",
    "caption": " Training loss landscapes of ResNet18 on CIFAR10 trained with SGD, SAM, SSAM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer leads to a smoother loss landscape?",
    "answer": "SSAM.",
    "rationale": "The SSAM loss landscape appears smoother than the SGD and SAM loss landscapes. This can be seen by comparing the sharpness of the peaks and valleys in each landscape.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.05177v2",
    "pdf_url": null
  },
  {
    "instance_id": "6153ff0764284d1f8e791a85da956909",
    "figure_id": "2112.01388v1-Figure2-1",
    "image_file": "2112.01388v1-Figure2-1.png",
    "caption": " A comparison of test performance over 10 independent trials using RPP-EMLP and equivalent EMLP and MLP models on the inertia (top) and double pendulum (bottom) datasets in which we have three varying levels of symmetries. The boxes represent the interquartile range, and the whiskers the remainder of the distribution. Left: perfect symmetries in which EMLP and the equivariant components of RPP-EMLP exactly capture the symmetries in the data. Center: approximate symmetries in which the perfectly symmetric systems have been modified to include some non-equivariant components. Right: mis-specified symmetries in which the symmetric components of EMLP and RPP-EMLP do not reflect the symmetries present in the data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in the case of exact symmetries?",
    "answer": "RPP",
    "rationale": "The box plot for RPP is the lowest in the \"Exact Symmetries\" column, indicating that it has the lowest mean squared error (MSE).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.01388v1",
    "pdf_url": null
  },
  {
    "instance_id": "8112bd4241814ef2b772637c0593c1fd",
    "figure_id": "1904.03834v2-Figure4-1",
    "image_file": "1904.03834v2-Figure4-1.png",
    "caption": " Sample distribution of the total memory estimator d̄ in four different simulation settings.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which simulation setting resulted in the most accurate estimate of the total memory?",
    "answer": "The \"Zero d:\" setting.",
    "rationale": "The figure shows that the \"Zero d:\" setting resulted in a distribution of estimates that was centered around the true total memory value of 0. This suggests that this setting produced the most accurate estimates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03834v2",
    "pdf_url": null
  },
  {
    "instance_id": "49e431b812844df6a566cde8df9bb069",
    "figure_id": "2305.19475v1-Figure14-1",
    "image_file": "2305.19475v1-Figure14-1.png",
    "caption": " tGF−→GF+DS over the 4 runs of (A-Adult), (B-Adult), (A-Census1990), and (BCensus1990).",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which dataset and number of clusters resulted in the lowest tGF−→GF+DS time?",
    "answer": " Dataset B-Census1990 with 3 clusters.",
    "rationale": " The plot shows the tGF−→GF+DS time for different numbers of clusters on four datasets. The lowest point on the plot for B-Census1990 is at 3 clusters, indicating the lowest time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19475v1",
    "pdf_url": null
  },
  {
    "instance_id": "464fbffa6899415e8d6fd3935479734d",
    "figure_id": "2106.04803v2-Figure1-1",
    "image_file": "2106.04803v2-Figure1-1.png",
    "caption": " Comparison for model generalization and capacity under different data size. For fair comparison, all models have similar parameter size and computational cost.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on ImageNet-1K and JFT datasets?",
    "answer": "ViT_ne performs the best on both datasets.",
    "rationale": "ViT_ne has the lowest training loss and the highest evaluation accuracy and precision@1 on both datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04803v2",
    "pdf_url": null
  },
  {
    "instance_id": "202cc673d0bb498d81cfa4586ca3e745",
    "figure_id": "1904.10293v1-Figure3-1",
    "image_file": "1904.10293v1-Figure3-1.png",
    "caption": " Example image patches and the corresponding attention maps. In (a)-(f), from left to right: the reference image, one non-reference image, and the attention map applied on the nonreference image. (a), (b) and (c) display attention maps for the significantly misaligned regions. (d), (e) and (f) show the attention maps can highlight useful regions.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which panels in the figure show examples of attention maps that highlight useful regions?",
    "answer": "Panels (d), (e), and (f).",
    "rationale": "The caption states that \"(d), (e), and (f) show the attention maps can highlight useful regions.\" The attention maps in these panels are shown in the rightmost column, and they highlight regions of the image that are relevant to the task at hand.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.10293v1",
    "pdf_url": null
  },
  {
    "instance_id": "706e56af86df4521b544b8384ca4c00f",
    "figure_id": "1811.00206v4-Figure5-1",
    "image_file": "1811.00206v4-Figure5-1.png",
    "caption": " Sparsity-Perplexity curves of various sparsity patterns on PTB dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sparsity pattern achieves the lowest perplexity on the PTB dataset?",
    "answer": "Balanced Sparsity",
    "rationale": "The figure shows the perplexity of different sparsity patterns on the PTB dataset. The lower the perplexity, the better the model performs. The Balanced Sparsity curve is the lowest of all the curves, indicating that it achieves the lowest perplexity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.00206v4",
    "pdf_url": null
  },
  {
    "instance_id": "667360e7b8754479adfd94d783896f70",
    "figure_id": "2208.06399v1-Figure8-1",
    "image_file": "2208.06399v1-Figure8-1.png",
    "caption": " Performance of AutoShard against baselines. We report the mean and standard deviation across five runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sharding strategy achieves the highest degree of balance for MetaProd?",
    "answer": "AutoShard",
    "rationale": "The figure shows that AutoShard achieves a degree of balance of approximately 90% for MetaProd, which is higher than any of the other sharding strategies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.06399v1",
    "pdf_url": null
  },
  {
    "instance_id": "b15fcd44fa714736873b05d20abf7cdf",
    "figure_id": "2211.07381v2-Figure1-1",
    "image_file": "2211.07381v2-Figure1-1.png",
    "caption": " Comparison of evaluation speed in frames per second (FPS) and image-level area under the receiver operating curve (AUROC) (%) on the MVTecAD test set. The figure shows the values reported in the paper.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms shown in the figure has the highest detection accuracy?",
    "answer": "Ours.",
    "rationale": "The figure shows the image-level AUROC for each algorithm, and \"Ours\" has the highest value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.07381v2",
    "pdf_url": null
  },
  {
    "instance_id": "cffad9c3224d4cbc95ea420ac92193b5",
    "figure_id": "2307.16754v1-Figure26-1",
    "image_file": "2307.16754v1-Figure26-1.png",
    "caption": " Duality gap as a function of the number of full (or equivalent) gradient computations for ECyclicPDA, MP, CFR+, and PCFR+, using a quadratic averaging scheme for ECyclicPDA and MP.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm appears to converge to the optimal solution most quickly for the Goofspiel game?",
    "answer": "CFR+",
    "rationale": "The CFR+ line in the Goofspiel plot is the first to reach a duality gap of zero, indicating that it has converged to the optimal solution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.16754v1",
    "pdf_url": null
  },
  {
    "instance_id": "cd7a5ac6827a483ebe351d1f73aa7370",
    "figure_id": "2203.02459v3-Figure4-1",
    "image_file": "2203.02459v3-Figure4-1.png",
    "caption": " Comparative BLEU scores versus AL at three regimes, low, medium, and high latency, for IWSLT 2020 simultaneous text-to-text track participants, RWTH, ON-TRAC, KIT and our streaming MT (STR-MT) system on the MuST-C corpus.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which system performed the best at low latency?",
    "answer": "STR-MT",
    "rationale": "The figure shows that the STR-MT system has the highest BLEU score at the low latency regime.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.02459v3",
    "pdf_url": null
  },
  {
    "instance_id": "eabf8fe849584cd59db6fdbafb42c0cb",
    "figure_id": "2101.04898v2-Figure3-1",
    "image_file": "2101.04898v2-Figure3-1.png",
    "caption": " (a): Comparison between ‘bus’ (clean) class, ‘ship’ (unlearnable) class and the overall accuracy. (b): Clean test accuracy of RN-18/RN-50/DN-121 on unlearnable CIFAR-10 with errorminimizing noise crafted on ImageNet. (c): Prediction confusion matrix of RN-18 trained on CIFAR10 with only 4 classes (‘airplane’, ‘car’, ‘ship’, ‘truck’) are unlearnable by ImageNet transferred noise, and the confusion matrix is computed on CIFAR-10 clean test set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the clean test set when trained with error-minimizing noise crafted on ImageNet?",
    "answer": "ResNet-18.",
    "rationale": "Figure (b) shows the clean test accuracy of different models on the unlearnable CIFAR-10 dataset with error-minimizing noise crafted on ImageNet. ResNet-18 achieves the highest accuracy among the models shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.04898v2",
    "pdf_url": null
  },
  {
    "instance_id": "aebad366d0fb4c73a579cd5e66c0777b",
    "figure_id": "2302.06072v1-Figure4-1",
    "image_file": "2302.06072v1-Figure4-1.png",
    "caption": " Visualization of the action selections by the baseline method (Chen et al. 2021) and our AACL. The green boxes denote the correct actions and the red boxes denote the wrong ones. After step 6 marked with the grey dashed box, the baseline and AACL make different trajectories.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method correctly predicts the agent's trajectory after step 6?",
    "answer": "AACL",
    "rationale": "The figure shows that the baseline method predicts the agent to turn left after step 6, while AACL correctly predicts the agent to go forward. This is indicated by the green box around the \"go forward\" action in the AACL column.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.06072v1",
    "pdf_url": null
  },
  {
    "instance_id": "0835626aad9d4b28a103be6cd510f629",
    "figure_id": "2108.12084v2-Figure5-1",
    "image_file": "2108.12084v2-Figure5-1.png",
    "caption": " Confusion matrix of Classifier C3",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many instances were correctly classified by Classifier C3?",
    "answer": "242",
    "rationale": "The confusion matrix shows that 242 instances were correctly classified as class 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.12084v2",
    "pdf_url": null
  },
  {
    "instance_id": "c7c6b97ea48b4f6ca2da026287c28bf9",
    "figure_id": "2010.04091v1-Figure4-1",
    "image_file": "2010.04091v1-Figure4-1.png",
    "caption": " Average computation time per decision vs. average final cumulative regret for (a) Figure 3(a); (b) Figure 3(b); (c) Figure 3(c); (d) Figure 3(d).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms shown in the figure is the most computationally efficient?",
    "answer": "Laplace-TS",
    "rationale": "The figure shows the average computation time per decision vs. the average final cumulative regret for each algorithm. The algorithm with the lowest computation time per decision is Laplace-TS.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.04091v1",
    "pdf_url": null
  },
  {
    "instance_id": "9d79a5c89d5b4ac2a0313385cd5eb868",
    "figure_id": "1903.12473v2-Figure7-1",
    "image_file": "1903.12473v2-Figure7-1.png",
    "caption": " Detection results on three benchmarks and several representative comparisons of curve texts on CTW1500. More examples are provided in the supplementary materials.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, PSENet or CTD+TLOC, appears to be more accurate at detecting curved text?",
    "answer": "PSENet.",
    "rationale": "In the figure, the green boxes represent the ground truth bounding boxes of the text, while the yellow boxes represent the predicted bounding boxes. In the PSENet results, the predicted bounding boxes are much closer to the ground truth bounding boxes than in the CTD+TLOC results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.12473v2",
    "pdf_url": null
  },
  {
    "instance_id": "f4b2ec1f6cd4476fb8c36ed35979c74f",
    "figure_id": "1910.12845v3-Figure5-1",
    "image_file": "1910.12845v3-Figure5-1.png",
    "caption": " Copula-EM vs nonparametric algorithms: The imputation error for each data type on synthetic data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which imputation method performed the best for continuous data with a missing ratio of 50%?",
    "answer": "xPCA",
    "rationale": "The plot for continuous data shows that xPCA has the lowest SMAE at a missing ratio of 50%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.12845v3",
    "pdf_url": null
  },
  {
    "instance_id": "1acda9d9b2b545c88b3acd00eaafffb8",
    "figure_id": "2101.11203v3-Figure4-1",
    "image_file": "2101.11203v3-Figure4-1.png",
    "caption": " Training loss (top) and test accuracy (bottom) for three models on MNIST with hyperparameters setting: local learning rate 0.1, global learning rate 1.0, worker number 10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on MNIST with hyperparameters setting: local learning rate 0.1, global learning rate 1.0, worker number 10?",
    "answer": "CNN.",
    "rationale": "The CNN model has the lowest training loss and highest test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.11203v3",
    "pdf_url": null
  },
  {
    "instance_id": "1d4f5ffe85374174993fef14caab5d4f",
    "figure_id": "2006.05467v3-Figure1-1",
    "image_file": "2006.05467v3-Figure1-1.png",
    "caption": " Layer-collapse leads to a sudden drop in accuracy. Top-1 test accuracy as a function of the compression ratio for a VGG-16 model pruned at initialization and trained on CIFAR100. Colored arrows represent the critical compression of the corresponding pruning algorithm. Only our algorithm, SynFlow, reaches the theoretical limit of max compression (black dashed line) without collapsing the network. See Sec. 7 for more details on the experiments.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pruning algorithm achieves the highest compression ratio without collapsing the network?",
    "answer": "SynFlow",
    "rationale": "The figure shows that the SynFlow pruning algorithm reaches the theoretical limit of maximum compression (black dashed line) without collapsing the network, while other pruning algorithms experience a sudden drop in accuracy at a lower compression ratio.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.05467v3",
    "pdf_url": null
  },
  {
    "instance_id": "0c575d0eec31430a8923e06a5d11d059",
    "figure_id": "2201.12105v1-Figure1-1",
    "image_file": "2201.12105v1-Figure1-1.png",
    "caption": " Training E2E SLU models when entity spoken order is unknown via data augmentation and entity reordering.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two different ways to train E2E SLU models when the entity spoken order is unknown?",
    "answer": "Data augmentation and entity reordering.",
    "rationale": "The figure shows two different ways to train E2E SLU models when the entity spoken order is unknown. The first way is to use data augmentation, which involves randomly reordering the training data. The second way is to use entity reordering, which involves using a hybrid ASR system or an attention-based SLU system to infer the spoken order of the entities.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.12105v1",
    "pdf_url": null
  },
  {
    "instance_id": "6f27334646674044ac9c4d1575b823f6",
    "figure_id": "2308.13411v1-Figure2-1",
    "image_file": "2308.13411v1-Figure2-1.png",
    "caption": " Pointwise RNFLT distribution at different percentiles.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which percentile of RNFLT has the highest peak value?",
    "answer": "75th percentile",
    "rationale": "The figure shows the pointwise RNFLT distribution at different percentiles. The 75th percentile RNFLT distribution has the highest peak value, as indicated by the red color in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.13411v1",
    "pdf_url": null
  },
  {
    "instance_id": "c187295b9e6a459bb17e4c4e8db5a9f6",
    "figure_id": "2310.02714v1-Figure12-1",
    "image_file": "2310.02714v1-Figure12-1.png",
    "caption": " Comparison of 2D StyleGAN2 [12] with our GETAvatar.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the two methods, StyleGAN2 or GETAvatar, produces more realistic-looking images?",
    "answer": " GETAvatar",
    "rationale": " The images produced by GETAvatar (on the right) are more realistic-looking than the images produced by StyleGAN2 (on the left). The GETAvatar images have more detail and are more lifelike.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.02714v1",
    "pdf_url": null
  },
  {
    "instance_id": "d4c69fea59a94f9f819f242fbbdef080",
    "figure_id": "2105.15053v1-Figure2-1",
    "image_file": "2105.15053v1-Figure2-1.png",
    "caption": " Results of our human evaluation. Although the VAE baseline is the best at preserving question meaning, it is the worst at introducing variation to the output. SEPARATOR offers the best balance between dissimilarity and meaning preservation, and is more fluent than both DiPS and Latent BoW.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of meaning preservation?",
    "answer": "VAE",
    "rationale": "The figure shows that VAE has the highest relative preference score for meaning, which is 58%. This indicates that VAE is the best method at preserving the meaning of the original question.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.15053v1",
    "pdf_url": null
  },
  {
    "instance_id": "e6737981e32b4ed3b27a50da84747b16",
    "figure_id": "1907.05820v2-Figure5-1",
    "image_file": "1907.05820v2-Figure5-1.png",
    "caption": " Results of online refinement on YouTube videos. From left to right: input images, feed-forward (FF) results, OFT results, PFT results. The results of online refinement, OFT, are noticeably better than the feed-forward model – comparable to PFT but much faster.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the best results, according to the caption?",
    "answer": "PFT",
    "rationale": "The caption states that \"The results of online refinement, OFT, are noticeably better than the feed-forward model – comparable to PFT but much faster.\" This implies that PFT produced the best results, followed closely by OFT.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.05820v2",
    "pdf_url": null
  },
  {
    "instance_id": "1d82f65cba7c4cc18ac2e06a74de11de",
    "figure_id": "2306.07553v1-Figure4-1",
    "image_file": "2306.07553v1-Figure4-1.png",
    "caption": " Area maps of the real-world road networks Jinan12, Hangzhou16, NewYork48 and NewYork196. Red dots are the traffic signals controlled by the agents and the roads in these intersections are marked by white lines.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which city has the most traffic signals controlled by agents?",
    "answer": "Upper East, New York, USA",
    "rationale": "The figure shows that the Upper East Side of New York has the most red dots, which represent traffic signals controlled by agents.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.07553v1",
    "pdf_url": null
  },
  {
    "instance_id": "6dfc267e3bd14d23b8c67614c765f41f",
    "figure_id": "2303.00980v1-Figure6-1",
    "image_file": "2303.00980v1-Figure6-1.png",
    "caption": " Results on Depth-only and Width-only growth. LiGO saves 51.7% FLOPS when expanding depth-only, and 41.6% FLOPS when expanding width-only.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the lowest perplexity with the least amount of FLOPS when expanding depth-only?",
    "answer": "LiGO",
    "rationale": "The figure shows the log perplexity of different methods as a function of FLOPS. LiGO has the lowest perplexity with the least amount of FLOPS when expanding depth-only.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.00980v1",
    "pdf_url": null
  },
  {
    "instance_id": "2a55b72bb42d4969962ac3b1a98e048b",
    "figure_id": "2006.14308v1-Figure3-1",
    "image_file": "2006.14308v1-Figure3-1.png",
    "caption": " Fractions of images under extreme conditions for different sets. Extreme conditions include large head pose, exaggerated expression, non-uniform illumination, unrecognizable makeup, object occlusion, and blurry shot.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which extreme condition is most prevalent in the test set?",
    "answer": "Occlusion",
    "rationale": "The bar for \"Occlusion\" in the test set is the highest among all the extreme conditions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.14308v1",
    "pdf_url": null
  },
  {
    "instance_id": "49be12c1b76c4aa8b88ae8d16a080ecd",
    "figure_id": "2307.16377v2-Figure4-1",
    "image_file": "2307.16377v2-Figure4-1.png",
    "caption": " Visualization of cross-attention weights in the last refining layer. We randomly sample 100 persons from 3DPW test set and average the attention weights for visualization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature has the highest attention weight for 3D Feature?",
    "answer": "Shape",
    "rationale": "The bar for Shape in the 3D Feature category is the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.16377v2",
    "pdf_url": null
  },
  {
    "instance_id": "0bafbfb49a9a4277aba40f147519ac59",
    "figure_id": "2012.13841v1-Figure5-1",
    "image_file": "2012.13841v1-Figure5-1.png",
    "caption": " (Top.) We divide the cross entropy loss into two parts as per (5). The cosine between the weight vectorw and−∇`pos is positive whereas the cosine between w and −∇`neg is negative. This suggests that network norm increases as subset of weights responsible for correct predictions grow in magnitude. (Bottom.) cos(w,−`pos) with `pos defined as per (5). We see that for a network with shuffled labels the gradient barely points in the radial direction, which would lead to less growth as per Figure 4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture shows the greatest difference in the cosine of the weight vector and the negative gradient of the positive loss between the original and shuffled labels?",
    "answer": "densenet121",
    "rationale": "The difference between the original and shuffled labels is greatest for densenet121, as shown in the bottom right plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.13841v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee1bc17373f644289c98834fb2508d2b",
    "figure_id": "2303.17569v2-Figure22-1",
    "image_file": "2303.17569v2-Figure22-1.png",
    "caption": " Complete comparisons with all methods and the reference image on the BAID test dataset. Our CLIP-LIT restores the human face most clearly and naturally.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the image that is most similar to the reference image?",
    "answer": "CLIP-LIT",
    "rationale": "The figure shows the results of different methods for restoring a low-light image. The reference image is shown in the bottom right corner. The CLIP-LIT image is the closest to the reference image in terms of clarity and naturalness.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.17569v2",
    "pdf_url": null
  },
  {
    "instance_id": "bec0daf585d44c4ab0a43f8478c0681d",
    "figure_id": "1902.10350v1-Figure4-1",
    "image_file": "1902.10350v1-Figure4-1.png",
    "caption": " Root mean squared errors as functions of active learning iteration for different methods on the Airline delays data set. Plots show median of the errors over 25 runs. NNGP initially has a much higher error, but shows the rapid improvement and becomes the best method near iteration 300.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the Airline delays dataset after 300 iterations of active learning?",
    "answer": "NNGP",
    "rationale": "The plot shows that the NNGP algorithm has the lowest test RMSE after 300 iterations of active learning.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.10350v1",
    "pdf_url": null
  },
  {
    "instance_id": "dcd9529353eb4c0d9a020554a3136fc6",
    "figure_id": "2007.11301v3-Figure5-1",
    "image_file": "2007.11301v3-Figure5-1.png",
    "caption": " Comparison of interpolations between one-stage autoregressive (top row, in green), onestage feed-forward (2nd row, in pink), ours – Hungarian (3rd row, in orange) and ours – ordered (bottom row, in blue). Ordered generally leads to the smoothest interpolations. The last two examples show interpolations where Hungarian yields visually more meaningful shape transitions. For a better visualization of these transitions, paths are colored according to their index (or order for one-stage architectures).",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which interpolation method leads to the smoothest transitions between shapes?",
    "answer": "Ordered interpolation.",
    "rationale": "The caption states that \"Ordered generally leads to the smoothest interpolations.\" This is also evident in the figure, where the bottom row (blue) shows smoother transitions between shapes compared to the other rows.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.11301v3",
    "pdf_url": null
  },
  {
    "instance_id": "bfc80c421b72499c8c41474b980e317e",
    "figure_id": "2012.05292v1-Figure12-1",
    "image_file": "2012.05292v1-Figure12-1.png",
    "caption": " Directed edge labels for the topological map are represented as discretized polar coordinates. The chart is represented as a top-down view with the agent in the center, and orientations represented as compass directions.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which direction are the longest edges?",
    "answer": "The longest edges are in the northeast direction.",
    "rationale": "The figure shows that the green area, which represents edges longer than 5 meters, is largest in the northeast direction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.05292v1",
    "pdf_url": null
  },
  {
    "instance_id": "8cd0ddbaa1284eca9792e54b545c2da8",
    "figure_id": "2212.09686v2-Figure13-1",
    "image_file": "2212.09686v2-Figure13-1.png",
    "caption": " Divergence of the bias term (after softmax is performed to map bias onto probability simplex) from the unigram distribution of the respective training set for models trained on different data sets. Same models as used in Fig. 3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model shows the greatest divergence of the bias term from the unigram distribution?",
    "answer": "AfroMT EN -> RUN",
    "rationale": "The figure shows the divergence of the bias term from the unigram distribution for different models and training steps. The AfroMT EN -> RUN model has the highest peak, indicating the greatest divergence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.09686v2",
    "pdf_url": null
  },
  {
    "instance_id": "82581cf03f2f459db6b210f7931fd06c",
    "figure_id": "1911.11907v2-Figure6-1",
    "image_file": "1911.11907v2-Figure6-1.png",
    "caption": " Top-1 accuracy v.s. FLOPs on ImageNet dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network has the highest accuracy for a given number of FLOPs?",
    "answer": "GhostNet.",
    "rationale": "The plot shows the accuracy of different networks as a function of the number of FLOPs. GhostNet is the highest line on the plot, which means it has the highest accuracy for a given number of FLOPs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.11907v2",
    "pdf_url": null
  },
  {
    "instance_id": "ef7dc281b6794d53af93512f47c0a6c4",
    "figure_id": "2010.04529v1-Figure7-1",
    "image_file": "2010.04529v1-Figure7-1.png",
    "caption": " Factual errors made by the BertSumExtAbs model.",
    "figure_type": "\"other\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "According to the study, what percentage of British men have never washed their own car?",
    "answer": "31%",
    "rationale": "The document states that \"31 percent of British men say they have never washed their own car.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.04529v1",
    "pdf_url": null
  },
  {
    "instance_id": "096010dbe18f4fb98e7bf721b4bcb672",
    "figure_id": "2106.12535v2-Figure10-1",
    "image_file": "2106.12535v2-Figure10-1.png",
    "caption": " Comparison in terms of test error rates and PAC-Bayesian bound values. Each block corresponds to a different number of predictorsM . We report the means (bars) and standard deviations (vertical, magenta lines) over 10 different runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest test error rate for M = 32?",
    "answer": "Bin",
    "rationale": "The bar for Bin is the shortest among all the bars for M = 32.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.12535v2",
    "pdf_url": null
  },
  {
    "instance_id": "ebf828d4e6ee4d21b3d6e58670d64a12",
    "figure_id": "1903.05511v1-Figure2-1",
    "image_file": "1903.05511v1-Figure2-1.png",
    "caption": " Comparison of the runtime of the original (unabstracted) approach with the proposed one (using summarized abstraction). Confidence intervals mark standard error.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach is faster, the original or the proposed one?",
    "answer": "The proposed approach is faster.",
    "rationale": "The figure shows that the runtime of the proposed approach is consistently lower than the runtime of the original approach for all values of the number of vertices.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.05511v1",
    "pdf_url": null
  },
  {
    "instance_id": "a1070b28d64b4da8b6c78e396649a89e",
    "figure_id": "2110.04260v3-Figure13-1",
    "image_file": "2110.04260v3-Figure13-1.png",
    "caption": " Switch(t) w/ load balancing. Left: average routing confidence; Right: load of experts.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which expert has the higher load?",
    "answer": "Expert 1",
    "rationale": "The right plot shows the load of each expert over time. The solid blue line represents Expert 1, and the dashed orange line represents Expert 2. The solid blue line is consistently above the dashed orange line, indicating that Expert 1 has a higher load than Expert 2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.04260v3",
    "pdf_url": null
  },
  {
    "instance_id": "33e750d6c902423abb4f097acae83807",
    "figure_id": "2207.14502v4-Figure1-1",
    "image_file": "2207.14502v4-Figure1-1.png",
    "caption": " Illustrative puzzles and solutions that were synthesized by the Codex language model: the first is a simple equation; the second requires finding a palindrome (string same forwards and backwards) with exactly n=4600 copies of each of a given list of substrings.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the output of the second puzzle?",
    "answer": "\"Hello, there, you!\"",
    "rationale": "The second puzzle asks for a palindrome with exactly n=4600 copies of each of the substrings in the list [\"Hello\", \"there\", \"you!\"]. The solution is the string \"Hello, there, you!\" repeated 4600 times.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.14502v4",
    "pdf_url": null
  },
  {
    "instance_id": "f87225cadae846928658eaa35f889c61",
    "figure_id": "2012.07762v1-Figure2-1",
    "image_file": "2012.07762v1-Figure2-1.png",
    "caption": " Results for power system design of UAVs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of minimizing the objective value for both energy and mass objectives?",
    "answer": "MerCBO",
    "rationale": "The MerCBO line is consistently below the other lines in both plots, indicating that it achieves the lowest objective values for both energy and mass.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.07762v1",
    "pdf_url": null
  },
  {
    "instance_id": "5ec168cb540e4721b68aa52639536cc4",
    "figure_id": "2305.16548v1-Figure5-1",
    "image_file": "2305.16548v1-Figure5-1.png",
    "caption": " Case study for ENDERANKER where it identifies an error correctly in the example on the left, but fails in the right-side example. The rank threshold T=3. The SOIs are highlighted both in the original sentence and in the candidates list sorted by score in descending order.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which candidate is most likely to be the correct answer in the example on the left?",
    "answer": "\"the team\"",
    "rationale": "The candidate \"the team\" has the highest score in the example on the left, and the correctness label is also marked as a checkmark.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16548v1",
    "pdf_url": null
  },
  {
    "instance_id": "3dd279b298ff401e927c7620af340c31",
    "figure_id": "2306.07881v2-Figure1-1",
    "image_file": "2306.07881v2-Figure1-1.png",
    "caption": " Minens dataset – training samples. We show 24 random examples from the dataset. Each training example consists of 3 images (shown) and associated camera poses.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many images are shown in each training example?",
    "answer": "3",
    "rationale": "The caption states that \"Each training example consists of 3 images (shown) and associated camera poses.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.07881v2",
    "pdf_url": null
  },
  {
    "instance_id": "317b2928bcd143c4afd55fbed21716da",
    "figure_id": "1806.07564v2-Figure6-1",
    "image_file": "1806.07564v2-Figure6-1.png",
    "caption": " Effect on the F-score of the threshold τ .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "For which value of r is the F-score highest for the BMM method?",
    "answer": "r = 13",
    "rationale": "The figure shows the F-score for different values of r and for both the BMM and Otsu methods. The highest F-score for the BMM method is achieved when r = 13.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.07564v2",
    "pdf_url": null
  },
  {
    "instance_id": "8a9b04a3860e42bb82a8965d84a8bb2d",
    "figure_id": "2305.16283v4-Figure6-1",
    "image_file": "2305.16283v4-Figure6-1.png",
    "caption": " Qualitative limitations. We show some examples in different scenarios where some interpenetrating phenomena happen.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the lamp and the chair in the living room?",
    "answer": "The lamp is above and smaller than the chair.",
    "rationale": "The figure shows a scene graph of the living room, which includes nodes for the lamp and the chair. The lamp node is connected to the chair node by an arrow labeled \"above\" and \"smaller\", indicating that the lamp is above and smaller than the chair.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16283v4",
    "pdf_url": null
  },
  {
    "instance_id": "a375e3c7daa54fd7bead9d44f5b59202",
    "figure_id": "2211.14980v3-Figure4-1",
    "image_file": "2211.14980v3-Figure4-1.png",
    "caption": " Training time of trees generated by CART, GUIDE, IAI, Evtree, OSRT.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest training time for a maximum depth of 7 and a number of leaves of 25?",
    "answer": "evtree",
    "rationale": "The figure shows that the training time for evtree is higher than the training time for any other algorithm at a maximum depth of 7 and a number of leaves of 25.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.14980v3",
    "pdf_url": null
  },
  {
    "instance_id": "7ec05630c6ba4efb9f028bf3b48402e0",
    "figure_id": "2302.01849v1-Figure4-1",
    "image_file": "2302.01849v1-Figure4-1.png",
    "caption": " Performance with different parameter budgets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the FB15k-237 dataset?",
    "answer": "RotateE",
    "rationale": "The figure shows the MRR (Mean Reciprocal Rank) for different models on two datasets. The RotateE model has the highest MRR on the FB15k-237 dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.01849v1",
    "pdf_url": null
  },
  {
    "instance_id": "71d4ffe9b92641ecad9b8357bb3178a4",
    "figure_id": "2306.01650v1-Figure9-1",
    "image_file": "2306.01650v1-Figure9-1.png",
    "caption": " Precision-recall graph for anonymous users.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best for anonymous users?",
    "answer": "ORES",
    "rationale": "The ORES line is the highest on the graph, indicating that it has the highest precision for any given recall value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01650v1",
    "pdf_url": null
  },
  {
    "instance_id": "b6bb7db821e24d279400cd55d4d609d2",
    "figure_id": "1811.08996v1-Figure5-1",
    "image_file": "1811.08996v1-Figure5-1.png",
    "caption": " HyperAdam performs best compared with other optimizers on neural networks with different structures.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performs best on the 7-hidden-layer MLP with MNIST data?",
    "answer": "DMoptimizer",
    "rationale": "Subfigure (b) shows the loss curves for different optimizers on the 7-hidden-layer MLP with MNIST data. The DMoptimizer curve has the lowest final loss value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.08996v1",
    "pdf_url": null
  },
  {
    "instance_id": "84fb2baedbc74663b25d635373714219",
    "figure_id": "2308.12964v1-Figure5-1",
    "image_file": "2308.12964v1-Figure5-1.png",
    "caption": " Comparison with different layout-guided text-to-image methods. While MAS [19] and SpaText [2] are specifically trained for layout control, SD-Pww [4] and DenseDiffusion (ours) are training-free methods based on pre-trained Stable Diffusion models [46]. Nevertheless, our results adhere to layout conditions comparably to SpaText [2], and even outperform MAS [19] in many cases.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is able to generate images that are most consistent with the text prompt?",
    "answer": "Dense Diffusion.",
    "rationale": "The figure shows that Dense Diffusion is able to generate images that are most consistent with the text prompt, even though it is a training-free method. For example, in the first row, the image generated by Dense Diffusion is the only one that shows a red car in front of a snowy mountain.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.12964v1",
    "pdf_url": null
  },
  {
    "instance_id": "03487cb0a2b743788d2dd00cbfdfef73",
    "figure_id": "2304.03283v1-Figure8-1",
    "image_file": "2304.03283v1-Figure8-1.png",
    "caption": " Visualizations on inpainting of three different algorithms. Generations of DiffMAE have finer details than MAE and are more semantically meaningful than RePaint.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three algorithms produces the most realistic images?",
    "answer": "DiffMAE",
    "rationale": "The figure shows that DiffMAE is able to generate images with finer details and that are more semantically meaningful than the other two algorithms. This is evident in the images of the dog, monkey, and shower cap, where DiffMAE is able to accurately reproduce the details of the original images, such as the fur on the dog, the wrinkles on the monkey's face, and the folds in the shower cap.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.03283v1",
    "pdf_url": null
  },
  {
    "instance_id": "03f2e5f7544743ae880cb534971b8ffb",
    "figure_id": "2004.14648v3-Figure1-1",
    "image_file": "2004.14648v3-Figure1-1.png",
    "caption": " A typical example on adversarial SQuAD. By breaking the question and context down into smaller units, we can expose the incorrect entity match and use explicit constraints to fix it. The solid lines denote edges from SRL and coreference, and the dotted lines denote the possible alignments between the arguments (desired in red, actual in black).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the correct answer to the question \"What day was Super Bowl 50 played on?\"",
    "answer": "February 7, 2016.",
    "rationale": "The figure shows that the sentence \"The game was played on February 7, 2016...\" is the correct answer to the question \"What day was Super Bowl 50 played on?\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14648v3",
    "pdf_url": null
  },
  {
    "instance_id": "89b28240294948aeaf9b6eff4d44499b",
    "figure_id": "1903.01344v3-Figure6-1",
    "image_file": "1903.01344v3-Figure6-1.png",
    "caption": " Results of the experiments on the four tasks. Left column: Success rate of each method during training. Right column: Mean episode reward of each method during training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best on the Chase and Attack task?",
    "answer": "H-PPO",
    "rationale": "The success rate of H-PPO is higher than the other algorithms in the Chase and Attack task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.01344v3",
    "pdf_url": null
  },
  {
    "instance_id": "cef5e3151e0d47769b6590903f8257f9",
    "figure_id": "2210.03022v3-Figure4-1",
    "image_file": "2210.03022v3-Figure4-1.png",
    "caption": " Test-time results for SAF, MAPPO and IPPO on TeamTogether, TeamSupport and KeyForTreasure environments on varying levels of heterogeneity. The coordination level is fixed at 1. All algorithms show decreased performance as heterogeneity increases. SAF shows better performance in more cases.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best in the KeyForTreasure environment with high heterogeneity?",
    "answer": "SAF",
    "rationale": "The box plots for SAF are generally higher than those for MAPPO and IPPO in the KeyForTreasure environment, particularly at higher levels of heterogeneity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.03022v3",
    "pdf_url": null
  },
  {
    "instance_id": "37da8ea71fca48a6b8232ebc0388997f",
    "figure_id": "1908.10703v1-Figure2-1",
    "image_file": "1908.10703v1-Figure2-1.png",
    "caption": " Distribution of gender.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What emotion is most likely to be expressed by a female?",
    "answer": "Happiness",
    "rationale": "The bar for happiness is higher for females than for any other emotion.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.10703v1",
    "pdf_url": null
  },
  {
    "instance_id": "4b5ddcce205e4f3cbfff60600f610b66",
    "figure_id": "1906.12086v1-Figure3-1",
    "image_file": "1906.12086v1-Figure3-1.png",
    "caption": " Parameters safe set in light blue and unsafe set in white. The top row shows the safe set when the context is 0◦C and time is 0 and 25 days. The bottom row shows the final safe set for −5◦C and +5◦C. The initial PI parameters are shown with a black circle while the final optimized parameters with a black star.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of increasing the temperature from -5°C to 5°C on the safe set of parameters?",
    "answer": "The safe set of parameters decreases in size.",
    "rationale": "The safe set is shown in light blue. In the bottom row, the safe set for -5°C is larger than the safe set for 5°C.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.12086v1",
    "pdf_url": null
  },
  {
    "instance_id": "612d0c944608441bbafcf8deb89d21b8",
    "figure_id": "1905.13649v6-Figure2-1",
    "image_file": "1905.13649v6-Figure2-1.png",
    "caption": " CDF of (a) GS and (b) RCS for YelpNYC. The more the distance of a CDF (corresponding to a method) from y-axis, the better the performance of the method.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best according to the GS metric?",
    "answer": "DeFrauder",
    "rationale": "The CDF of DeFrauder is the furthest from the y-axis in (a), indicating that it has the best performance according to the GS metric.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.13649v6",
    "pdf_url": null
  },
  {
    "instance_id": "22bedf736e06414592a8e170bf90362c",
    "figure_id": "2106.04927v3-Figure3-1",
    "image_file": "2106.04927v3-Figure3-1.png",
    "caption": " Generalization result on TPC-H dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models performs the best on the TPC-H dataset with a problem size of 100?",
    "answer": "PPO-BiHyb (TPC-H 100 train)",
    "rationale": "The figure shows that the PPO-BiHyb (TPC-H 100 train) model has the highest improvement with respect to the critical path for a problem size of 100.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04927v3",
    "pdf_url": null
  },
  {
    "instance_id": "0c50b93d5df84adfa9846a5271d3c0f5",
    "figure_id": "2212.13738v2-Figure9-1",
    "image_file": "2212.13738v2-Figure9-1.png",
    "caption": " Visualization on background change.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which step in the stir fry recipe involves adding vegetables to the wok?",
    "answer": "Step 6",
    "rationale": "The figure shows a series of steps for making stir fry. Step 6 is \"add vegetables to the wok and stir.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.13738v2",
    "pdf_url": null
  },
  {
    "instance_id": "c21a976440ba4b058d7e98505ee586d8",
    "figure_id": "2103.08490v2-Figure10-1",
    "image_file": "2103.08490v2-Figure10-1.png",
    "caption": " Average F1 gains over the baseline on QA tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on QA tasks when τ=2?",
    "answer": "mBERT",
    "rationale": "The figure shows the average F1 gains over the baseline on QA tasks for different values of τ. When τ=2, mBERT has the highest F1 gain.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.08490v2",
    "pdf_url": null
  },
  {
    "instance_id": "42ccb6b8cadf457099c6fca0153309f8",
    "figure_id": "2106.07880v2-Figure2-1",
    "image_file": "2106.07880v2-Figure2-1.png",
    "caption": " Test accuracy of: (a) approximate NTK methods (GRADRF, NTKSKETCH and NTKRF) on MNIST and (b) approximate CNTK methods (GRADRF and CNTKSKETCH) on CIFAR-10.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods (GRADRF, NTKSKETCH, NTKRF) has the highest test accuracy on MNIST?",
    "answer": "NTKRF.",
    "rationale": "The test accuracy for NTKRF is highest for most of the feature dimensions and for all wall-clock times shown in the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07880v2",
    "pdf_url": null
  },
  {
    "instance_id": "67ec5448149b40f285449ee6fdb0ea46",
    "figure_id": "2205.06440v2-Figure6-1",
    "image_file": "2205.06440v2-Figure6-1.png",
    "caption": " (a)-(d) show the effect of _𝑉𝐿 , _𝑉𝐺 , 𝐾 , and 𝐷 on model performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which hyperparameter has the most significant impact on model performance?",
    "answer": "D",
    "rationale": "The figure shows that increasing the value of D leads to a significant increase in NDCG@5 for all models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.06440v2",
    "pdf_url": null
  },
  {
    "instance_id": "41035936e9c64ed08843811abb5da5b3",
    "figure_id": "2202.09082v1-Figure3-1",
    "image_file": "2202.09082v1-Figure3-1.png",
    "caption": " AB preference test results with 95% confidence intervals for different combinations of phoneme duration and 𝐹0, where ‘GG’ denotes Ground-truth duration and Ground-truth F0, ‘GP’ denotes Ground-truth duration and Predicted F0, and ‘PP’ denotes Predicted duration and Predicted F0.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which combination of phoneme duration and F0 resulted in the highest percentage of participants preferring option B?",
    "answer": "Ground-truth duration and predicted F0 (GP)",
    "rationale": "The figure shows that for all the combinations tested, GP had the highest percentage of participants preferring option B. This is evident from the orange bars in the figure, which represent the percentage of participants who preferred option B.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.09082v1",
    "pdf_url": null
  },
  {
    "instance_id": "d103339e326646b4bf3b01d49ec5bcf4",
    "figure_id": "2106.03039v3-Figure4-1",
    "image_file": "2106.03039v3-Figure4-1.png",
    "caption": " Regret comparison on Mnist+NotMnist with 𝐻2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest regret on the Mnist dataset?",
    "answer": "LinUCB",
    "rationale": "The figure shows that the LinUCB line is consistently below the other lines, indicating that it has the lowest regret.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03039v3",
    "pdf_url": null
  },
  {
    "instance_id": "c982932c69a44c2d80dbcef007f9c990",
    "figure_id": "1909.03341v2-Figure3-1",
    "image_file": "1909.03341v2-Figure3-1.png",
    "caption": " The numbers of languages symbols have shared across Ar, He, Ru, Ko and It (from X-En). Note that these languages have mutually different character sets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of symbols are shared by exactly two languages according to the Char model?",
    "answer": "Approximately 40%.",
    "rationale": "The Char model is represented by the pink bars in the figure. The height of the bar at x=2 corresponds to the percentage of symbols shared by exactly two languages. This height is approximately 40%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.03341v2",
    "pdf_url": null
  },
  {
    "instance_id": "021950f4dcc341d6a20293948f58fb5f",
    "figure_id": "1911.10699v1-Figure5-1",
    "image_file": "1911.10699v1-Figure5-1.png",
    "caption": " Impact of latent components numbers on three real datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the best performance in terms of RMSE and MAE?",
    "answer": "MovieLens",
    "rationale": "The figure shows the RMSE and MAE for three different datasets: Yelp, Amazon, and MovieLens. The MovieLens dataset has the lowest RMSE and MAE, indicating that it has the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.10699v1",
    "pdf_url": null
  },
  {
    "instance_id": "7f4af056193b4c36a419b08c00006ce3",
    "figure_id": "2212.09686v2-Figure8-1",
    "image_file": "2212.09686v2-Figure8-1.png",
    "caption": " ALC scores on WMT’14 De→En. Setup is the same as in Fig. 3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which initialization method resulted in the highest BLEU ALC score?",
    "answer": "Unigram Init",
    "rationale": "The figure shows the BLEU ALC scores for different initialization methods. The Unigram Init method has the highest score, as indicated by the blue dot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.09686v2",
    "pdf_url": null
  },
  {
    "instance_id": "eae895b1c9344f54ace16ec800967dfb",
    "figure_id": "2205.11713v3-Figure11-1",
    "image_file": "2205.11713v3-Figure11-1.png",
    "caption": " Example run of Thalamus on split MNIST tasks. We provide an example where the 5th task ”task 3” was not seen through out the training sequence but performance on it increases after the model forms latent representations of the other tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task is the model performing best on?",
    "answer": "Task 1.",
    "rationale": "The plot in panel B shows the accuracy of the model on each task. Task 1 has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.11713v3",
    "pdf_url": null
  },
  {
    "instance_id": "5ad6483246eb45cfafab80f23b8c9d25",
    "figure_id": "2104.08200v3-Figure4-1",
    "image_file": "2104.08200v3-Figure4-1.png",
    "caption": " Id→Jv machine translation tasks’ human evaluation metrics summary for the baseline models on fluency (top left, 5 is best), adequacy (top right, 5 is best) and rank (bottom, 1 is best).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest median fluency score?",
    "answer": "Ground Truth",
    "rationale": "The Ground Truth violin plot for fluency is the highest on the graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.08200v3",
    "pdf_url": null
  },
  {
    "instance_id": "52b00fd553784e2d8a89f72e85ddd06b",
    "figure_id": "2106.05933v2-Figure129-1",
    "image_file": "2106.05933v2-Figure129-1.png",
    "caption": " Sparsity over layers for wav2vec-base finetuned for Mandarin zh-TW at 80% sparsity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the sparsity of the zh_TW_bert_0.8_mask layer?",
    "answer": "75.777%",
    "rationale": "The sparsity of the zh_TW_bert_0.8_mask layer is shown in the figure as 75.777%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05933v2",
    "pdf_url": null
  },
  {
    "instance_id": "bdd0a1e1b9214b768ea9fa6135c563cc",
    "figure_id": "1804.03429v2-Figure2-1",
    "image_file": "1804.03429v2-Figure2-1.png",
    "caption": " Samples on the MNIST dataset. The results of (a) are comparable to those reported in [8]. The mixture k is fixed in each column of (b) and (c). k is fixed in each row of (d), which is from [7].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model produces the most diverse set of digits?",
    "answer": "GAN-G.",
    "rationale": "Each of the other models tends to produce clusters of similar digits, while GAN-G produces a more varied set of digits. This can be seen in the figure, where each column of images represents the output of a different model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.03429v2",
    "pdf_url": null
  },
  {
    "instance_id": "08ae3fc51e0249939c7e0d6a90301477",
    "figure_id": "2106.03849v2-Figure15-1",
    "image_file": "2106.03849v2-Figure15-1.png",
    "caption": " Log-likelihood & KL comparison across view-supervised models. We show five independent runs of GQN and NeRF-VAE for three GECO log-likelihood thresholds each. For SIMONe-VS, we show three independent runs (trained as usual without GECO). Note that SIMONeVS’s KL shown here is the sum (not average) over K object latents.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of reconstruction log-likelihood and KL divergence?",
    "answer": "SIMONe-VS",
    "rationale": "The figure shows that SIMONe-VS has the highest reconstruction log-likelihood and the lowest KL divergence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03849v2",
    "pdf_url": null
  },
  {
    "instance_id": "15d54e7e0dfc4fd0b5d6a88515ef4c81",
    "figure_id": "2012.08791v2-Figure2-1",
    "image_file": "2012.08791v2-Figure2-1.png",
    "caption": " Transform time for MiniRocket versus Rocket",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the transform time of MiniRocket and Rocket?",
    "answer": "MiniRocket is generally faster than Rocket.",
    "rationale": "The figure shows that the transform time for MiniRocket is generally less than the transform time for Rocket for a given number of dimensions. The data points for MiniRocket are mostly below the diagonal line, which indicates that MiniRocket is faster. The figure also shows two dashed lines, one indicating where MiniRocket is 10x faster and another indicating where MiniRocket is 100x faster than Rocket.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.08791v2",
    "pdf_url": null
  },
  {
    "instance_id": "4f61da11f0024c3499907ee167f06903",
    "figure_id": "2104.07611v2-Figure7-1",
    "image_file": "2104.07611v2-Figure7-1.png",
    "caption": " Comparing mention detection accuracy on test set for different active learning strategies across reading/labeling configurations. The plots are formatted in the same way as Figure 6. Generally, mention detection improves most from ment-ent sampling.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which active learning strategy performs the best for mention detection?",
    "answer": "Ment-ent sampling.",
    "rationale": "The plots show that the mention detection accuracy is highest for the ment-ent sampling strategy, regardless of the number of spans, documents, or datasets used.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.07611v2",
    "pdf_url": null
  },
  {
    "instance_id": "e156ca2a40c94881a6511d7b3587e9bc",
    "figure_id": "2307.16715v2-Figure7-1",
    "image_file": "2307.16715v2-Figure7-1.png",
    "caption": " Visualization of Joint moment retrieval and highlight detection on (a) QVHighlights, and Moment Retrieval on (b) Charades-STA, (c) Ego4D, (d) TACoS. Textual queries are mostly natural sentences.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the task that the woman in the image is performing?",
    "answer": "The woman is cleaning up after preparing the kiwi.",
    "rationale": "The image shows a sequence of frames from a video, and the caption states that the woman is cleaning up after preparing the kiwi.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.16715v2",
    "pdf_url": null
  },
  {
    "instance_id": "fcf0c65b626042aea7da3090c26814a9",
    "figure_id": "2106.02105v2-Figure1-1",
    "image_file": "2106.02105v2-Figure1-1.png",
    "caption": " Targeted transfer attack success rate against ImageNet classifiers using adversarial examples optimized with respect to ε-robust ResNet50 source models. Success rate is the fraction of adversarial examples classified as their adversarial target by the destination network. Higher is a more successful attack. Baseline refers the rate at which unperturbed images are classified as the target class. (Best viewed in color.)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which destination network is the most robust to targeted transfer attacks when the source model is an ε-robust ResNet50 with a maximum perturbation size of 24?",
    "answer": "CLIP",
    "rationale": "The figure shows the targeted transfer attack success rate against different destination networks for different maximum perturbation sizes and source model ε-robustness. The success rate for CLIP is the lowest for all perturbation sizes, indicating that it is the most robust to targeted transfer attacks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02105v2",
    "pdf_url": null
  },
  {
    "instance_id": "5c4eed9aa2ea47b5abd4ac216622fee2",
    "figure_id": "1912.03582v1-Figure2-1",
    "image_file": "1912.03582v1-Figure2-1.png",
    "caption": " Synthetic experiments on Gaussian data (noisy dimensions are uniform in [−2, 2]). For clarity, we split the results into two figures. y−axis measures how many of the 100 true anomalies were reported by the algorithm in the top 100 anomalies.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best when the noise dimension is 18?",
    "answer": "PIDForest.",
    "rationale": "The figure shows the accuracy of different algorithms for different noise dimensions. When the noise dimension is 18, PIDForest has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.03582v1",
    "pdf_url": null
  },
  {
    "instance_id": "9a4e263916014a47a97f2b56b431361e",
    "figure_id": "2003.13549v3-Figure3-1",
    "image_file": "2003.13549v3-Figure3-1.png",
    "caption": " Histogram of the variance along the depth axis of filter kernels which can be explained using only one principal component per filter. The filters are grouped by convolution stages (stage 1: blue, stage 2: orange, stage 3: green, stage 4: red). These quantitative results show that a large portion of CNN filters can potentially be represented using our BSConv formulation. This figure is best viewed in color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three architectures has the most filters that can be represented using only one principal component?",
    "answer": "ResNet-50",
    "rationale": "The figure shows that ResNet-50 has the highest filter count for each color, which represents the different convolution stages. This means that it has the most filters that can be represented using only one principal component.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.13549v3",
    "pdf_url": null
  },
  {
    "instance_id": "06ff50322b6b4670bd123c4a0a16e962",
    "figure_id": "2210.13542v3-Figure2-1",
    "image_file": "2210.13542v3-Figure2-1.png",
    "caption": " Demonstration of differentiable planners with algorithmic differentiation, which cannot scale up to large tasks or more iterations, due to coupled forward and backward pass.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the task of size 49 x 49?",
    "answer": "SymVIN",
    "rationale": "The figure shows the success rate of different models on different tasks. The task of size 49 x 49 is shown in the third column of the figure. The SymVIN model has the highest success rate for this task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.13542v3",
    "pdf_url": null
  },
  {
    "instance_id": "b64d7e7fdbf14638b93f1d62627fee00",
    "figure_id": "2104.09283v1-Figure9-1",
    "image_file": "2104.09283v1-Figure9-1.png",
    "caption": " Comparison of proposed method against state-of-the-art single image shape estimation methods - GT is ground-truth and DH is DeepHuman. Differences are highlighted in red circle.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate 3D human pose estimation results?",
    "answer": "The proposed method.",
    "rationale": "The figure shows the results of different methods for estimating 3D human pose from a single image. The proposed method is shown to produce results that are closest to the ground truth (GT), as indicated by the red circles highlighting the differences between the estimated poses and the GT poses.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.09283v1",
    "pdf_url": null
  },
  {
    "instance_id": "923765c05d6440cd8a2749f63d4ce7b9",
    "figure_id": "1808.02651v2-Figure10-1",
    "image_file": "1808.02651v2-Figure10-1.png",
    "caption": " A quantitative comparison using parametric norm-balls shows the fact that adversarial lighting/geometry perturbations have a higher success rate (%) in fooling classifiers comparing to random perturbations in the parametric spaces.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of perturbation is more effective in fooling classifiers: adversarial lighting or random lighting?",
    "answer": "Adversarial lighting.",
    "rationale": "The figure shows that adversarial lighting perturbations have a success rate of 61.0% in fooling classifiers, while random lighting perturbations have a success rate of only 9.1%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.02651v2",
    "pdf_url": null
  },
  {
    "instance_id": "1f40c5cc189f48879804f43e1a0592bb",
    "figure_id": "2108.01368v2-Figure5-1",
    "image_file": "2108.01368v2-Figure5-1.png",
    "caption": " Average test SSIM in various scenarios, across a range of acceleration factors R. Higher R indicates a smaller number of acquired measurements. Our approach mostly shows the best performance and lowest reconstruction variance both in- and out-of-distribution at test-time. Shaded regions indicate 95% confidence intervals. Note that we trained baselines on MVUE images and hence these numerical values should not be compared with those in literature trained on RSS images (see Appendix A.1 for a more detailed discussion).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach achieves the best performance in terms of average test SSIM in various scenarios?",
    "answer": "The Langevin approach.",
    "rationale": "The Langevin approach has the highest SSIM across all R values and scenarios, as shown by the blue line with markers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.01368v2",
    "pdf_url": null
  },
  {
    "instance_id": "7cd7ce0ab03d492aadb7992b04e144ae",
    "figure_id": "1811.12104v4-Figure10-1",
    "image_file": "1811.12104v4-Figure10-1.png",
    "caption": " Targets’ saliency of RefGTA, RefCOCO and RefCOCO (human) calculated as Fig. 5. As the saliency becomes higher, the ratio of human instances becomes larger in RefCOCO.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest proportion of instances with high saliency values?",
    "answer": "RefCOCO (human)",
    "rationale": "The figure shows the distribution of saliency values for three datasets. The RefCOCO (human) dataset has the highest proportion of instances with high saliency values, as its distribution is skewed towards the right.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.12104v4",
    "pdf_url": null
  },
  {
    "instance_id": "0e502a4710fa49838fcddf44ad9436e1",
    "figure_id": "1910.07475v3-Figure4-1",
    "image_file": "1910.07475v3-Figure4-1.png",
    "caption": " XLM F1 score stratified by English difficulty",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the highest total F1 score?",
    "answer": "English",
    "rationale": "The blue bars represent the total F1 score for each language. The blue bar for English is the tallest, indicating that it has the highest total F1 score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.07475v3",
    "pdf_url": null
  },
  {
    "instance_id": "1a0198c645cf4e7a98b24e465d941009",
    "figure_id": "1910.03206v2-Figure1-1",
    "image_file": "1910.03206v2-Figure1-1.png",
    "caption": " Temporal distribution of videos in V .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "During what month did the number of videos in V increase the most?",
    "answer": "September 2017",
    "rationale": "The figure shows a sharp increase in the number of videos in V during September 2017, with the number of videos peaking at over 1100. This is significantly higher than the number of videos in any other month.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.03206v2",
    "pdf_url": null
  },
  {
    "instance_id": "8cc6cb04fae44a03a91564aec5625521",
    "figure_id": "2110.02871v1-Figure14-1",
    "image_file": "2110.02871v1-Figure14-1.png",
    "caption": " Bootstrapped distribution of the 20 % trimmed means of the difference in edge coherence between models that included pseudo labels and their counterparts. Equivalent distributions were obtained for all other techniques and metrics in the ablation study.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the bootstrapped distribution, can we reject the null hypothesis that the difference in edge coherence between models that included pseudo labels and their counterparts is zero?",
    "answer": "Yes, we can reject the null hypothesis.",
    "rationale": "The p-value is 0.0039, which is less than the typical significance level of 0.05. This means that there is less than a 5% chance of observing the data if the null hypothesis were true. Therefore, we can reject the null hypothesis and conclude that there is a significant difference in edge coherence between models that included pseudo labels and their counterparts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02871v1",
    "pdf_url": null
  },
  {
    "instance_id": "69de908c2a89450b94c79f41a193e97c",
    "figure_id": "2206.04763v2-Figure6-1",
    "image_file": "2206.04763v2-Figure6-1.png",
    "caption": " MSE (y-axis) after epochs of training (x-axis), where NBD performs best in the symmetric (left) and asymmetric (center, right) Bregman learning tasks. We see Mahalanobis performs well in the symmetric task (left) since it is correctly specified for the ground truth, but performs relatively poorly in the asymmetric case (right). Deep-div is unable to learn effectively in either (not shown on left because the error was too high).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which normalization technique performs best in the symmetric Bregman learning task?",
    "answer": "NBD",
    "rationale": "The left plot shows the MSE after epochs of training for the symmetric Bregman learning task. NBD has the lowest MSE of all the techniques, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.04763v2",
    "pdf_url": null
  },
  {
    "instance_id": "03fe16b62bb94ada9e93df71faa6948d",
    "figure_id": "1809.03036v5-Figure2-1",
    "image_file": "1809.03036v5-Figure2-1.png",
    "caption": " Long-term motion synthesis on walking activity on test sequence. Snapshots are shown at 160, 560, 1000, 2000 and 4000 milliseconds (from top-to-bottom) along the prediction time-axis. We see that GRU-d and VTLN-GRU-d are qualitatively closer to the ground-truth sequence than MBR-long and VTLN-GRU-ac.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is qualitatively closest to the ground-truth sequence?",
    "answer": "GRU-d and VTLN-GRU-d.",
    "rationale": "The figure shows that the poses predicted by GRU-d and VTLN-GRU-d are more similar to the ground-truth poses than the poses predicted by the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.03036v5",
    "pdf_url": null
  },
  {
    "instance_id": "60b47039feff464b853324439154b45f",
    "figure_id": "2205.15301v1-Figure20-1",
    "image_file": "2205.15301v1-Figure20-1.png",
    "caption": " Impact of the selection of layers affected by INLP. Dots represented different languages, the squares indicate the mean %.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the highest average percentage of PIE per layer affected by INLP?",
    "answer": "German (de)",
    "rationale": "The figure shows the average percentage of PIE per layer affected by INLP for different languages. The squares indicate the mean percentage for each language. The square for German is the highest, indicating that it has the highest average percentage of PIE per layer affected by INLP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.15301v1",
    "pdf_url": null
  },
  {
    "instance_id": "a3510eacd0e64019846db20bbefb6b51",
    "figure_id": "2011.09011v2-Figure1-1",
    "image_file": "2011.09011v2-Figure1-1.png",
    "caption": " Comparison of AttentiveNAS with prior NAS approaches [3, 10, 35, 36, 43] on ImageNet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which NAS approach has the highest Top-1 validation accuracy?",
    "answer": "AttentiveNAS-A6",
    "rationale": "The figure shows that AttentiveNAS-A6 has the highest Top-1 validation accuracy among all the NAS approaches compared.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.09011v2",
    "pdf_url": null
  },
  {
    "instance_id": "e68036e569e84892ab29b2249d373fb1",
    "figure_id": "2106.06959v5-Figure3-1",
    "image_file": "2106.06959v5-Figure3-1.png",
    "caption": " Qualitative Robustness Test on the W-space of the StyleGAN2 (Karras et al., 2020b) trained on FFHQ. Each traversal image is generated by the linear traversal on W except for (d) under the strong perturbation intensity I of up to 12. The intensity is linearly increased from 0 to 12 for each column. We infer the deterioration of the traversal image along the global method is due to the escape of the latent traversal from the latent manifold. (See the appendix for the additional Robustness Test results along the first 10 components of Local Basis.)",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods produces the most robust traversals?",
    "answer": "Iterative Curve-Traversal (Ours)",
    "rationale": "The figure shows that the traversals produced by Iterative Curve-Traversal (Ours) are the most robust to perturbations. This is because the latent traversal does not escape the latent manifold. The other three methods produce traversals that become increasingly distorted as the perturbation intensity increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.06959v5",
    "pdf_url": null
  },
  {
    "instance_id": "922237f0ba5d4e9eb6be2c76971ac3fc",
    "figure_id": "2204.01016v1-Figure22-1",
    "image_file": "2204.01016v1-Figure22-1.png",
    "caption": " UAS for English (en). Note the kink in the y-axis and the different scales of the two halves.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in round 2 and round 3 according to the UAS metric?",
    "answer": "MonoA[es]",
    "rationale": "The figure shows the UAS metric for different methods across different rounds. In round 2 and round 3, the MonoA[es] method has the highest UAS values compared to the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.01016v1",
    "pdf_url": null
  },
  {
    "instance_id": "b03a6164a966448cae99f7495cccb949",
    "figure_id": "1910.00359v3-Figure5-1",
    "image_file": "1910.00359v3-Figure5-1.png",
    "caption": " For reference we record the test accuracy of all models from 1 in the left plot and the relative change in parameters in the right plot.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest test accuracy?",
    "answer": "WideResNet 16-x",
    "rationale": "The left plot shows the test accuracy of each model as a function of the number of parameters. The WideResNet 16-x model has the highest test accuracy, which is approximately 75%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.00359v3",
    "pdf_url": null
  },
  {
    "instance_id": "024b717a8d4d4c9cb9730ee12dacb6b1",
    "figure_id": "2201.08207v2-Figure1-1",
    "image_file": "2201.08207v2-Figure1-1.png",
    "caption": " Initially, each node is occupied by a resident (red), except for a seed set S of nodes occupied by mutants (blue). In each step of the voter process with bias r, one random node adopts the type of one of its neighbors, with mutants having a relative (dis)advantage r.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In the figure, what is the type of the node labeled \"?\".",
    "answer": "The type of the node labeled \"?\" is unknown.",
    "rationale": "The figure shows a network of nodes, with some nodes labeled \"r\" and others labeled \"1\". The node labeled \"?\" is not labeled with either \"r\" or \"1\", so its type is unknown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.08207v2",
    "pdf_url": null
  },
  {
    "instance_id": "1c739494cd8444cba90a38839cae7fec",
    "figure_id": "2105.14376v1-Figure4-1",
    "image_file": "2105.14376v1-Figure4-1.png",
    "caption": " Histograms of the spatially-average amounts of hierarchical structural artifacts on CelebA-HQ. We observe clear margins between distributions of real and fake (ProGAN or StyleGAN).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method generates images with the least amount of artifacts?",
    "answer": "Real images have the least amount of artifacts.",
    "rationale": "The histograms show that the distribution of real images has the lowest values, which indicates that they have the least amount of artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14376v1",
    "pdf_url": null
  },
  {
    "instance_id": "f60735a0d5bc4cf5bbd99a5f324a1af4",
    "figure_id": "1902.00448v2-Figure16-1",
    "image_file": "1902.00448v2-Figure16-1.png",
    "caption": " Neural architecture search experiment.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest minimum NAS?",
    "answer": "COMBO",
    "rationale": "The table on the right shows the minimum NAS for each method. COMBO has the lowest minimum NAS of 0.1846 ± 0.0005.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.00448v2",
    "pdf_url": null
  },
  {
    "instance_id": "2a7a65200f764e608db0436df6624a0a",
    "figure_id": "2112.04137v2-Figure3-1",
    "image_file": "2112.04137v2-Figure3-1.png",
    "caption": " Analytical experiments conducted on W→A (Office-31) based on DANN. (a): Visualization of optimization paths of different optimization schemes. (b): Objective scale sensitivity analysis of ParetoDA by varying the scale factors of objectives.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization scheme achieves the best trade-off between domain alignment loss and source classification loss?",
    "answer": "ParetoDA (Ideal)",
    "rationale": "The Pareto Front in Figure (a) shows the optimal trade-off between the two objectives. The ParetoDA (Ideal) curve is closest to the Pareto Front, indicating that it achieves the best trade-off.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.04137v2",
    "pdf_url": null
  },
  {
    "instance_id": "bdee7dedfbfb464c8d26879af55c0843",
    "figure_id": "2007.11503v2-Figure20-1",
    "image_file": "2007.11503v2-Figure20-1.png",
    "caption": " Experimental results of OFDM frequency bins in polarization mismatch setup.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which subcarrier IDs experienced the greatest increase in channel gain when the metasurface was used?",
    "answer": "Subcarrier IDs 1, 2, and 3.",
    "rationale": "The red bars in the plot represent the channel gain with the metasurface, while the blue bars represent the channel gain without the metasurface. The greatest difference between the red and blue bars occurs at subcarrier IDs 1, 2, and 3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.11503v2",
    "pdf_url": null
  },
  {
    "instance_id": "76310d32569844b69b67922b0f1ce73b",
    "figure_id": "2103.06643v2-Figure8-1",
    "image_file": "2103.06643v2-Figure8-1.png",
    "caption": " Accuracy/loss vs. training epoch on Pascal VOC. As the training goes, the loss functions try to drag the output to binary and the local minimum with bad properties makes the cross-entropytype loss explosion. Since the accuracy will be very close to 0 after gradient explosion, we truncate the descending curves and keep them unchanged for better visualization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of accuracy?",
    "answer": "Acc-F",
    "rationale": "The plot on the left shows the accuracy of three different models (Acc-F, Acc-H, and Acc-P) over time. The Acc-F curve is consistently higher than the other two curves, indicating that it has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.06643v2",
    "pdf_url": null
  },
  {
    "instance_id": "e2d9870ae3c844b5ab4e9355bb2f190b",
    "figure_id": "2011.12328v1-Figure17-1",
    "image_file": "2011.12328v1-Figure17-1.png",
    "caption": " Running average accuracy of individual tasks after training for the top 5 approaches on Easy-CHASY",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach performs the best on average across all tasks?",
    "answer": "GVCL-F",
    "rationale": "The figure shows the running average accuracy of the top 5 approaches on Easy-CHASY. GVCL-F has the highest average accuracy across all tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.12328v1",
    "pdf_url": null
  },
  {
    "instance_id": "4c6c86aa20344e5a9012790d28ff6cea",
    "figure_id": "2304.05868v1-Figure8-1",
    "image_file": "2304.05868v1-Figure8-1.png",
    "caption": " Texture transfer from real images (ScanNet, CompCars) to arbitrary shape geometry of the same class category (ShapeNet). Under this challenging scenario, our NOCguided patch optimization enables plausible texturing for image queries.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most realistic textures?",
    "answer": "Our method.",
    "rationale": "The figure shows the results of different texture transfer methods. The textures produced by our method are the most realistic, as they are closest to the textures of the real images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.05868v1",
    "pdf_url": null
  },
  {
    "instance_id": "3872871f8ada493d949f5d2daa66997f",
    "figure_id": "2010.02808v2-Figure2-1",
    "image_file": "2010.02808v2-Figure2-1.png",
    "caption": " Learning from the natural hierarchy present in the videos. Each video in a dataset consists of multiple shots (indicated in the gray boxes), each shot consists of multiple frames. This hierarchy can be used to formulate a contrastive loss for learning image representations [68] (cf. Section 3). We extend this hierarchy to the object level by using an off-the-shelf detector.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How are shots and frames related in a video?",
    "answer": "Shots are made up of multiple frames.",
    "rationale": "The figure shows that each shot consists of multiple frames. The frames are the individual images that make up the video, and the shots are the sequences of frames that are grouped together.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02808v2",
    "pdf_url": null
  },
  {
    "instance_id": "f0d7e43d335a4b17b8a964c0c2decebb",
    "figure_id": "2010.06986v2-Figure17-1",
    "image_file": "2010.06986v2-Figure17-1.png",
    "caption": " Results on the COMPAS Recidivism dataset with African American as the protected group.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of $\\theta$ minimizes the underranking metric for both nDCG at top 20 and top 100 ranks?",
    "answer": "$\\theta=0.05$",
    "rationale": "The figure shows that the underranking metric is minimized at $\\theta=0.05$ for both nDCG at top 20 and top 100 ranks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.06986v2",
    "pdf_url": null
  },
  {
    "instance_id": "a9992a02b01d4171b9d20fc2fe013135",
    "figure_id": "2010.09208v2-Figure3-1",
    "image_file": "2010.09208v2-Figure3-1.png",
    "caption": " MAB vs. PDTool total end-to-end workload time for static workloads.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tool is the fastest for the TPC-H Skew workload?",
    "answer": "NoIndex",
    "rationale": "The figure shows that NoIndex has the shortest bar for the TPC-H Skew workload.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.09208v2",
    "pdf_url": null
  },
  {
    "instance_id": "91bcfd8eb34c4ef8a98714dcbbd26ab4",
    "figure_id": "2205.08221v1-Figure4-1",
    "image_file": "2205.08221v1-Figure4-1.png",
    "caption": " Distribution of summary lengths and compression ratios. The maximum frequencies of HC are cropped.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced summaries with the shortest lengths on average?",
    "answer": "HC (Hill Climbing)",
    "rationale": "The left plot shows the distribution of summary lengths for each method. The HC bars are taller on the left side of the plot, indicating that HC produced more summaries with shorter lengths than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.08221v1",
    "pdf_url": null
  },
  {
    "instance_id": "713ecf51ac6a4e68b79dc999a84d04fb",
    "figure_id": "2207.02098v3-Figure2-1",
    "image_file": "2207.02098v3-Figure2-1.png",
    "caption": " Performance curves on three tasks. The dashed vertical red line is the training range, meaning that sequences to the right have not been seen during training and thus measure generalization.",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three tasks shown in the figure do the models perform the best on?",
    "answer": "Parity Check.",
    "rationale": "The Parity Check task has the highest accuracy for all models, and the accuracy stays relatively high even for longer sequence lengths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.02098v3",
    "pdf_url": null
  },
  {
    "instance_id": "04082cbe4e684be594f5745ab732f52a",
    "figure_id": "1906.00588v5-Figure16-1",
    "image_file": "1906.00588v5-Figure16-1.png",
    "caption": " Comparison between NN and R+O-corrected outputs. Each dot represents a data point. The horizontal axis denote the original NN predictions, and the vertical axis the corresponding predictions after R+O corrections. The solid line indicates where NN predictions are same as R+O-corrected predictions (i.e., no change in output).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the most significant difference between the NN outputs and the R+O-corrected outputs?",
    "answer": "MSD",
    "rationale": "The MSD plot shows the largest deviation from the solid line, indicating that the R+O corrections made the most significant difference for this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.00588v5",
    "pdf_url": null
  },
  {
    "instance_id": "99db102f5b6c47328995254deb727326",
    "figure_id": "2006.09128v2-Figure2-1",
    "image_file": "2006.09128v2-Figure2-1.png",
    "caption": " Samples generated from various models by performing gradient ascent on random inputs (see § -5.1.2). While none of the generated samples are realistic, samples obtained from score-matched and gradient-norm regularized models are smoother and less noisy.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models generated the most realistic samples?",
    "answer": "None of the models generated realistic samples.",
    "rationale": "The caption states that \"none of the generated samples are realistic.\" This is evident from the figure, which shows that all of the samples are noisy and distorted.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.09128v2",
    "pdf_url": null
  },
  {
    "instance_id": "948331f253af4bb889b605efd793cdf6",
    "figure_id": "2005.04551v1-Figure8-1",
    "image_file": "2005.04551v1-Figure8-1.png",
    "caption": " Early stage or late stage where we can add epipolar transformer to the backbone model. (a) Hourglass networks [23] on InterHand. (b) ResNet-50 detector [37] on Human3.6M [13].",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two stages of the backbone model where the epipolar transformer can be added?",
    "answer": "The early stage and the late stage.",
    "rationale": "The figure shows two different backbone models, one with an hourglass architecture and the other with a ResNet-50 architecture. In both models, the epipolar transformer can be added at either the early stage (after the first two convolutional layers) or the late stage (before the final convolutional layer).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.04551v1",
    "pdf_url": null
  },
  {
    "instance_id": "d5a3ef4309da47f09221ee160a9122f9",
    "figure_id": "2110.13541v2-Figure1-1",
    "image_file": "2110.13541v2-Figure1-1.png",
    "caption": " Behavioral disparities in trivial attacks. They do not amplify the behavioral differences caused by quantization. [Left] On each of 40 pre-trained AlexNets, we add Gaussian noise to its parameters and measure the accuracy drop (0–13%). [Right] We construct 40 backdoored models and measure the difference in attack success rate caused by quantization (7–12%).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of quantization leads to the greatest decrease in accuracy for a clean model?",
    "answer": "4-bit quantization.",
    "rationale": "The left plot shows the change in accuracy for clean models with different types of quantization. The 4-bit quantization curve is shifted furthest to the right, indicating a greater decrease in accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.13541v2",
    "pdf_url": null
  },
  {
    "instance_id": "9cb9b7115a6f42778f32f5ce25c7d462",
    "figure_id": "2009.08330v3-Figure3-1",
    "image_file": "2009.08330v3-Figure3-1.png",
    "caption": " Relative score improvements against models with M-BERT embeddings for three tasks. Models are equipped with the CRF layer.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the NER task?",
    "answer": "The model with F + W embeddings.",
    "rationale": "The figure shows that the model with F + W embeddings has the highest relative score for the NER task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.08330v3",
    "pdf_url": null
  },
  {
    "instance_id": "da6cc60c1a644a61b24aa471872c1fdb",
    "figure_id": "2105.03664v1-Figure4-1",
    "image_file": "2105.03664v1-Figure4-1.png",
    "caption": " A screenshot of the web survey.",
    "figure_type": "**  \"other\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " How many different models are being compared in the survey? ",
    "answer": " 4 ",
    "rationale": " The survey instructions state that the participant will be rating slides generated by 4 different models. This is also evident in Task 2, where the participant is asked to sort the 4 models based on their preference. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.03664v1",
    "pdf_url": null
  },
  {
    "instance_id": "7d7153e382904426ac4761b01f596929",
    "figure_id": "2304.04952v1-Figure3-1",
    "image_file": "2304.04952v1-Figure3-1.png",
    "caption": " Probability distributions of CLS token Gradients varying the training steps. ViT-BIQA and DEIQT are models without and with the proposed decoder, respectively. By introducing the decoder, variations in the gradients decrease considerably faster than those without the decoder, indicating that the decoder can greatly improve training efficiency.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of introducing the decoder on the training efficiency of ViT-BIQA?",
    "answer": "The decoder greatly improves training efficiency.",
    "rationale": "The figure shows that the variations in the gradients of the CLS token decrease considerably faster for DEIQT (with decoder) than for ViT-BIQA (without decoder). This indicates that the decoder helps the model converge to a solution more quickly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.04952v1",
    "pdf_url": null
  },
  {
    "instance_id": "de66204e1e9947e99d48089bfd44edbf",
    "figure_id": "2309.17430v1-Figure18-1",
    "image_file": "2309.17430v1-Figure18-1.png",
    "caption": " Slices retrieved by FACTS for the airways class from NICO++90. Note that the dominant context for airways is outdoor.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the dominant context for airways, as shown in the image?",
    "answer": "Outdoor.",
    "rationale": "The image shows slices retrieved by FACTS for the airways class from NICO++90. The slices show a variety of outdoor scenes, such as mountains, forests, lakes, and deserts. This suggests that the dominant context for airways is outdoor.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.17430v1",
    "pdf_url": null
  },
  {
    "instance_id": "f4df104eb90443a6889847a08bcda385",
    "figure_id": "2110.09192v3-Figure3-1",
    "image_file": "2110.09192v3-Figure3-1.png",
    "caption": " Shaping Class-Conditional Inefficiency on CIFAR: Possible inefficiency reductions, in percentage change, per class (blue) and the impact on the overall, average inefficiency across classes (green). Left: Significant inefficiency reductions are possible for all classes on CIFAR10. Middle: The same strategy applies to groups of classes, e.g., “vehicles” vs “animals”, as well. Right: Similarly, on CIFAR100, we group classes by their coarse class (20 groups à 5 classes), see (Krizhevsky, 2009), allowing inefficiency improvements of more than 30% per individual group.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which group of classes in CIFAR100 has the largest potential for inefficiency improvement?",
    "answer": "Group 0",
    "rationale": "The rightmost plot shows the inefficiency reduction by group for CIFAR100. Group 0 has the largest bar, indicating the greatest potential for improvement.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.09192v3",
    "pdf_url": null
  },
  {
    "instance_id": "216dec46bec2452598da2d5782b64356",
    "figure_id": "2009.05697v2-Figure8-1",
    "image_file": "2009.05697v2-Figure8-1.png",
    "caption": " Accuracy (mAP) and speed (FPS) of different block size pruning results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the accuracy and speed of the different block size pruning results?",
    "answer": "The accuracy decreases as the speed increases.",
    "rationale": "The plot shows that the mAP (accuracy) decreases as the FPS (speed) increases. This means that the more blocks are pruned, the faster the model becomes, but the less accurate it is.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.05697v2",
    "pdf_url": null
  },
  {
    "instance_id": "9c3a5ac677ae4977bad3d7f8693906de",
    "figure_id": "2110.02128v2-Figure2-1",
    "image_file": "2110.02128v2-Figure2-1.png",
    "caption": " Average rewards and confidence bounds of different policies for deadline scheduling.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which policy performs the best in terms of average rewards?",
    "answer": "NeurWIN.",
    "rationale": "The figure shows the average rewards of different policies for deadline scheduling. The NeurWIN policy has the highest average rewards across all three scenarios.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02128v2",
    "pdf_url": null
  },
  {
    "instance_id": "7e146d7a092e4ec08f38118d5121b010",
    "figure_id": "2306.03228v1-Figure10-1",
    "image_file": "2306.03228v1-Figure10-1.png",
    "caption": " Notropis percobromus",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common value for the code level 0 Q?",
    "answer": "35",
    "rationale": "The x-axis of the histogram represents the values of the code level 0 Q, and the y-axis represents the frequency of each value. The highest bar in the histogram is at 35, indicating that this is the most common value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.03228v1",
    "pdf_url": null
  },
  {
    "instance_id": "930c0638e63f4275a429e2d851c4b9f7",
    "figure_id": "2302.10866v3-Figure4.2-1",
    "image_file": "2302.10866v3-Figure4.2-1.png",
    "caption": "2: Preliminary \"scaling law\" of language models on The Pile. Comparison of our approach (red) based on long convolutions and gating (Hyena) and a standard GPT (blue) (Brown et al., 2020). We reach perplexity of GPT with a smaller training FLOP budget.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better in terms of perplexity on The Pile dataset?",
    "answer": "Hyena",
    "rationale": "The plot shows that Hyena (red line) achieves lower perplexity values than GPT (blue line) for the same FLOPs budget.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.10866v3",
    "pdf_url": null
  },
  {
    "instance_id": "9199561230d540e89991f74010723060",
    "figure_id": "2206.12411v2-Figure6-1",
    "image_file": "2206.12411v2-Figure6-1.png",
    "caption": " The optimization curves of top-10 average on optimizing isomer-based oracles.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which molecule had the most diverse set of optimization curves?",
    "answer": "isomers_c7h8n2o2",
    "rationale": "The figure shows the optimization curves for four different molecules. The molecule with the most diverse set of curves is isomers_c7h8n2o2, which has a wider range of values than the other three molecules. This suggests that the optimization process for this molecule was more varied and less consistent than for the other molecules.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.12411v2",
    "pdf_url": null
  },
  {
    "instance_id": "374b6ab4c1644a0b87d7b25a19949cfa",
    "figure_id": "2106.07876v3-Figure2-1",
    "image_file": "2106.07876v3-Figure2-1.png",
    "caption": " The generalization error of the original training set and different augmentation training sets. With Original (Red) → ISA (Orange) → NSA (Green) → REM (Blue), the distance between T and T̃ is getting farther and farther, and the generalization error decreases accordingly.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following training sets has the lowest generalization error?",
    "answer": "REM",
    "rationale": "The generalization error is plotted on the y-axis of the figure, and the REM training set (blue dashed line) has the lowest point on the y-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07876v3",
    "pdf_url": null
  },
  {
    "instance_id": "51ed7f1ba973455cb0998a785877eeeb",
    "figure_id": "2008.07870v1-Figure1-1",
    "image_file": "2008.07870v1-Figure1-1.png",
    "caption": " Visualization of predicted trajectories with H = 40 using several state-of-the-art methods: a) location-LSTM; b) CNN; c) MBT1; d) MACRO VRNN1, e) SocialGAN4; f) MBT4l (ours); red: attackers, blue: defenders, orange: ball, grey: input history of predicted player, yellow: prediction, green: ground truth. A video animation is included in the Supplementary Material.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method predicts the most accurate trajectory for the attacker?",
    "answer": " MBT4l",
    "rationale": " The figure shows the predicted trajectories for the attacker (red) using different methods. The green line represents the ground truth, and the yellow line represents the prediction. The MBT4l method has the closest predicted trajectory to the ground truth, indicating that it is the most accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.07870v1",
    "pdf_url": null
  },
  {
    "instance_id": "847e84e9390c48ef9f6653b5443113f5",
    "figure_id": "1905.10881v3-Figure4-1",
    "image_file": "1905.10881v3-Figure4-1.png",
    "caption": " (Left:) Recalls based on one-step LPs and one-step DNLPs. (Right:) Results over the Citeseer (left), Cora (mid) and PubMed (right) networks. Recalls (mean ± std) of different PRs vs steps.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest recall on the Citeseer network?",
    "answer": "IPR-d 0.99.",
    "rationale": "The plot in the middle of the right column shows the recall of different algorithms on the Citeseer network. The line for IPR-d 0.99 is consistently higher than the lines for the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10881v3",
    "pdf_url": null
  },
  {
    "instance_id": "6da41ad900f8416dba8c9e0903a6b482",
    "figure_id": "2003.14030v1-Figure6-1",
    "image_file": "2003.14030v1-Figure6-1.png",
    "caption": " Visualization of the flow error map of our optical flow network on the KITTI 2015 testing benchmark. Larger errors are encoded in red, while blue pixels represents good optical flow estimates with respect to the ground-truth.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the color red in the flow error map indicate?",
    "answer": "Larger errors in the optical flow estimates.",
    "rationale": "The caption states that \"Larger errors are encoded in red.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.14030v1",
    "pdf_url": null
  },
  {
    "instance_id": "e188e4081c10429c93b8a62846ad2f95",
    "figure_id": "2203.09210v3-Figure5-1",
    "image_file": "2203.09210v3-Figure5-1.png",
    "caption": " The performance of WMT16 En2Ro when decoding with different number of iterations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best according to the plot?",
    "answer": "+CeMAT",
    "rationale": "The plot shows that +CeMAT has the highest BLEU score at each iteration, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.09210v3",
    "pdf_url": null
  },
  {
    "instance_id": "5de2c587e5d84531ad8af766262f2745",
    "figure_id": "2206.04890v2-Figure12-1",
    "image_file": "2206.04890v2-Figure12-1.png",
    "caption": " Illustration of the performance in GNFC and TCGA. The grey bar denotes the standard error (×0.3 for brevity) of 3 random seeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in the TCGA dataset for the t1_bias2 condition?",
    "answer": "SCIGAN",
    "rationale": "The figure shows the root mean square error (RMSE) for each method in each condition. In the TCGA dataset, for the t1_bias2 condition, SCIGAN has the lowest RMSE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.04890v2",
    "pdf_url": null
  },
  {
    "instance_id": "b225e044cfde46fca085e052c3c9d3e7",
    "figure_id": "1802.02209v1-Figure12-1",
    "image_file": "1802.02209v1-Figure12-1.png",
    "caption": " CDF of Trolley Tracking",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of the time does the proposed IONet method have an error of less than 1 meter?",
    "answer": "Approximately 60% of the time.",
    "rationale": "The CDF (Cumulative Distribution Function) plot shows the percentage of data points that fall below a certain error threshold. Looking at the green line, which represents the IONet method, we can see that it intersects the 60% mark on the Y-axis at an error of approximately 1 meter on the X-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1802.02209v1",
    "pdf_url": null
  },
  {
    "instance_id": "b7f82ece6b11466ca5ffd24b055c9ac1",
    "figure_id": "2004.12214v1-Figure2-1",
    "image_file": "2004.12214v1-Figure2-1.png",
    "caption": " Average reward vs. wall-clock time for MuJoCo locomotion tasks. In each condition, we perform 5 runs with different random seeds. Shaded areas represent 1 standard deviation. The grey horizontal line indicates the prescribed threshold at which the task is considered ‘solved’. Measurements are performed on Intel Xeon E7-8890 v3 processors and Nvidia GeForce RTX 2080 Ti GPUs. We list the number of cores used for each experiment.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What task took the least amount of time to solve?",
    "answer": "Hopper-v2",
    "rationale": "The figure shows that the Hopper-v2 task reached the prescribed threshold (grey horizontal line) in the least amount of time compared to the other tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.12214v1",
    "pdf_url": null
  },
  {
    "instance_id": "d0758fc55a3e49dc826bcee68d3abe54",
    "figure_id": "2107.03190v2-Figure5-1",
    "image_file": "2107.03190v2-Figure5-1.png",
    "caption": " Examples of conditional queries.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two conditional queries in the image is not identifiable? Why?",
    "answer": "The conditional query in (b) is not identifiable.",
    "rationale": "The conditional query in (b) is not identifiable because the factor P(Y_x = y | Z_x = z, X = x') is not identifiable. This factor cannot be identified because there is no way to distinguish between the causal effect of Z on Y and the effect of X on Y through Z.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.03190v2",
    "pdf_url": null
  },
  {
    "instance_id": "f6515688c4004726b1a4582f6059b408",
    "figure_id": "2108.08023v1-Figure4-1",
    "image_file": "2108.08023v1-Figure4-1.png",
    "caption": " Visualizations of test samples in the 4-Joint case. (a), (b), (c) and (d) are the input, density maps of JT, DKPNet(c=5,k=2) and Ground-Truth, resp.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the density maps in (b), (c), and (d) is most accurate?",
    "answer": "(d) is the most accurate density map.",
    "rationale": "(d) is the ground-truth density map, which means it is the actual density of people in the image. The other density maps are generated by different methods, and they are not as accurate as the ground-truth density map.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.08023v1",
    "pdf_url": null
  },
  {
    "instance_id": "2d5a1f7c425a445789c89c0d6d1daee8",
    "figure_id": "1806.05034v4-Figure5-1",
    "image_file": "1806.05034v4-Figure5-1.png",
    "caption": " Reproduction of the probabilities of the segmentation modes on the Cityscapes task. The artificial flipping of 5 classes results in 32 modes with different ground truth probability (x-axis). The y-axis shows the frequency of how often the model predicted this variant in the whole test set. Agreement with the bisector line indicates calibration quality.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is the most well-calibrated?",
    "answer": "The M-Heads model.",
    "rationale": "The M-Heads model has the most points that are close to the bisector line, which indicates that the model's predicted probabilities are close to the true probabilities.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.05034v4",
    "pdf_url": null
  },
  {
    "instance_id": "839af367073343d691aefc18b3fc0065",
    "figure_id": "2012.03236v2-Figure6-1",
    "image_file": "2012.03236v2-Figure6-1.png",
    "caption": " Transfer Learning scenario: Top-1 test accuracy for two combinations from CIFAR-100 to STL-10 or Tiny-ImageNet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which knowledge distillation method achieves the highest accuracy on STL-10 when transferring from CIFAR-100 with VGG-8 and ResNet-32x4?",
    "answer": "KD (Knowledge Distillation)",
    "rationale": "The figure shows the top-1 test accuracy for different knowledge distillation methods on STL-10 when transferring from CIFAR-100 with VGG-8 and ResNet-32x4. The KD line is the highest among all the methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.03236v2",
    "pdf_url": null
  },
  {
    "instance_id": "09c5210497fd485fb0fb536643993948",
    "figure_id": "2301.11781v2-Figure5-1",
    "image_file": "2301.11781v2-Figure5-1.png",
    "caption": " We reproduce our experiments on the HSLS dataset with multi-group and multi-label pre-processing. On the right, we also demonstrate that FairFront can take into account multiple fairness considerations at once. We show how the fairness-accuracy curve changes as we add new types of group fairness constraints (i.e., adding OAE and SP constraints in addition to EO).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest accuracy on the HSLS dataset when considering only equalized odds (EO) constraints?",
    "answer": "FairFront with only EO constraints.",
    "rationale": "The figure on the right shows the accuracy of different methods as a function of the maximum equalized odds. The curve for FairFront with only EO constraints is the highest of all the curves, indicating that it achieves the highest accuracy for any given level of equalized odds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.11781v2",
    "pdf_url": null
  },
  {
    "instance_id": "386ebc5b1aaa4617b6ff01d528c3edaf",
    "figure_id": "2202.12837v2-Figure12-1",
    "image_file": "2202.12837v2-Figure12-1.png",
    "caption": " Performance gap from using the demonstrations with gold labels to using the demonstrations with random labels. Datasets are sorted in descending order. The top two figures use random labels that are sampled at uniform, with Channel MetaICL and Channel GPT-J, respectively. The bottom two figures use random labels that are sampled from a true distribution of labels on the training data, with Channel MetaICL and Channel GPT-J, respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four figures shows the performance gap for Channel GPT-J using the true distribution of labels?",
    "answer": "The bottom right figure.",
    "rationale": "The caption states that the bottom two figures use random labels that are sampled from a true distribution of labels on the training data, and the right two figures use Channel GPT-J.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.12837v2",
    "pdf_url": null
  },
  {
    "instance_id": "006fa82f17fe45a0855c04ef98fcddca",
    "figure_id": "2210.06456v2-Figure9-1",
    "image_file": "2210.06456v2-Figure9-1.png",
    "caption": " Results on all extractive QA OOD settings when training on SQuAD with pre-trained models of increasing size.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which BERT model performs best on the BioASQ dataset when trained on SQuAD?",
    "answer": "BERT Large",
    "rationale": "The figure shows the F1 score for each BERT model on the BioASQ dataset. The F1 score for BERT Large is higher than the F1 score for any other BERT model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06456v2",
    "pdf_url": null
  },
  {
    "instance_id": "3fdc03e2f08e4d4b82f8925f3bb84a11",
    "figure_id": "2307.06233v1-Figure4-1",
    "image_file": "2307.06233v1-Figure4-1.png",
    "caption": " MS-SSIM rate-distortion curve of different compression methods on (a) high quality images from the CLIC pro test set [2] and (b) nearly noiseless test images from NIND [8].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which compression method achieves the highest MS-SSIM for a given bitrate on high-quality images?",
    "answer": "JDC-CN",
    "rationale": "The MS-SSIM rate-distortion curve for JDC-CN is consistently higher than the curves for the other compression methods on high-quality images. This means that JDC-CN achieves a higher MS-SSIM for a given bitrate than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.06233v1",
    "pdf_url": null
  },
  {
    "instance_id": "6236305ca88c491d9f84a50fab7a9283",
    "figure_id": "2101.12463v3-Figure5-1",
    "image_file": "2101.12463v3-Figure5-1.png",
    "caption": " (a) Rainy image. (b) Derained result of RLNet. (c) Groundtruth. (d) Embedding residual map (×0.5) not recrified by the error detector. (e) Error map multiplied by 10 for visualization. (f) Final generated residual map.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the result of the RLNet algorithm?",
    "answer": "Image (b)",
    "rationale": "The caption states that image (b) is the derained result of RLNet.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.12463v3",
    "pdf_url": null
  },
  {
    "instance_id": "1634080af62a4b20bcfbc16640ec3e49",
    "figure_id": "2103.06175v2-Figure9-1",
    "image_file": "2103.06175v2-Figure9-1.png",
    "caption": " Empirical values during the training process.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest accuracy for f?",
    "answer": "DD (minimax on L_1)",
    "rationale": "The plot in (a) shows the accuracy of f for each method over time. The red line, which represents DD (minimax on L_1), is consistently higher than the other two lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.06175v2",
    "pdf_url": null
  },
  {
    "instance_id": "9224945bef9547eb886d67f3a645044e",
    "figure_id": "2106.01207v1-Figure1-1",
    "image_file": "2106.01207v1-Figure1-1.png",
    "caption": " Model scores for a) BERT, b) RoBERTa, c) Chinese BERT, and d) Chinese RoBERTa at the pronoun grouped by antecedent; stimuli derived from Ferstl et al. (2011) and Hartshorne et al. (2013)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the object-bias verb bias stimuli?",
    "answer": "RoBERTa.",
    "rationale": "The boxplot for RoBERTa in panel b) shows the highest median score and the smallest interquartile range for the object-bias verb bias stimuli.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01207v1",
    "pdf_url": null
  },
  {
    "instance_id": "24246a91ff6a41aa927c6bdb180ea908",
    "figure_id": "2004.00626v2-Figure7-1",
    "image_file": "2004.00626v2-Figure7-1.png",
    "caption": " Choice of Foreground layer. For baseline algorithms, IM and LFM, that do not predict the foreground layer F , we observe that F = I produces less visible artifacts compared to predicting F from the matting equation using the captured background B′. Notice how some of the brick texture creeps into the foreground when solving for F with the matting equation. We also show that our approach, which jointly estimates F and α, produces less artifacts in compositing.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods produces the least visible artifacts in compositing?",
    "answer": "Ours",
    "rationale": "The figure shows that the \"Ours\" method produces the least visible artifacts in compositing. This can be seen by comparing the different methods in the figure. The \"Ours\" method produces a more realistic image than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.00626v2",
    "pdf_url": null
  },
  {
    "instance_id": "9b9e14b9996d464c93b7aba64c218b98",
    "figure_id": "2010.15054v1-Figure1-1",
    "image_file": "2010.15054v1-Figure1-1.png",
    "caption": " Attribution maps of a network before and after network compression. These figures are examples that the networks before and after compression predicted the same correct labels (bus, cat, sofa), but exhibit different attribution maps. Observe that for compressed networks, the max value of the heatmaps (blue circle) is evicted outside the segmentation boundaries (white line) while our method maintains the dot.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the following methods preserves the most information about the object of interest after network compression? ",
    "answer": " KD with Ours",
    "rationale": " The figure shows that the KD with Ours method is the only one that maintains the \"hot spot\" (blue circle) within the segmentation boundaries (white line) after network compression. This suggests that this method preserves the most information about the object of interest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.15054v1",
    "pdf_url": null
  },
  {
    "instance_id": "d6170fff3dd4413dbeb32512b5e549a1",
    "figure_id": "2107.00644v2-Figure5-1",
    "image_file": "2107.00644v2-Figure5-1.png",
    "caption": " Training and test performance. We compare SVEA to DrQ with and without random convolution augmentation, as well as a set of ablations. Data-mixing only indiscriminately applies our data-mixing strategy to all data streams, and (α = 0, β = 1) only augments Q-predictions but without data-mixing. We find both components to contribute to SVEA’s success. Top: episode return on the training environment during training. Bottom: generalization measured by episode return on the color_hard benchmark of DMControl-GB. Mean of 5 seeds, shaded area is ±1 std. deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following algorithms has the highest average episode return on the \"Ball in cup, catch\" task during training?",
    "answer": "DrQ + conv",
    "rationale": "The plot in the bottom right corner of the figure shows the episode return for each algorithm on the \"Ball in cup, catch\" task during training. The DrQ + conv line is the highest, indicating that it has the highest average episode return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.00644v2",
    "pdf_url": null
  },
  {
    "instance_id": "8e37fd6c1ee149519c1bb5fe9a9a7f1a",
    "figure_id": "2201.06578v2-Figure7-1",
    "image_file": "2201.06578v2-Figure7-1.png",
    "caption": " FID curves for training unconditional and conditional StyleGAN2, as well as our method, on ImageNet Carnivores with 50 classes and 500 images per class.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method had the lowest FID score at the end of training?",
    "answer": "Our method.",
    "rationale": "The FID score for our method (green line) is lower than the FID scores for the unconditional and conditional StyleGAN2 methods (blue and red lines, respectively) at the end of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.06578v2",
    "pdf_url": null
  },
  {
    "instance_id": "b2591f77191d47a9bc0f0741b323c20c",
    "figure_id": "2104.08336v2-Figure4-1",
    "image_file": "2104.08336v2-Figure4-1.png",
    "caption": " Distributions of (a)–(b) coverage and (c)–(d) localization scores for Dense-CNN and DCRNNs on correctly predicted 60-s EEG clips for focal and generalized seizures. (e)–(f) Example occlusion maps with focal seizure whose localization >0.9 from pre-trained Corr-DCRNN. Only regions with normalized occlusion value >0.5 are colored. Red boxes are annotated seizure regions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better for focal seizures, Dense-CNN or DCRNN?",
    "answer": "DCRNN performs better for focal seizures.",
    "rationale": "The figure shows the distribution of coverage and localization scores for Dense-CNN and DCRNNs on correctly predicted 60-s EEG clips for focal and generalized seizures. For focal seizures, DCRNNs have a higher fraction of EEG clips with high coverage and localization scores compared to Dense-CNN.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.08336v2",
    "pdf_url": null
  },
  {
    "instance_id": "6bf6a05987524ac284f87863a9ee5046",
    "figure_id": "1910.04732v2-Figure2-1",
    "image_file": "1910.04732v2-Figure2-1.png",
    "caption": " Histograms of HardConcrete parameters during training. We show the changes of histograms for the first SRU layer (left figure) and the last layer (right figure). We compute the histogram every 3,000 training steps.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which SRU layer shows a more dramatic change in the distribution of its parameters during training?",
    "answer": "The first SRU layer.",
    "rationale": "The histograms for the first SRU layer (left figure) show a more significant shift in the distribution of parameters over time compared to the last layer (right figure). In the first layer, the distribution starts with a wider spread and gradually becomes more concentrated around specific values. This suggests that the first layer is learning more actively and adapting its parameters more significantly during training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.04732v2",
    "pdf_url": null
  },
  {
    "instance_id": "326dd411d78f46b592863740ed586328",
    "figure_id": "2106.14747v1-Figure5-1",
    "image_file": "2106.14747v1-Figure5-1.png",
    "caption": " The classification system of the Purpose-driven Affordance Dataset (PAD), which contains 4,002 images covering 72 object classes and 31 affordance classes. See the supplementary material for more details of each affordance category.",
    "figure_type": "\"schematic\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which affordance class has the most objects?",
    "answer": "\"Hit\"",
    "rationale": "The \"Hit\" affordance class is the largest slice of the pie chart, indicating that it has the most objects.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.14747v1",
    "pdf_url": null
  },
  {
    "instance_id": "b3228a84731f47558c32a90f1d8fc6e4",
    "figure_id": "2011.06110v1-Figure1-1",
    "image_file": "2011.06110v1-Figure1-1.png",
    "caption": " RNN-T output probability lattice, following [5].",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the predicted output at time step t=3?",
    "answer": "The predicted output at time step t=3 is y(3,2).",
    "rationale": "The red arrows in the figure indicate the path of the predicted output. At time step t=3, the red arrow points to the node labeled y(3,2).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.06110v1",
    "pdf_url": null
  },
  {
    "instance_id": "7528b08796114a809d7372934cbec9a0",
    "figure_id": "2105.14781v1-Figure6-1",
    "image_file": "2105.14781v1-Figure6-1.png",
    "caption": " The before-attack (a) and after-attack accuracy (b) of methods with different sample sizes on SocialIQA. The after-attack accuracy of Pro-A, CGA and Self-Talk is below 20.0%, and thus omitted in (b).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest before-attack accuracy?",
    "answer": "SEQ-A.",
    "rationale": "The figure shows that SEQ-A has the highest before-attack accuracy, as its line is the highest on the chart in (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14781v1",
    "pdf_url": null
  },
  {
    "instance_id": "a951ec035db644f39f34af03fb6697ab",
    "figure_id": "2105.04183v1-Figure2-1",
    "image_file": "2105.04183v1-Figure2-1.png",
    "caption": " An illustration of our UGRec model: 1) The directed relations are translated into the corresponding relation space for optimization; 2) the undirected co-occurrence relations are optimized on the hyperplanes projected from the entity space.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two types of relations considered in the UGRec model?",
    "answer": "Directed and undirected relations.",
    "rationale": "The figure shows two distinct sections, one for directed relations and one for undirected relations. The directed relations are translated into the corresponding relation space for optimization, while the undirected co-occurrence relations are optimized on the hyperplanes projected from the entity space.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.04183v1",
    "pdf_url": null
  },
  {
    "instance_id": "579e12793f154bfbb203b34640fa3d9b",
    "figure_id": "2306.16527v2-Figure4-1",
    "image_file": "2306.16527v2-Figure4-1.png",
    "caption": " Heatmap displaying the joint distribution of the number of tokens and the number of images in OBELICS documents, accompanied by their respective marginal distributions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the heatmap, what is the most common number of images in OBELICS documents?",
    "answer": "1",
    "rationale": "The heatmap shows that the highest density of data points is located around 1 on the y-axis, which represents the number of images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.16527v2",
    "pdf_url": null
  },
  {
    "instance_id": "b113a8a2324b4054bb4b6343aaecff87",
    "figure_id": "2209.07833v1-Figure1-1",
    "image_file": "2209.07833v1-Figure1-1.png",
    "caption": " Example with cycle. The blue edges form a Hamiltonian cycle. The red nodes are corrupt nodes.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many nodes are there in the Hamiltonian cycle?",
    "answer": "5",
    "rationale": "The Hamiltonian cycle is formed by the blue edges. The blue edges connect all five nodes in the graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.07833v1",
    "pdf_url": null
  },
  {
    "instance_id": "a893ab23600942dfb1af299d893f6bf7",
    "figure_id": "2205.01411v2-Figure14-1",
    "image_file": "2205.01411v2-Figure14-1.png",
    "caption": " For the remaining 9 images, the Top-1 predictions are correct and the RAPS sets contain the true label. Here, for example, the true label is: Train.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the top prediction of the classifier for the image?",
    "answer": "Train",
    "rationale": "The caption states that the top prediction of the classifier for the image is \"Train\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.01411v2",
    "pdf_url": null
  },
  {
    "instance_id": "f83ba153ffb743eab1bd7338619016b7",
    "figure_id": "2301.13688v2-Figure2-1",
    "image_file": "2301.13688v2-Figure2-1.png",
    "caption": " ATimeline of Public Instruction TuningCollections specifies the collection release date, detailed information on the finetuned models (the base model, their size, and whether the model itself is Public (P) or Not Public (NP)), what prompt specification they were trained for (zero-shot, few-shot, or Chain-of-Thought), the number of tasks contained in the Flan 2022 Collection (released with this work), and core methodological contributions in each work. Note that the number of tasks and of examples vary under different assumptions and so are estimates. For instance, the definition of “task” and ”task category” vary by work, and are not easily simplified to one ontology. The reported counts for the number of tasks are reported using task definitions from the respective works. † indicates concurrent work.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which instruction-tuned model has the most number of examples?",
    "answer": "Flan 2022 (ours)",
    "rationale": "The table shows that Flan 2022 has the most number of examples, with 1836 examples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.13688v2",
    "pdf_url": null
  },
  {
    "instance_id": "85865bc3830447ab8892551990cd3bb9",
    "figure_id": "2201.00103v1-Figure4-1",
    "image_file": "2201.00103v1-Figure4-1.png",
    "caption": " Qualitative results on PASCAL VOC, MS COCO (48/17 and 65/15) and DIOR datasets. For each dataset, the first column and second column are the results of ZSD and GZSD, respectively. Seen classes are shown with green and unseen with red.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the most images with unseen classes?",
    "answer": "DIOR",
    "rationale": "The figure shows that all of the images in the DIOR dataset have unseen classes, while the other datasets have a mix of seen and unseen classes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.00103v1",
    "pdf_url": null
  },
  {
    "instance_id": "0cf21fff1c564b2f88d0fb676101d890",
    "figure_id": "2109.10052v1-Figure10-1",
    "image_file": "2109.10052v1-Figure10-1.png",
    "caption": " Spearman correlation between each pair of models computed over all social groups. This figure illustrates that there is fairly little correlation between any of the models when it comes to the emotion profiles that they capture.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which two models have the highest correlation in terms of the emotion profiles they capture?",
    "answer": "BERT-B and RoBERTa-B.",
    "rationale": "The figure shows a heatmap of Spearman correlation coefficients between each pair of models. The darker the blue color, the higher the correlation. The highest correlation is observed between BERT-B and RoBERTa-B, as indicated by the darkest blue square in the heatmap.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.10052v1",
    "pdf_url": null
  },
  {
    "instance_id": "10637ca07117402bae96309ca9f6458a",
    "figure_id": "2106.01863v1-Figure4-1",
    "image_file": "2106.01863v1-Figure4-1.png",
    "caption": " Qualitative Comparisons. We compare our results with ESRGAN [30], RankSRGAN [35], SRNTT [39], and TTSR [34]. All these methods are trained with GAN loss. Our results have better visual quality with more texture details.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the best results for the image of the cruise ship?",
    "answer": "C^2-Matching",
    "rationale": "The C^2-Matching column shows the most detailed and realistic results for the cruise ship image, compared to the other methods. The other methods either produce blurry or distorted results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01863v1",
    "pdf_url": null
  },
  {
    "instance_id": "42bbdb4861a748c8b58d996a5802bd04",
    "figure_id": "2105.07452v1-Figure2-1",
    "image_file": "2105.07452v1-Figure2-1.png",
    "caption": " BLiMP accuracy different amounts of training data and across layers, for three LMs. About 1000 sentences are needed before a plateau is reached (mean tokens per sentence = 15.1).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language model performs the best according to the figure?",
    "answer": "RoBERTa performs the best according to the figure.",
    "rationale": "The figure shows the accuracy of three language models (BERT, RoBERTa, and XLNet) on the BLiMP benchmark. RoBERTa consistently achieves the highest accuracy across all training sentence counts and layers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.07452v1",
    "pdf_url": null
  },
  {
    "instance_id": "ce49cde1dd3a489883fee86fd67ae4f2",
    "figure_id": "2212.04273v1-Figure5-1",
    "image_file": "2212.04273v1-Figure5-1.png",
    "caption": " Change in simlex-999 scores for MP and INLP after every iteration.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better in terms of correlation?",
    "answer": "INLP",
    "rationale": "The plot shows that INLP has a higher correlation than MP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.04273v1",
    "pdf_url": null
  },
  {
    "instance_id": "3998fc03148843b6ae5008505bc3ab3a",
    "figure_id": "2106.05933v2-Figure106-1",
    "image_file": "2106.05933v2-Figure106-1.png",
    "caption": " Sparsity over layers for wav2vec-base finetuned for Turkish tr at 30% sparsity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the sparsity of the 11th layer of the wav2vec-base model finetuned for Turkish tr at 30% sparsity?",
    "answer": "43.357%",
    "rationale": "The figure shows the sparsity of each layer of the model. The 11th layer has a sparsity of 43.357%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05933v2",
    "pdf_url": null
  },
  {
    "instance_id": "cb8871d9983240078a0e763a974f1ad1",
    "figure_id": "2306.06368v1-Figure8-1",
    "image_file": "2306.06368v1-Figure8-1.png",
    "caption": " The first three subfigures on the left: The average performance and running time of each considered algo-",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms has the highest average truss-size increase for low input trussness k?",
    "answer": "The RD algorithm.",
    "rationale": "The rightmost subfigure shows the average truss-size increase for low (5/10) and high (15/20) input trussness k. For low input trussness k, the RD algorithm has the highest average truss-size increase, with a value of 4.91x.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.06368v1",
    "pdf_url": null
  },
  {
    "instance_id": "edc5ca5e4b5d4be5b4396a0808de9e93",
    "figure_id": "2004.15014v2-Figure5-1",
    "image_file": "2004.15014v2-Figure5-1.png",
    "caption": " One-shot segmentation results compared to CANet[28] output. Row 1 indicates improved class-specificity - the mask is localized on the correct target class. Row 2 demonstrates better utilization of similar background context.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model produces the most accurate segmentation masks?",
    "answer": "The Ground Truth model.",
    "rationale": "The Ground Truth model is the gold standard for segmentation, and the other models are compared to it. The figure shows that the Ground Truth model produces the most accurate segmentation masks, followed by SimPropNet and then CANet.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.15014v2",
    "pdf_url": null
  },
  {
    "instance_id": "a363bccc2d534c6fb9a21fa1f748c550",
    "figure_id": "2106.04152v2-Figure2-1",
    "image_file": "2106.04152v2-Figure2-1.png",
    "caption": " Test performance comparison on DMControl where the lines denote the mean score and the shadow indicates the corresponding standard deviation (obtained by running each environment with 5 random seeds). Our PlayVirtual (marked with blue) outperforms Baseline (marked with orange) in most environments by a large margin at different environment steps.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods shown in the figure, Baseline or PlayVirtual, achieves higher scores in most of the environments tested?",
    "answer": "PlayVirtual.",
    "rationale": "The blue line in each plot represents PlayVirtual, while the orange line represents Baseline. The blue line is higher than the orange line in most of the plots, indicating that PlayVirtual achieves higher scores than Baseline in most of the environments tested.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04152v2",
    "pdf_url": null
  },
  {
    "instance_id": "267c16234f924efbaa0e5479a61d7cbd",
    "figure_id": "1908.05005v3-FigureB.7-1",
    "image_file": "1908.05005v3-FigureB.7-1.png",
    "caption": "Figure B.7: CD (left) and rCD (right) evaluated on Cityscapes for ICNet (set as reference architecture), FCN8s-VGG16, DilatedNet, ResNet-38, PSPNet, GSCNN w.r.t. image corruptions of category blur, noise, digital, weather, and geometric distortion. Each bar except for geometric distortion is averaged within a corruption category (error bars indicate the standard deviation). The CD of image corruption “jpeg compression” of category digital is not included in this barplot, since, contrary to the remaining image corruptions of that category, the respective CDs range between 107% and 133%. Bars above 100% represent a decrease in performance compared to the reference architecture. FCN8s-VGG16 and DilatedNet are vulnerable to corruptions of category blur. DilatedNet is more robust against corruptions of category noise, digital, and weather than the reference. ResNet-38 is robust against corruptions of category weather. The rCD of PSPNet is oftentimes higher than 100% for each category. GSCNN is vulnerable to image noise. The rCD is considerably high, indicating a high decrease of mIoU in the presence of this corruption. Best viewed in color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which architecture is the most robust against image corruptions of category noise?",
    "answer": "DilatedNet",
    "rationale": "The figure shows that DilatedNet has the lowest CD and rCD for image corruptions of category noise.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.05005v3",
    "pdf_url": null
  },
  {
    "instance_id": "8ae968dba62144399781f4d60fa0c2c1",
    "figure_id": "1805.08331v2-Figure5-1",
    "image_file": "1805.08331v2-Figure5-1.png",
    "caption": " Comparison of scalings of Hera and Sinkhorn (Alg. 1) as the number of points in diagram increases. log-log scale.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm scales better as the number of points in the diagram increases?",
    "answer": "Sinkhorn algorithm scales better than Hera.",
    "rationale": "The plot shows that the scaling factor of Sinkhorn increases at a slower rate than that of Hera as the number of points in the diagram increases. This means that Sinkhorn is more efficient for large datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.08331v2",
    "pdf_url": null
  },
  {
    "instance_id": "cb4086ade8a14a4daf192217b94375f9",
    "figure_id": "2106.07941v1-Figure4-1",
    "image_file": "2106.07941v1-Figure4-1.png",
    "caption": " Image deraining results tested on the synthetic datasets. From (a)-(h): (a) the rainy images, and the deraining results of (b) DDN, (c) RESCAN, (d) DAF-Net, (e) PReNet, (f) DualCNN, (g) ours and (h) the ground truth, respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which deraining method produces the most realistic results?",
    "answer": "Ours.",
    "rationale": "The figure shows the results of different deraining methods applied to a rainy image. The ground truth image is shown in (h), and our method's result is shown in (g). Visually comparing the images, our method's result is the closest to the ground truth, indicating it produces the most realistic results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07941v1",
    "pdf_url": null
  },
  {
    "instance_id": "ae8e2e5746c94a328532753be13def56",
    "figure_id": "2207.00328v3-Figure6-1",
    "image_file": "2207.00328v3-Figure6-1.png",
    "caption": " Qualitative evaluation of Patch2Pix (Zhou, Sattler, and Leal-Taixe 2021), LoFTR (Sun et al. 2021) and our method (TopicFM) on MegaDepth dataset. Based on the known ground truth camera poses, we can visualize the correct matches in green color and the wrong matches in red colors. Our method produces a much higher number of matches compared to Patch2Pix and LoFTR.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most correct matches?",
    "answer": "TopicFM.",
    "rationale": "The figure shows the correct matches in green and the wrong matches in red. TopicFM has the most green lines, indicating that it has the most correct matches.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.00328v3",
    "pdf_url": null
  },
  {
    "instance_id": "4d9949af9e814292b951305e32343d9a",
    "figure_id": "1804.04327v5-Figure3-1",
    "image_file": "1804.04327v5-Figure3-1.png",
    "caption": " Performance of Group Recommendation Methods in terms of prec@K (p < 0.0001) (Best viewed in color).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which group recommendation method performs the best on the MovieLens-Rand dataset?",
    "answer": "MoSaAN",
    "rationale": "The figure shows the prec@K values for different group recommendation methods on different datasets. For the MovieLens-Rand dataset, MoSaAN has the highest prec@K values for all values of K.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.04327v5",
    "pdf_url": null
  },
  {
    "instance_id": "c71fd66711174b7f943467c25343d88b",
    "figure_id": "1809.00088v1-Figure5-1",
    "image_file": "1809.00088v1-Figure5-1.png",
    "caption": " This figure compares a subset (17 hate groups of 6 categories) of the predictions made by the baseline CVAE and the HCVAE. The X and Y axes are the index of each hate group. The hate groups of the same category are grouped together as shown in the dashed squares. The color of the grid (i, j) is mapped from rhcvae(i, j)−rcvae(i, j), where r(i, j) is the fraction of the group i’s instances among the instances predicted as the group j. A higher difference value corresponds to a lighter color. A grid darker than the background is mapped from a negative value while a grid lighter than the background is mapped from a positive one.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which hate group is most likely to be misclassified by the CVAE?",
    "answer": "Hate group 16.",
    "rationale": "The grid corresponding to hate group 16 and hate group 15 is the lightest in the figure, indicating that the HCVAE predicts a higher fraction of group 16's instances as group 15 than the CVAE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.00088v1",
    "pdf_url": null
  },
  {
    "instance_id": "cec684cc949743fbafe4ce55f023ed00",
    "figure_id": "2212.03063v2-Figure14-1",
    "image_file": "2212.03063v2-Figure14-1.png",
    "caption": " Stylised images produced by the Fourier-based Style Transfer of FAFT on Digits-DG. The content and style images are visualised on the first row and column respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of Fourier-based Style Transfer (FAFT) on the Digits-DG dataset?",
    "answer": "FAFT stylizes the images in the Digits-DG dataset.",
    "rationale": "The figure shows the original images in the first row and the stylized images in the remaining rows. The stylized images retain the content of the original images but have a different style.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.03063v2",
    "pdf_url": null
  },
  {
    "instance_id": "61ca026374234db483371dd26f100a14",
    "figure_id": "2310.11518v3-Figure9-1",
    "image_file": "2310.11518v3-Figure9-1.png",
    "caption": " Tiny Hanabi. We omit payoffs of 0 at terminals.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the payoff to player 2 if they play action $a$ and player 1 plays action $b$?",
    "answer": "1",
    "rationale": "The figure shows that if player 2 plays action $a$ and player 1 plays action $b$, the game reaches a terminal node with a payoff of 1 to player 2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.11518v3",
    "pdf_url": null
  },
  {
    "instance_id": "217d1d66c4d64db2ab9e9c7cde3300b1",
    "figure_id": "1912.00381v2-Figure11-1",
    "image_file": "1912.00381v2-Figure11-1.png",
    "caption": " Accuracy-vs-complexity of state-of-the-art on Something-V1. Size indicates number of parameters (M, in millions).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest accuracy and the highest number of parameters?",
    "answer": "GSM-InceptionV3 En3",
    "rationale": "The figure shows the accuracy and GFLOPs of different models. The size of the bubble indicates the number of parameters. The GSM-InceptionV3 En3 bubble is the largest and has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.00381v2",
    "pdf_url": null
  },
  {
    "instance_id": "b1f10aa36c694cd49dd05d39da3fb17d",
    "figure_id": "2106.03904v3-Figure6-1",
    "image_file": "2106.03904v3-Figure6-1.png",
    "caption": " EPIFNP outperforms top 2 baselines during abnormal COVID-19 season 2019/20.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed best in the 2019/20 season according to the figure?",
    "answer": "EpiFNP",
    "rationale": "The figure shows the predicted and observed values for the 2019/20 season. The EpiFNP model is shown in red, and it closely follows the observed values (black line). The other two models, RNP and GP, do not perform as well.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03904v3",
    "pdf_url": null
  },
  {
    "instance_id": "df440f7c5340489483f30477f361d69c",
    "figure_id": "2206.02511v1-Figure7-1",
    "image_file": "2206.02511v1-Figure7-1.png",
    "caption": " Candidate points in our compact search space. The blue points refer to candidates and the red points refer to the global optimum.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has a larger search space?",
    "answer": "cpu_act.",
    "rationale": "The search space is represented by the blue points in the figure. The cpu_act dataset has more blue points than the musk dataset, so it has a larger search space.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.02511v1",
    "pdf_url": null
  },
  {
    "instance_id": "e9dd187052924242b5d192a0c374db05",
    "figure_id": "2003.13964v2-Figure9-1",
    "image_file": "2003.13964v2-Figure9-1.png",
    "caption": " Histogram of log-probabilities of (a) the predicted label, i.e., top-1 softmax score, and (b) the ground-truth label on misclassified samples by networks trained by the cross-entropy (baseline) and CS-KD. The networks are trained on ResNet-18 for Stanford Dogs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, cross-entropy or CS-KD, is more likely to produce a high-confidence prediction for the wrong class?",
    "answer": "Cross-entropy.",
    "rationale": "The histogram in Figure (a) shows that the log-probabilities of the predicted labels for misclassified samples are generally higher for the cross-entropy method than for CS-KD. This means that the cross-entropy method is more likely to produce a high-confidence prediction for the wrong class.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.13964v2",
    "pdf_url": null
  },
  {
    "instance_id": "387c956c5f1d4ffd855d04f5a68f1278",
    "figure_id": "2001.10238v1-Figure8-1",
    "image_file": "2001.10238v1-Figure8-1.png",
    "caption": " Two trajectories are shown in the pixel space, between an image and its transformed version, for three types of transformations: translation, scale and orientation. Red: shortest path (interpolation) between the two extremes of the trajectory. Blue: trajectory of the actual transformation. At each position along the trajectories, we report the corresponding image (best seen with zoom).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of transformation preserves the shape of the object?",
    "answer": "Translation.",
    "rationale": "In the figure, the blue line shows the trajectory of the actual transformation. For translation, the shape of the object remains the same, even though its position changes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.10238v1",
    "pdf_url": null
  },
  {
    "instance_id": "c7b15c5edab34ee18629cabd6e146dd4",
    "figure_id": "2210.12579v1-Figure7-1",
    "image_file": "2210.12579v1-Figure7-1.png",
    "caption": " Bar plot showing Top-100-Recall for domain=YuGiOh when indexing using 500 anchor items for FIXEDITEM and ITEMCUR and 500 anchor queries for ANNCUR.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest Top-100-Recall when 1000 items are retrieved?",
    "answer": "ANNCUR_200",
    "rationale": "The bar for ANNCUR_200 is the highest at 1000 items retrieved.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12579v1",
    "pdf_url": null
  },
  {
    "instance_id": "38d75fa9156d4a3ebb01cfff95b69c90",
    "figure_id": "2305.16380v4-Figure3-1",
    "image_file": "2305.16380v4-Figure3-1.png",
    "caption": " Growth factor χl(t) (Theorem 3) over time with fixed ηZ = 0.5 and changing ηY . Each solid line is χl(t) and the dotted line with the same color corresponds to the transition time t0 for a given ηY .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the transition time t0 change as ηY increases?",
    "answer": "The transition time t0 increases as ηY increases.",
    "rationale": "The figure shows that the dotted lines, which represent the transition time t0, shift to the right as ηY increases. This means that it takes longer for the growth factor χl(t) to reach its steady-state value when ηY is larger.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16380v4",
    "pdf_url": null
  },
  {
    "instance_id": "06db46f553e24dfb8e048f3e5b8e5a8c",
    "figure_id": "2103.00065v3-Figure68-1",
    "image_file": "2103.00065v3-Figure68-1.png",
    "caption": " Runge-Kutta. Refer to the Figure 48 caption for more information. Additionally, in this figure the sharpness drops at the end of training due to the cross-entropy loss.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the train loss change over time?",
    "answer": "The train loss decreases over time.",
    "rationale": "The plot of the train loss versus time shows that the train loss starts at a high value and then decreases rapidly before leveling off at a lower value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.00065v3",
    "pdf_url": null
  },
  {
    "instance_id": "b012d1d1701a41efac3cd361533d37cd",
    "figure_id": "2106.09373v1-Figure3-1",
    "image_file": "2106.09373v1-Figure3-1.png",
    "caption": " Effects of Pre-training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following models is the best performer in terms of TAU when trained on 10k samples?",
    "answer": "Pre-trained (Harbin)",
    "rationale": "The figure shows that the Pre-trained (Harbin) model has the highest TAU value of around 1.05 when trained on 10k samples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.09373v1",
    "pdf_url": null
  },
  {
    "instance_id": "7ad39e8857bd43c085ab2c982710e2e9",
    "figure_id": "2209.15517v2-Figure6-1",
    "image_file": "2209.15517v2-Figure6-1.png",
    "caption": " 100-shot results (AP%) from Table 3 with error bar",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best on the CVC-ClinicDB dataset?",
    "answer": "Ours (Manual)",
    "rationale": "The figure shows the AP% for different methods on different datasets. The AP% for Ours (Manual) on the CVC-ClinicDB dataset is the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.15517v2",
    "pdf_url": null
  },
  {
    "instance_id": "6b56dc809b2a4fa58318bce0aaab2e7a",
    "figure_id": "2205.00943v2-Figure2-1",
    "image_file": "2205.00943v2-Figure2-1.png",
    "caption": " Learning performances on the continuous control tasks from the DMC Suite (Selected). The proposed CCLF on SAC outperforms the other baseline methods in terms of sample efficiency and converges much faster, averaged by 6 random runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the continuous control tasks from the DMC Suite (Selected) is the easiest for the proposed CCLF on SAC to learn?",
    "answer": "Cartpole Swingup.",
    "rationale": "The figure shows the learning performance of different methods on various continuous control tasks. The Cartpole Swingup task has the highest evaluation score and the fastest convergence rate for CCLF on SAC, indicating that it is the easiest task for the proposed method to learn.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.00943v2",
    "pdf_url": null
  },
  {
    "instance_id": "6e904ea76ab5417184ff374f21536c87",
    "figure_id": "1712.08685v2-Figure1-1",
    "image_file": "1712.08685v2-Figure1-1.png",
    "caption": " Dependence on bipartite edge sample rate fm. Left: WRE on MOVIE. Right: 1− Cor(k) on GITHUB. Top-100 dense ranks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest relative error on MOVIE and GITHUB?",
    "answer": "SimAdapt",
    "rationale": "The SimAdapt method has the lowest relative error on both datasets. This can be seen in the figure because the red line, which represents SimAdapt, is the lowest line in both plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1712.08685v2",
    "pdf_url": null
  },
  {
    "instance_id": "39350fc2c80b4b528f3bcdd7beafade2",
    "figure_id": "2209.01347v1-Figure4-1",
    "image_file": "2209.01347v1-Figure4-1.png",
    "caption": " NDCG@5 Results with different update schedule settings (𝑝: number of updates in model training).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which update schedule setting leads to the best NDCG@5 results for the Beauty dataset?",
    "answer": "EC4SRec(SSL)",
    "rationale": "The figure shows the NDCG@5 results for different update schedule settings on the Beauty and Clothing datasets. For the Beauty dataset, EC4SRec(SSL) consistently achieves the highest NDCG@5 values across all values of p.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.01347v1",
    "pdf_url": null
  },
  {
    "instance_id": "019611e779f147c0b3072a745232d058",
    "figure_id": "1905.04663v1-Figure2-1",
    "image_file": "1905.04663v1-Figure2-1.png",
    "caption": " Validation error of a set of models on MNIST (top) and CIFAR-10 (bottom) when we rotate the validation set at different angles. We see similar behaviors in both datasets. Notably, the PARTIAL method performs best.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on both MNIST and CIFAR-10 datasets?",
    "answer": "PARTIAL",
    "rationale": "The figure shows the validation error of different models on the MNIST and CIFAR-10 datasets. The PARTIAL model has the lowest validation error on both datasets, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.04663v1",
    "pdf_url": null
  },
  {
    "instance_id": "502ff87e378549d1892788ce13bfd80b",
    "figure_id": "1905.00076v3-Figure6-1",
    "image_file": "1905.00076v3-Figure6-1.png",
    "caption": " Histograms of measures of uncertainty derived from ensemble, EnD2 and EnD2 +AUX on in-domain (ID) and test out-of-domain (OOD) data from CIFAR-100 and TinyImageNet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which uncertainty measure is consistently lower for EnD2 + AUX compared to EnD2 and Ensemble across all datasets and uncertainty types?",
    "answer": "Knowledge Uncertainty",
    "rationale": "In all subplots, the green bars representing EnD2 + AUX are consistently lower than the blue bars (Ensemble) and red bars (EnD2) for Knowledge Uncertainty (subplots c, f, i, l). This indicates that EnD2 + AUX consistently estimates lower knowledge uncertainty compared to the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.00076v3",
    "pdf_url": null
  },
  {
    "instance_id": "183ed1b2d6c44e4f8baeb14f2a10a249",
    "figure_id": "2107.09937v1-Figure3-1",
    "image_file": "2107.09937v1-Figure3-1.png",
    "caption": " Test error vs. iterations of different models on four attack methods on CIFAR10 automobile vs. truck (Fig. 3a- 3d) and MNIST8m 0v4 (Fig. 3e-3h). (Since adv-linear-SVM cannot run iteratively like adv-SVM, we do not include it here.)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the lowest test error for the FGSM attack on CIFAR10 automobile vs. truck?",
    "answer": "adv-SVM(C)",
    "rationale": "The test error for adv-SVM(C) is consistently lower than the other models in Figure 3a.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.09937v1",
    "pdf_url": null
  },
  {
    "instance_id": "6c5b0acb5fd640aba8496d7be0c270e4",
    "figure_id": "1904.07392v1-Figure1-1",
    "image_file": "1904.07392v1-Figure1-1.png",
    "caption": " Average Precision vs. inference time per image across accurate models (top) and fast models (bottom) on mobile device. The green curve highlights results of NASFPN combined with RetinaNet. Please refer to Figure 9 for details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest Average Precision (AP) on the P100 GPU?",
    "answer": "NAS-FPN",
    "rationale": "The green curve in the top plot shows the AP for NAS-FPN, and it is the highest curve in the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.07392v1",
    "pdf_url": null
  },
  {
    "instance_id": "a7cae91bd10746328979784910b21195",
    "figure_id": "2106.15535v2-Figure18-1",
    "image_file": "2106.15535v2-Figure18-1.png",
    "caption": " Relative ratio of FPR in the biased training node selection experiment. Remaining results on Cora besides Figure 5. Each row corresponds to a different dominant class of choice. See Figure 5 for the plot settings.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best for class 4?",
    "answer": "GCN.",
    "rationale": "The figure shows the relative ratio of FPR for different models and classes. For class 4, the GCN model has the lowest relative ratio of FPR, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15535v2",
    "pdf_url": null
  },
  {
    "instance_id": "5ddae7f283d245df877621049fa83799",
    "figure_id": "2302.06586v3-Figure7-1",
    "image_file": "2302.06586v3-Figure7-1.png",
    "caption": " Effect of stitching CNNs and CNN-ViT.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest Top-1 accuracy with the lowest FLOPs?",
    "answer": "Swin-Ti",
    "rationale": "The figure shows the Top-1 accuracy of different models as a function of their FLOPs. The Swin-Ti model has the highest Top-1 accuracy and the lowest FLOPs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.06586v3",
    "pdf_url": null
  },
  {
    "instance_id": "5d685c16abfb4472b9a5f2915f78f368",
    "figure_id": "2207.14465v3-Figure1-1",
    "image_file": "2207.14465v3-Figure1-1.png",
    "caption": " FRPT consists of three essential modules: the discriminative perturbation prompt module to zoom and even exaggerate the discriminative elements within objects, the frozen pre-trained backbone network to extract rich object representation, and the category-specific awareness head to compress the species discrepancies within the extracted semantic features.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which module in the FRPT framework is responsible for extracting rich object representation?",
    "answer": "The frozen pre-trained backbone network.",
    "rationale": "The figure shows that the backbone network is responsible for extracting semantic features from the input image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.14465v3",
    "pdf_url": null
  },
  {
    "instance_id": "caa481f4755246b8a82461ef57cd43c2",
    "figure_id": "2301.11443v3-Figure7-1",
    "image_file": "2301.11443v3-Figure7-1.png",
    "caption": " The Large-N Regime",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the resolvent differences as the number of particles, N, increases?",
    "answer": "The resolvent differences decrease.",
    "rationale": "The plot shows that the resolvent differences decrease as N increases. This indicates that the large-N limit is a good approximation for the system.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.11443v3",
    "pdf_url": null
  },
  {
    "instance_id": "7ba64d0ccaac407f8bda665407e7f4d6",
    "figure_id": "2201.02026v2-Figure4-1",
    "image_file": "2201.02026v2-Figure4-1.png",
    "caption": " Performance of SenDM-base and base sized baselines on fpb75 given different amounts of fine-tuning examples. Lines indicate the mean and shaded areas indicate the standard deviation over the five seeds (see Section 3.4 for details). The dashed horizontal line indicates the prior of the common class in the dataset. SenDM∗ is the same as SenDM when trained excluding financial documents.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best when given a small number of fine-tuning examples?",
    "answer": "SendDM-base.",
    "rationale": "The figure shows that SendDM-base has the highest accuracy when given a small number of fine-tuning examples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.02026v2",
    "pdf_url": null
  },
  {
    "instance_id": "471d2b74b2744d8c9e196c020d264f9c",
    "figure_id": "1909.11409v3-Figure5-1",
    "image_file": "1909.11409v3-Figure5-1.png",
    "caption": " Visual qualitative comparison on ×3 scale (barbara from Set14).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image reconstruction method produces the highest quality image?",
    "answer": "ESRN (ours)",
    "rationale": "The figure shows the original high-resolution image (Ground-truth HR) and several reconstructions of the image using different methods. The PSNR and SSIM values are also shown for each method. ESRN (ours) has the highest PSNR and SSIM values, indicating that it produces the highest quality image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.11409v3",
    "pdf_url": null
  },
  {
    "instance_id": "e37ad874853748c9b01752bfefcb626d",
    "figure_id": "2107.08173v1-Figure1-1",
    "image_file": "2107.08173v1-Figure1-1.png",
    "caption": " The change of BLEU/Entity F1 scores for each task during the whole learning process (i.e., after learning new tasks).",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method consistently achieves the highest BLEU score across all tasks?",
    "answer": "TPEM.",
    "rationale": "The TPEM line is consistently above all other lines in the BLEU score plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.08173v1",
    "pdf_url": null
  },
  {
    "instance_id": "5238db849438424cb1d5539c0ec7ed5b",
    "figure_id": "2201.09802v4-Figure9-1",
    "image_file": "2201.09802v4-Figure9-1.png",
    "caption": " Learning curves for the PointPush1 task and ObservablePointPush1 task which uses partially transparent box. We also show the baseline algorithms performance with a “pseudo-LiDAR” observation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best on the PointPush1 task?",
    "answer": "TRPO Lagrangian (10M steps, proprio)",
    "rationale": "The figure shows the learning curves for the PointPush1 task for four different algorithms. The TRPO Lagrangian (10M steps, proprio) algorithm has the highest average reward and the lowest cost regret.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.09802v4",
    "pdf_url": null
  },
  {
    "instance_id": "19f639f114ed4f56b27d8fd0c4a91307",
    "figure_id": "2303.11934v1-Figure15-1",
    "image_file": "2303.11934v1-Figure15-1.png",
    "caption": " The GABA switch occurs by epoch ∼100 and Top-K is fully implemented by ∼200 epochs. At this point, the RMS and Adam optimizers begin continuously killing off neurons such that they are never activated for any training or validation data. These results are for training directly on CIFAR10 images. See the text for a discussion of training on ConvMixer embeddings.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer(s) continuously kill off neurons after epoch 200?",
    "answer": "RMS and Adam",
    "rationale": "The fraction of dead neurons for RMS and Adam optimizers continuously increases after epoch 200.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.11934v1",
    "pdf_url": null
  },
  {
    "instance_id": "1a7d854c836746a69bf7c997c0a15941",
    "figure_id": "2005.03358v1-Figure4-1",
    "image_file": "2005.03358v1-Figure4-1.png",
    "caption": " Experiments on data in the wild. From left to right, each example shows a single input image, our result, the result from HMD[54], and the result from Tang et al. [41].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most accurate results?",
    "answer": "Our method.",
    "rationale": "The results from our method are shown in the second column of each example, and they are visually more similar to the input images than the results from the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.03358v1",
    "pdf_url": null
  },
  {
    "instance_id": "88d14a2c1e974cff9846c9251efba13a",
    "figure_id": "2204.02877v2-Figure6-1",
    "image_file": "2204.02877v2-Figure6-1.png",
    "caption": " Experimental results of algorithm generalization in training environment.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in the Ant-Wind environment?",
    "answer": "PPO",
    "rationale": "The figure shows the average return for each algorithm in each environment. In the Ant-Wind environment, PPO has the highest average return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.02877v2",
    "pdf_url": null
  },
  {
    "instance_id": "6a29d34293a44cbb93c8e8a94943c036",
    "figure_id": "2112.06113v1-Figure13-1",
    "image_file": "2112.06113v1-Figure13-1.png",
    "caption": " Training clothes",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different types of clothing are shown in the image?",
    "answer": "Six.",
    "rationale": "The image shows three pairs of pants, three shirts, three dresses, three skirts, and three pairs of shorts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.06113v1",
    "pdf_url": null
  },
  {
    "instance_id": "8c4ce345e25249b88c5ed27470bb11da",
    "figure_id": "2003.01747v2-Figure2-1",
    "image_file": "2003.01747v2-Figure2-1.png",
    "caption": " Austen plots preserve the qualitative conclusions of Imbens’ analysis without imposing any restriction on the modeling of the observed data. In each plot, the black solid line indicates the partial R2 and α values that would induce a bias of at least $1000. Each plot also includes estimates for the strength of confounding for each of the nine covariates (red circles) as well as recent lag in earnings (RE75 and pos75, yellow circles), and the all preprogram earnings (RE74, pos74, RE75, pos75, green circles).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the covariates in the figure has the strongest confounding effect on the outcome?",
    "answer": "Recent earnings (RE75 and pos75, yellow circles).",
    "rationale": "The confounding effect of a covariate is indicated by its position in the Austen plot. The closer a covariate is to the black solid line, the stronger its confounding effect. In this figure, recent earnings are closest to the black solid line, indicating that they have the strongest confounding effect on the outcome.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.01747v2",
    "pdf_url": null
  },
  {
    "instance_id": "533d4f4b733c472cb65293321d830471",
    "figure_id": "1902.08412v1-Figure6-1",
    "image_file": "1902.08412v1-Figure6-1.png",
    "caption": " Change in accuracy of Deepwalk on CORA-ML.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best when a large number of edges are changed?",
    "answer": "A-Meta-Train.",
    "rationale": "The plot shows that A-Meta-Train has the highest accuracy when the percentage of edges changed is high.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.08412v1",
    "pdf_url": null
  },
  {
    "instance_id": "fd308057ff8c459db800d8149eac63cc",
    "figure_id": "1909.12673v2-Figure12-1",
    "image_file": "1909.12673v2-Figure12-1.png",
    "caption": " UFC101 error landscape.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which error landscape is more accurate, the actual or the estimated?",
    "answer": "The actual error landscape is more accurate.",
    "rationale": "The actual error landscape is based on the true values of the error, while the estimated error landscape is based on an approximation of the error. The actual error landscape is therefore more likely to be accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.12673v2",
    "pdf_url": null
  },
  {
    "instance_id": "9d1b908349ba4d5e9701dab39f6179bf",
    "figure_id": "2011.02687v1-Figure5-1",
    "image_file": "2011.02687v1-Figure5-1.png",
    "caption": " Span-F1 and Span-EM of baseline models and BLANC trained on NaturalQ. We categorize NaturalQ dataset into five groups by number answer texts appeared in a passage: n = 1, 2, 3, 4, and n ≥ 5. BLANC outperforms baseline models on every groups and the performance gap increases as the number of answer texts in a passage increases.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the NaturalQ dataset when there are 5 or more answer texts in a passage?",
    "answer": "BLANC",
    "rationale": "The figure shows that BLANC has the highest Span-F1 and Span-EM scores for all categories of the NaturalQ dataset, including the category with 5 or more answer texts in a passage.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.02687v1",
    "pdf_url": null
  },
  {
    "instance_id": "bb3121dbcf764898bd62123b931e8ddc",
    "figure_id": "2110.04866v1-Figure4-1",
    "image_file": "2110.04866v1-Figure4-1.png",
    "caption": " Test accuracy vs. user node degree for Eedi.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best when the user node degree is high?",
    "answer": "CoRGi",
    "rationale": "The plot shows that the CoRGi model has the highest test accuracy for all values of D, which represents the user node degree.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.04866v1",
    "pdf_url": null
  },
  {
    "instance_id": "b1d9fc444c2e4ae4a14505eb7b6f8d33",
    "figure_id": "2202.06866v1-Figure1-1",
    "image_file": "2202.06866v1-Figure1-1.png",
    "caption": " Example of an approximated Delaunay graph GD = GD(R ∪ E) (solid edges) obtained from the Voronoi cells (dashed gray lines) of the considered R and E points as well as the distilled Delaunay graph GDD containing three connected components (solid dark colored and gray edges) used in the final evaluation of R and E. See Section 3 for furher details.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many connected components are there in the distilled Delaunay graph GDD?",
    "answer": "Three",
    "rationale": "The caption states that the distilled Delaunay graph GDD contains three connected components. This can be seen in the figure by the three distinct clusters of nodes connected by solid dark colored and gray edges.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.06866v1",
    "pdf_url": null
  },
  {
    "instance_id": "13fcdcc90fdd45f48735a480c1ac98a1",
    "figure_id": "2306.01304v2-Figure7-1",
    "image_file": "2306.01304v2-Figure7-1.png",
    "caption": " Case study of Robustness. The left is the predictions of CREPE, and the right is the predictions of JEPOO.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better at predicting the fundamental frequency of a signal?",
    "answer": "CREPE.",
    "rationale": "The left panel shows the predictions of CREPE, which are closer to the ground truth than the predictions of JEPOO, shown in the right panel.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01304v2",
    "pdf_url": null
  },
  {
    "instance_id": "7f10c64d087f47b6a3d281df24e97192",
    "figure_id": "1801.01725v1-Figure4-1",
    "image_file": "1801.01725v1-Figure4-1.png",
    "caption": " The Chinese Word segmentation, NER and Term Weighting preprocessing results for a given product title.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the brand of the product?",
    "answer": "YinMan",
    "rationale": "The brand of the product is listed in the NER row of the table.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1801.01725v1",
    "pdf_url": null
  },
  {
    "instance_id": "2a14839200584e28a75ffa5f13a14b7d",
    "figure_id": "1908.07070v1-Figure3-1",
    "image_file": "1908.07070v1-Figure3-1.png",
    "caption": " Qualitative comparison of horizon line predictions. From top row to bottom row: InteriorNet, ScanNet and SUN360. Our trained model outperforms other baselines in terms of accuracy on all three datasets.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best at predicting the horizon line in the images?",
    "answer": "Our trained model.",
    "rationale": "The figure shows that our trained model (orange dotted line) is closest to the ground truth (blue solid line) in all of the images. This means that our model is the most accurate at predicting the horizon line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.07070v1",
    "pdf_url": null
  },
  {
    "instance_id": "0186030ecf1d49ceb1dcff3460f34231",
    "figure_id": "2305.19500v1-Figure3-1",
    "image_file": "2305.19500v1-Figure3-1.png",
    "caption": " Prompt performance and variation on each dataset using RoBERTa-large. The vertical axis represents the metric of each prompt over Xtrain. MRPC uses f1 metric, and others use accuracy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the most consistent prompt performance?",
    "answer": "SST-2",
    "rationale": "The box plot for SST-2 has the smallest interquartile range (IQR), which indicates that the data points are more closely clustered together. This suggests that the prompt performance on SST-2 is more consistent than on the other datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19500v1",
    "pdf_url": null
  },
  {
    "instance_id": "c371c03a4a0147b7aa69c070f74e0b99",
    "figure_id": "1910.11481v1-Figure7-1",
    "image_file": "1910.11481v1-Figure7-1.png",
    "caption": " To visualize how the sampled images are located on the image manifold, we sampled 10 outpainted images for 100 testing input images in CelebA datast[29]. Then, we extracted features for all images using pretrained VGG network[39] and ran t-sne[31] on the features to visualize the manifold in two dimension. The colored points (pink, orange, green, red, cyan) indicate the sampled images for five specific conditional inputs. Within the same color points, the more spread the points indicate more diverse the sampled outputs for the specific conditional input. The blue dots represent the rest of sampled images.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the six methods produces the most diverse set of sampled images for a specific conditional input?",
    "answer": " RNDiv(Ours)",
    "rationale": " The figure shows the t-SNE visualization of the image manifold for six different methods. The colored points indicate the sampled images for five specific conditional inputs. Within the same color points, the more spread the points, the more diverse the sampled outputs for the specific conditional input. We can see that the points for RNDiv(Ours) are the most spread out, indicating that this method produces the most diverse set of sampled images for a specific conditional input.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.11481v1",
    "pdf_url": null
  },
  {
    "instance_id": "6ca779b70973472a90a0dbd952a98de4",
    "figure_id": "2103.05137v2-Figure41-1",
    "image_file": "2103.05137v2-Figure41-1.png",
    "caption": " Top: Distribution of object scale in ObjectNet dataset (113 annotated classes), Bottom: Distribution object scale in ILSVRC2012-2014 single-object localization (dark green) and PASCAL VOC 2012 (light blue) validation sets. Object scale is fraction of image area occupied by an average object instance. Please see Fig. 16 in Russakovsky et al. (2015) .",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the largest proportion of objects with a scale of 0.2?",
    "answer": "PASCAL VOC 2012.",
    "rationale": "The bottom plot shows the distribution of object scales for three datasets. The light blue bars represent the PASCAL VOC 2012 dataset, and the bar at 0.2 is the tallest of the three datasets at that scale.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.05137v2",
    "pdf_url": null
  },
  {
    "instance_id": "6457593bfebe430799cf402c4ea9faf4",
    "figure_id": "2106.05931v3-Figure14-1",
    "image_file": "2106.05931v3-Figure14-1.png",
    "caption": " Uncurated samples generated by LSGM with the SGM prior applied to the latent variables of 32×32 spatial dimensions, on the CelebA-HQ-256 dataset. Sampling in the latent space is done using the probability flow ODE.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the image show?",
    "answer": "The image shows 16 photographs of people.",
    "rationale": "The image contains 16 photos, each showing the face of a person.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05931v3",
    "pdf_url": null
  },
  {
    "instance_id": "a644327fcf12401a97cf0e96ef065a9f",
    "figure_id": "2202.13437v1-Figure7-1",
    "image_file": "2202.13437v1-Figure7-1.png",
    "caption": " Histogram of gradient strength of grad1 = ∇x′gθ(x′, x, y) and grad2 = ∇x′ ĉX (x′, x) on MNIST and CIFAR10 dataset. We use L2 norm for the cost function cX (x′, x) , τ = η and λ = 1",
    "figure_type": "** \nplot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \nWhich of the following histograms has the most values close to zero?",
    "answer": " \n(a) CIFAR10, grad1",
    "rationale": " \nThe histogram in (a) has the highest peak at zero, which indicates that most of the values are close to zero. This can be seen by comparing the heights of the peaks in the different histograms. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.13437v1",
    "pdf_url": null
  },
  {
    "instance_id": "23542cf316e2408f86f3acdd2d82f7b2",
    "figure_id": "2208.07448v4-Figure2-1",
    "image_file": "2208.07448v4-Figure2-1.png",
    "caption": " Visualization of EEG graphs before, begining and during a typical seizure (SZ) incident. Darker colors indicate higher values.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which brain regions show the highest level of activity during a seizure?",
    "answer": "The frontal and central regions.",
    "rationale": "The figure shows that the nodes in the frontal and central regions have the darkest colors, which indicates higher values of activity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.07448v4",
    "pdf_url": null
  },
  {
    "instance_id": "e6764249e5c34febb7c63166560231ed",
    "figure_id": "2002.00743v2-Figure3-1",
    "image_file": "2002.00743v2-Figure3-1.png",
    "caption": " This graph shows the accuracy of bilingual translation pairs. The red bar indicate translation accuracy using the barycenter of all languages (HR, EN, FI, FR, DE, RU, IT, TR), while the blue bar correspond to the barycenter of (HR, EN, FR, DE, IT, RU).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language pair has the highest translation accuracy when using the barycenter of all languages?",
    "answer": "hr-it",
    "rationale": "The orange bar for hr-it is the tallest, indicating that it has the highest accuracy score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.00743v2",
    "pdf_url": null
  },
  {
    "instance_id": "2a9d3a74b1574201a52608b261a59aec",
    "figure_id": "2202.01169v2-Figure19-1",
    "image_file": "2202.01169v2-Figure19-1.png",
    "caption": " RL-R performance for 64E continues to scale well compared to dense up to 7B base model size.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture performs better as the model size increases, RL-R or Dense?",
    "answer": "RL-R",
    "rationale": "The plot shows that the validation loss for RL-R is consistently lower than the validation loss for Dense, across all model sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.01169v2",
    "pdf_url": null
  },
  {
    "instance_id": "49692c884a5a4f02a9935296311f2fcc",
    "figure_id": "2112.12707v1-Figure5-1",
    "image_file": "2112.12707v1-Figure5-1.png",
    "caption": " Test accuracy of the models on Cifar100 dataset under (a) PGD attack and (b) FGSM attack",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is the most robust to FGSM attack?",
    "answer": "ResNet34-GP",
    "rationale": "The figure shows the test accuracy of different models under FGSM attack. ResNet34-GP has the highest test accuracy for all values of epsilon, which indicates that it is the most robust to FGSM attack.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.12707v1",
    "pdf_url": null
  },
  {
    "instance_id": "64c8408be71341a693443e3bd34444be",
    "figure_id": "2003.09712v1-Figure2-1",
    "image_file": "2003.09712v1-Figure2-1.png",
    "caption": " Experimental results for the problem in Figure 1c. (a, e): robustness of teaching for ∆Q0 -imperfect teacher. (b, f): for ∆η-imperfect teacher, the learner’s error could be high when the teacher overestimates η or the teaching set size could be arbitrary large when the teacher underestimates η. (c, g): robustness of teaching for ∆Z -imperfect teacher. (d, h): robustness of teaching for ∆φ-imperfect teacher.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of imperfect teacher leads to the largest teaching set size?",
    "answer": "∆η-imperfect teacher",
    "rationale": "In Figure 1f, the teaching set size for the ∆η-imperfect teacher is much larger than the teaching set size for the other types of imperfect teachers. This is because the ∆η-imperfect teacher underestimates the value of η, which leads to a larger teaching set size.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.09712v1",
    "pdf_url": null
  },
  {
    "instance_id": "72de39f1430f46f28e86336760c0080b",
    "figure_id": "2205.06924v1-Figure12-1",
    "image_file": "2205.06924v1-Figure12-1.png",
    "caption": " FID curves on the CIFAR-10 dataset. The FID score is reported every 5 epochs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves a lower FID score on the CIFAR-10 dataset, CoopFlow or CoopFlow (Pre)?",
    "answer": "CoopFlow (Pre)",
    "rationale": "The FID curve for CoopFlow (Pre) is consistently lower than the FID curve for CoopFlow, indicating that CoopFlow (Pre) achieves a lower FID score on the CIFAR-10 dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.06924v1",
    "pdf_url": null
  },
  {
    "instance_id": "840c13e68dc74893995539beba1ee022",
    "figure_id": "2004.05479v1-Figure1-1",
    "image_file": "2004.05479v1-Figure1-1.png",
    "caption": " Recurrent Neural Network for BSM.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of learning rule is used in the connections between the output layer and the hidden layer?",
    "answer": "Anti-Hebbian learning rule.",
    "rationale": "The legend in the figure indicates that the connections between the output layer and the hidden layer are marked with a red dot, which corresponds to the Anti-Hebbian learning rule.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.05479v1",
    "pdf_url": null
  },
  {
    "instance_id": "67d945163d1a42fab09bfe24891f3e4f",
    "figure_id": "1811.10928v2-Figure4-1",
    "image_file": "1811.10928v2-Figure4-1.png",
    "caption": " Learning curves of A3C for the 4 chosen learning rates (4e-4, 2e-4, 1e-4, 5e-5) on the Sokoban level generator.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which learning rate performed the best?",
    "answer": "5e-5",
    "rationale": "The line for the learning rate of 5e-5 has the highest ratio of tasks solved at the end of the experiment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.10928v2",
    "pdf_url": null
  },
  {
    "instance_id": "b8edabf0bfc741739bf91c61d5d5c4a2",
    "figure_id": "1805.10309v2-Figure3-1",
    "image_file": "1805.10309v2-Figure3-1.png",
    "caption": " Learning curves for various ensembles on sparse locomotion tasks. Mean and standard-deviation over 3 random seeds are plotted.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the SparseAnt task?",
    "answer": "Si-interact-JS",
    "rationale": "The figure shows the learning curves for various algorithms on the SparseAnt task. The Si-interact-JS algorithm has the highest reward at the end of the training period, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.10309v2",
    "pdf_url": null
  },
  {
    "instance_id": "85952ac7a1d046d89980fcc41f6fb555",
    "figure_id": "2305.18978v2-Figure9-1",
    "image_file": "2305.18978v2-Figure9-1.png",
    "caption": " Benchmark the inverse design tasks using all the training data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization algorithm achieves the lowest MSE on the SCF task?",
    "answer": "RS",
    "rationale": "The figure shows the MSE achieved by different optimization algorithms on the SCF task. The RS algorithm achieves the lowest MSE, as its line is the lowest on the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.18978v2",
    "pdf_url": null
  },
  {
    "instance_id": "c09cf13302294fc4b5adb188a60778f7",
    "figure_id": "2310.04128v1-Figure3-1",
    "image_file": "2310.04128v1-Figure3-1.png",
    "caption": " Training statistics computed over ten trials and one epoch of PPO, with FFM in orange. FFM trains nearly two orders of magnitude faster on the GPU than the fastest RNNs (note the logscale x axis). Even on the CPU, FFM trains roughly one order of magnitude faster than RNNs on the GPU. FFM has sub-millisecond CPU inference latency, making it useful for on-policy algorithms. FFM memory usage is on-par with RNNs, providing scalability to long sequences. Despite efficiency improvements, FFM still attains greater episodic reward than other models on POPGym.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest reward and the lowest peak memory usage?",
    "answer": "FFM.",
    "rationale": "The figure shows that FFM has the highest reward and the lowest peak memory usage. This is evident from the bar graphs, where FFM is the rightmost bar in the reward graph and the leftmost bar in the peak memory usage graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.04128v1",
    "pdf_url": null
  },
  {
    "instance_id": "750603dfd3b24bb2b06fa5d5337cf66c",
    "figure_id": "2008.13367v2-Figure2-1",
    "image_file": "2008.13367v2-Figure2-1.png",
    "caption": " An example of the output from the FCOS head which includes a classification score, a bounding box and a centerness score.",
    "figure_type": "photograph",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the centerness score for the person in the bottom left box?",
    "answer": "0.33",
    "rationale": "The centerness score is a measure of how close the predicted bounding box is to the center of the object. In this case, the centerness score for the person in the bottom left box is 0.33. This means that the predicted bounding box is not very close to the center of the person.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.13367v2",
    "pdf_url": null
  },
  {
    "instance_id": "63a3df9a56cd4ceb85d345b91df94f43",
    "figure_id": "1812.06190v1-Figure1-1",
    "image_file": "1812.06190v1-Figure1-1.png",
    "caption": " The encoder (left) and decoder (right) for each of the baselines and our model. Shaded nodes represent conditioning variables. Dotted arrows represent adversarially trained prediction networks used to minimize mutual information between variables.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models depicted in the figure has the most complex encoder?",
    "answer": "CSVAE (ours)",
    "rationale": "The encoder for CSAVE has three nodes, while the encoders for CondVAE and CondVAE-info have only two nodes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.06190v1",
    "pdf_url": null
  },
  {
    "instance_id": "ecb45524c553462f80611a092fac2943",
    "figure_id": "2211.00522v2-Figure3-1",
    "image_file": "2211.00522v2-Figure3-1.png",
    "caption": " A visual illustration of timeline and latency metrics of a streaming ASR system.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which delay metric is most important for user experience?",
    "answer": "User Sensitive Delay",
    "rationale": "The figure shows three delay metrics: First Token Emission Delay, Last Token Emission Delay, and Endpoint Delay. The User Sensitive Delay is the sum of the First Token Emission Delay and the Endpoint Delay. This is the total time that the user has to wait before they receive the final transcript. Therefore, this is the most important delay metric for user experience.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.00522v2",
    "pdf_url": null
  },
  {
    "instance_id": "3391a49aa97448b685c67beb5336f8f6",
    "figure_id": "2010.04440v3-Figure11-1",
    "image_file": "2010.04440v3-Figure11-1.png",
    "caption": " Average cosine similarity between gradient measurements. AVEC empirically reduces the variance compared to PPO or PPO without a baseline (PPO-nobaseline). Trajectory size used in estimation of the gradient variance: 3000 (upper row), 6000 (middle row), 9000 (lower row). Lines are average performances and shaded areas represent one standard deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest variance in gradient measurements?",
    "answer": "AVEC-PPO",
    "rationale": "The figure shows the average pairwise cosine similarity between gradient measurements for different methods. AVEC-PPO has the lowest variance, as indicated by the smaller shaded areas around the lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.04440v3",
    "pdf_url": null
  },
  {
    "instance_id": "551378cd085145468e793d32df47ea44",
    "figure_id": "2209.15200v5-Figure1-1",
    "image_file": "2209.15200v5-Figure1-1.png",
    "caption": " Main architecture of TDANet. N and T ′ denote the number of channels and length of features, respectively. By down-sampling S times, TDANet contains S + 1 features with different temporal resolutions. Here, we set S to 3. The red, blue, and orange arrows indicate bottom-up, top-down, and lateral connections, respectively. (a) The structure of the encoder, where “DWConv” denotes a depthwise convolutional layer with a kernel size of 5 and stride size of 2 followed by GLN. (b) The “Up-sample” layer denotes nearest neighbor interpolation. (c) The structure of decoder, where the LA layer adaptively modulates features of different scales by a set of learnable parameters.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three types of connections used in TDANet?",
    "answer": "Bottom-up, top-down, and lateral connections.",
    "rationale": "The figure shows the three types of connections in different colors: red for bottom-up, blue for top-down, and orange for lateral.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.15200v5",
    "pdf_url": null
  },
  {
    "instance_id": "a02f1469c60d42878c7e1d7fd5575c30",
    "figure_id": "2103.05137v2-Figure10-1",
    "image_file": "2103.05137v2-Figure10-1.png",
    "caption": " Performance of models on object bounding boxes (from left to right: ResNet, Inception3, and MNASNet).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed best on the task of object bounding boxes?",
    "answer": "ResNet.",
    "rationale": "The figure shows the performance of three models on the task of object bounding boxes. The x-axis shows the accuracy of each model, and the y-axis shows the different objects. ResNet has the highest accuracy for most of the objects, which indicates that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.05137v2",
    "pdf_url": null
  },
  {
    "instance_id": "4a4140a5e7e2481eb20e5b490b0686cf",
    "figure_id": "1806.05034v4-Figure12-1",
    "image_file": "1806.05034v4-Figure12-1.png",
    "caption": " Histograms showing the amount of (ground truth) ambiguous and unambiguous lesions as a function of the number of times the model produces a sample with a lesion in it (out of 16 samples). Each histogram corresponds to one model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is most likely to produce a sample with a lesion in it, even if the lesion is ambiguous?",
    "answer": "The Probabilistic U-Net model.",
    "rationale": "The Probabilistic U-Net model has the highest frequency of ambiguous lesions for each number of predicted samples with lesions. This suggests that the model is more likely to produce a sample with a lesion in it, even if the lesion is ambiguous.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.05034v4",
    "pdf_url": null
  },
  {
    "instance_id": "b65b8c31aeba4aee906c42d0db907e3f",
    "figure_id": "2201.13093v1-Figure4-1",
    "image_file": "2201.13093v1-Figure4-1.png",
    "caption": " Average MUSHRA scores with 13 listeners and Student’s t distribution comparing PostGAN with other GAN-based models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GAN-based model achieved the highest average MUSHRA score for item1?",
    "answer": "Reference",
    "rationale": "The bar for the Reference model for item1 is the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.13093v1",
    "pdf_url": null
  },
  {
    "instance_id": "7e9ae446483e4ea994236d35a0879937",
    "figure_id": "2003.10423v1-Figure3-1",
    "image_file": "2003.10423v1-Figure3-1.png",
    "caption": " Example matches between EPC and MADDPG trained agents in Grassland",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the different types of agents shown in the figure?",
    "answer": "Sheep and wolves.",
    "rationale": "The figure shows two different types of agents, sheep and wolves, interacting in a simulated environment. The sheep are shown as blue circles and the wolves are shown as red circles. The green circles represent landmarks in the environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.10423v1",
    "pdf_url": null
  },
  {
    "instance_id": "6aca696d2d8d4f2da1138e9851da2dc9",
    "figure_id": "1912.01991v1-Figure4-1",
    "image_file": "1912.01991v1-Figure4-1.png",
    "caption": " Invariance of PIRL representations. Distribution of l2 distances between unit-norm image representations, f(vI)/‖f(vI)‖2, and unit-norm representations of the transformed image, g(vIt )/‖g(vIt )‖2. Distance distributions are shown for PIRL and Jigsaw representations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, PIRL or Jigsaw, produces representations that are more invariant to transformations?",
    "answer": "PIRL.",
    "rationale": "The plot shows the distribution of l2 distances between unit-norm image representations and unit-norm representations of the transformed image for both PIRL and Jigsaw. The l2 distance measures how similar two representations are. A smaller l2 distance indicates that the representations are more similar. The plot shows that the l2 distances for PIRL are generally smaller than the l2 distances for Jigsaw. This indicates that PIRL representations are more similar to the representations of the transformed image, which means that they are more invariant to transformations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.01991v1",
    "pdf_url": null
  },
  {
    "instance_id": "0228e21447034a40925d6149232b3605",
    "figure_id": "1810.12348v3-Figure2-1",
    "image_file": "1810.12348v3-Figure2-1.png",
    "caption": " Top-1 ImageNet validation error (%) for the proposed (left) GE-θ− and (right) GE-θ designs based on a ResNet-50 architecture (the baseline label indicates the performance of the original ResNet-50 model in both plots). For reference, ResNet-101 achieves a top-1 error of 22.20%. See Sec. 3 for further details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which design achieved the lowest Top-1 error?",
    "answer": "GE-θ−",
    "rationale": "The plot on the left shows that GE-θ− has a lower Top-1 error than GE-θ for all extent ratios.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.12348v3",
    "pdf_url": null
  },
  {
    "instance_id": "3b0e240bd31949ed9a0a1a703caa3b35",
    "figure_id": "2211.14575v2-Figure5-1",
    "image_file": "2211.14575v2-Figure5-1.png",
    "caption": " Video prediction on the BAIR dataset. The model predicts future frames conditioned on a single initial frame. Thanks to VQGAN, RIVER can be used to generate high resolution videos.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the robot arm doing in the image?",
    "answer": "The robot arm is picking up a toy zebra.",
    "rationale": "The figure shows a series of frames from a video, and in each frame, the robot arm is moving closer to the zebra. In the final frame, the robot arm has grasped the zebra and is lifting it off the table.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.14575v2",
    "pdf_url": null
  },
  {
    "instance_id": "651f3ea683504bf3887131beb5ae3b21",
    "figure_id": "2101.02098v1-Figure3-1",
    "image_file": "2101.02098v1-Figure3-1.png",
    "caption": " DLP and FP values after the classifer for each concert evaluated with Re-MOVE (120,30), categorized by genre.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which music genre tends to have the highest DLP values?",
    "answer": "Electro",
    "rationale": "The plot shows the DLP values for each concert evaluated with Re-MOVE (120,30), categorized by genre. The Electro genre has the highest DLP values, with most of the data points clustered around 80%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.02098v1",
    "pdf_url": null
  },
  {
    "instance_id": "949494f791624927b45837259d8d016d",
    "figure_id": "1806.05034v4-Figure3-1",
    "image_file": "1806.05034v4-Figure3-1.png",
    "caption": "Fig. 3b shows samples of each approach in the comparison given one input image. In Appendix G we show further samples of other images, produced by our approach. Fig. 4b shows that the Probabilistic U-Net on the Cityscapes task outperforms the baseline methods when sampling 4, 8 and 16 times in terms of the energy distance (numerical results can be found in Table 3). This edge in segmentation performance at 16 samples is highly significant according to the Wilcoxon signed-rank test (p-value ∼ O(10−77)). We have also conducted ablation experiments in order to explore which elements of our architecture contribute to its performance. These were (1) Fixing the prior, (2) Fixing the prior, and not using the context in the posterior and (3) Injecting the latent features at the beginning of the U-Net. Each of these variations resulted in a lower performance. Detailed results can be found in Appendix D.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the probability of flipping 'sidewalk' to 'sidewalk 2'?",
    "answer": "8/17",
    "rationale": "The text states that \"we create ambiguities by artificial random flips of five classes to newly introduced classes. We flip 'sidewalk' to 'sidewalk 2' with a probability of 8/17.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.05034v4",
    "pdf_url": null
  },
  {
    "instance_id": "05393b858c0146a9bd5b06206039ef1d",
    "figure_id": "2205.12870v2-Figure7-1",
    "image_file": "2205.12870v2-Figure7-1.png",
    "caption": " Comparison of translation performance on duplicate and non-duplicate sentences. Duplicate sentences are ones that appear in the training set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of sentence has higher translation performance according to ROUGE and BLEU scores?",
    "answer": "Non-duplicate sentences.",
    "rationale": "The figure shows that the bars for non-duplicate sentences are consistently higher than the bars for duplicate sentences for all of the metrics shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.12870v2",
    "pdf_url": null
  },
  {
    "instance_id": "f590d1af8b0d4f95a0f62bd123aeef5d",
    "figure_id": "2202.01587v2-Figure2-1",
    "image_file": "2202.01587v2-Figure2-1.png",
    "caption": " Strengths of MiDaS. (a) It rapidly finds overall the most representative sub-hypergraphs among 13 approaches from 11 real-world hypergraphs. (b) Especially, it accurately preserves the degree distribution. See Section 5 for details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach is the fastest and most accurate?",
    "answer": "MiDaS",
    "rationale": "Figure (a) shows that MiDaS has the lowest average ranking and the fastest elapsed time. \nThis means that MiDaS is both the fastest and most accurate approach.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.01587v2",
    "pdf_url": null
  },
  {
    "instance_id": "a5e38f6b4a2349bf8cc25ede048cf981",
    "figure_id": "2012.13245v1-Figure5-1",
    "image_file": "2012.13245v1-Figure5-1.png",
    "caption": " Regret of different bandit methods on simulated dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which bandit method has the lowest regret?",
    "answer": "LMDB",
    "rationale": "The figure shows that the LMDB method has the lowest regret at all time steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.13245v1",
    "pdf_url": null
  },
  {
    "instance_id": "195f4fa9d1f149ef8a7c13e324114559",
    "figure_id": "2106.02105v2-Figure10-1",
    "image_file": "2106.02105v2-Figure10-1.png",
    "caption": " CIFAR-10 data: t-SNE plots of destination-network representations of representationtargeted adversarial examples generated by using whitebox ResNet50 models of specified ε-robustness. (Best viewed in color and magnified.)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network appears to be the most robust to adversarial examples?",
    "answer": "CLIP",
    "rationale": "CLIP's t-SNE plots show the least amount of change across different values of epsilon, indicating that its representations are less sensitive to adversarial perturbations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02105v2",
    "pdf_url": null
  },
  {
    "instance_id": "26751f0296194a05bc5169280520ee62",
    "figure_id": "1810.13243v1-Figure1-1",
    "image_file": "1810.13243v1-Figure1-1.png",
    "caption": " Validation accuracy corresponding to models on the following 6 different curves - curve GA represents curve connecting mode G (one found with default hyperparameters) and mode A (using large batch size), similarly, curve GB connects mode G and mode B (using Adam), curve GC connects to mode C (using linearly decaying learning rate), curve GD to mode D (with lesser L2 regularization), curve GE to mode E (using a poor initialization), and curve GF to mode F (without using data augmentation). t = 0 corresponds to mode G for all plots.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model had the lowest validation accuracy at the 20th epoch?",
    "answer": "Model GE (Bad initialization (variance = 3x))",
    "rationale": "Curve GE is the orange curve, and at the 20th epoch, it is the lowest of all the curves.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.13243v1",
    "pdf_url": null
  },
  {
    "instance_id": "0f4c80840a7041f9b61c03faf4679b24",
    "figure_id": "1806.09842v3-Figure2-1",
    "image_file": "1806.09842v3-Figure2-1.png",
    "caption": " Convergence of different solvers for QDFSM over three different real datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which solver converges the fastest for the mushroom dataset?",
    "answer": "QRCDM-SPE",
    "rationale": "The figure shows that the QRCDM-SPE curve decreases the fastest, meaning it reaches the smallest gap in the least amount of time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.09842v3",
    "pdf_url": null
  },
  {
    "instance_id": "a36140a66c43418a81935bc7a859055a",
    "figure_id": "2107.07702v1-Figure8-1",
    "image_file": "2107.07702v1-Figure8-1.png",
    "caption": " Visualization of time series mixup. Each of the series is \"mixed up\" with the its horizontal neighbor time series: (a) with (b) and (c) with (d)).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two time series are most similar to each other?",
    "answer": "(a) and (b)",
    "rationale": "The two time series share the same general trend and shape, with some minor differences in the amplitude of the peaks and valleys.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.07702v1",
    "pdf_url": null
  },
  {
    "instance_id": "a92b7f6fa995421f8e19753cb5dec02c",
    "figure_id": "1810.09225v2-Figure3-1",
    "image_file": "1810.09225v2-Figure3-1.png",
    "caption": " Heatmaps of robust test error using our cost-sensitive robust classifier on MNIST for various real-valued cost tasks: (a) small-large; (b) large-small.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which digit is most likely to be misclassified as a 9 in the small-large cost task?",
    "answer": "Digit 4.",
    "rationale": "The heatmap in (a) shows that the robust test error for classifying a 4 as a 9 is 5.9%, which is the highest value in the row for digit 4.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.09225v2",
    "pdf_url": null
  },
  {
    "instance_id": "96027c499e4c467c99bd1f3367dd70b7",
    "figure_id": "2005.09669v2-Figure2-1",
    "image_file": "2005.09669v2-Figure2-1.png",
    "caption": "Fig 2. Approximately sampling from π ∝ e−‖·‖ by sampling from πβ ∝ e−‖·‖−β‖·−1‖2 (β = .0005). Algorithms are initialized at a random X0 with ‖X0‖ = 1000. The plot shows the squared distance of the running means to 0.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges to the target distribution the fastest?",
    "answer": "NLA.",
    "rationale": "The plot shows the squared distance of the running means to 0 for each algorithm. NLA's squared distance decreases the fastest, which indicates that it converges to the target distribution the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.09669v2",
    "pdf_url": null
  },
  {
    "instance_id": "f81f3957d2024de5bdc3dbcc0496996a",
    "figure_id": "2206.12327v1-Figure3-1",
    "image_file": "2206.12327v1-Figure3-1.png",
    "caption": " The performance of SL-VAE with different forward models under the SI diffusion pattern.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which forward model achieved the best AUC performance on the Jazz dataset?",
    "answer": "DeepI5",
    "rationale": "The AUC score for SL-VAE with DeepI5 is the highest among all forward models on the Jazz dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.12327v1",
    "pdf_url": null
  },
  {
    "instance_id": "9390491417cf4ea688c7647f5314bb50",
    "figure_id": "2008.02217v3-FigureA.5-1",
    "image_file": "2008.02217v3-FigureA.5-1.png",
    "caption": "Figure A.5: (a): change of count density during training is depicted for the first 20, 000 steps. (b): the corresponding distribution of the Frobenius norm of the Jacobian of the softmax function is depicted. The gradients with respect to the weights are determined by the Jacobian J defined in Eq. (59) as can be seen in Eq. (418), Eq. (429), and Eq. (435).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the density of counts change during training?",
    "answer": "The density of counts increases during training.",
    "rationale": "The figure shows that the density of counts is higher in the later stages of training than in the earlier stages. This is because the model is learning to better represent the data and is therefore able to generate more accurate predictions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.02217v3",
    "pdf_url": null
  },
  {
    "instance_id": "6ed631d5be614a9a960946703035eb54",
    "figure_id": "2211.10772v4-Figure13-1",
    "image_file": "2211.10772v4-Figure13-1.png",
    "caption": " More qualitative results on CTW1500.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of business does the sign with the lobster advertise?",
    "answer": "The sign with the lobster advertises a restaurant.",
    "rationale": "The sign has the word \"lobster\" on it, which is a type of food typically served at restaurants. Additionally, the sign mentions \"Perkins Cove,\" which is likely the name of the restaurant.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.10772v4",
    "pdf_url": null
  },
  {
    "instance_id": "caf9a731435a4b47a5a94c8392f21c08",
    "figure_id": "2210.07128v3-Figure3-1",
    "image_file": "2210.07128v3-Figure3-1.png",
    "caption": " A PROPARA example (left) and its corresponding Python code (right). We use a string to represent a concrete location (e.g., soil), UNK to represent an unknown location, and None to represent non-existence.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the state_0 variable track in the PROPARA example?",
    "answer": "The location/state of water.",
    "rationale": "The figure shows that the state_0 variable is initialized to \"soil\" in the init() function. The roots_absorb_water_from_soil() function then sets the state_0 variable to \"roots\", indicating that the water has been absorbed by the roots. Finally, the water_flows_to_leaf() function sets the state_0 variable to \"leaf\", indicating that the water has flowed to the leaf. This shows that the state_0 variable tracks the location/state of water.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.07128v3",
    "pdf_url": null
  },
  {
    "instance_id": "88ae5c38c5e94eef9de85e84725c89fc",
    "figure_id": "2307.16779v1-Figure1-1",
    "image_file": "2307.16779v1-Figure1-1.png",
    "caption": " Comparison of our approach (LADR) and baselines when retrieving using TAS-B [13] on the TREC Deep Learning 2019 dataset, in terms of nDCG, Recall@1000, and latency (single CPU). LADR establishes a new Pareto frontier with higher nDCG and Recall at latencies below 16ms/query.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest nDCG at a latency of 16 ms/q?",
    "answer": "LADR",
    "rationale": "The figure shows the nDCG and Recall@1000 of different methods at different latencies. At a latency of 16 ms/q, the orange line representing LADR is the highest, indicating that it has the highest nDCG.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.16779v1",
    "pdf_url": null
  },
  {
    "instance_id": "e39c122e035c435795b8d31cf6d1795c",
    "figure_id": "2109.05463v2-Figure3-1",
    "image_file": "2109.05463v2-Figure3-1.png",
    "caption": " AUC evaluation metric. The smaller area under the curve, the better the result.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which curve in the figure represents a model with better performance?",
    "answer": "Curve 2.",
    "rationale": "The AUC evaluation metric measures the area under the curve. The smaller the area under the curve, the better the model performs. In this figure, Curve 2 has a smaller area under the curve than Curve 1, indicating that it is the better-performing model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.05463v2",
    "pdf_url": null
  },
  {
    "instance_id": "d044f200e1b14c898420d91301aace09",
    "figure_id": "2007.00201v1-Figure2-1",
    "image_file": "2007.00201v1-Figure2-1.png",
    "caption": " The principle behind our planar to spatial deployment system. Top row: all members of a family are parallel and rigid, the kinematic linkage can move freely in the plane. Bottom row: non parallel layout produces a deadlock when trying to change the shape, inner members are too long. Allowing members to elastically deform, they buckle out of plane.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Why does the kinematic linkage become deadlocked in the non-parallel layout?",
    "answer": "The inner members are too long.",
    "rationale": "The figure shows that in the non-parallel layout, the inner members are longer than the outer members. This means that when the linkage tries to change shape, the inner members will collide with the outer members, preventing the linkage from moving.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.00201v1",
    "pdf_url": null
  },
  {
    "instance_id": "aba1c8b951d2411bae38c677a20ba7d6",
    "figure_id": "2302.09694v1-Figure2-1",
    "image_file": "2302.09694v1-Figure2-1.png",
    "caption": " The causal graph for the proposed Disentangled Mediation Analysis Variational AutoEncoder (DMAVAE). T is the treatment, M is the mediator, Y is the outcome and X represents the set of the proxy attributes. ZTM , ZTY and ZMY denote the disentangled representations of the three types of latent confounders.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which latent confounder is most likely to be responsible for the observed relationship between the mediator and the outcome?",
    "answer": "ZMY",
    "rationale": "The figure shows that ZMY is the only latent confounder that has a direct causal effect on both the mediator (M) and the outcome (Y). This means that ZMY could be responsible for the observed relationship between M and Y, even if there is no direct causal effect of M on Y.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.09694v1",
    "pdf_url": null
  },
  {
    "instance_id": "b904af43dc684c4baddb096f8ed9beeb",
    "figure_id": "2005.00700v3-Figure2-1",
    "image_file": "2005.00700v3-Figure2-1.png",
    "caption": " Properties of various QA datasets included in this study: 5 extractive (EX), 3 abstractive (AB), 9 multiplechoice (MC), and 3 yes/no (YN). ‘idk’ denotes ‘I don’t know’ or unanswerable questions. BoolQ represents both the original dataset and its contrast-sets extension BoolQ-CS; similarly for ROPES, Quoref, and DROP.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the most explicit candidate answers?",
    "answer": "QASC.",
    "rationale": "The figure shows that QASC has 8 explicit candidate answers, which is the most of any dataset in the table.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.00700v3",
    "pdf_url": null
  },
  {
    "instance_id": "4bb8a969b28d4d83a7887cad01594154",
    "figure_id": "2205.01883v1-Figure7-1",
    "image_file": "2205.01883v1-Figure7-1.png",
    "caption": " Answer length distributions per dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest percentage of answers with one word?",
    "answer": "VQA2 - COCO",
    "rationale": "The figure shows that the blue line, which represents VQA2 - COCO, is the highest at the point where the x-axis value is 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.01883v1",
    "pdf_url": null
  },
  {
    "instance_id": "e40aa249199f45d9a7524fa2a8527a58",
    "figure_id": "2212.04214v1-Figure4-1",
    "image_file": "2212.04214v1-Figure4-1.png",
    "caption": " Relationships between the number of the cited papers in each ease (X-axis) and R (the average of RG-1, RG-2 and RG-L) of four models. Best viewed in color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest average RG score when considering all numbers of cited papers?",
    "answer": "GSSG",
    "rationale": "The GSSG line is consistently above the other lines in the plot, indicating that it has the highest average RG score for all numbers of cited papers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.04214v1",
    "pdf_url": null
  },
  {
    "instance_id": "ed6b4dda136a4cdf8e86dae38b63dedb",
    "figure_id": "1908.05656v1-Figure6-1",
    "image_file": "1908.05656v1-Figure6-1.png",
    "caption": " Mean AUCCESS on PHYRE-{B, 2B} of six DQN variants of the Baseline in the main text. Error bars show one standard deviation. FuseFirst, FuseAll, and FuseGlobal DQN agents perform fusion of observation and action features in alternative locations via channel-wise bias and gain modulation (akin to [37]): Baseline fuses with the input to the ResNet-18 conv5 stage; FuseFirst fuses with the input to the conv2 stage; FuseAll fuses with the inputs to each stage from conv2 to conv5; and FuseGlobal fuses with the globally max-pooled output of the conv5 stage. Act1024 and Act1024×2 DQN agents use Baseline fusion but larger action encoder networks with one or two hidden layers of 1024 units, respectively. The NoBalancing agents trains the Baseline DQN without balancing the positive and negative examples in the batches. We refer the reader to our code release on https://phyre.ai for full details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which DQN variant performs the best on the PHYRE-B (cross) task?",
    "answer": "FuseAll",
    "rationale": "The figure shows the mean AUCCESS of six DQN variants on the PHYRE-B (cross) task. The FuseAll DQN agent has the highest mean AUCCESS, indicating that it performs the best on this task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.05656v1",
    "pdf_url": null
  },
  {
    "instance_id": "f7daefc1edaf4628a970c2d41fdbd806",
    "figure_id": "1903.04480v1-Figure9-1",
    "image_file": "1903.04480v1-Figure9-1.png",
    "caption": " Ablation studies of our method. Top left: GT. Top right: w/o segmentation label map and flow. Bottom left:w/o flow. Bottom right: our full model. Our method preserve better the visual quality.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the images shows the ground truth (GT) for the scene?",
    "answer": "The top left image.",
    "rationale": "The caption states that \"Top left: GT.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.04480v1",
    "pdf_url": null
  },
  {
    "instance_id": "e3fc8c77bf0a40789199a32c1d7fe4af",
    "figure_id": "2111.03811v3-Figure3-1",
    "image_file": "2111.03811v3-Figure3-1.png",
    "caption": " The distribution of the cosine similarity between the predicted speaker embedding and target speaker’s average embedding extracted by ECAPA-TDNN SV system",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two models, SIG-VC or AGAIN-VC, generally produces utterances that are more similar to the target speaker's voice?",
    "answer": "AGAIN-VC",
    "rationale": "The plot shows that the distribution of cosine similarities for AGAIN-VC is shifted towards higher values compared to SIG-VC. This indicates that the embeddings generated by AGAIN-VC are, on average, more similar to the target speaker's embedding.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.03811v3",
    "pdf_url": null
  },
  {
    "instance_id": "3134bdcd97f546ab9e2c85668a372816",
    "figure_id": "1911.04448v4-Figure8-1",
    "image_file": "1911.04448v4-Figure8-1.png",
    "caption": " Comparison between RTAC and SAC in RTMDP versions of the autonomous driving tasks. We can see that RTAC under real-time constraints outperforms SAC even without real-time constraints. Mean and 95% confidence interval are computed over four training runs per environment.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better in the RaceSolo-v0 environment?",
    "answer": "RTAC",
    "rationale": "The plot shows the average return for each algorithm in the RaceSolo-v0 environment. RTAC has a higher average return than SAC in both RTMDP(E) and E versions of the environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.04448v4",
    "pdf_url": null
  },
  {
    "instance_id": "bea93b90965c406ba8830e21e77b3f4a",
    "figure_id": "2211.00680v1-Figure2-1",
    "image_file": "2211.00680v1-Figure2-1.png",
    "caption": " Fourier transform (amplitude) of the artificial fingerprint estimated from 1000 image residuals. Top row: from left to right ProGAN [20], BigGan [21], StyleGAN2 [22], Taming Transformers [23], DALL·E Mini [24]. Bottom row: GLIDE [5], Latent Diffusion [25], Stable Diffusion [4], ADM [26], DALL·E 2 [3]",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model produces the most artifacts in the generated images?",
    "answer": "ProGAN",
    "rationale": "The Fourier transform of the image residuals shows that ProGAN has the most high-frequency components, which correspond to artifacts in the generated images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.00680v1",
    "pdf_url": null
  },
  {
    "instance_id": "4539e97afc7243d6875bf93abb920ec4",
    "figure_id": "2102.06203v2-Figure2-1",
    "image_file": "2102.06203v2-Figure2-1.png",
    "caption": " Comparison of pre-training and co-training on mix-1 and mix-2. > denotes a pre-training step and + denotes a co-training. As an example, WebMath > mix2 > mix1 + tactic signifies a model successively pre-trained on WebMath then mix2 and finally co-trained as a finetuning step on mix1 and tactic. Columns mix1, mix2, tactic report the optimal validation loss achieved on these respective datasets. We provide a detailed description of experiment runtime and computing infrastructure in Appendix B.",
    "figure_type": "** Table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which model achieved the highest pass rate on the tactic dataset?",
    "answer": " WebMath > mix2 + tactic + tactic",
    "rationale": " The table shows the pass rate for each model on the tactic dataset. The model with the highest pass rate is WebMath > mix2 + tactic + tactic, which achieved a pass rate of 48.4%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.06203v2",
    "pdf_url": null
  },
  {
    "instance_id": "41bc6b336d15480f98f7f785df3d9ca9",
    "figure_id": "2203.00887v3-Figure8-1",
    "image_file": "2203.00887v3-Figure8-1.png",
    "caption": " Results on the JEE 2009 dataset with gender as the protected group. For Fair ϵ-greedy we use ϵ = 0.5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm consistently achieves the highest fraction of rankings in the top 100 ranks?",
    "answer": "GDL21.",
    "rationale": "The figure shows the fraction of rankings achieved by different algorithms at different ranks. GDL21 consistently has the highest fraction of rankings in the top 100 ranks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.00887v3",
    "pdf_url": null
  },
  {
    "instance_id": "d3f1609a501d4f0a89b636bc3f5fc975",
    "figure_id": "2101.12578v4-Figure4-1",
    "image_file": "2101.12578v4-Figure4-1.png",
    "caption": " Pairwise comparison of three methods of training NN on the synthesized data with different true ρ value (left) and different standard deviation of noise (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best when the true ρ value is 0.3 and the standard deviation of noise is 0.01?",
    "answer": "w/ compared to w/o",
    "rationale": "The plot on the left shows that when the true ρ value is 0.3, the w/ compared to w/o method has the highest percentage of beating the first method. The plot on the right shows that when the standard deviation of noise is 0.01, the w/ compared to w/o method still has the highest percentage of beating the first method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.12578v4",
    "pdf_url": null
  },
  {
    "instance_id": "eb5b7dd5b7c14465b3ec8471735beffe",
    "figure_id": "1901.10879v6-Figure1-1",
    "image_file": "1901.10879v6-Figure1-1.png",
    "caption": " An overview of the span model",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the blue box in the figure represent?",
    "answer": "The blue box represents the input to the model.",
    "rationale": "The blue box is labeled \"Repeat customers\" and it has an arrow pointing to it from the \"Repeat\" node. This indicates that the blue box is the input to the model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.10879v6",
    "pdf_url": null
  },
  {
    "instance_id": "4d77177c7c7544bf9f5c475002b10d1c",
    "figure_id": "2309.10153v1-Figure6-1",
    "image_file": "2309.10153v1-Figure6-1.png",
    "caption": " Qualitative comparison between similarity-based (regular) and volume-preserving (ours) methods trained on the Brain Tumor Segmentation (BraTS20) dataset. Specifically, the VTN, VXM, and TransMorph (Trans.) networks were tested, both with their regular similarity-based registration versions and our proposed volume-preserving version. The left side of the figure shows two sets of images: Fixed and Ground Truth (GT), and Moving and GT. The first row of the figure displays the warped moving image, while the second row illustrates the organ outlines in green and red for the moving and fixed images, respectively. The yellow overlay highlights the tumors. Our proposed volume-preserving (VP) method ensures the preservation of tumor volume while aligning the images, as demonstrated by reduced number of visible changes in tumor size. In the third row, the Jacobian matrix of the deformation field is visualized. The green and red lines represent the organ and tumor outlines, respectively. The white areas indicate a large Jacobian, which corresponds to a more significant change in volume. The method without volume-preserving loss demonstrates a larger white area in the tumor, indicating a greater volume change of tumor volume. The last row of the figure displays the deformation field.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method preserves tumor volume while aligning the images?",
    "answer": "Our proposed volume-preserving (VP) method.",
    "rationale": "The figure shows that the VP method results in less visible changes in tumor size compared to the similarity-based methods. This is also evident in the Jacobian matrix visualization, where the VP method shows a smaller white area in the tumor, indicating a smaller volume change.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.10153v1",
    "pdf_url": null
  },
  {
    "instance_id": "168cccb9053a4f74bc420bbec39e44ae",
    "figure_id": "1809.03057v1-Figure7-1",
    "image_file": "1809.03057v1-Figure7-1.png",
    "caption": " Convergence of MCCFR and MCCFR+ on logarithmic scale. For the first 106 iterations, MCCFR+ performs similllary to the MCCFR. After approximately 107 iterations, the difference in favor of MCCFR starts to be visible and the gap in exploitability widens as the number of iterations grows.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges faster, MCCFR or MCCFR+?",
    "answer": "MCCFR+ converges faster than MCCFR.",
    "rationale": "The plot shows that the exploitability of MCCFR+ decreases faster than the exploitability of MCCFR. This means that MCCFR+ is able to find a better strategy in fewer iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.03057v1",
    "pdf_url": null
  },
  {
    "instance_id": "ce2a4e87aebe460fab68e897ea952e24",
    "figure_id": "2109.05198v1-Figure16-1",
    "image_file": "2109.05198v1-Figure16-1.png",
    "caption": " Sensitivity of OASIS w.r.t. (β2, α), Deterministic Logistic regression.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset and hyperparameter combination shows the fastest convergence?",
    "answer": "ijcnn1 and 0.995-1e-07.",
    "rationale": "The plot shows the convergence of the objective function F(w) for different datasets and hyperparameter combinations. The ijcnn1 and 0.995-1e-07 combination reaches the lowest value of F(w) in the least number of effective passes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.05198v1",
    "pdf_url": null
  },
  {
    "instance_id": "84c0904cbe5248fc89667772352aac2e",
    "figure_id": "1904.05514v1-Figure2-1",
    "image_file": "1904.05514v1-Figure2-1.png",
    "caption": " Three Player Game: Linear Example",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the variable z represent in the figure?",
    "answer": "The latent variable.",
    "rationale": "The figure shows a three-player game, with x as the input, z as the latent variable, and s and t as the outputs of players D and T, respectively. The latent variable is a hidden variable that is not directly observed but is inferred from the observed data. In this case, the latent variable z is inferred from the input x.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.05514v1",
    "pdf_url": null
  },
  {
    "instance_id": "24e22318302b45b98b6b226c977bfd20",
    "figure_id": "2305.10010v1-Figure9-1",
    "image_file": "2305.10010v1-Figure9-1.png",
    "caption": " Comparison of the attribution gap between teacher and student on training set and development set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, MRPC or QNLI, results in a smaller attribution gap between the teacher and student models on the development set?",
    "answer": "MRPC.",
    "rationale": "The figure shows that the MSE of attribution maps for MRPC is lower than that for QNLI on the development set, indicating a smaller attribution gap.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.10010v1",
    "pdf_url": null
  },
  {
    "instance_id": "2f646ca4eb6843d680d0440775d92248",
    "figure_id": "2303.14773v2-Figure15-1",
    "image_file": "2303.14773v2-Figure15-1.png",
    "caption": " Grad-CAM on Biased-MNIST. While baseline methods attend to the background rather than digit shape, our BlackVIP can bypass this spurious feature through a widely scattered visual prompt and focus more of the attention on the shape of the digit.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method focuses the most attention on the shape of the digit?",
    "answer": "BlackVIP",
    "rationale": "The figure shows the results of Grad-CAM on Biased-MNIST for different methods. Grad-CAM visualizes the importance of different regions of the input image for the model's prediction. The figure shows that BlackVIP focuses the most attention on the shape of the digit, while other methods attend more to the background.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.14773v2",
    "pdf_url": null
  },
  {
    "instance_id": "f6d3f2bb6a1847d9a6793be2fac5b5be",
    "figure_id": "2011.03677v1-Figure5-1",
    "image_file": "2011.03677v1-Figure5-1.png",
    "caption": " Qualitative comparison on aerial images from HAI Dataset with water bodies and sky regions",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the dehazing methods is most effective in removing haze from the sky region?",
    "answer": "SkyGAN.",
    "rationale": "The figure shows that SkyGAN is the only method that is able to completely remove the haze from the sky region, resulting in a clear and blue sky.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.03677v1",
    "pdf_url": null
  },
  {
    "instance_id": "a1c22d693b4343b5b7d1a3b795b7a707",
    "figure_id": "1906.02319v1-Figure6-1",
    "image_file": "1906.02319v1-Figure6-1.png",
    "caption": " Running time per epoch (best seen in color)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods is the most efficient in terms of running time per epoch?",
    "answer": "GAT",
    "rationale": "The figure shows the running time per epoch for four different methods: DEMO-Net (hash), DEMO-Net (weight), GCN, and GAT. The GAT method has the lowest running time per epoch for all numbers of nodes, which means it is the most efficient.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02319v1",
    "pdf_url": null
  },
  {
    "instance_id": "c6aef3b9d2864f64a415c1efe21c5c15",
    "figure_id": "1904.03870v1-Figure4-1",
    "image_file": "1904.03870v1-Figure4-1.png",
    "caption": " Qualitative results on ActivityNet Captions dataset. The arrows represent ground-truth events (red) and events in the predicted event sequence from our event sequence generation network (blue) for input videos. Note that the events in the event sequence are selected in the order of its index. For the predicted events, we show the captions generated independently (ESGN-Ind) and sequentially (SDVC). More consistent captions are obtained by our sequential captioning network, where words for comparison are marked in bold-faced black.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model generated more consistent captions for the two videos shown?",
    "answer": "SDVC generated more consistent captions.",
    "rationale": "The figure shows the ground-truth events (red) and events in the predicted event sequence from the event sequence generation network (blue) for two input videos. The predicted events are shown with captions generated independently (ESGN-Ind) and sequentially (SDVC). For both videos, the captions generated by SDVC are more consistent with each other and with the ground-truth events than the captions generated by ESGN-Ind. This is because SDVC takes into account the temporal order of the events when generating captions, while ESGN-Ind does not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03870v1",
    "pdf_url": null
  },
  {
    "instance_id": "6bce528b50e34d8d9a71887b9d292013",
    "figure_id": "2208.13298v4-Figure8-1",
    "image_file": "2208.13298v4-Figure8-1.png",
    "caption": " Complete ContinuousSeek Results for d = 20.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best across all learning rates and batch sizes?",
    "answer": "DDPG+HER",
    "rationale": "The DDPG+HER line is consistently above the other lines in all of the plots, indicating that it achieves the highest success rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.13298v4",
    "pdf_url": null
  },
  {
    "instance_id": "0fc995d2ecee42fb94c47f93bdf6b1ee",
    "figure_id": "2210.07105v4-Figure3-1",
    "image_file": "2210.07105v4-Figure3-1.png",
    "caption": " (a) Performance profiles after online tuning (b) Probability of improvement of ReBRAC to other algorithms after online tuning. The curves (Agarwal et al., 2021) are for D4RL benchmark spanning AntMaze and Adroit cloned datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the D4RL benchmark after online tuning?",
    "answer": "ReBRAC",
    "rationale": "Figure (a) shows the performance profiles of the algorithms after online tuning. ReBRAC is the only algorithm that achieves a score of 1.0, which means that it performs the best on all of the tasks in the benchmark.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.07105v4",
    "pdf_url": null
  },
  {
    "instance_id": "1acfbfdf19e44652b62bc0751bd582a9",
    "figure_id": "1904.08364v2-Figure3-1",
    "image_file": "1904.08364v2-Figure3-1.png",
    "caption": " Word error rate (left) and character error rate (right) of ACE loss on validation set under regression and cross entropy perspective.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function, regression or cross-entropy, results in a lower word error rate for the ACE model?",
    "answer": "Regression.",
    "rationale": "The left plot in the figure shows the word error rate (WER) for the ACE model trained with both regression and cross-entropy loss functions. The blue line, which represents the regression loss function, is consistently lower than the red line, which represents the cross-entropy loss function. This indicates that the regression loss function results in a lower word error rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.08364v2",
    "pdf_url": null
  },
  {
    "instance_id": "9c42dff62ad94a61996a02818791d0f9",
    "figure_id": "2003.11249v2-Figure16-1",
    "image_file": "2003.11249v2-Figure16-1.png",
    "caption": " Individual results for CIFAR-100+[45:55]",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which active learning algorithm achieves the highest mean accuracy for all three trials?",
    "answer": "The proposed algorithm.",
    "rationale": "In all three trials, the red line, which represents the proposed algorithm, is consistently higher than the other lines, indicating that it achieves the highest mean accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.11249v2",
    "pdf_url": null
  },
  {
    "instance_id": "bf279c43618b412b8d59c9a570661e72",
    "figure_id": "2205.00048v1-Figure5-1",
    "image_file": "2205.00048v1-Figure5-1.png",
    "caption": " The trade-off between II-F and GG-F when directly optimizing the recommender model towards a combination of both objectives.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset results in a lower GG-F value when directly optimizing the recommender model towards a combination of both objectives, MovieLens100k or MovieLens1M?",
    "answer": "MovieLens1M",
    "rationale": "The plot in Figure (b) shows that the GG-F values for MovieLens1M are lower than the GG-F values for MovieLens100k for all values of II-F.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.00048v1",
    "pdf_url": null
  },
  {
    "instance_id": "0492c2003f03445394abf51ab35f74b3",
    "figure_id": "2207.03729v1-Figure3-1",
    "image_file": "2207.03729v1-Figure3-1.png",
    "caption": " Figure shows the behavior brought by each component of our model in isolation (Middle and Right column results are without and with using a component respectively). 1st two rows show the benefits of Subject-Object addition to edge prediction model, the use of GloVe embeddings and class-balancing edge loss. 3rd depicts advantage of using external-knowledge for node prediction. The last two rows show the advantage of Cluster-Aware BFS.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which component of the model is responsible for adding subject-object information to the edge prediction model?",
    "answer": "GraphRNN* w/ Cluster-Aware BFS.",
    "rationale": "The figure shows that the GraphRNN* w/ Cluster-Aware BFS model is able to add subject-object information to the edge prediction model, as evidenced by the presence of edges labeled \"on\" and \"in\" in the generated graphs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.03729v1",
    "pdf_url": null
  },
  {
    "instance_id": "6a3d520ca93a4e43ab054f2b69d27696",
    "figure_id": "1811.06094v1-Figure4-1",
    "image_file": "1811.06094v1-Figure4-1.png",
    "caption": " LVM applied to the synthetic test data. The title of the plot is the shared dimension of the latent space for the target and background samples. The dimension of the target latent space is 2. When the shared dimension is zero, the method is equivalent to applying PPCA to the target dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the distribution of the data points as the shared dimension of the latent space increases?",
    "answer": "The data points become more clustered together.",
    "rationale": "In the figure, as the shared dimension of the latent space increases from 0 to 12, the data points become more clustered together. This is because the shared dimension of the latent space allows the data points to be represented in a lower-dimensional space, which can lead to more clustering.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.06094v1",
    "pdf_url": null
  },
  {
    "instance_id": "87a8a7ef693b4a638d347a6c777b4f69",
    "figure_id": "1906.04556v2-Figure2-1",
    "image_file": "1906.04556v2-Figure2-1.png",
    "caption": " Comparison of PeNFAC, DDPG and deterministic PPO over 60 different seeds for each algorithm in HalfCheetah.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the highest average reward in this environment?",
    "answer": "PeNFAC",
    "rationale": "The plot shows the average reward achieved by each algorithm over the course of training. PeNFAC consistently achieves a higher average reward than the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04556v2",
    "pdf_url": null
  },
  {
    "instance_id": "24737062a07f4265932adff98a875e3f",
    "figure_id": "2303.15472v1-Figure5-1",
    "image_file": "2303.15472v1-Figure5-1.png",
    "caption": " Matching accuracies according to varying degree of rotations on Roto-360.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature point extraction method performs best when the image is rotated by 90 degrees?",
    "answer": "SuperPoint",
    "rationale": "The green line, which represents SuperPoint, has the highest MMA value at 90 degrees.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.15472v1",
    "pdf_url": null
  },
  {
    "instance_id": "9240a44b0c364a159d3534d7cd1bfa1e",
    "figure_id": "2302.02813v2-Figure5-1",
    "image_file": "2302.02813v2-Figure5-1.png",
    "caption": " Monthly median news sentiment and positive reply stance share timelines by country.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which country had the most positive news sentiment in April?",
    "answer": "Germany",
    "rationale": "The figure shows the monthly median news sentiment and positive reply stance share timelines by country. In April, the line for Germany is the highest, indicating that it had the most positive news sentiment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.02813v2",
    "pdf_url": null
  },
  {
    "instance_id": "9948a75aabc547318b3cb138e8fdf4aa",
    "figure_id": "1712.05231v2-Figure1-1",
    "image_file": "1712.05231v2-Figure1-1.png",
    "caption": " The similarity geometric transformation representation achieves more accurate and robust tracking results.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in this image?",
    "answer": "OURS",
    "rationale": "The figure shows the results of four different tracking algorithms. OURS is the only algorithm that correctly tracks the object in all four frames.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1712.05231v2",
    "pdf_url": null
  },
  {
    "instance_id": "c74cc903aaed4dc28c22094473903482",
    "figure_id": "2107.01850v2-Figure1-1",
    "image_file": "2107.01850v2-Figure1-1.png",
    "caption": " Completely identifying a DAG can require exponentially more interventions than identifying the matching intervention. Consider a graph constructed by joining r size-4 cliques, where the matching intervention has the source node as the only perturbation target, as pictured in (a) with r = 2 and the source node in purple; (b) shows the minimum size set intervention (in purple) that completely identifies the DAG, which grows as O(r) (Squires et al., 2020a). In Theorem 2, we show that the matching intervention can be identified in O(log r) single-node interventions.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many more interventions are needed to completely identify the DAG in (b) than in (a)?",
    "answer": "1",
    "rationale": "The figure shows that in (a), only one node needs to be intervened on to identify the matching intervention, while in (b), two nodes need to be intervened on to completely identify the DAG.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.01850v2",
    "pdf_url": null
  },
  {
    "instance_id": "363dc05255474aeeb7140f8304c38bd6",
    "figure_id": "1802.04240v2-Figure4-1",
    "image_file": "1802.04240v2-Figure4-1.png",
    "caption": " Log of ratio of solution time to the number of customer nodes using different algorithms.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best for VRP100?",
    "answer": "OR-Tools",
    "rationale": "The figure shows that OR-Tools has the lowest log(time/# of customer nodes) for VRP100, which means it has the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1802.04240v2",
    "pdf_url": null
  },
  {
    "instance_id": "91da5cc2edfb4ab3a55f39ed85d0417a",
    "figure_id": "1901.08907v1-Figure3-1",
    "image_file": "1901.08907v1-Figure3-1.png",
    "caption": " The results of Precision@K in top-K recommendation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the MovieLens-1M dataset?",
    "answer": "PER",
    "rationale": "The figure shows the Precision@K for different models on the MovieLens-1M dataset. PER has the highest Precision@K for all values of K.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.08907v1",
    "pdf_url": null
  },
  {
    "instance_id": "865fe769d78f4cf8ada8cdf9d88607ba",
    "figure_id": "2304.06813v2-Figure8-1",
    "image_file": "2304.06813v2-Figure8-1.png",
    "caption": " S-OOD in MS-OOD DETECTION. The results are averaged over eight S-OOD datasets. X-axis: ACC of classifying the ID examples; Y-axis: FPR(S-OOD)@TPR(ID+)=95 for wrongly accepting S-OOD examples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the best performance in terms of both accuracy and FPR(S-OOD)?",
    "answer": "CLIP_ResNet50",
    "rationale": "The figure shows that CLIP_ResNet50 has the highest accuracy and the lowest FPR(S-OOD) among all the methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.06813v2",
    "pdf_url": null
  },
  {
    "instance_id": "ca5961b8336241279c6b961ed9869895",
    "figure_id": "2011.03506v1-Figure5-1",
    "image_file": "2011.03506v1-Figure5-1.png",
    "caption": "Figure 5",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the probability of transitioning from state S1 to state S2 when action a is taken?",
    "answer": "p^a_12",
    "rationale": "The arrow from S1 to S2 is labeled with p^a_12, 0, which indicates that the probability of transitioning from S1 to S2 when action a is taken is p^a_12. The 0 indicates that there is no reward for taking this action.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.03506v1",
    "pdf_url": null
  },
  {
    "instance_id": "55f92ff957464cf092879189c51fd5f7",
    "figure_id": "1905.12681v5-Figure10-1",
    "image_file": "1905.12681v5-Figure10-1.png",
    "caption": " Top-Bottom 20 classes based on improvement of G-Blend to RGB model. The improved classes are indeed audio-relevant, while those have performance drop are not very audio semantically-related.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class of events saw the biggest drop in accuracy when using G-Blend compared to the RGB model?",
    "answer": "Building a cabinet",
    "rationale": "The figure shows the top 20 classes with the biggest drop in accuracy when using G-Blend compared to the RGB model. The class with the biggest drop in accuracy is building a cabinet, which is represented by the tallest bar in the \"Top 20 Dropped Class Accuracy\" plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12681v5",
    "pdf_url": null
  },
  {
    "instance_id": "85c512d2ea534c618438844592530b87",
    "figure_id": "2108.09936v1-Figure4-1",
    "image_file": "2108.09936v1-Figure4-1.png",
    "caption": " Qualitative comparisons on the Completion3D dataset (1/2).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate point cloud completion for the table?",
    "answer": "VE-PCN.",
    "rationale": "The figure shows the point cloud completion results for different methods on the Completion3D dataset. The VE-PCN method produces the most accurate point cloud completion for the table, as it is able to capture the shape and structure of the table more accurately than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.09936v1",
    "pdf_url": null
  },
  {
    "instance_id": "aa8ccab0d69a467c930d041cada3aac8",
    "figure_id": "1907.00235v3-Figure3-1",
    "image_file": "1907.00235v3-Figure3-1.png",
    "caption": " Illustration of different attention mechanism between adjacent layers in Transformer.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attention mechanism is the most computationally efficient?",
    "answer": "LogSparse Self Attention",
    "rationale": "The figure shows that LogSparse Self Attention only attends to a subset of the input tokens, which reduces the number of computations required.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.00235v3",
    "pdf_url": null
  },
  {
    "instance_id": "a42be2939fbf4e3985b7843760e27894",
    "figure_id": "2204.04724v1-Figure4-1",
    "image_file": "2204.04724v1-Figure4-1.png",
    "caption": " Provider fairness of ProFairRec and baseline methods based on different partition of protected provider group. Fairness is better if ER@K is closer to 1 and rND@K is closer to 0.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which methods achieve the best fairness performance on the protected provider group at TOP K = 10?",
    "answer": "ProFairRec and OFAIR.",
    "rationale": "The figure shows the ER@K and rND@K values for different methods at different TOP K values. At TOP K = 10, ProFairRec and OFAIR have the highest ER@K values and the lowest rND@K values, indicating that they achieve the best fairness performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.04724v1",
    "pdf_url": null
  },
  {
    "instance_id": "1bca2c9cfca84acc8c788d1af93ecec7",
    "figure_id": "2301.12247v2-Figure10-1",
    "image_file": "2301.12247v2-Figure10-1.png",
    "caption": " Unedited image generated from the prompt ‘a house at a lake’. Included for reference of subsequent ablations.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What season is depicted in the photograph?",
    "answer": "Autumn",
    "rationale": "The trees in the background are displaying fall foliage, which is a characteristic of autumn.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.12247v2",
    "pdf_url": null
  },
  {
    "instance_id": "e6c558d202884cbd8fe725eb545aa68e",
    "figure_id": "2106.03476v1-Figure1-1",
    "image_file": "2106.03476v1-Figure1-1.png",
    "caption": " Running time",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the shortest running time for the Facebook dataset?",
    "answer": "ST",
    "rationale": "The figure shows the running time of different algorithms for different datasets. The ST algorithm has the shortest running time for the Facebook dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03476v1",
    "pdf_url": null
  },
  {
    "instance_id": "1182a247d6894161b8e99de1f87f4198",
    "figure_id": "2012.15738v1-Figure13-1",
    "image_file": "2012.15738v1-Figure13-1.png",
    "caption": " Excerpt from AMT HIT instructions: Discouraging use of morally-charged language.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of words should be avoided when writing action sentences?",
    "answer": "Morally-charged words.",
    "rationale": "The image provides a list of morally-charged words to avoid when writing action sentences. These words can bias the reader's perception of the characters and their actions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.15738v1",
    "pdf_url": null
  },
  {
    "instance_id": "e293b2bf8b3f4acebd74ccf942015761",
    "figure_id": "2004.00666v1-Figure3-1",
    "image_file": "2004.00666v1-Figure3-1.png",
    "caption": " Illustration of over-complete distribution generation while generating hard samples between two classes. The boundary of the distribution would be decided from equations 1 and 2. Since µOC is the average between one class to other competing classes, the boundary would be extended based on the new obtained µOC .",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the purpose of generating hard samples between two classes?",
    "answer": "To extend the boundary of the distribution.",
    "rationale": "The figure shows that the generated hard samples are located between two classes. This is done to extend the boundary of the distribution, which is decided by equations 1 and 2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.00666v1",
    "pdf_url": null
  },
  {
    "instance_id": "d1d63dc068f7427aa5d2de0da0528aac",
    "figure_id": "2006.10732v4-Figure22-1",
    "image_file": "2006.10732v4-Figure22-1.png",
    "caption": " Illustration of the monotonicity of the bias term under Σθ = Id. We consider two distributions of eigenvalues for ΣX : two equally weighted point masses (circle) and a uniform distribution (star), and vary the condition number κX and overparameterization level γ. In all cases the bias in monotone in α ∈ [0, 1].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of eigenvalue distribution of ΣX has the largest bias?",
    "answer": "The point mass distribution.",
    "rationale": "The figure shows that the bias for the point mass distribution is larger than the bias for the uniform distribution for all values of γ and κX.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.10732v4",
    "pdf_url": null
  },
  {
    "instance_id": "246cfa82c4754c77ad89793a5a878e57",
    "figure_id": "2106.12288v2-Figure5-1",
    "image_file": "2106.12288v2-Figure5-1.png",
    "caption": " ROC curves on malware variants detection.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest true positive rate for a given false positive rate?",
    "answer": "MG-DVD.",
    "rationale": "The ROC curve for MG-DVD is consistently above the other curves, indicating that it has a higher true positive rate for any given false positive rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.12288v2",
    "pdf_url": null
  },
  {
    "instance_id": "6bf82d1dec48476689bcd811ceb6b821",
    "figure_id": "2307.04285v1-Figure4-1",
    "image_file": "2307.04285v1-Figure4-1.png",
    "caption": " User interface for data annotation. We divide the overall step into four notations: A, B, C, and D. A, B, and C are for the entity annotation step, and D is for the relation annotation. The annotators detect the named entity of the Korean text in A, find the parallel entity of the Hanja text in B, and annotate the parallel relationship, shown as the blue line in C. After checking the entity detection, the annotators move to D, where they annotate the relation between the entities, choose the relation class, and add the indices of evidence sentences.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which step in the data annotation process involves finding the parallel entity of the Hanja text?",
    "answer": "Step B.",
    "rationale": "The caption states that \"the annotators detect the named entity of the Korean text in A, find the parallel entity of the Hanja text in B\". This means that step B is specifically dedicated to finding the parallel entity in the Hanja text.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.04285v1",
    "pdf_url": null
  },
  {
    "instance_id": "896993a09a2a47f79165f880218267dc",
    "figure_id": "2106.02993v1-Figure11-1",
    "image_file": "2106.02993v1-Figure11-1.png",
    "caption": " Comparison of the predicted and exact solutions of Schrödinger equation corresponding to 𝑡 = 0.79 snapshot.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods produced the most accurate prediction of the solution to the Schrödinger equation?",
    "answer": "APINN-Drop",
    "rationale": "The figure shows the predicted and exact solutions for four different methods: PID-GAN, PIG-GAN, PINN-Drop, and APINN-Drop. The predicted solution for APINN-Drop is closest to the exact solution, as evidenced by the overlap of the blue and red lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02993v1",
    "pdf_url": null
  },
  {
    "instance_id": "634d5eff5aa74963adae9a0dedf58f0b",
    "figure_id": "2003.09373v1-Figure5-1",
    "image_file": "2003.09373v1-Figure5-1.png",
    "caption": " Sample face images from Adience with the corresponding quality predictions from four face quality assessment methods. SER-FIQ refers to our same model approach based on ArcFace.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which face quality assessment method appears to be the most accurate?",
    "answer": "FaceQnet.",
    "rationale": "The image shows the quality predictions from four different face quality assessment methods for five different face images. The quality predictions from FaceQnet are consistently higher than the other methods, indicating that it is the most accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.09373v1",
    "pdf_url": null
  },
  {
    "instance_id": "fc2f7b6d43aa43bfaa88e508cd84c8a8",
    "figure_id": "2104.01374v2-Figure14-1",
    "image_file": "2104.01374v2-Figure14-1.png",
    "caption": " Application of diverse samples for instance segmentation. We use the diverse denoised samples from HDN to obtain diverse segmentation results using the procedure presented in Prakash et al. (2021). We then use the diverse segmentation results as input to a consensus algorithm which leverages the diversity present in the samples to obtain a higher quality segmentation estimate than any of the individual diverse segmentation maps. The segmentation performance is compared against available ground truth in terms of Average Precision (Lin et al., 2014) and SEG score (Ulman et al., 2017). Higher scores represent better segmentation. The segmentation performance corresponding to our HDN MMSE estimate obtained by averaging different number of denoised samples improves with increasing number of samples used for obtaining the denoised MMSE estimate. Clearly, the segmentation results obtained with consensus method using our HDN samples outperforms even segmentation on high SNR images using as little as 3 diverse segmentation samples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed best for instance segmentation according to the figure?",
    "answer": "HDN consensus.",
    "rationale": "The figure shows that the HDN consensus method achieved the highest Average Precision and SEG score, regardless of the number of samples used.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.01374v2",
    "pdf_url": null
  },
  {
    "instance_id": "f211e476922f4d88a15a5c272d11189e",
    "figure_id": "2303.06242v1-Figure4-1",
    "image_file": "2303.06242v1-Figure4-1.png",
    "caption": " Median radius per action class is an indicator of the understandability of each action class (vector graphics, please zoom-in for better readability).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which action class has the highest median radius?",
    "answer": "pushing other person",
    "rationale": "The bar for \"pushing other person\" is the longest, indicating that it has the highest median radius.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.06242v1",
    "pdf_url": null
  },
  {
    "instance_id": "e59dea6f0de14efe88b615a1e60f3c2a",
    "figure_id": "2006.04884v3-Figure5-1",
    "image_file": "2006.04884v3-Figure5-1.png",
    "caption": " Box plots showing the fine-tuning performance of (a) BERT, (b) RoBERTa, (c) ALBERT for different learning rates α with and without bias correction (BC) on RTE. For BERT and ALBERT, having bias correction leads to more stable results and allows to train using larger learning rates. For RoBERTa, the effect is less pronounced but still visible.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model shows the most stable results with the largest learning rate?",
    "answer": "RoBERTa",
    "rationale": "The figure shows that RoBERTa has the smallest variance in accuracy across different learning rates, and it can be trained with a larger learning rate without significant degradation in performance. This is evident from the tighter box plots and higher maximum accuracy values for RoBERTa compared to BERT and ALBERT.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04884v3",
    "pdf_url": null
  },
  {
    "instance_id": "e916912ffd4746709bad57d124c54504",
    "figure_id": "1712.00643v1-Figure3-1",
    "image_file": "1712.00643v1-Figure3-1.png",
    "caption": " Incorporating known spreader states leads to gains in accuracy for both infection and spreader prediction tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better at predicting infection when the proportion of known spreaders is low?",
    "answer": "y-PALS-TT",
    "rationale": "The y-PALS-TT model has a higher mean AUC than the other models when the proportion of known spreaders is low. This can be seen in the top panel of the figure, where the y-PALS-TT line is above the other lines at the left side of the x-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1712.00643v1",
    "pdf_url": null
  },
  {
    "instance_id": "47224d29be1e4154a10a9e3d7fdb7e6f",
    "figure_id": "2306.00312v1-Figure3-1",
    "image_file": "2306.00312v1-Figure3-1.png",
    "caption": " DIS2 may be invalid when the features are explicitly learned to violate Assumption 3.5. Domain-adversarial representation learning algorithms such as DANN and CDAN indirectly minimize maxh′∈H∆(ĥ, h′), meaning the necessary condition is less likely to be satisfied. Nevertheless, when DIS2 does overestimate accuracy, it almost always does so by less than prior methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method generally has the highest target accuracy prediction?",
    "answer": "Dis2",
    "rationale": "The figure shows that the points for Dis2 are generally closer to the y = x line than the points for the other methods. This means that Dis2 generally has a higher target accuracy prediction than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.00312v1",
    "pdf_url": null
  },
  {
    "instance_id": "c3e7158cd0a24a519a3f4a1b4ba81051",
    "figure_id": "2210.01348v1-Figure2-1",
    "image_file": "2210.01348v1-Figure2-1.png",
    "caption": " Learning curves for simplified long-time-scale learning problem with gradient descent (markers) and with gradient flow (solid lines). Gradient descent is done with learning rate 1. Time difference t1 − t0 is set to 10. Dashed lines are lower bounds given in Tab. 1 fitted to each learning curve with suitable translation. These lower bounds well approximate asymptotic convergence of gradient flow. Results of refine and fast gates are explained in Section 5.3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods, \"softsign\", \"sigmoid\", \"refine\", and \"fast\", appears to converge to its asymptotic value the fastest?",
    "answer": "\"fast\"",
    "rationale": "The figure shows that the \"fast\" method (red line) reaches its asymptotic value (around 0.99999) in fewer iterations than the other three methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.01348v1",
    "pdf_url": null
  },
  {
    "instance_id": "59a26eefaeb345d49750b7c4d8df0376",
    "figure_id": "1904.02365v2-Figure10-1",
    "image_file": "1904.02365v2-Figure10-1.png",
    "caption": " Qualitative results of the discovered models - (arch0 and arch1) - on the validation set of CityScapes. The last row shows failure cases.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which architecture performs better on the Cityscapes validation set, arch0 or arch1?",
    "answer": "arch0",
    "rationale": "The figure shows qualitative results of the discovered models (arch0 and arch1) on the Cityscapes validation set. The last row shows failure cases. It can be seen that arch0 produces more accurate segmentations than arch1, especially in the failure cases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.02365v2",
    "pdf_url": null
  },
  {
    "instance_id": "0575adf8168a46f4b9804a7f6b41df45",
    "figure_id": "1904.10754v2-Figure9-1",
    "image_file": "1904.10754v2-Figure9-1.png",
    "caption": " Shape interpolation between two humans. Note that PointNet autoencoder produces shapes with local area distortion, while the interpolation from nearest neighbor (NN) retrieval is not continuous.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces shapes with local area distortion?",
    "answer": "PointNet autoencoder",
    "rationale": "The figure shows that the shapes produced by PointNet autoencoder have local area distortion, while the shapes produced by NN retrieval do not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.10754v2",
    "pdf_url": null
  },
  {
    "instance_id": "ded98940bb184a88a906cc49f9e94352",
    "figure_id": "2012.01203v2-Figure7-1",
    "image_file": "2012.01203v2-Figure7-1.png",
    "caption": " Distribution of triangle angles in the reconstructed meshes. Our method produces better shaped triangles than all other methods except for ball pivoting which sacrifices mesh manifoldness. We show the angle variance next to each method.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the mesh with the most acute angles?",
    "answer": "Ball Pivoting",
    "rationale": "The plot shows the density of triangle angles for each method. The Ball Pivoting method has the highest density of angles near 0 degrees, indicating that it produced the most acute angles.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.01203v2",
    "pdf_url": null
  },
  {
    "instance_id": "a29cb72b9e474bbc8a93f9834b8fc5b5",
    "figure_id": "2110.03921v2-Figure1-1",
    "image_file": "2110.03921v2-Figure1-1.png",
    "caption": " AP and latency (milliseconds) summarized in Table 2. The text in the plot indicates the backbone model size.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture achieves the highest AP while maintaining a latency of less than 100 MS per image?",
    "answer": "YOLOS with a \"nano\" backbone model.",
    "rationale": "The plot shows that YOLOS with a \"nano\" backbone model achieves an AP of approximately 42 while having a latency of less than 80 MS per image. All other models with a latency of less than 100 MS per image have a lower AP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.03921v2",
    "pdf_url": null
  },
  {
    "instance_id": "7b8d786e1297448cab5e1ae416b32523",
    "figure_id": "2205.15759v2-Figure5-1",
    "image_file": "2205.15759v2-Figure5-1.png",
    "caption": " An illustration of Exposure Template Search. Here we set request length 𝐿 = 4, beam size 𝐵 = 2, top ad slot 𝑙top = 2, and min ad gap Δ𝑙min = 2.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the maximum number of ads that can be displayed in the Ranked Ad-List?",
    "answer": "2",
    "rationale": "The figure shows that the beam size B is set to 2, which means that only the top 2 nodes are retained at each level of the tree. Therefore, the maximum number of ads that can be displayed in the Ranked Ad-List is 2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.15759v2",
    "pdf_url": null
  },
  {
    "instance_id": "9a77ca6782294301aed5188f648b6108",
    "figure_id": "2306.01323v3-Figure3-1",
    "image_file": "2306.01323v3-Figure3-1.png",
    "caption": " Performance comparison between GCN and MLP-based models. Each bar represents the accuracy gap on a specific node subgroup exhibiting a homophily ratio within the range specified on the x-axis. MLP-based models often outperform GCN on heterophilic nodes in homophilic graphs and homophilic nodes in heterophilic graphs with a positive value.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which of the four datasets shown does GCN perform best compared to MLP?",
    "answer": "Squirrel (h=0.25)",
    "rationale": "The figure shows the difference in accuracy between GCN and MLP-based models on four different datasets. For each dataset, the bars represent the difference in accuracy for different homophily ratio ranges. A positive value indicates that MLP outperforms GCN, while a negative value indicates that GCN outperforms MLP. In the Squirrel dataset, GCN has a positive difference in accuracy for all homophily ratio ranges, indicating that it performs better than MLP on this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01323v3",
    "pdf_url": null
  },
  {
    "instance_id": "efb13b22444b4549946f58b4c3788774",
    "figure_id": "2002.06922v1-Figure3-1",
    "image_file": "2002.06922v1-Figure3-1.png",
    "caption": " Average rate-distortion curves on the five 8K video sequences of simulcast and pre/post-processing approaches.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest PSNR-Y value at a bitrate of 50 mb/s?",
    "answer": "simulcast_VTM",
    "rationale": "The PSNR-Y value for simulcast_VTM at a bitrate of 50 mb/s is approximately 37.5, which is higher than the PSNR-Y values for the other two methods at the same bitrate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.06922v1",
    "pdf_url": null
  },
  {
    "instance_id": "f2055479f89144a3a8efbb5bca649a2b",
    "figure_id": "2106.14568v4-Figure3-1",
    "image_file": "2106.14568v4-Figure3-1.png",
    "caption": " t-SNE projection of training trajectories of ensemble learners discovered by DST Ensemble and EDST Ensemble with Wide ResNet28-10 on CIFAR-10/100. The sparsity level is S = 0.8.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four panels shows the most diverse set of trajectories?",
    "answer": "Panel (d)",
    "rationale": "Panel (d) shows the trajectories of the EDST Ensemble on CIFAR-100. The trajectories in this panel are more spread out than in the other panels, indicating that the ensemble learners are more diverse.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.14568v4",
    "pdf_url": null
  },
  {
    "instance_id": "406498ec07ee4de698b4542ff655be70",
    "figure_id": "2006.06676v2-Figure7-1",
    "image_file": "2006.06676v2-Figure7-1.png",
    "caption": " (a-c) FID as a function of training set size, reported as median/min/max over 3 training runs. (d) Average of 10k random images generated using the networks trained with 5k subset of FFHQ. ADA matches the average of real data, whereas the xy-translation augmentation in bCR [53] has leaked to the generated images, significantly blurring the average image.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the augmentation methods, ADA or bCR, leads to a more realistic average image?",
    "answer": "ADA",
    "rationale": "The average image generated by ADA is sharper and more realistic than the average image generated by bCR, which is significantly blurred. This can be seen in the (d) Mean image subfigure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06676v2",
    "pdf_url": null
  },
  {
    "instance_id": "77d7fe31ce254db28ed8a6d4dc03adbf",
    "figure_id": "2205.11876v1-Figure5-1",
    "image_file": "2205.11876v1-Figure5-1.png",
    "caption": " Ablation analysis of loss functions in CPSTN on RoadScene dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the images shows the result of the CPSTN model without the L_cross and L_pst loss functions?",
    "answer": "Image (c).",
    "rationale": "The caption states that the figure shows the results of an ablation analysis of loss functions in the CPSTN model. The image labels indicate which loss functions were used or not used in each case. Image (c) is labeled \"w/o L_cross, L_pst\", which means that it was generated without using the L_cross and L_pst loss functions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.11876v1",
    "pdf_url": null
  },
  {
    "instance_id": "0ca27ee66b444fecb29a16839d83ad3f",
    "figure_id": "1902.05796v2-Figure8-1",
    "image_file": "1902.05796v2-Figure8-1.png",
    "caption": " Timeseries with complaints to selected domains and complainants causing complaints’ bursts.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which domain received the most complaints in the first quarter of 2017?",
    "answer": "mgp3taringa.net",
    "rationale": "The figure shows the number of complaints received by each domain over time. In the first quarter of 2017, mgp3taringa.net received the most complaints, as shown by the green line in the top right plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.05796v2",
    "pdf_url": null
  },
  {
    "instance_id": "dbd621c39c1e42b0b2f2721e45ac9541",
    "figure_id": "2103.00065v3-Figure16-1",
    "image_file": "2103.00065v3-Figure16-1.png",
    "caption": " The effect of depth: cross-entropy. We use gradient flow to train networks of various depths, ranging from 1 hidden layer to 4 hidden layers, using cross-entropy loss. We train each network from five different random initializations (different colors). Observe that progressive sharpening occurs to a greater degree for deeper networks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does the effect of progressive sharpening occur to a greater degree for deeper networks?",
    "answer": "Yes.",
    "rationale": "The figure shows that the sharpness of the network increases as the depth of the network increases. This is evident from the fact that the curves for deeper networks are steeper than the curves for shallower networks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.00065v3",
    "pdf_url": null
  },
  {
    "instance_id": "4773420ef37d4cff8885c073aae7dd5f",
    "figure_id": "2106.16115v1-Figure1-1",
    "image_file": "2106.16115v1-Figure1-1.png",
    "caption": " Illustrations of Key Definitions",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between Y2 and Y3?",
    "answer": "Y3 is a subset of Y2.",
    "rationale": "The figure shows that Y3 is completely contained within Y2. This means that every scenario in Y3 is also in Y2, but not every scenario in Y2 is in Y3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.16115v1",
    "pdf_url": null
  },
  {
    "instance_id": "f01511cac3e84e7ca118b25eb26d0693",
    "figure_id": "2306.14421v1-Figure10-1",
    "image_file": "2306.14421v1-Figure10-1.png",
    "caption": " Prototype system.",
    "figure_type": "Screenshot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the estimated energy consumption for this trip?",
    "answer": "0.8258 kWh",
    "rationale": "The figure shows a map with a route highlighted in green and red. The text \"0.8258 kWh\" is displayed next to the route, indicating the estimated energy consumption for the trip.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.14421v1",
    "pdf_url": null
  },
  {
    "instance_id": "6bf89d5c8aa44390bc96afe8ab7defb3",
    "figure_id": "2007.07011v2-FigureC.1-1",
    "image_file": "2007.07011v2-FigureC.1-1.png",
    "caption": "Figure C.1: Performance with the true policy vs other policies. Percent gap (∆) indicates task diversity. Body parts (BP) domains are more diverse than gravity (G) domains, and Walker-2D (W) and Hopper (Ho) domains are more varied than HalfCheetah (HC) domains.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment exhibits the greatest task diversity according to the performance gap between the true policy and other policies?",
    "answer": "Walker-2D Body Parts (W_BP)",
    "rationale": "The figure shows the average reward achieved by the true policy and other policies for each environment. The percent gap (Δ) between the two bars indicates the task diversity, with a larger gap indicating greater diversity. The W_BP environment has the largest gap (55%), indicating that it has the greatest task diversity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.07011v2",
    "pdf_url": null
  },
  {
    "instance_id": "f6ad5f583084435ca91235ca4f047123",
    "figure_id": "2006.13205v2-Figure9-1",
    "image_file": "2006.13205v2-Figure9-1.png",
    "caption": " Comparison of visual planning & control approaches. Execution traces of Visual Foresight (left), GCP-tree with non-hierarchical planning (middle) and GCP-tree with hierarchical planning (right) on two 25-room navigation tasks. Visualized are start and goal observation for all approaches as well as predicted subgoals for hierarchical planning. Both GCP-based approaches can reach faraway goals reliably, but GCP with hierarchical planning finds shorter trajectories to the goal.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three approaches, Visual Foresight, GCP-Flat, and GCP-Hierarchical, finds the shortest trajectory to the goal?",
    "answer": "GCP-Hierarchical.",
    "rationale": "The figure shows that the GCP-Hierarchical approach finds a shorter trajectory to the goal than the other two approaches. This is because the GCP-Hierarchical approach uses hierarchical planning, which allows it to break down the task into smaller subtasks and find more efficient solutions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.13205v2",
    "pdf_url": null
  },
  {
    "instance_id": "34d25570cb434ddc8468e3688104edb1",
    "figure_id": "1904.05003v1-Figure8-1",
    "image_file": "1904.05003v1-Figure8-1.png",
    "caption": " The ego network of a “game” group. The left side is the ego network, in which “game” groups are in red and “non-game” groups are in blue. The right side is the internal structure of the ego “game” group, in which a bigger node indicates a larger importance, and a darker color implies a larger node degree.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the user and the game group?",
    "answer": "The user is a member of the game group.",
    "rationale": "The user is represented by the green node in the center of the ego network. The game group is represented by the red nodes. The user is connected to the game group by a solid line, which indicates that the user is a member of the group.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.05003v1",
    "pdf_url": null
  },
  {
    "instance_id": "68fc4652ae8b4a888adfa22efed276d6",
    "figure_id": "2112.07471v6-Figure6-1",
    "image_file": "2112.07471v6-Figure6-1.png",
    "caption": " Qualitative comparison on real data. Unlike the baselines, Ours generates complete images and accurate geometry even when extrapolating expressions beyond the training data. The examples become more challenging from top to bottom.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method generates the most accurate geometry even when extrapolating expressions beyond the training data?",
    "answer": "Ours",
    "rationale": "The figure shows that the Ours method generates complete images and accurate geometry even when extrapolating expressions beyond the training data. This is evident in the bottom two rows of the figure, where the other methods generate incomplete or inaccurate images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.07471v6",
    "pdf_url": null
  },
  {
    "instance_id": "7528b1709702481c8f9e4d6258c70080",
    "figure_id": "2310.18894v1-Figure7-1",
    "image_file": "2310.18894v1-Figure7-1.png",
    "caption": " Few shot image synthesis datasets and qualitative comparison resutlts between our methods and FastGAN [25].",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the two methods produced more realistic images?",
    "answer": " Our method produced more realistic images.",
    "rationale": " The images in (b) show that our method produced images that are more similar to the real images in (a), while the images produced by FastGAN are more blurry and less realistic.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.18894v1",
    "pdf_url": null
  },
  {
    "instance_id": "629f3ae7cabd4817b3d722f9b8db116e",
    "figure_id": "2011.02893v1-Figure7-1",
    "image_file": "2011.02893v1-Figure7-1.png",
    "caption": " Top-10 retrosynthesis accuracy per reaction category with given reaction type.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best for reaction category 5?",
    "answer": "RetroSim",
    "rationale": "The figure shows the top-10 retrosynthesis accuracy per reaction category with given reaction type for three different methods: RetroSim, GLN, and RetroXpert. For reaction category 5, the bar for RetroSim is the highest, indicating that it has the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.02893v1",
    "pdf_url": null
  },
  {
    "instance_id": "21abce918f6649319b012eb8a91c2598",
    "figure_id": "2206.00783v1-Figure4-1",
    "image_file": "2206.00783v1-Figure4-1.png",
    "caption": " (Top) Parameter recovery for 𝑛 = 100, 𝑘 ∈ {2, 3}, 𝑐 = [1.5, 2.5], _ = 2.5. Legend: c0 corresponds to off-by-one parameters and c corresponds to the actual parameters. (Bottom) Average Runtime of Sampling Using the Ball Dropping Method for hypergraphs of orders 𝑘 ∈ {2, 3} and 50 − 500 nodes with a step of 50 nodes for an 1-layer instance with 𝑏 = 3, 𝑐 = 1.5. The dashed line is the function is the expected number of edges of a 𝑘-uniform CIGAM hypergraph.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which parameter has the highest posterior probability? ",
    "answer": " c.1 ",
    "rationale": " The top plot shows the posterior probability distribution for each parameter. The vertical red line represents the true value of each parameter, and the height of the bars represents the posterior probability. The parameter c.1 has the highest posterior probability, as the bar for c.1 is the tallest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.00783v1",
    "pdf_url": null
  },
  {
    "instance_id": "a2c0aa6c72ce40d7ad4c59e7765321f3",
    "figure_id": "1804.06459v2-Figure2-1",
    "image_file": "1804.06459v2-Figure2-1.png",
    "caption": " (a) Improvements of LIRPG augmented agents over A2C baseline agents. (b) Improvements of LIRPG augmented agents over live-bonus augmented A2C baseline agents. In both figures, the columns correspond to different games labeled on the x-axes and the y-axes show human score normalized improvements.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which game had the biggest improvement with LIRPG augmentation compared to the A2C baseline?",
    "answer": "UpNDown",
    "rationale": "The figure shows the relative performance of LIRPG augmented agents compared to A2C baseline agents for different games. The UpNDown game has the highest bar in the figure, indicating that it had the biggest improvement with LIRPG augmentation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.06459v2",
    "pdf_url": null
  },
  {
    "instance_id": "90eb99af9d15479f9d42b58045516dc7",
    "figure_id": "1809.04288v1-Figure3-1",
    "image_file": "1809.04288v1-Figure3-1.png",
    "caption": " Confusion Matrix for Using Our Model on ARVSU Dataset. Each cell shows number of utterances. The term “LoS Entities” corresponds to ”Line-of-Sight Entities” class. Average accuracy was 62.5%",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class had the highest number of correctly classified utterances?",
    "answer": "\"Others\"",
    "rationale": "The confusion matrix shows the number of correctly classified utterances for each class on the diagonal. The cell corresponding to the \"Others\" class has the highest value of 25,840.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.04288v1",
    "pdf_url": null
  },
  {
    "instance_id": "897ec8158fd94d73ba62ff735d5492db",
    "figure_id": "2010.07070v1-Figure4-1",
    "image_file": "2010.07070v1-Figure4-1.png",
    "caption": " The trivial profile d1.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the value of DB1(d1)?",
    "answer": "5/16",
    "rationale": "The figure shows the trivial profile d1, which is defined as the profile where DB1(d1) = 5/16.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.07070v1",
    "pdf_url": null
  },
  {
    "instance_id": "503fc4d2cc3d4d32b6206c320de0ecf4",
    "figure_id": "2003.06417v3-Figure5-1",
    "image_file": "2003.06417v3-Figure5-1.png",
    "caption": " The three environments used for testing SGM. PointEnv is a small maze with coordinate observations. We increase its difficulty by thinning the walls. ViZDoom is a large environment, which can take up to 5 minutes and 5k steps to traverse entirely. ViZDoom actions are discrete and observations are first-person camera views. SafetyGym is another large environment with first-person view observations, and supports continuous actions.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three environments has the most complex observation space?",
    "answer": "ViZDoom.",
    "rationale": "The figure shows that ViZDoom uses first-person camera views as observations, which are typically more complex than coordinate observations or top-down views.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.06417v3",
    "pdf_url": null
  },
  {
    "instance_id": "68d739e6e0e0448388cde5350d1ccde7",
    "figure_id": "2301.08834v2-Figure9-1",
    "image_file": "2301.08834v2-Figure9-1.png",
    "caption": " Results of ablation studies on different objective combinations",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which objective combination resulted in the highest accuracy?",
    "answer": "Case 1",
    "rationale": "The top left plot shows the accuracy for each case. Case 1 has the highest accuracy value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.08834v2",
    "pdf_url": null
  },
  {
    "instance_id": "fe811f25e3294a83bd58b77838ccd011",
    "figure_id": "1912.10589v2-Figure7-1",
    "image_file": "1912.10589v2-Figure7-1.png",
    "caption": " Qualitative comparison to state-of-the-art. Visual comparison of our results and those produced by Pixel2Mesh [38], AtlasNet [16], OccNet [27], and IM-NET [6] . In all examples our results are more consistent with the input images. For some of the methods, e.g. [27, 6] the strong shape priors result in meshes that are close to what the network considers a reasonable object but very far away from the input image.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most accurate 3D reconstruction of the chair?",
    "answer": "Our method.",
    "rationale": "The figure shows that our method produced a 3D reconstruction of the chair that is most similar to the input image. The other methods produced reconstructions that were either too smooth or too detailed.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.10589v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd095a97859449e19e685b75af8a75d5",
    "figure_id": "2210.09049v2-Figure3-1",
    "image_file": "2210.09049v2-Figure3-1.png",
    "caption": " The case visualization of the global boundary matrix. The span could be an entity if the corresponding color is dark. (Best viewed in color.)",
    "figure_type": "\"plot\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which word is most likely to be part of the same entity as \"essential thrombocythemia\"?",
    "answer": "\"Myeloproliferative disease\"",
    "rationale": "The figure shows that the words \"essential thrombocythemia\" and \"myeloproliferative disease\" are close together in the matrix and have a high color intensity, indicating that they are likely to be part of the same entity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.09049v2",
    "pdf_url": null
  },
  {
    "instance_id": "159da6865f1243438eef089a11f41f91",
    "figure_id": "2006.02572v2-Figure4-1",
    "image_file": "2006.02572v2-Figure4-1.png",
    "caption": " Average run-time of Newton-Schulz and EVD to compute on CPUs and GPUs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is faster for computing eigenvalues on a CPU?",
    "answer": "Newton-Schulz",
    "rationale": "The plot on the left shows that the blue line (Newton-Schulz) is below the red line (EVD) for all values of d, indicating that Newton-Schulz takes less time to compute eigenvalues on a CPU.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.02572v2",
    "pdf_url": null
  },
  {
    "instance_id": "e1bf910faa7f4ec2af9128adb619d0b3",
    "figure_id": "2011.07831v2-Figure17-1",
    "image_file": "2011.07831v2-Figure17-1.png",
    "caption": " Per-task test set performance comparison of the best catbAbI runs (first part).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on task 14?",
    "answer": "Transformer-XL.",
    "rationale": "The plot for task 14 shows that the Transformer-XL model achieves the highest accuracy, reaching a value of approximately 0.9.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.07831v2",
    "pdf_url": null
  },
  {
    "instance_id": "5c6b37df89fa4f6d9a97a2cc95c46c78",
    "figure_id": "2006.05779v2-Figure8-1",
    "image_file": "2006.05779v2-Figure8-1.png",
    "caption": " Comparison of NDCGwhen only using Q-learning for recommendations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better for purchase predictions?",
    "answer": "GRU-SAC performs better for purchase predictions.",
    "rationale": "The figure shows that GRU-SAC has the highest NDCG score for purchase predictions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.05779v2",
    "pdf_url": null
  },
  {
    "instance_id": "e545feea14184076b18a6a7859efe9b5",
    "figure_id": "2303.16058v1-Figure1-1",
    "image_file": "2303.16058v1-Figure1-1.png",
    "caption": " Comparison with SOTA methods. “ZS” and “FT” refer to “zero-shot” and “fine-tuned”. “T2V” means video-text retrieval. For Kinetics action recognition, [86] and [76] are excluded since they utilize model ensemble. With only public sources for pre-training, our approach achieves SOTA performances on scene-related, temporalrelated and complex video-language benchmarks. Compared with CoCa [90], our method is much more environmentally friendly with 70× reduction in carbon emissions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the best performance on Kinetics-700?",
    "answer": "Our method with public sources.",
    "rationale": "The figure shows that our method with public sources achieved the highest score on Kinetics-700 (83), which is higher than all other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.16058v1",
    "pdf_url": null
  },
  {
    "instance_id": "52e90d16530d476ca5bed8e023476c8c",
    "figure_id": "2010.00694v2-Figure4-1",
    "image_file": "2010.00694v2-Figure4-1.png",
    "caption": " Empirical comparison I: Quantitative analysis of the proposed CKE method with Bayesian DeepPrior against the other methods applied on the standard version. Evaluated datasets: ICVL (Left), BigHand2.2 (Middle) and NYU (Right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the ICVL dataset?",
    "answer": "MCD CKE",
    "rationale": "The plot on the left shows that the MCD CKE method has the lowest average joint error for all numbers of annotated images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.00694v2",
    "pdf_url": null
  },
  {
    "instance_id": "2bdaf3152591444c883b4bc13bddba32",
    "figure_id": "1908.00151v2-Figure5-1",
    "image_file": "1908.00151v2-Figure5-1.png",
    "caption": " Our multi-path training process. Left: Joint augmented input batch with uniformly sampled SO(3) views of n object instances; Right: Individual reconstruction targets xj for each decoder.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " How many decoders are shown in the figure? ",
    "answer": " Three ",
    "rationale": " The figure shows three decoders labeled Decoder 1, Decoder 2, and Decoder n.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.00151v2",
    "pdf_url": null
  },
  {
    "instance_id": "21a98dbc65c24ee0a46b2582803d240e",
    "figure_id": "1904.00069v3-Figure5-1",
    "image_file": "1904.00069v3-Figure5-1.png",
    "caption": " Qualitative comparison on 3D-EPN dataset.",
    "figure_type": "** Photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the methods produced the most accurate reconstruction of the airplane?",
    "answer": " Ours+.",
    "rationale": " The figure shows that the Ours+ method produced a reconstruction that is closest to the ground truth (GT) for the airplane. This can be seen by comparing the shapes of the airplane in the different columns.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.00069v3",
    "pdf_url": null
  },
  {
    "instance_id": "caccfd69dff94ec9ab095b3976ee0f80",
    "figure_id": "2010.14407v2-Figure2-1",
    "image_file": "2010.14407v2-Figure2-1.png",
    "caption": " Latent traversals of a trained model that perfectly disentangles the dataset’s FoVs. In each column, all latent variables but one are fixed.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which latent variable controls the color of the cube?",
    "answer": "The second latent variable from the left.",
    "rationale": "The second column from the left shows that the color of the cube changes while all other aspects of the image remain the same. This indicates that the second latent variable controls the color of the cube.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.14407v2",
    "pdf_url": null
  },
  {
    "instance_id": "def61cbd856b4997935bc8522cccf259",
    "figure_id": "2206.01342v3-Figure4-1",
    "image_file": "2206.01342v3-Figure4-1.png",
    "caption": " Overall matching score χ̄+ (Eqn. 16) with InfoNCE (top row) and quadratic loss (bottom row). When P = 1, linear model works well regardless of the degree of over-parameterization β, while ReLU requires large over-parameterization to perform well. When each Rk has multiple patterns (P > 1) related to generators, ReLU models can capture diverse patterns better than linear ones in the over-parameterization region β > 1. We found similar trend for other homogeneous activations such as LeakyReLU (with negative slope 0.05) and quadratic. In contrast, linear models are much less affected by over-parameterization. While the trends are similar, quadratic loss is not as effective as InfoNCE in feature learning. Each setting is repeated 3 times and mean/std are reported. See Appendix (Fig. 9 and Fig. 10) for χ̄−.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activation function performs best when P = 1 and the degree of overparameterization is low?",
    "answer": "Linear",
    "rationale": "The figure shows that when P = 1, the linear model has the highest overall matching score for all degrees of overparameterization.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.01342v3",
    "pdf_url": null
  },
  {
    "instance_id": "822daacef67b4927a95fe892730a58c2",
    "figure_id": "1811.00207v5-Figure1-1",
    "image_file": "1811.00207v5-Figure1-1.png",
    "caption": " Example where acknowledging an inferred feeling is appropriate",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What emotion is the speaker likely feeling based on the image?",
    "answer": "The speaker is likely feeling proud.",
    "rationale": "The image shows the speaker saying \"I finally got promoted today at work\" with a smiley face emoji. The text \"feels proud\" is also written next to the speaker.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.00207v5",
    "pdf_url": null
  },
  {
    "instance_id": "4623f05c914c472c8fb9c2f033a19664",
    "figure_id": "2210.09257v2-Figure3-1",
    "image_file": "2210.09257v2-Figure3-1.png",
    "caption": " Sweeps and ablation studies showing the average solver gap of three experiment seeds evaluated over 512 sampled games against the number of train steps. Subfigure (a) shows 4 × 4 games over different equilibrium selection, (b) shows MECCE over games with different numbers of players and strategies, (c) shows CE and CCE concepts on 8× 8 games, and (d) shows ablation experiments on MECCE 4× 4× 4 games.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which equilibrium selection method performs the best for 4x4 games?",
    "answer": "MWME",
    "rationale": "In subfigure (a), the MWME line has the lowest solver gap for most iterations, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.09257v2",
    "pdf_url": null
  },
  {
    "instance_id": "1c0dd507518041988926c4cf892dc22d",
    "figure_id": "2103.10153v3-Figure3-1",
    "image_file": "2103.10153v3-Figure3-1.png",
    "caption": " Instance of the described state-space model, visualized as a directed graphical model. Shaded variables are observed. Either only data, only mechanistic knowledge, or both sources of information can be conditioned on during inference (recall Figure 1).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What information is used to estimate the state variables in the model?",
    "answer": "The data and the mechanistic knowledge.",
    "rationale": "The figure shows that the state variables (X) are estimated based on the data (Y) and the mechanistic knowledge (Z). The arrows in the figure indicate the dependencies between the variables.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.10153v3",
    "pdf_url": null
  },
  {
    "instance_id": "d52359a13eac48b395c7c5c90fd17c6e",
    "figure_id": "1812.01945v1-Figure1-1",
    "image_file": "1812.01945v1-Figure1-1.png",
    "caption": " Traditional vs. Our methods in outlier detection. Green arrows/edges indicate correct annotations, while red arrows represent the outliers. The numbers indicate the number of votes received by each edge.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more likely to correctly identify outliers?",
    "answer": "Our method.",
    "rationale": "The figure shows that the traditional method incorrectly identifies the edge between nodes o1,o2 and o3,o4 as an outlier, even though it received 3 votes, which is the highest number of votes among all edges. In contrast, our method correctly identifies the edge between nodes o5,o6 and o3,o4 as an outlier, as it received only 1 vote.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.01945v1",
    "pdf_url": null
  },
  {
    "instance_id": "8db42ecf7f1048999fd0f04d2b730132",
    "figure_id": "2302.04308v2-Figure6-1",
    "image_file": "2302.04308v2-Figure6-1.png",
    "caption": " Comparison of baseline and enhanced features ACN, RFNet, and mmFormer by varying the full modality count from 100% to 40% (Fig. 7a). In order to retain sufficient samples for each combination task in metatraining, we assume that at least 50% of the subjects have partial modalities. Hence we show our results only on 50% and 40% proportions of full modality data. The fact that even with 50% full modality samples, we match the evaluation scores of SOTA at 100% setting is noteworthy. A sharp degradation can be noticed in the average WT DSC of SOTA once the number of full modality data decreases. On the other hand, our method shows only a minor drop of 0.29%. This is due to ACN being heavily dependent on the full modality for knowledge distillation. RFNet and mmFormer require full modality data as input to the network. They under-fit since their overall sample count decreases. Our method efficiently utilizes even limited samples of full modality data for feature adaptation in meta-testing. Owing to the above reasons, our approach is resilient to change in full modality proportion. Results for other tumor regions are provided in supplementary (Sec. 8).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most robust to changes in the proportion of full modality data?",
    "answer": "Our method.",
    "rationale": "Figure 7a shows that the average WT DSC of ACN, RFNet, and mmFormer all decrease significantly as the proportion of full modality data decreases, while our method shows only a minor drop. This is because our method efficiently utilizes even limited samples of full modality data for feature adaptation in meta-testing.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.04308v2",
    "pdf_url": null
  },
  {
    "instance_id": "9a7fb715a681402593547a6786779133",
    "figure_id": "2103.06779v2-Figure4-1",
    "image_file": "2103.06779v2-Figure4-1.png",
    "caption": " Percentage of Preference of Original Quatrains vs Quatrains rewritten by MERMAID",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of quatrain was preferred by the participants?",
    "answer": "Mermaid Quatrains.",
    "rationale": "The pie chart shows that 68% of participants preferred the Mermaid Quatrains, while only 32% preferred the original Quatrains.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.06779v2",
    "pdf_url": null
  },
  {
    "instance_id": "766b67efe7d348a0babd664c3c8c5457",
    "figure_id": "2012.13841v1-Figure9-1",
    "image_file": "2012.13841v1-Figure9-1.png",
    "caption": " Quantiles of log ∣∣mi wi ∣∣ for weightwi with buffermi as a function of time for DQN [Mnih et al., 2015] trained on various Atari games. The gradient signal is small compared to the weights, but the ratio differs dramatically between parameters.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which Atari game has the most stable gradient signal over time?",
    "answer": "Space Invaders.",
    "rationale": "The figure shows that the quantiles of log ∣∣mi wi ∣∣ for weight wi with buffer mi as a function of time for DQN trained on various Atari games. The gradient signal is small compared to the weights, but the ratio differs dramatically between parameters. The Space Invaders game has the most stable gradient signal over time because the quantiles are relatively close together and do not change much over time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.13841v1",
    "pdf_url": null
  },
  {
    "instance_id": "aae7583ce73d4bd6a6850b0cbddc45d7",
    "figure_id": "2104.06644v2-Figure4-1",
    "image_file": "2104.06644v2-Figure4-1.png",
    "caption": " BPPL scores per model per test scenario.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on test sentence F1?",
    "answer": "Model M1 performs best on test sentence F1.",
    "rationale": "The BPPL score for model M1 on test sentence F1 is 104.1, which is the highest score for that sentence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.06644v2",
    "pdf_url": null
  },
  {
    "instance_id": "83949296a08340c788860730cb498aa9",
    "figure_id": "1906.02385v1-Figure5-1",
    "image_file": "1906.02385v1-Figure5-1.png",
    "caption": " Illustration of a complication in cyclic models",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which figure shows a cycle that could lead to infinite recursion?",
    "answer": "Figure (b)",
    "rationale": "In figure (b), there is a cycle from V1 to V2 to V4 and back to V1. This cycle could lead to infinite recursion because there is no stopping condition.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02385v1",
    "pdf_url": null
  },
  {
    "instance_id": "13a7ceb5ea6f42f487000ff0b9870f3c",
    "figure_id": "2012.14905v4-Figure20-1",
    "image_file": "2012.14905v4-Figure20-1.png",
    "caption": " Convolutions are competitive to the standard fully connected setup.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on average, VSM FC or VSM CNN?",
    "answer": "VSM FC",
    "rationale": "The figure shows the accuracy of the two models on different datasets and training sets. VSM FC has a higher accuracy on average across all datasets and training sets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.14905v4",
    "pdf_url": null
  },
  {
    "instance_id": "6fac4b9992a643ae818df67d8e88c009",
    "figure_id": "2209.13284v2-Figure4-1",
    "image_file": "2209.13284v2-Figure4-1.png",
    "caption": " We show comparisons against the state-of-the-art methods by Park et al. [42] (ABME) and Reda et al. [48] (FILM).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most similar to ours?",
    "answer": "FILM",
    "rationale": "The figure shows that the arrows for our method and FILM are pointing in the same direction, which suggests that the two methods are producing similar results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.13284v2",
    "pdf_url": null
  },
  {
    "instance_id": "11086946b19448bbbfc745cd8375d99f",
    "figure_id": "2012.07320v1-Figure6-1",
    "image_file": "2012.07320v1-Figure6-1.png",
    "caption": " Results for core placement optimization in multicore chips (minimization) over 300 iterations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two algorithms, L2S-DISCO or BOCS, performs better for core placement optimization in multicore chips?",
    "answer": "L2S-DISCO performs better than BOCS.",
    "rationale": "The plot shows the best function value achieved by each algorithm over the number of iterations. The best function value for L2S-DISCO is consistently lower than that of BOCS, indicating that L2S-DISCO finds a better solution to the optimization problem.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.07320v1",
    "pdf_url": null
  },
  {
    "instance_id": "a202686bdefd4446b901e5e528233f84",
    "figure_id": "2104.13346v2-Figure5-1",
    "image_file": "2104.13346v2-Figure5-1.png",
    "caption": " Variation in partial Pearson correlation when omitting error types. Higher variation indicates greater influence of an error type in the overall correlation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of error has the greatest influence on the overall correlation between the BLEU score and human judgements?",
    "answer": "Verifiability errors.",
    "rationale": "The figure shows the variation in partial Pearson correlation when omitting different types of errors. The higher the variation, the greater the influence of that type of error on the overall correlation. We can see that Verifiability errors have the highest variation, indicating that they have the greatest influence on the overall correlation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.13346v2",
    "pdf_url": null
  },
  {
    "instance_id": "f25f0b98ca3a44469ebf15b101d58183",
    "figure_id": "2011.10233v2-Figure3-1",
    "image_file": "2011.10233v2-Figure3-1.png",
    "caption": " For each fine-tuning method, we evaluate the performance by adjusting the learning rate α in the range of 10−6 to 0.05. We took the average of the SI-SNRi scores in all testing datasets.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which fine-tuning method performs the best at a learning rate of 10^-5?",
    "answer": "m.",
    "rationale": "The figure shows that the Si-SNRi score for the m method is the highest at a learning rate of 10^-5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.10233v2",
    "pdf_url": null
  },
  {
    "instance_id": "875aba4d69c44af1a320e0b23a13a357",
    "figure_id": "2212.09095v2-Figure15-1",
    "image_file": "2212.09095v2-Figure15-1.png",
    "caption": " Spearman’s rank correlation coefficients between the attention head importance rankings for various tasks in the five-shot setting. All p-values < 0.01.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task has the highest correlation with HellaSwag in terms of attention head importance rankings?",
    "answer": "PIQA",
    "rationale": "The Spearman's rank correlation coefficient between HellaSwag and PIQA is 0.4, which is the highest among all the tasks shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.09095v2",
    "pdf_url": null
  },
  {
    "instance_id": "b03f5611e5c447a8910a685f41c8d62f",
    "figure_id": "2211.11727v3-Figure6-1",
    "image_file": "2211.11727v3-Figure6-1.png",
    "caption": " Prediction bias across ‘Old’/‘New’ classes. We show the per-class prediction distributions. Both works, especially UNO+, are prone to make biased predictions. Across all classes, the predictions are unexpectedly biased towards the head classes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the main difference between the prediction bias of UNO+ and GCD?",
    "answer": "UNO+ has a higher prediction bias than GCD.",
    "rationale": "The figure shows the per-class prediction distributions for UNO+ and GCD on the CIFAR100 and CUB datasets. The prediction bias is defined as the difference between the predicted probability of a class and the ground truth probability of that class. In all four plots, the curves for UNO+ are consistently higher than the curves for GCD, indicating that UNO+ has a higher prediction bias.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11727v3",
    "pdf_url": null
  },
  {
    "instance_id": "19cad747e21547648d50ae735fb0d144",
    "figure_id": "2104.14129v2-Figure7-1",
    "image_file": "2104.14129v2-Figure7-1.png",
    "caption": " Training throughput vs batch size. Red cross mark means out-of-memory. The shaded yellow region denotes the possible batch sizes with full precision training given the memory budget.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the training methods achieves the highest training throughput for ResNet-50?",
    "answer": "DTR",
    "rationale": "The figure shows the training throughput for different training methods and different batch sizes. For ResNet-50, DTR has the highest throughput at all batch sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.14129v2",
    "pdf_url": null
  },
  {
    "instance_id": "84e103d0700746a7bcc57329f5e794fa",
    "figure_id": "1810.13338v1-Figure2-1",
    "image_file": "1810.13338v1-Figure2-1.png",
    "caption": " Rate of location retrieval for F = 201. Figure 3: Rate of weight retrieval for F = 201.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of M and K has the highest rate of weight retrieval?",
    "answer": "M = 4, K = 2",
    "rationale": "The color of the heatmap in Figure 3 represents the rate of weight retrieval, with darker colors indicating a higher rate. The cell with the darkest color is at M = 4, K = 2, indicating that this combination has the highest rate of weight retrieval.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.13338v1",
    "pdf_url": null
  },
  {
    "instance_id": "217854fb7fc645e5beabe429362ff95f",
    "figure_id": "2202.02790v1-Figure6-1",
    "image_file": "2202.02790v1-Figure6-1.png",
    "caption": " Results from 40 different NES runs with 16 workers each (using random seeds) show that our method is able to identify SEs that allow agents to solve the target tasks. Each thin line corresponds to the average of 16 worker evaluation scores returned by EvaluateAgent in our algorithm as a function of the NES outer loop iterations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment was more difficult for the agent to learn?",
    "answer": "Acrobat-v1",
    "rationale": "The figure shows the cumulative reward for two environments, CartPole-v0 and Acrobat-v1, over the course of training. The CartPole-v0 environment reaches the solved threshold much faster than the Acrobat-v1 environment, indicating that it is easier for the agent to learn.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.02790v1",
    "pdf_url": null
  },
  {
    "instance_id": "48ac083bb68a452bb87d78af2cb58340",
    "figure_id": "2305.16947v1-Figure5-1",
    "image_file": "2305.16947v1-Figure5-1.png",
    "caption": " The CoNLL performance (average of MUC, B3 and CEAFϕ4 ) of each k-Sentence-Incremental model on the OntoNotes dev set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which k-Sentence Setting has the highest F1 score?",
    "answer": "25",
    "rationale": "The green line in the plot represents the F1 score. The highest point on the green line is at k-Sentence Setting = 25.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16947v1",
    "pdf_url": null
  },
  {
    "instance_id": "67765c14c6e6457aa5c2ae4455b17027",
    "figure_id": "1802.05074v5-Figure6-1",
    "image_file": "1802.05074v5-Figure6-1.png",
    "caption": " Training progress of the DNC. (a) Training loss (equals test loss) on the Differential Neural Computer architecture. See Fig. 2 for details. The L4 optimizers use default settings, whereas RMSProp and Adam use best performing learning rates 0.005 and 0.01, respectively. We see high stochasticity in training, particularly with Adam. Both L4 optimizers match or beat RMSProp in performance. (b) Effective learning rate η of L4Adam and plain Adam. The L4Adam displays a huge variance in the selected stepsize. This however has a stabilizing effect on the training progress.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer shows the highest stochasticity in training?",
    "answer": "Adam",
    "rationale": "The figure shows that the training loss for Adam fluctuates more than the other optimizers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1802.05074v5",
    "pdf_url": null
  },
  {
    "instance_id": "20d91391866a4406bb701d77225a4c20",
    "figure_id": "2211.03064v3-Figure1-1",
    "image_file": "2211.03064v3-Figure1-1.png",
    "caption": " Explaining the predictions of ViT-B/16 on four images. The saliency maps by ViT-CX are clearly more meaningful than those by previous attention-based methods (CGW1, CGW2, TAM), highlighting all regions apparently important to predictions. They are also more faithful to the model as measured by the deletion (Del) and insertion (Ins) AUC metrics. In contrast, the direct application of Score-CAM to ViTs can lead to nonsensical explanations (e.g., the goldfish and head cabbage example).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which explanation method produces the most accurate results for the \"goldfish\" image, according to the deletion and insertion AUC metrics?",
    "answer": "ViT-CX",
    "rationale": "The figure shows the deletion and insertion AUC metrics for each explanation method. For the \"goldfish\" image, ViT-CX has the highest deletion and insertion AUC metrics, indicating that it is the most accurate explanation method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.03064v3",
    "pdf_url": null
  },
  {
    "instance_id": "eafdb946990648dfa8ce54ffc7e83705",
    "figure_id": "2101.05930v2-Figure6-1",
    "image_file": "2101.05930v2-Figure6-1.png",
    "caption": " Examples of backdoored CIFAR-10 images by the 6 attacks.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attack resulted in the most visually imperceptible backdoor?",
    "answer": "The Refool attack.",
    "rationale": "The Refool image appears to be the most visually similar to a normal image, while the other attacks have obvious visual artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.05930v2",
    "pdf_url": null
  },
  {
    "instance_id": "33a65bde8c3f4610961c910d5b8e151c",
    "figure_id": "2303.03955v1-FigureA.6-1",
    "image_file": "2303.03955v1-FigureA.6-1.png",
    "caption": "Figure A.6: These plots show the actor’s gradients mean and standard deviation of DDPG-AE for all environments and all rollout horizons. For readability, we clip large gradients to 102 (Halfcheetah environment).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment has the largest gradients for the actor in DDPG-AE?",
    "answer": "Halfcheetah",
    "rationale": "The plots show that the Halfcheetah environment has the largest gradients for the actor in DDPG-AE, as the gradients are clipped to 102 for readability.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.03955v1",
    "pdf_url": null
  },
  {
    "instance_id": "39d1d1bc15604fc1bb8c82828a6a0863",
    "figure_id": "2208.06979v1-Figure8-1",
    "image_file": "2208.06979v1-Figure8-1.png",
    "caption": " Averaged Pearson correlation coefficients of traffic patterns between the links and their neighbors.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which city has the highest average Pearson correlation coefficient for 1-hop neighbors?",
    "answer": "Tianjin",
    "rationale": "The figure shows that the average Pearson correlation coefficient for 1-hop neighbors is highest for Tianjin (0.33), followed by Beijing (0.27) and Shanghai (0.26).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.06979v1",
    "pdf_url": null
  },
  {
    "instance_id": "d39dafec2704409ab28be376423fc244",
    "figure_id": "2005.01571v3-Figure6-1",
    "image_file": "2005.01571v3-Figure6-1.png",
    "caption": " Distribution of evaluation time of the best XGBoost config found by each method",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest to find the best XGBoost configuration?",
    "answer": "RS",
    "rationale": "The RS curve reaches 100% cumulative percentage the fastest, which means it finds the best XGBoost configuration in the shortest amount of time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.01571v3",
    "pdf_url": null
  },
  {
    "instance_id": "d3851396ea7e461cb226098dcd95222c",
    "figure_id": "2308.04163v1-Figure3-1",
    "image_file": "2308.04163v1-Figure3-1.png",
    "caption": " The comparison of synthetic images generated by our enhanced IFP and Feng et al. [6]. Our enhanced IFP generates haziness and contrast distortion, which can also be observed in real-world captured UDC images. PSNR and SSIM are calculated by using the captured UDC images as the reference.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods, \"Ours\" or \"Feng et al.\", generates images that are more similar to the real-world captured UDC images?",
    "answer": "\"Ours\"",
    "rationale": "The caption states that the enhanced IFP generates haziness and contrast distortion, which can also be observed in real-world captured UDC images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.04163v1",
    "pdf_url": null
  },
  {
    "instance_id": "888b9b1e096b45fa83b917d8deb6a748",
    "figure_id": "2202.06105v1-Figure2-1",
    "image_file": "2202.06105v1-Figure2-1.png",
    "caption": " Model convergence comparison of Myopic [9] with two baseline EH schedulers Round Robin and Greedy for EHFL.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three EH schedulers has the highest testing accuracy?",
    "answer": "Myopic",
    "rationale": "The figure shows that the Myopic scheduler has the highest testing accuracy at the end of the training process. This is evident from the blue line, which represents the Myopic scheduler, being above the other two lines throughout the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.06105v1",
    "pdf_url": null
  },
  {
    "instance_id": "301005cce4864c89b7eae6458b72c9f9",
    "figure_id": "2112.10483v1-Figure2-1",
    "image_file": "2112.10483v1-Figure2-1.png",
    "caption": " Cross-modal matching results: (left) FOP vs other losses used in F-V methods. (right) Our method vs state-of-the-art methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function performs the best when the number of images in the gallery is small?",
    "answer": "Our loss function.",
    "rationale": "The left plot shows that our loss function (green line) has the highest V-F Matching ACC for all values of n, including small values of n.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.10483v1",
    "pdf_url": null
  },
  {
    "instance_id": "6c6ae9a0b52445a9840bae35d7da7f58",
    "figure_id": "2311.08357v1-Figure3-1",
    "image_file": "2311.08357v1-Figure3-1.png",
    "caption": " A comparison of the best gradient size reduction achieved by DP-AdaFEST, DP-FEST, and DP-SGD with exponential selection [ZMH21] compared to DP-SGD at different thresholds for utility difference. A higher curve indicates a better utility/efficiency trade-off. DP-AdaFEST consistently outperforms DP-FEST.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm consistently achieves the best utility/efficiency trade-off across all datasets and thresholds for utility difference?",
    "answer": "DP-AdaFEST.",
    "rationale": "The figure shows that the curves for DP-AdaFEST are consistently higher than the curves for DP-FEST and DP-SGD with exponential selection across all datasets and thresholds for utility difference. A higher curve indicates a better utility/efficiency trade-off.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2311.08357v1",
    "pdf_url": null
  },
  {
    "instance_id": "869cac3d1eac4c04ac8e61bce788842d",
    "figure_id": "1903.11367v2-Figure7-1",
    "image_file": "1903.11367v2-Figure7-1.png",
    "caption": " Smoothed distribution of specificity scores.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which response type has the highest median specificity score?",
    "answer": "iResp",
    "rationale": "The peak of the iResp distribution is shifted to the right compared to the other two distributions, indicating that the median specificity score for iResp is higher.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.11367v2",
    "pdf_url": null
  },
  {
    "instance_id": "0ccebe21a7394a83b1bbdbe2a9ce123e",
    "figure_id": "2012.13866v1-Figure5-1",
    "image_file": "2012.13866v1-Figure5-1.png",
    "caption": " BLEU scores [%] and translation speed [tokens/sec] against decoder depth on the En-De task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has a higher BLEU score at a decoder depth of 12?",
    "answer": "Base-RPR",
    "rationale": "The figure shows that the red line (Base-RPR) is higher than the blue line (Deep-RPR) at a decoder depth of 12.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.13866v1",
    "pdf_url": null
  },
  {
    "instance_id": "702ec72f532b48adb43335cbe0a1e85c",
    "figure_id": "2303.15247v2-Figure5-1",
    "image_file": "2303.15247v2-Figure5-1.png",
    "caption": " Histogram of the number of queries per number of ground truths for the CIRCO dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many queries have at least 5 ground truths?",
    "answer": "175",
    "rationale": "The bar for 5 ground truths shows a value of approximately 175 on the y-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.15247v2",
    "pdf_url": null
  },
  {
    "instance_id": "4aff67fb79ae4b1a8fed854cadd46bfc",
    "figure_id": "2105.14095v2-Figure2-1",
    "image_file": "2105.14095v2-Figure2-1.png",
    "caption": " Extension to weighted-sample training. The weighted training algorithm can be easily extended from the weights on tasks to the weights on samples. As for the experiments on weightedsample training, we use the PoS tagging on entity words as the source task and named entity classification on entity words as the target task. Note that the settings for the weighted-sample training are quite different from those for the weighted-task training in the remaining parts because the weightedsample training is much more costly compared to the weighted-task training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which learning method achieved the highest performance on the named entity classification task?",
    "answer": "Weighted-Sample Joint Training",
    "rationale": "The figure shows the performance of different learning methods on the named entity classification task. The bar for Weighted-Sample Joint Training is the highest, indicating that it achieved the highest performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14095v2",
    "pdf_url": null
  },
  {
    "instance_id": "16c3d50dd6f143d5a3de64bd54291ffd",
    "figure_id": "1905.11680v1-Figure2-1",
    "image_file": "1905.11680v1-Figure2-1.png",
    "caption": "Figure 2 The example for non-existence of equilibria in directed discrete preference games.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which node has the highest in-degree in the graph shown in Figure 2(b)?",
    "answer": "Node (0, b) has the highest in-degree of 3.",
    "rationale": "The in-degree of a node is the number of edges that point to it. In Figure 2(b), node (0, b) has three edges pointing to it, which is the highest among all the nodes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11680v1",
    "pdf_url": null
  },
  {
    "instance_id": "46e65ab7a4304b9997b3b24572270e75",
    "figure_id": "2310.01551v2-Figure4-1",
    "image_file": "2310.01551v2-Figure4-1.png",
    "caption": " Test accuracy plateaus for large k. All runs averaged over 10 random train-test splits with maximum depth fixed to 3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest test accuracy for large values of k?",
    "answer": "Hayes-roth",
    "rationale": "The plot for Hayes-roth shows that the test accuracy is highest for large values of k compared to the other two datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.01551v2",
    "pdf_url": null
  },
  {
    "instance_id": "596a90d18fcc468091d494bbcae7458c",
    "figure_id": "2306.00576v1-Figure4-1",
    "image_file": "2306.00576v1-Figure4-1.png",
    "caption": " Number of videos per each mammal category. We rank the categories according to their trimmed videos frequency.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which mammal category has the highest number of full-length videos?",
    "answer": "Canis",
    "rationale": "The figure shows the number of videos for each mammal category, with the bars representing the number of trimmed videos (red) and full-length videos (teal). The Canis category has the highest teal bar, indicating that it has the most full-length videos.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.00576v1",
    "pdf_url": null
  },
  {
    "instance_id": "25dba345b7064698b28d1924df5c3c55",
    "figure_id": "2202.07646v3-Figure12-1",
    "image_file": "2202.07646v3-Figure12-1.png",
    "caption": " Text examples that are memorized by the 125M model (according to true-continuation match), but not memorized by larger models (the generated texts do not match the true continuation, nor any other training examples). The first column shows the prompt. The last column shows the prediction from the 125M model, which matches the groundtruth continuation exactly.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model memorized the phrase \"Kingdom.\" in the first example?",
    "answer": "The 125M model.",
    "rationale": "The text in the 125M model column for the first example matches the ground truth continuation exactly, including the word \"Kingdom.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.07646v3",
    "pdf_url": null
  },
  {
    "instance_id": "febaa2b7b7cb45eca322c2b6fd55b979",
    "figure_id": "2205.12688v2-Figure5-1",
    "image_file": "2205.12688v2-Figure5-1.png",
    "caption": " Results of head-to-head comparison between models with and without Canary on PROSOCIALDIALOG via human judgements (§6.2).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed best in terms of overall score?",
    "answer": "GPT-3 + Canary",
    "rationale": "The figure shows that GPT-3 + Canary has the highest bar for the \"Overall\" category.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.12688v2",
    "pdf_url": null
  },
  {
    "instance_id": "76e1bbf473ec4a2aab3d8c9cb1964a02",
    "figure_id": "2305.03403v5-Figure18-1",
    "image_file": "2305.03403v5-Figure18-1.png",
    "caption": " Dataset description for Kaggle_health-insurance-lead-prediction-raw-data.",
    "figure_type": "Other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of machine learning problem is this dataset suitable for?",
    "answer": "Binary Classification",
    "rationale": "The passage states that \"For the data and objective, it is evident that this is a Binary Classification Problem\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.03403v5",
    "pdf_url": null
  },
  {
    "instance_id": "d681afe09464427282851cd122213da1",
    "figure_id": "2303.03374v2-Figure3-1",
    "image_file": "2303.03374v2-Figure3-1.png",
    "caption": " Train and test accuracy of individual models from three differently behaving SSEs (left plots) and StarSSEs (right plots) on CIFAR-100 with self-supervised pre-training (with a single fine-tuned model for comparison). Hyperparameters for more local and more semi-local experiments are the same for SSE and StarSSE, while hyperparameters for the optimal experiments may differ.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which experiment type, local, semi-local, or optimal, resulted in the highest test accuracy for StarSSE models?",
    "answer": "Optimal",
    "rationale": "The right plots show the test accuracy for StarSSE models. The red line represents the optimal experiment, and it is consistently higher than the other two lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.03374v2",
    "pdf_url": null
  },
  {
    "instance_id": "f8908613bb834ddd927bcf888cdf601d",
    "figure_id": "2012.03137v1-Figure10-1",
    "image_file": "2012.03137v1-Figure10-1.png",
    "caption": " Sampling M starting from the output node. Labels on edges denote probabilities of transition. Numbers in boxes correspond to rewards accumulated at each hidden node. Straight lines show a potential sample path in sampling, with total ward B1,1 +B2,1.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the total reward for the sample path shown in the figure?",
    "answer": "B1,1 + B2,1",
    "rationale": "The straight lines in the figure show a potential sample path in sampling. The numbers in boxes correspond to the rewards accumulated at each hidden node. The total reward for the sample path is the sum of the rewards at each node, which is B1,1 + B2,1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.03137v1",
    "pdf_url": null
  },
  {
    "instance_id": "0a36068134b8489fa633b5ce6bb7111e",
    "figure_id": "2112.08414v1-Figure1-1",
    "image_file": "2112.08414v1-Figure1-1.png",
    "caption": " The product titles are truncated in order to fit within the e-commerce search mobile APP.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the purpose of truncating the product titles in the e-commerce search mobile APP?",
    "answer": "To fit more products on the screen.",
    "rationale": "The image shows a screenshot of an e-commerce search mobile APP. The product titles are truncated, which means that they are shortened so that they can fit within the limited space on the screen. This allows the user to see more products at a glance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.08414v1",
    "pdf_url": null
  },
  {
    "instance_id": "34297a4238d044e2b6fbc3adaa7ebea9",
    "figure_id": "2304.06813v2-Figure7-1",
    "image_file": "2304.06813v2-Figure7-1.png",
    "caption": " Histogram of ID+, ID−, C-OOD+, and C-OOD− data at different g(x, f). We use the ImageNet-R as C-OOD. We use ResNet50 as the model f , and MSP and Energy for g(x, f).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the greatest overlap between in-distribution (ID) and corruption out-of-distribution (C-OOD) data?",
    "answer": "MSP",
    "rationale": "The figure shows the density plots of the scores for ID+, ID−, C-OOD+, and C-OOD− data for both MSP and Energy methods. The MSP plot shows a greater overlap between the ID and C-OOD distributions than the Energy plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.06813v2",
    "pdf_url": null
  },
  {
    "instance_id": "dcf2596903394d2dbe131143768e148d",
    "figure_id": "2004.13912v2-Figure2-1",
    "image_file": "2004.13912v2-Figure2-1.png",
    "caption": " Accurately Fitting the Toy Dataset: Training predictions learned by a single hidden layer neural network with 1024 (a) standard ReLU, and (b) ReLU-n with ExU hidden units trained for 10,000 epochs on the binary classification dataset described in Section 2. We can see that the ReLU network has learned a fairly smooth function while the ExU network has learned a very jumpy function. We find that a DNN with three hidden layers also learned smooth functions (see Figure A.3).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network learned a smoother function, the ReLU network or the ExU network?",
    "answer": "The ReLU network.",
    "rationale": "The figure shows the predictions of the two networks on the toy dataset. The ReLU network's predictions are smoother than the ExU network's predictions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.13912v2",
    "pdf_url": null
  },
  {
    "instance_id": "41f3ed824e334558a74adafdc971a145",
    "figure_id": "2112.15025v3-Figure9-1",
    "image_file": "2112.15025v3-Figure9-1.png",
    "caption": " The unnormalized sum of rewards of Π15, and the policy sets constructed by DIAYN, SMP and DSP. Since the policy sets constructed by the prior methods depend on their particular initialization, their plots are obtained by running each of the constructed policy sets for 5 runs and then averaging over their results. For each task, the agent was evaluated on 1000 episodes. The plot for Π15 is obtained in a similar way as in Fig. 8.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which policy set achieved the highest average sum of rewards?",
    "answer": "Π15 achieved the highest average sum of rewards.",
    "rationale": "The plot shows the sum of rewards for each policy set. The lines for each policy set are plotted on the same axes, so we can compare their performance directly. We can see that the line for Π15 is consistently higher than the lines for the other policy sets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.15025v3",
    "pdf_url": null
  },
  {
    "instance_id": "2fb48dfb0c904ec69c4c643337ea0b16",
    "figure_id": "1809.03075v1-Figure1-1",
    "image_file": "1809.03075v1-Figure1-1.png",
    "caption": " Sequential action space for the first player in the game of Kuhn poker. denotes an observation point; represents the end of the decision process.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the possible actions for the first player in the game of Kuhn poker if their first card is a king?",
    "answer": "The possible actions are \"check\" or \"raise\".",
    "rationale": "The figure shows a tree of possible actions for the first player in the game of Kuhn poker. The first player's first card is shown at the top of the tree. If the first card is a king, the possible actions are shown on the branches below the king node.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.03075v1",
    "pdf_url": null
  },
  {
    "instance_id": "810e5dafff404a359930061f4b2804bb",
    "figure_id": "2208.00368v1-Figure8-1",
    "image_file": "2208.00368v1-Figure8-1.png",
    "caption": " Spectrum importance scores on different actions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which action has the highest importance score for the combined filtering g(1, 1)?",
    "answer": "Walking",
    "rationale": "The figure shows the importance scores for different actions and combined filterings. For the combined filtering g(1, 1), the blue bar representing walking is higher than the red bar representing sitting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.00368v1",
    "pdf_url": null
  },
  {
    "instance_id": "2d0d1a1cacdb4400bf56fde4dd8a7435",
    "figure_id": "2308.11261v1-Figure25-1",
    "image_file": "2308.11261v1-Figure25-1.png",
    "caption": " Qualitative comparison to the state of the part method [15] in MC scenario. Top: Ground truth in orange, Middle: HMD-NeMo, Bottom: Jiang et al. [15].",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is better at predicting the future position of the pedestrian, HMD-NeMo or Jiang et al. [15]?",
    "answer": "HMD-NeMo",
    "rationale": "The figure shows that the predictions made by HMD-NeMo are closer to the ground truth than the predictions made by Jiang et al. [15]. This can be seen by comparing the orange and blue figures to the ground truth figures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.11261v1",
    "pdf_url": null
  },
  {
    "instance_id": "58fefd02e2894aa3b0126c199a2f194f",
    "figure_id": "1911.11098v1-Figure9-1",
    "image_file": "1911.11098v1-Figure9-1.png",
    "caption": " Synthetic chair variations. We show a few examples of the 96 structural variations of a bench, including variations to the legs, base, backrest, and armrests.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different types of benches are shown in the image?",
    "answer": "4",
    "rationale": "The caption states that there are 96 structural variations of a bench, including variations to the legs, base, backrest, and armrests. The image shows a few examples of these variations, including benches with different types of legs, bases, backrests, and armrests.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.11098v1",
    "pdf_url": null
  },
  {
    "instance_id": "eb6bf73f4fc14b8f87aada043a95bc2b",
    "figure_id": "2206.07883v3-Figure5-1",
    "image_file": "2206.07883v3-Figure5-1.png",
    "caption": " Error Probabiltiy for Experiment 2",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest error probability at T = 600?",
    "answer": "LUCB",
    "rationale": "The figure shows that the blue line, which represents the LUCB algorithm, is the lowest at T = 600.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.07883v3",
    "pdf_url": null
  },
  {
    "instance_id": "5c20f7bb7a8d42d99613e875493f65bc",
    "figure_id": "2310.15484v1-Figure5-1",
    "image_file": "2310.15484v1-Figure5-1.png",
    "caption": " Effect of number of layers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does using backup improve the F1 score of NuTrea?",
    "answer": "Yes.",
    "rationale": "The figure shows that the F1 score of NuTrea with backup is consistently higher than the F1 score of NuTrea without backup, regardless of the number of layers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.15484v1",
    "pdf_url": null
  },
  {
    "instance_id": "ea08a8b2572842c89c1769a26a32cfc7",
    "figure_id": "2012.12631v2-Figure13-1",
    "image_file": "2012.12631v2-Figure13-1.png",
    "caption": " Comparison of all baselines on the Spl stream",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which baseline method achieves the best accuracy on the Spl stream while using the least amount of memory and FLOPs?",
    "answer": "MNTDP-D",
    "rationale": "The figure shows the accuracy, memory usage, and FLOPs for each baseline method on the Spl stream. MNTDP-D has the highest accuracy and the lowest memory usage and FLOPs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.12631v2",
    "pdf_url": null
  },
  {
    "instance_id": "29d9b33ffbd54fde98faad5b9cb24a90",
    "figure_id": "2304.04408v1-Figure3-1",
    "image_file": "2304.04408v1-Figure3-1.png",
    "caption": " Average accuracy rate on observed learning stages on Split CIFAR100 and Split MiniImageNet while the buffer size is 1000.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best on both Split CIFAR100 and Split MiniImageNet datasets?",
    "answer": "ER.",
    "rationale": "The figure shows the average accuracy rate of different algorithms on both datasets. ER consistently achieves the highest accuracy rate across all learning tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.04408v1",
    "pdf_url": null
  },
  {
    "instance_id": "5ca847f31dbc45fe8331e43670c32449",
    "figure_id": "2303.11327v1-Figure6-1",
    "image_file": "2303.11327v1-Figure6-1.png",
    "caption": " More Examples of 3DMV-VQA Dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many chairs are there in the room with the sofa?",
    "answer": "6",
    "rationale": "The figure shows a floor plan of the room with the sofa and the corresponding photograph. The floor plan shows six chairs in the room, and the photograph confirms this.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.11327v1",
    "pdf_url": null
  },
  {
    "instance_id": "6d4680684c94432ba42c386c15830e23",
    "figure_id": "2007.15531v2-Figure3-1",
    "image_file": "2007.15531v2-Figure3-1.png",
    "caption": " Maps of 20 highest weighted nodes for layers 1, 2, and 3. Black star is the forecasted node.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the most nodes located in the Hollywood area?",
    "answer": "Layer 2.",
    "rationale": "The figure shows that layer 2 has the most nodes located in the Hollywood area, as evidenced by the concentration of orange circles in that area.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.15531v2",
    "pdf_url": null
  },
  {
    "instance_id": "2aff14d1abd142af926338afce23f517",
    "figure_id": "1807.03848v3-Figure5-1",
    "image_file": "1807.03848v3-Figure5-1.png",
    "caption": " Comparison memory requirement at the training and test phases among other types of networks. (a) Training. (b) Test. (The batch size is 8.)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture has the lowest memory requirement for training and testing?",
    "answer": "Inception-v3",
    "rationale": "The figure shows the memory requirement for training and testing for different network architectures. Inception-v3 has the lowest memory requirement for both training and testing.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1807.03848v3",
    "pdf_url": null
  },
  {
    "instance_id": "9993840201f54474a9945e170ad66d65",
    "figure_id": "1911.12409v1-Figure7-1",
    "image_file": "1911.12409v1-Figure7-1.png",
    "caption": " Confusion matrices for testing P&C performance on the three datasets(from left to right): NW-UCLA(10 actions); UWA3D V4(30 actions); NTU-RGBD Cross-View(60 actions).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the most accurate P&C performance?",
    "answer": "NW-UCLA.",
    "rationale": "The NW-UCLA confusion matrix has the highest values on the diagonal, which indicates that the model is correctly predicting the true labels more often than for the other two datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.12409v1",
    "pdf_url": null
  },
  {
    "instance_id": "89539723da254d1c80bf5bab6c9c2731",
    "figure_id": "2104.12690v1-Figure7-1",
    "image_file": "2104.12690v1-Figure7-1.png",
    "caption": " Semi-supervised learning advances online labeling. The learning problem in online-labeling can be seen as a semi-supervised problem (Sec. 4.4). We compare two semi-supervised techniques: PseudoLabeling [27] and MixMatch [1]. Both improve annotation efficiency on all subsets, particularly for fine-grained datasets. Surprisingly, Pseudolabels performs slightly better than a modified version of MixMatch (Sec. 4.4).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the semi-supervised learning techniques performed better in the online labeling task?",
    "answer": "PseudoLabeling",
    "rationale": "The caption states that \"Pseudolabels performs slightly better than a modified version of MixMatch.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.12690v1",
    "pdf_url": null
  },
  {
    "instance_id": "ddd2e274d96f4993a3e00d3a4894e26a",
    "figure_id": "2103.03905v3-Figure13-1",
    "image_file": "2103.03905v3-Figure13-1.png",
    "caption": " ImageNet32x32 generations. Left: VAE; Right: K++.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two models, VAE or K++, generates images that are more visually appealing and realistic?",
    "answer": "K++",
    "rationale": "The images generated by K++ on the right are sharper and more detailed than those generated by VAE on the left. For example, the images of the cat and the airplane in the K++ column are more easily recognizable than their counterparts in the VAE column.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.03905v3",
    "pdf_url": null
  },
  {
    "instance_id": "7ac049920618405cbb3f95c8d854d58d",
    "figure_id": "1809.06404v3-Figure5-1",
    "image_file": "1809.06404v3-Figure5-1.png",
    "caption": " The top and bottom rows show the path followed by a 2D point-mass agent (yellow) to reach the target (green) in training and testing environment, respectively.",
    "figure_type": "schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which environment does the agent take a shorter path to reach the target?",
    "answer": "Training environment.",
    "rationale": "The figure shows that the agent in the top row (training environment) takes a shorter path to reach the target than the agent in the bottom row (testing environment).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.06404v3",
    "pdf_url": null
  },
  {
    "instance_id": "75d8cb4e55374b4a965d8b4f089a18cc",
    "figure_id": "2012.11522v1-Figure15-1",
    "image_file": "2012.11522v1-Figure15-1.png",
    "caption": " The mean of the synthesis score of the top 100 molecules proposed by each method, over a series of ten GuacaMol tasks [10, §3.2].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three generative methods achieved the highest mean synthesizable score for Scaffold Hop?",
    "answer": "SMILES LSTM",
    "rationale": "The bar for SMILES LSTM in the Scaffold Hop column is the highest among the three generative methods (SMILES LSTM, SMILES GA, and Graph GA).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.11522v1",
    "pdf_url": null
  },
  {
    "instance_id": "41d5a73157c143b5a00239abe3880717",
    "figure_id": "2307.02064v2-Figure9-1",
    "image_file": "2307.02064v2-Figure9-1.png",
    "caption": " Generation MSE per imagination step of alternative S4WM architectures. Each environment is labeled with (context steps | query steps). S4WM-Full-Posterior performs the best as imagination horizon increases.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which S4WM architecture performs the best as the imagination horizon increases?",
    "answer": "S4WM-Full-Posterior",
    "rationale": "The figure shows that the S4WM-Full-Posterior architecture has the lowest generation MSE for both the Four Rooms and Ten Rooms environments as the number of imagination steps increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.02064v2",
    "pdf_url": null
  },
  {
    "instance_id": "c4dcc55af7aa402abde235a437604456",
    "figure_id": "2206.04192v2-Figure1-1",
    "image_file": "2206.04192v2-Figure1-1.png",
    "caption": " (a) Interpretation of relation parameters (orange dashed) as a parallelogram (green solid) in the j-th correlation subspace; (b) Multiple relation embeddings with the following properties: Symmetry (rB), Anti-Symmetry (rA, rD, rE , rF ), Inversion (rD = r−1A ), Hierarchy rA(X,Y )⇒ rC(X,Y ), Intersection rD(X,Y ) ∧ rE(X,Y )⇒ rF (X,Y ), Mutual Exclusion (e.g., rA ∩ rB = ∅).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which relation embedding is depicted as anti-symmetric?",
    "answer": "rA, rD, rE, and rF are anti-symmetric.",
    "rationale": "The figure shows that these relation embeddings do not lie on the identity line, which means that they are not symmetric.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.04192v2",
    "pdf_url": null
  },
  {
    "instance_id": "25f66821a85648b197a7ec0b5aede156",
    "figure_id": "1802.08898v5-Figure4-1",
    "image_file": "1802.08898v5-Figure4-1.png",
    "caption": " Logarithmic plot comparing an estimate for the quantity used to bound the numerical error of second-order integrators using the Euclidean Lipschitz constant and the infinity-norm Lipschitz constant, at different values of d.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which norm produces a tighter bound on the numerical error of second-order integrators?",
    "answer": "The Euclidean norm.",
    "rationale": "The figure shows that the quantity used to bound the numerical error of second-order integrators using the Euclidean Lipschitz constant (blue line) is smaller than the quantity used to bound the numerical error using the infinity-norm Lipschitz constant (black line) for all values of d.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1802.08898v5",
    "pdf_url": null
  },
  {
    "instance_id": "117ccd40682048a1bf750b272362be9d",
    "figure_id": "2203.02343v2-Figure10-1",
    "image_file": "2203.02343v2-Figure10-1.png",
    "caption": " Affinity network for the dataset 2002-Presidential with a threshold at 12%",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which candidate has the most connections to other candidates?",
    "answer": "Jospin",
    "rationale": "The figure shows that Jospin has the most connections to other candidates, as indicated by the number of lines connecting Jospin's circle to other circles.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.02343v2",
    "pdf_url": null
  },
  {
    "instance_id": "5e8573c9a3e04dff916683a38c061f41",
    "figure_id": "2306.06063v1-Figure4-1",
    "image_file": "2306.06063v1-Figure4-1.png",
    "caption": " Relative prompt transfer performance (transfer performance / original VNT performance) on the target tasks of the virtual node prompts trained on the source tasks.",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method of prompt reuse resulted in better performance on target task 5?",
    "answer": " Using learned prompts as an initializer.",
    "rationale": " The figure shows the relative prompt transfer performance for two methods of prompt reuse: learned prompts reuse and learned prompts as an initializer. The performance on target task 5 is shown in the fifth row of each heatmap. For learned prompts reuse, the performance is 0.78, while for learned prompts as an initializer, the performance is 1.3. Therefore, using learned prompts as an initializer resulted in better performance on target task 5.\n\n**Figure type:** Plot",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.06063v1",
    "pdf_url": null
  },
  {
    "instance_id": "3112b05d8e264e6ea9f6f9a187dec210",
    "figure_id": "2106.09686v3-Figure24-1",
    "image_file": "2106.09686v3-Figure24-1.png",
    "caption": " Pareto frontier estimates in meta-LOOCV settings on the LR-tuned error and memory matrices. Each error bar is the standard error across datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest convergence error in Setting I?",
    "answer": "ED-MF (PEPPP)",
    "rationale": "The plot in (a) shows the convergence error for each method in Setting I. The ED-MF (PEPPP) method has the lowest mean convergence error, as indicated by the red circle.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.09686v3",
    "pdf_url": null
  },
  {
    "instance_id": "a632eaea2ceb4abe8a0a89db0fff1837",
    "figure_id": "2106.00672v1-Figure47-1",
    "image_file": "2106.00672v1-Figure47-1.png",
    "caption": " Analysis of choice critic MLP depth (C3): 95th percentile of performance scores conditioned on choice (top) and distribution of choices in top 5% of configurations (bottom).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment has the largest difference in performance between the expert and human policies?",
    "answer": "Door",
    "rationale": "The figure shows the performance of expert and human policies in different environments. The difference in performance between the expert and human policies is largest for the Door environment, where the expert policy performs significantly better than the human policy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.00672v1",
    "pdf_url": null
  },
  {
    "instance_id": "9a3bae724eb84117a67faeb6ab37d9c1",
    "figure_id": "2110.12467v1-Figure10-1",
    "image_file": "2110.12467v1-Figure10-1.png",
    "caption": " Qualitative results on Cityscapes, Google Maps, CMP Facade. Similar to Figure 2.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms is able to recover the most detail in the perturbed images?",
    "answer": "UGAC (ours)",
    "rationale": "The figure shows the results of different algorithms on three different datasets: Cityscapes, Google Maps, and CMP Facade. The UGAC algorithm is able to recover the most detail in the perturbed images, as evidenced by the fact that the images produced by UGAC are the most similar to the ground truth images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.12467v1",
    "pdf_url": null
  },
  {
    "instance_id": "80b6e15ed9ee4f5abc276c9952a878ab",
    "figure_id": "2305.00286v1-Figure13-1",
    "image_file": "2305.00286v1-Figure13-1.png",
    "caption": " Full timescale results in non-stationary MuJoCo environments",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in the Cheetah-Direction-Non-Stat environment?",
    "answer": "RL2",
    "rationale": "The figure shows the average return for each algorithm in the Cheetah-Direction-Non-Stat environment. RL2 has the highest average return, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.00286v1",
    "pdf_url": null
  },
  {
    "instance_id": "cb80c01b475a437397b9909f00a9e0b5",
    "figure_id": "2008.07905v3-Figure4-1",
    "image_file": "2008.07905v3-Figure4-1.png",
    "caption": " Performance under different source input length on WMT14 DE-EN",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on average across all source input lengths?",
    "answer": "GLAT",
    "rationale": "The figure shows that the GLAT model consistently achieves the highest BLEU score across all source input lengths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.07905v3",
    "pdf_url": null
  },
  {
    "instance_id": "cb1ab3c4a944443fb98c339459832ae5",
    "figure_id": "2204.05426v2-Figure4-1",
    "image_file": "2204.05426v2-Figure4-1.png",
    "caption": " Number of unique 5-nearest training examples to each prototype (blue+red), and the number of examples associated with only 1 prototype (blue-only). Without normalization, very few examples (out of 100) are close to all prototypes; with normalization, we observe more diversity: different training examples are near different prototypes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does normalization lead to a greater diversity of training examples associated with each prototype?",
    "answer": "Yes.",
    "rationale": "The figure shows that with normalization, there are more unique 5-nearest training examples to each prototype (blue+red bars) and that more examples are associated with only 1 prototype (blue-only bars). This suggests that normalization helps to diversify the training examples associated with each prototype.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.05426v2",
    "pdf_url": null
  },
  {
    "instance_id": "5672194715324eefa578e49da69ec93c",
    "figure_id": "1912.02249v3-Figure13-1",
    "image_file": "1912.02249v3-Figure13-1.png",
    "caption": " The Discriminator’s architecture, used in the CycleGAN. “c4s2-64-LR” reads as 2D convolutional layer with kernel size 4 × 4 pixels, stride 2, 64 output channels, followed by a Leaky ReLU activation layer. S stands for the sigmoid activation layer.",
    "figure_type": "** schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " How many convolutional layers are used in the Discriminator's architecture?",
    "answer": " 4",
    "rationale": " The figure shows a schematic of the Discriminator's architecture, which includes four layers labeled with \"c4s2\". These layers represent 2D convolutional layers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.02249v3",
    "pdf_url": null
  },
  {
    "instance_id": "cb3e2f29d0b9476e88a74a2b6760dfde",
    "figure_id": "2201.09863v1-Figure34-1",
    "image_file": "2201.09863v1-Figure34-1.png",
    "caption": " WingspanMaximizer-v0",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the robot in the simulation?",
    "answer": "The robot falls over.",
    "rationale": "The figure shows a sequence of frames from a simulation of a robot trying to walk. In the first frame, the robot is standing upright. In the second frame, the robot starts to lean forward. In the third frame, the robot has fallen over.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.09863v1",
    "pdf_url": null
  },
  {
    "instance_id": "5719216aa75d4eaea47aa53c9a0dacf4",
    "figure_id": "2303.02854v2-Figure1-1",
    "image_file": "2303.02854v2-Figure1-1.png",
    "caption": " Experimental results (two subfigures above for phase retrieval and two below for DRO).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges the fastest for the phase retrieval problem?",
    "answer": "SPIDER",
    "rationale": "The top left subfigure shows the convergence of different algorithms for the phase retrieval problem. The x-axis is the number of iterations, and the y-axis is the function value. SPIDER converges the fastest, as its curve reaches the lowest function value in the fewest iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.02854v2",
    "pdf_url": null
  },
  {
    "instance_id": "1de59a1f42db417eaefefc6d12e80f0c",
    "figure_id": "2303.01959v1-Figure8-1",
    "image_file": "2303.01959v1-Figure8-1.png",
    "caption": " (a) Pre-trained PCN improves PointCert in Scenario III. (b) Impact of the fraction of a customer’s labeled point clouds on PointCert in Scenario III.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does pre-training improve the certified accuracy of PointCert in Scenario III?",
    "answer": "Yes.",
    "rationale": "In Figure (a), the blue line (with pre-training) is consistently higher than the orange line (without pre-training), indicating that pre-training leads to higher certified accuracy for all values of t (number of added points).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.01959v1",
    "pdf_url": null
  },
  {
    "instance_id": "e753b89cdcf240e7baf8725ff9f6a718",
    "figure_id": "2306.05609v1-Figure2-1",
    "image_file": "2306.05609v1-Figure2-1.png",
    "caption": " Mean model precision vs. Wu-Palmer distance between WordNet synsets associated with fullypartitioned tokens.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best when the Wu-Palmer distance is between 0.65 and 0.7?",
    "answer": "BERT-MLM",
    "rationale": "The figure shows that the BERT-MLM model has the highest mean model precision for all Wu-Palmer distance ranges, including the range between 0.65 and 0.7.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.05609v1",
    "pdf_url": null
  },
  {
    "instance_id": "0ef37644600a4f06bc94127492522bb8",
    "figure_id": "1911.09071v3-Figure1-1",
    "image_file": "1911.09071v3-Figure1-1.png",
    "caption": " Example items from the three datasets labeled according to shape (top) and texture (bottom). GST items reproduced with permission of [36, 91]. ImageNet-C items from [44].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the datasets is most likely to be used for training a model to recognize objects based on their shape?",
    "answer": "Navon.",
    "rationale": "The Navon dataset contains images of letters that are made up of smaller letters. This suggests that the dataset is focused on shape recognition, as the letters are all the same color and have no texture.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.09071v3",
    "pdf_url": null
  },
  {
    "instance_id": "7b91d51ec67a41d8b0aa3949cfe08005",
    "figure_id": "2210.03022v3-Figure8-1",
    "image_file": "2210.03022v3-Figure8-1.png",
    "caption": " Ablation study for Out-of-Distribution generalization. In order to understand the robustness of different ablated models to shifts in either coordination or heterogeneity levels, models trained with coordination level 2 and heterogeneity level 1 are tested across different coordination levels (Figure 8(a)). In a similar manner, The models trained on coordination level 1 and heterogeneity level 2 are tested across different heterogeneity levels (Figure 8(b)). KS indicates SAF without pool of policies whereas, KS PP means SAF.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which coordination level results in the highest episodic returns for KS_PP in TeamTogetherEnv?",
    "answer": "Coordination level 3",
    "rationale": "In Figure 8(a), the boxplot for KS_PP at coordination level 3 has the highest median value and the largest interquartile range, indicating that the episodic returns are generally higher at this coordination level.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.03022v3",
    "pdf_url": null
  },
  {
    "instance_id": "df7981e1fbe24845a79d2c24f2dab348",
    "figure_id": "2106.15535v2-Figure1-1",
    "image_file": "2106.15535v2-Figure1-1.png",
    "caption": " Test accuracy disparity across subgroups by aggregated-feature distance. Each figure corresponds to a dataset, and each bar cluster corresponds to a model. Bars labeled 1 to 5 represent subgroups with increasing distance to training set. Results are averaged over 40 independent trials with different random splits of the data, and the error bar represents the standard error of the mean.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Cora dataset across all subgroups?",
    "answer": "GCN",
    "rationale": "The GCN model has the highest accuracy for all subgroups in the Cora dataset, as shown in Figure (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15535v2",
    "pdf_url": null
  },
  {
    "instance_id": "019fdeaaacec428fac18514749d5368e",
    "figure_id": "1901.04530v2-Figure10-1",
    "image_file": "1901.04530v2-Figure10-1.png",
    "caption": " Objective evaluations comparing between CrossNet, CycleGAN and AGGAN for foreground extraction on the UT Zappos50K dataset and HDSeg.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the UT Zappos50K dataset for shoes foreground extraction?",
    "answer": "CrossNet performed the best with an AUC of 0.93.",
    "rationale": "The figure shows the ROC curves for each model on the UT Zappos50K dataset. The AUC is a measure of how well the model performs, and the higher the AUC, the better the model performs. CrossNet has the highest AUC of all the models, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.04530v2",
    "pdf_url": null
  },
  {
    "instance_id": "9b7048b73e3e484c810d98351fd89b30",
    "figure_id": "2104.09554v1-Figure3-1",
    "image_file": "2104.09554v1-Figure3-1.png",
    "caption": " An example of the constant alignment that AXE chooses after training the model. Given the German source “danke fürs zuhören”, the model tries to predict “thank you for listening”. Because the model is trained with teacher forcing, it can simply learn to predict its input at each position, and assume that AXE will align the prediction with the previous token (which is identical to the input). For example, p2 predicts “thank” with very high probability because teacher forcing uses the previous target y1 as the decoder’s input in the second position. Notice how the final prediction p6 is used twice to predict both “.” and EOS.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the model predict at position 4?",
    "answer": "The model predicts \"you\" at position 4.",
    "rationale": "The table shows the model predictions at each position. The prediction at position 4 is \"you\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.09554v1",
    "pdf_url": null
  },
  {
    "instance_id": "11363cb840da48fca05dda27154f54d7",
    "figure_id": "1908.06112v1-Figure1-1",
    "image_file": "1908.06112v1-Figure1-1.png",
    "caption": " The class-wise test accuracy of an 8-layer CNN on CIFAR-10 trained by (a) CE on clean labels with classbiased phenomenon, (b) CE on 40% symmetric/uniform noisy labels with amplified class-biased phenomenon and under learning on hard classes (e.g., class 3), (c) LSR under the same setting to (b) with under learning on hard classes still existing, (d) our proposed SL under the same setting to (b) exhibiting overall improved learning on all classes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function performs the best on noisy data?",
    "answer": "SL (Symmetric Loss)",
    "rationale": "The figure shows the class-wise test accuracy of an 8-layer CNN on CIFAR-10 trained by different loss functions. The plots show that SL performs the best on noisy data, as it exhibits overall improved learning on all classes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.06112v1",
    "pdf_url": null
  },
  {
    "instance_id": "e689c8c670994684a0cadcfdff117d7c",
    "figure_id": "2111.04906v1-Figure14-1",
    "image_file": "2111.04906v1-Figure14-1.png",
    "caption": " Comparing the testing accuracy curves of DPAdam, ADADP and DPAdamWOSM models across hyperparameter tuning grid from Table 2 with σ = 8. The limits for the y-axes are adjusted based on the dataset while maintaining a 15% range for all.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the MNIST dataset?",
    "answer": "DPAdam",
    "rationale": "The figure shows the testing accuracy curves for different models on the MNIST dataset. The DPAdam curve is consistently higher than the other curves, indicating that it performs better.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.04906v1",
    "pdf_url": null
  },
  {
    "instance_id": "865f142d139d417cbcc6ce4076dbb964",
    "figure_id": "2307.08695v2-Figure5-1",
    "image_file": "2307.08695v2-Figure5-1.png",
    "caption": " Qualitative comparisons. DeepV2D [34] and Robust-CVD [13] show obvious artifacts in those videos. We draw the scanline slice over time; fewer zigzagging pattern means better consistency. Compared with the other video depth methods, our NVDS is more robust on natural scenes and achieves better spatial accuracy and temporal consistency.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which video depth estimation method shows the least amount of zigzagging in the scanline slice over time?",
    "answer": "Our(DPT)",
    "rationale": "The figure shows the scanline slice over time for different video depth estimation methods. The scanline slice is a horizontal line that is drawn across the image, and the color of the line represents the depth at each point. The less zigzagging in the scanline slice, the more consistent the depth estimation is over time. The figure shows that Our(DPT) has the least amount of zigzagging in the scanline slice, which means that it is the most consistent video depth estimation method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.08695v2",
    "pdf_url": null
  },
  {
    "instance_id": "bb40e94839864a64b4e59da2c91c1b1a",
    "figure_id": "2211.04124v1-Figure1-1",
    "image_file": "2211.04124v1-Figure1-1.png",
    "caption": " Music dereverberation procedure of proposed method.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the purpose of the WPE block in the music dereverberation procedure?",
    "answer": "The WPE block estimates the filter coefficients.",
    "rationale": "The figure shows that the WPE block takes the wet signals as input and outputs the estimated filter coefficients. These coefficients are then used by the DDRM block to estimate the dry signals.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.04124v1",
    "pdf_url": null
  },
  {
    "instance_id": "a0276c5dd3054c1d915258f8d2043641",
    "figure_id": "2006.08684v3-Figure11-1",
    "image_file": "2006.08684v3-Figure11-1.png",
    "caption": " Top: Final episodic return in Half-Cheetah environment. Bottom: Learning curves in Half-Cheetah environment. H-UCRL outperforms greedy and Thompson sampling, particularly when the actoin penalty increases.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best in the Half-Cheetah environment when the action penalty is 1.0?",
    "answer": "H-UCRL",
    "rationale": "The top plot shows that H-UCRL has the highest return when the action penalty is 1.0. The bottom plot shows that H-UCRL consistently outperforms the other algorithms throughout the learning process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.08684v3",
    "pdf_url": null
  },
  {
    "instance_id": "d4c10c813a3b4d0fa9603248aa25ef7a",
    "figure_id": "2203.07682v3-Figure7-1",
    "image_file": "2203.07682v3-Figure7-1.png",
    "caption": " Visual comparison of our ACT against state-of-the-art color JPEG artifact removal methods [13, 23] with quality factor of 10. Our ACT better removes the artifacts and produces accurate structures than the baselines.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate structures in the images?",
    "answer": "ACT.",
    "rationale": "The figure shows that ACT produces the most accurate structures in the images, as it better removes the artifacts than the baselines. This is evident in the zoomed-in crops of the images, where ACT produces sharper and more detailed structures than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.07682v3",
    "pdf_url": null
  },
  {
    "instance_id": "7d05a76f3a1e48b182a5e827079b2524",
    "figure_id": "1911.08453v1-Figure10-1",
    "image_file": "1911.08453v1-Figure10-1.png",
    "caption": " Complete 2D Navigation Results",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of final distance to the goal and final success rate?",
    "answer": "LEAP",
    "rationale": "The left plot shows the final distance to the goal for each algorithm, and LEAP has the lowest final distance to the goal. The right plot shows the final success rate for each algorithm, and LEAP has the highest final success rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.08453v1",
    "pdf_url": null
  },
  {
    "instance_id": "e2cbca17615f424d9e8667b457bea946",
    "figure_id": "2012.08270v2-Figure1-1",
    "image_file": "2012.08270v2-Figure1-1.png",
    "caption": " Comparison with SoTA approaches. (a) STD (Ma, Cavalheiro, and Karaman 2019), (b) DeepLidar (Qiu et al. 2019), (c) CSPN++ (Cheng et al. 2020), (d) Ours.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate depth map?",
    "answer": "Ours.",
    "rationale": "The figure shows the depth maps produced by four different methods, with the ground truth depth map shown in (d). The depth map produced by our method is the most similar to the ground truth, indicating that it is the most accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.08270v2",
    "pdf_url": null
  },
  {
    "instance_id": "e395b9f3e7ee40889d59fb21327175f0",
    "figure_id": "2102.09200v1-Figure3-1",
    "image_file": "2102.09200v1-Figure3-1.png",
    "caption": " Comparison of TNN vs. state-of-the-art methods using Rand Index normalized to K-means",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which clustering method performed the best according to the Normalized Rand Index?",
    "answer": "TNN",
    "rationale": "The figure shows the Normalized Rand Index for different clustering methods. TNN has the highest bar, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.09200v1",
    "pdf_url": null
  },
  {
    "instance_id": "4beeffe3c50b433ebba2081a77b98ad5",
    "figure_id": "1909.02583v2-Figure5-1",
    "image_file": "1909.02583v2-Figure5-1.png",
    "caption": " DDQN Lunar Lander box plots showing average cumulative reward across 10 episodes for each attack method. Plots (a), (b) and (c) are attacked with a horizon of 5 time steps with budget value of 3, 4, and 5 respectively. (d), (e), and (f) are attacked with a horizon value of 10 time steps with budget value of 3, 4, and 5 respectively. Given the same horizon and budget, it is evident LAS attacks are more severe than MAS attacks, which in turn are generally more effective than random attacks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attack method is more effective for a horizon of 5 time steps and a budget of 3: LAS or MAS?",
    "answer": "LAS",
    "rationale": "Comparing the box plots in (a) for LAS and MAS, the median of the LAS attack is lower than the median of the MAS attack, indicating that the LAS attack is more effective.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.02583v2",
    "pdf_url": null
  },
  {
    "instance_id": "a8525cef4efa43b6842bbbbafbfa7545",
    "figure_id": "2211.11448v3-Figure13-1",
    "image_file": "2211.11448v3-Figure13-1.png",
    "caption": " More visual comparisons.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods is the most effective at inverting age, pose, and smile?",
    "answer": "CLCAE w/+.",
    "rationale": "The figure shows the results of different methods for inverting age, pose, and smile. CLCAE w/+ is the only method that successfully inverts all three attributes in all of the examples shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11448v3",
    "pdf_url": null
  },
  {
    "instance_id": "8a0f110628144b7f8f0f2088eb99db75",
    "figure_id": "2106.09129v2-Figure6-1",
    "image_file": "2106.09129v2-Figure6-1.png",
    "caption": " Using Full Precision weights in lottery ticket Initialization still provides robustness gains: While initialization pruning methods with binary weights yield the greatest robustness gains over the baseline, randomly initialized networks with full-precision weights pruned using Edgepopup are capable of providing improved robustness over the dense baseline.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \n\nDoes using full-precision weights in lottery ticket initialization provide robustness gains over the baseline? ",
    "answer": " Yes.",
    "rationale": " \n\nThe figure shows the relative CIFAR-10 accuracy for different pruning methods and network architectures. The baseline (dense) network is shown in black, and the Edgepopup pruning method is shown in different colors depending on the specific pruning technique used. The reference accuracy for each network architecture is also shown. For all network architectures, the Edgepopup pruning method with full-precision weights provides a higher relative CIFAR-10 accuracy than the baseline, indicating that it provides robustness gains.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.09129v2",
    "pdf_url": null
  },
  {
    "instance_id": "a68288af12ff4a849df69eccbb7b11f8",
    "figure_id": "2210.16541v1-Figure5-1",
    "image_file": "2210.16541v1-Figure5-1.png",
    "caption": " The effect on F1s with regards to different numbers of paths in bags.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best when the number of paths is greater than 16?",
    "answer": "ECRIM(Full)",
    "rationale": "The figure shows that the F1 score for ECRIM(Full) is the highest when the number of paths is greater than 16.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.16541v1",
    "pdf_url": null
  },
  {
    "instance_id": "d7d03e2c0cc94ab297722b59895e0eda",
    "figure_id": "2004.04572v2-Figure8-1",
    "image_file": "2004.04572v2-Figure8-1.png",
    "caption": " Reconstruction example using different types of spatial features. XYZ: absolute coordinates, L2: Euclidean distances to each joint, RBF: Radial basis function based distance to each joint. The proposed RBF preserves notably more details.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three spatial features preserves the most detail in the reconstruction of the woman's body?",
    "answer": "RBF",
    "rationale": "The figure shows three different reconstructions of the woman's body, each using a different spatial feature. The RBF reconstruction preserves the most detail, as evidenced by the smoother and more accurate representation of the woman's clothing and body shape.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.04572v2",
    "pdf_url": null
  },
  {
    "instance_id": "420c8444554149b1bb364afe291c73b7",
    "figure_id": "2103.04285v1-Figure3-1",
    "image_file": "2103.04285v1-Figure3-1.png",
    "caption": " Learning curves that represent FID scores of the translated images (apple⇒ orange or orange⇒ apple) over epochs. The blue and yellow curves represent the result achieved by p and G respectively. The results suggest that p refines the results provided by G.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better in terms of FID score?",
    "answer": "Model P.",
    "rationale": "The blue curve, which represents model P, consistently has a lower FID score than the orange curve, which represents model G. This indicates that model P produces images that are more similar to the real images than model G.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.04285v1",
    "pdf_url": null
  },
  {
    "instance_id": "24b7c07e9d9c49d2b3e57e73cb804134",
    "figure_id": "2211.12572v1-Figure9-1",
    "image_file": "2211.12572v1-Figure9-1.png",
    "caption": " Quantitative evaluation. We measure CLIP cosine similarity (higher is better) and DINO-ViT self-similarity distance (lower is better) to quantify the fidelity to text and preservation of structure, respectively. We report these metrics on three benchmarks: (a) Wild-TI2I for which an ablation of our method is included, (b) ImageNet-R-TI2I, and (c) Generated-ImageNet-R-TI2I. Note that we could compare to P2P only for (b) and (c) due to their prompts restriction. All baselines struggle to achieve both low structure distance and a high CLIP score. Our method exhibit a better balance between these two ends across all benchmarks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the best balance between structure distance and CLIP score on the Wild-TI2I benchmark?",
    "answer": "Our method (full)",
    "rationale": "The figure shows that our method (full) achieved the lowest structure distance and the highest CLIP score among all the methods compared on the Wild-TI2I benchmark. This indicates that our method achieved the best balance between preserving structure and fidelity to text.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.12572v1",
    "pdf_url": null
  },
  {
    "instance_id": "140c9e944714461588e93222bfed5115",
    "figure_id": "2007.08259v2-Figure15-1",
    "image_file": "2007.08259v2-Figure15-1.png",
    "caption": " Reliability diagrams for the model from Real-World to Art before and after calibration.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the calibration methods shown in the figure results in the most accurate model?",
    "answer": "Oracle",
    "rationale": "The Oracle method is the most accurate because its red line is closest to the diagonal line. This means that the predicted probabilities are very close to the actual probabilities.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.08259v2",
    "pdf_url": null
  },
  {
    "instance_id": "f258c9e3c05942e78c289c9a5f0a769a",
    "figure_id": "2010.16402v2-FigureC.1-1",
    "image_file": "2010.16402v2-FigureC.1-1.png",
    "caption": "Figure C.1: Evolution of ImageNet validation accuracy over training. Each curve represents a different model. For each loss function, curves terminate at the epoch that provided the highest holdout set accuracy. Validation accuracy rises rapidly due to the use of an exponential moving average of the weights for evaluation. Some loss functions, such as logit normalization, appear to provide higher accuracy than vanilla softmax cross-entropy over the entire training run.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function appears to provide the highest accuracy over the entire training run?",
    "answer": "Logit normalization.",
    "rationale": "The curve for logit normalization is consistently higher than the other curves throughout the training run.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.16402v2",
    "pdf_url": null
  },
  {
    "instance_id": "a2c74e9320a549f8b7f748d8c6b32f08",
    "figure_id": "1902.02449v3-Figure6-1",
    "image_file": "1902.02449v3-Figure6-1.png",
    "caption": " Reconstructed images for sparse-view CT from (a) Input image (144 views) : (b) FISTA with backtracking, (c) Proposed diag-learned SGP, all at the 10th iteration. (d) Proposed method yielded faster convergence rate than FISTA-b.",
    "figure_type": "photograph(s) and plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm produced the best reconstructed image?",
    "answer": "Diag-learned SGP.",
    "rationale": "The figure shows that the reconstructed image produced by Diag-learned SGP (c) is sharper and has more details than the reconstructed image produced by FISTA-b (b). Additionally, the plot in (d) shows that Diag-learned SGP converges to a lower NMSE (normalized mean squared error) than FISTA-b, indicating that it produces a more accurate reconstruction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.02449v3",
    "pdf_url": null
  },
  {
    "instance_id": "cd1819de971c40079937e14f6386c5b2",
    "figure_id": "2206.06320v1-Figure2-1",
    "image_file": "2206.06320v1-Figure2-1.png",
    "caption": " Yearly exchange-wise Tweet activity statistics",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which exchange experienced the largest increase in tweet activity between 2018 and 2019?",
    "answer": "Binance",
    "rationale": "The figure shows that Binance's tweet activity increased from around 25,000 tweets in 2018 to over 100,000 tweets in 2019, which is the largest increase among all exchanges shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.06320v1",
    "pdf_url": null
  },
  {
    "instance_id": "bf6d162bb1a6413dba4fb20530d74eb2",
    "figure_id": "2103.00063v3-Figure3-1",
    "image_file": "2103.00063v3-Figure3-1.png",
    "caption": " Regret comparison (LOCB) on Synthetic and Yelp datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest regret on the Yelp dataset?",
    "answer": "CLUB",
    "rationale": "The figure shows the regret of different algorithms on the Yelp dataset. The CLUB algorithm has the highest regret, as its line is the highest on the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.00063v3",
    "pdf_url": null
  },
  {
    "instance_id": "6483e5ca48884cb5a674055d394b541f",
    "figure_id": "2110.13827v2-Figure8-1",
    "image_file": "2110.13827v2-Figure8-1.png",
    "caption": " Comparison of CoPO with or without centralized critics.",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following scenarios benefited the most from using centralized critics?",
    "answer": "The Tollgate scenario.",
    "rationale": "The figure shows that the CoPO algorithm with centralized critics (blue line) performed significantly better than the CoPO algorithm without centralized critics (green line) in the Tollgate scenario. This suggests that centralized critics can be beneficial for the CoPO algorithm in some scenarios.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.13827v2",
    "pdf_url": null
  },
  {
    "instance_id": "edadc6df67474edba6e16d45437fb41d",
    "figure_id": "1908.10383v2-Figure3-1",
    "image_file": "1908.10383v2-Figure3-1.png",
    "caption": " Comparison of extractive methods under FAR and SAR reflects their capability of extracting salient and non-redundant sentences.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of extracting salient and non-redundant sentences?",
    "answer": "UnifiedSum(E)",
    "rationale": "The figure shows that UnifiedSum(E) has the highest SAR and lowest FAR, which indicates that it is able to extract more salient and non-redundant sentences than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.10383v2",
    "pdf_url": null
  },
  {
    "instance_id": "f72de457d8864be689abf618f6a6f25c",
    "figure_id": "2307.07994v1-Figure4-1",
    "image_file": "2307.07994v1-Figure4-1.png",
    "caption": " SUPPORTER progressively enhances the elicitation intensity and exhibits robust adjustment ability in the later stage of the conversation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model shows the most consistent increase in positive emotion distance over the course of the dialogue?",
    "answer": "SUPPORTER",
    "rationale": "The plot shows the positive emotion distance for each model over the course of the dialogue. The SUPPORTER model is the only one that shows a consistent increase in positive emotion distance over the course of the dialogue.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.07994v1",
    "pdf_url": null
  },
  {
    "instance_id": "0e3c245391db4894918d753abbc9f4ee",
    "figure_id": "2307.16895v1-Figure2-1",
    "image_file": "2307.16895v1-Figure2-1.png",
    "caption": " Results for 4-week ahead COVID-19 death forecasting in California. The left column shows the COVID-19 Forecast Hub ensemble model, and the right column shows conformal PID control using the tan integrator, and a scorecaster given by ℓ1-penalized quantile regression on all past forecasts, cases, and deaths from all 50 states. The top row plots coverage, averaged over a trailing window of 10 weeks. The nominal coverage level is 1− α = 0.8 and marked by a gray dotted line. The bottom row plots the prediction sets in gold along with the ground-truth times series (death counts). Miscoverage events are indicated by red dots. Summary statistics such as the coverage and average set size are available in Table 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has a higher coverage, the Base Forecaster or the Conformal PID Control?",
    "answer": "The Conformal PID Control has a higher coverage.",
    "rationale": "The top row of the figure shows the coverage of each method. The coverage of the Conformal PID Control is closer to the nominal coverage level of 0.8, which is marked by a gray dotted line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.16895v1",
    "pdf_url": null
  },
  {
    "instance_id": "b735d8fd01eb4124b2bbada132e0e58b",
    "figure_id": "2010.06897v2-Figure7-1",
    "image_file": "2010.06897v2-Figure7-1.png",
    "caption": " Comparison between our method and the best baseline, showing the top1 images retrieved for the target scenario Snow. The images with green border correspond with the ground truth, while the ones with a red border are wrong predictions.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more accurate in retrieving images for the target scenario \"Snow\"?",
    "answer": "Our method.",
    "rationale": "The images with green borders are the ground truth, and the images with red borders are wrong predictions. Our method has more images with green borders than the best baseline, which means it is more accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.06897v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd268ab09f164b3ba892780b69aa8856",
    "figure_id": "2202.10816v2-Figure5-1",
    "image_file": "2202.10816v2-Figure5-1.png",
    "caption": " Proof of Theorem 11, Case 2a illustration",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the random variables $T$ and $S_m$?",
    "answer": "$T$ and $S_m$ are independent random variables.",
    "rationale": "The figure shows that $T$ and $S_m$ are not connected by any arrows, which indicates that they are independent.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.10816v2",
    "pdf_url": null
  },
  {
    "instance_id": "6603b24240084b53b9cdceb762db821e",
    "figure_id": "2202.11202v3-Figure6-1",
    "image_file": "2202.11202v3-Figure6-1.png",
    "caption": " Visualization of the poisoning noise for MoCo v2 and BYOL. Note that for the sample-wise noise, we randomly sample one from each class to visualize.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which poisoning noise method appears to be more effective in distorting the images?",
    "answer": "BYOL",
    "rationale": "The BYOL noise appears to be more effective in distorting the images, as the images in the BYOL section are more difficult to recognize than the images in the MoCo v2 section.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.11202v3",
    "pdf_url": null
  },
  {
    "instance_id": "c4840232806b4e96b211d606d4047f03",
    "figure_id": "2106.12052v2-Figure4-1",
    "image_file": "2106.12052v2-Figure4-1.png",
    "caption": " Qualitative results for reconstructed geometries of objects from the DTU dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods shown in the figure produces the most accurate reconstruction of the snowman?",
    "answer": "IDR",
    "rationale": "The snowman reconstructed by IDR has the most detail and is the closest to the original object. This can be seen in the sharp edges of the hat and the clear definition of the facial features.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.12052v2",
    "pdf_url": null
  },
  {
    "instance_id": "8e9e53ddf6c54e6a90cd14e690943c39",
    "figure_id": "2104.02008v1-Figure5-1",
    "image_file": "2104.02008v1-Figure5-1.png",
    "caption": " Evaluation on the hyper-parameter α on (a) PACS, (b) person re-ID datasets and (c) Coinrun. In (b), M and D denote Market1501 and Duke respectively.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which value of α results in the highest accuracy on the PACS dataset?",
    "answer": " 0.1",
    "rationale": " The plot in Figure (a) shows that the accuracy on the PACS dataset is highest when α is 0.1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.02008v1",
    "pdf_url": null
  },
  {
    "instance_id": "e6a3843e13b442fc865e78138ca711f6",
    "figure_id": "2205.10012v3-Figure3-1",
    "image_file": "2205.10012v3-Figure3-1.png",
    "caption": " Distribution of number of languages in which Wi-",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which has a larger fraction of Wikidata entities with descriptions in only one language, articles or descriptions?",
    "answer": "Articles",
    "rationale": "The plot shows that the fraction of articles with descriptions in only one language is much higher than the fraction of descriptions with descriptions in only one language.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.10012v3",
    "pdf_url": null
  },
  {
    "instance_id": "fb1ff9225ca7493f854fcf19812b169a",
    "figure_id": "2308.11916v1-Figure10-1",
    "image_file": "2308.11916v1-Figure10-1.png",
    "caption": " Sensitivity test for Lpdc geo/Lgeo/Lscale.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which parameter has the largest impact on the accuracy of the model?",
    "answer": "The coefficient of L_scale.",
    "rationale": "The figure shows that the coefficient of L_scale has the largest impact on the accuracy of the model. This is because the coefficient of L_scale is the only parameter that is consistently high across all scales. The coefficients of L_geo and L_pdc-geo are both relatively low, and they vary depending on the scale.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.11916v1",
    "pdf_url": null
  },
  {
    "instance_id": "4bed3479f4814331ac55391438e8f831",
    "figure_id": "2104.07705v2-Figure2-1",
    "image_file": "2104.07705v2-Figure2-1.png",
    "caption": " The validation-set loss of 24hBERT compared to the original BERT model configurations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model configuration has the lowest validation-set loss after 24 hours of training?",
    "answer": "24hBERT",
    "rationale": "The plot shows the validation-set loss of the three model configurations over time. After 24 hours of training, the blue line representing 24hBERT is the lowest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.07705v2",
    "pdf_url": null
  },
  {
    "instance_id": "2d79d14c2c3749959d01532b35038f6a",
    "figure_id": "2301.11494v3-Figure8-1",
    "image_file": "2301.11494v3-Figure8-1.png",
    "caption": " Future prediction of our DVP method compared to baseline methods on a real video sequence. Our method generates a predicted sequence that best matches the input video within its duration, and remains visually plausible way beyond its duration.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method best predicts the future of the video sequence?",
    "answer": "Our method.",
    "rationale": "The figure shows that our method generates a predicted sequence that best matches the input video within its duration, and remains visually plausible way beyond its duration. The other methods either fail to match the input video within its duration or become visually implausible beyond its duration.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.11494v3",
    "pdf_url": null
  },
  {
    "instance_id": "942171c5ae884c67859396924e7c449e",
    "figure_id": "2207.10606v1-Figure5-1",
    "image_file": "2207.10606v1-Figure5-1.png",
    "caption": "Noisy Pose Estimation Dashed lines are averages for the method, while the black diamonds show the average for that method and model. Here Fuzzy Metaballs win in all statistical measures, typically by a factor of ≈ 2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on average?",
    "answer": "Fuzzy Metaballs.",
    "rationale": "The black diamonds show the average pose error for each method and model. The dashed line for Fuzzy Metaballs is lower than the dashed lines for all other methods, indicating that it has the lowest average pose error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.10606v1",
    "pdf_url": null
  },
  {
    "instance_id": "5dba2a5a0f0c46eeb615deaacd398d82",
    "figure_id": "1902.02236v1-Figure10-1",
    "image_file": "1902.02236v1-Figure10-1.png",
    "caption": " Revenue Per Offer by Each Pricing System Over Time",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pricing system resulted in the highest average revenue per offer over the 29-day period?",
    "answer": "APP-LM",
    "rationale": "The figure shows the revenue per offer for each pricing system over time. The APP-LM line is consistently higher than the other two lines, indicating that it generated the highest average revenue per offer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.02236v1",
    "pdf_url": null
  },
  {
    "instance_id": "d97a4acddf504a048a193621f4b150a9",
    "figure_id": "1812.02766v1-Figure11-1",
    "image_file": "1812.02766v1-Figure11-1.png",
    "caption": " Knocking-off a real-world API. Performance of the knockoff achieved with two choices of PA.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset and model combination resulted in the highest accuracy?",
    "answer": "CelebA and resnet101.",
    "rationale": "The plot on the left shows the accuracy of the knockoff model on the CelebA dataset, with resnet101 achieving the highest accuracy of around 80%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.02766v1",
    "pdf_url": null
  },
  {
    "instance_id": "3a0e6b2829454d8d8fd01f00b297c213",
    "figure_id": "2209.13533v1-Figure7-1",
    "image_file": "2209.13533v1-Figure7-1.png",
    "caption": " Visualization of the forward diffusion process. The two valid codewords are represented as a blue cross and a red cross. The other six words are denoted by black crosses.",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which sub-figure(s) does the trajectory visit all eight codewords?",
    "answer": "Sub-figure 1, 3, and 6.",
    "rationale": "The trajectory is shown as a series of connected dots in the figure. In sub-figures 1, 3, and 6, the trajectory passes through all eight codewords, which are represented by the crosses.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.13533v1",
    "pdf_url": null
  },
  {
    "instance_id": "5754de52b83f4f448d228e89c11027a5",
    "figure_id": "2003.06468v1-Figure4-1",
    "image_file": "2003.06468v1-Figure4-1.png",
    "caption": " (a) The effect of the variance σ on the ratio of correctly classified queries C to the total number of queries N at boundary point xB . (b) Effect of λ on the performance of the algorithm. (c) Comparison of two extreme cases of query distributions, i.e., single iteration (λ→ 0) and uniform distribution (λ = 1) with optimal distribution (λ = 0.6).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of λ minimizes the l2 distance between the predicted and true labels?",
    "answer": "λ = 0.6",
    "rationale": "The plot in Figure (c) shows that the optimal distribution (λ = 0.6) has the lowest l2 distance for a given number of queries.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.06468v1",
    "pdf_url": null
  },
  {
    "instance_id": "45b4f1ac18f243d995775aa9e32773f8",
    "figure_id": "2305.11255v4-Figure5-1",
    "image_file": "2305.11255v4-Figure5-1.png",
    "caption": " Error analysis.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest percentage of unsolvable instances?",
    "answer": "Unsup. T5 (11B)",
    "rationale": "The figure shows that the Unsup. T5 (11B) model has the highest percentage of unsolvable instances, with a value of 48.27%. This is evident from the length of the orange bar, which represents the percentage of unsolvable instances.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.11255v4",
    "pdf_url": null
  },
  {
    "instance_id": "2f3fc604ac0049128862193b111e0ead",
    "figure_id": "2203.07671v1-Figure12-1",
    "image_file": "2203.07671v1-Figure12-1.png",
    "caption": " Training and Test Results of AC",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach has the highest test data loss?",
    "answer": "DIFF AI+",
    "rationale": "The table shows that DIFF AI+ has the highest test data loss for all data sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.07671v1",
    "pdf_url": null
  },
  {
    "instance_id": "b8df69c52b3c42dfaf9a6220b78fe6b3",
    "figure_id": "2009.03300v3-Figure17-1",
    "image_file": "2009.03300v3-Figure17-1.png",
    "caption": " A Business Ethics example.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following tactics is most likely to involve physically attacking a company's operations?",
    "answer": "Violent direct action.",
    "rationale": "The passage states that violent direct action may involve physically attacking a company's operations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.03300v3",
    "pdf_url": null
  },
  {
    "instance_id": "2d24bbc5e3d9466ca20a074228e4970c",
    "figure_id": "2103.10814v1-Figure7-1",
    "image_file": "2103.10814v1-Figure7-1.png",
    "caption": " Visualization results of the ablation study. Minor degeneracies can be seen in models without offsets or the fidelity loss. Major performance drop is seen if either the activation strengths or the coverage loss is removed.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models in the figure performs the worst?",
    "answer": "The model without activation strengths.",
    "rationale": "The caption states that \"Major performance drop is seen if either the activation strengths or the coverage loss is removed.\" This suggests that the model without activation strengths performs worse than the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.10814v1",
    "pdf_url": null
  },
  {
    "instance_id": "d010b17d88964da2b91c141c79f71721",
    "figure_id": "2004.01215v2-FigureI-1",
    "image_file": "2004.01215v2-FigureI-1.png",
    "caption": "Figure I.2: NSP9 Replicase correlation between molecule properties and synthesis.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which molecule property is most negatively correlated with synthesis steps?",
    "answer": "QED",
    "rationale": "The figure shows a heatmap of the correlation between molecule properties and synthesis steps. The color of each cell indicates the strength and direction of the correlation, with blue indicating a negative correlation and yellow indicating a positive correlation. The cell corresponding to QED and synthesis steps is the darkest blue, indicating the strongest negative correlation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.01215v2",
    "pdf_url": null
  },
  {
    "instance_id": "25cb07059dda4fbdab446113346a8b35",
    "figure_id": "1811.06156v1-Figure6-1",
    "image_file": "1811.06156v1-Figure6-1.png",
    "caption": " An example of multi-scale layer over MedicalQA#2 dataset.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of nephritis is associated with diffuse mesangial proliferation and shows c3 granular deposition on immunofluorescence testing?",
    "answer": "Diffuse mesangial proliferative glomerulonephritis",
    "rationale": "The figure shows that diffuse mesangial proliferative glomerulonephritis is associated with diffuse mesangial proliferation and shows c3 granular deposition on immunofluorescence testing.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.06156v1",
    "pdf_url": null
  },
  {
    "instance_id": "6b9cc93ef43b4eacbf8e32ba0f905479",
    "figure_id": "1906.00823v3-Figure7-1",
    "image_file": "1906.00823v3-Figure7-1.png",
    "caption": " Frequency-estimation performance of DeepFreq compared to other methodologies. Standard error bars for the DeepFreq method are shown in Appendix B. The experiment is described in Section 3.4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of chamber error?",
    "answer": "DeepFreq",
    "rationale": "The figure shows that DeepFreq has the lowest chamber error for all SNR values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.00823v3",
    "pdf_url": null
  },
  {
    "instance_id": "f550842000944cabab9267adb3f24f53",
    "figure_id": "2212.10029v3-Figure9-1",
    "image_file": "2212.10029v3-Figure9-1.png",
    "caption": " Like GPT-3 (text-davinci-003), ChatGPT also seems to have incoherent mental pictures of everyday things.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the image, what is the relationship between the shell and the shell membrane in an egg?",
    "answer": "The shell membrane surrounds the shell.",
    "rationale": "The image shows two statements about the relationship between the shell and the shell membrane in an egg. The first statement says that the shell is surrounded by the shell membrane, which is true. The second statement says that the shell membrane is surrounded by the egg white, which is also true. This means that the shell membrane is located between the shell and the egg white.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10029v3",
    "pdf_url": null
  },
  {
    "instance_id": "e8058eac17c94b9c92d582bc960e43a6",
    "figure_id": "1911.11288v2-Figure13-1",
    "image_file": "1911.11288v2-Figure13-1.png",
    "caption": " Some example images from the PD dataset that we used for synthetic CSS training.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the speed limit on the road in the bottom right image?",
    "answer": "40 mph",
    "rationale": "The speed limit sign in the bottom right image shows \"40\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.11288v2",
    "pdf_url": null
  },
  {
    "instance_id": "740bc2bbf3114e188f131a5a8e3bc528",
    "figure_id": "2108.01368v2-Figure1-1",
    "image_file": "2108.01368v2-Figure1-1.png",
    "caption": " Comparison of reconstruction methods for in-distribution, sampling-shift, and anatomy-shift images. All methods and hyperparameters were optimized on T2-weighted brain scans with a vertical sampling mask, and tested at higher accelerations, horizontal masks, and on knee & abdomen scans. Our reconstructions are competitive with state-of-the-art methods, and introduce fewer artifacts out of distribution. All measurements are multicoil k-space from the NYU fastMRI dataset and the supervised baselines are trained from scratch on MVUE targets for a fair comparison.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed best for reconstructing anatomy-shift images?",
    "answer": "Langevin (Ours)",
    "rationale": "The figure shows the results of different reconstruction methods for different types of images. For anatomy-shift images, Langevin (Ours) has the highest SSIM and PSNR values, indicating that it produced the most accurate reconstructions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.01368v2",
    "pdf_url": null
  },
  {
    "instance_id": "873c6d684c374482bc06c422357cecf9",
    "figure_id": "2112.12004v1-Figure3-1",
    "image_file": "2112.12004v1-Figure3-1.png",
    "caption": " Evolution of the pseudo-labels during training. For each method (FixMatch, our composite loss, and SSL-thenFixMatch from left to right), we show the ratio of images with correct and confident pseudo-labels (bottom green area), incorrect but confident pseudo-labels (red area) and unconfident pseudo-labels (grey area). We also plot the test accuracy with a black line. For SSL-then-FixMatch, early training corresponds to self-supervised learning, thus these information are not available.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most effective at producing accurate pseudo-labels?",
    "answer": "Our composite loss method.",
    "rationale": "The green area in the \"Ours\" plot is larger than the green areas in the other two plots, indicating that a higher ratio of images have correct and confident pseudo-labels when using our composite loss method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.12004v1",
    "pdf_url": null
  },
  {
    "instance_id": "2935c6458f6f47b5927ddd67e0627d8a",
    "figure_id": "2205.07246v3-Figure7-1",
    "image_file": "2205.07246v3-Figure7-1.png",
    "caption": " Confusion matrix on the test set of CIFAR-10 (10). Rows correspond to the rows in Figure 4. Columns correspond to different SSL methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which SSL method has the highest accuracy for the most prototypical labeled samples?",
    "answer": "FixMatch",
    "rationale": "The confusion matrix for the most prototypical labeled samples shows that FixMatch has the highest accuracy, as indicated by the darkest blue squares along the diagonal.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.07246v3",
    "pdf_url": null
  },
  {
    "instance_id": "cda4e7fbbfff43e8973adbacc6ac4132",
    "figure_id": "2108.13753v2-Figure5-1",
    "image_file": "2108.13753v2-Figure5-1.png",
    "caption": " Alternative diagram of Figure 1 in the band style following Figure 8.1 of MacKay (2003).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between $I(u; v_1)$ and $I(u; v_1, v_2)$?",
    "answer": "$I(u; v_1)$ is a subset of $I(u; v_1, v_2)$.",
    "rationale": "The figure shows that $I(u; v_1)$ is a subset of $I(u; v_1, v_2)$ because the rectangle representing $I(u; v_1)$ is contained within the rectangle representing $I(u; v_1, v_2)$.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.13753v2",
    "pdf_url": null
  },
  {
    "instance_id": "2751e46ef00c452d8141e9ccfbeef79e",
    "figure_id": "1905.10837v5-Figure13-1",
    "image_file": "1905.10837v5-Figure13-1.png",
    "caption": " Loss and AUC curves for architecture variants when training on all thirty tasks simultaneously.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which anti-overfitting measure seems to be the most effective in preventing overfitting based on the figure?",
    "answer": "Dropout.",
    "rationale": "The figure shows the training and testing loss and AUC for three different architectures: baseline, weight decay, and dropout. The baseline architecture shows a large gap between the training and testing loss and AUC, indicating that the model is overfitting. The weight decay architecture reduces the gap somewhat, but the dropout architecture has the smallest gap between the training and testing loss and AUC, indicating that it is the most effective in preventing overfitting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10837v5",
    "pdf_url": null
  },
  {
    "instance_id": "eea5af76f10e459390bb72ff71fd09a8",
    "figure_id": "2112.07315v2-Figure6-1",
    "image_file": "2112.07315v2-Figure6-1.png",
    "caption": " Qualitative comparison of our method with other MFSR approaches on real-world BurstSR dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the image that is most similar to the ground truth?",
    "answer": "Our method.",
    "rationale": "The figure shows the results of different MFSR approaches on two different images. The images produced by our method are visually more similar to the ground truth images than the images produced by the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.07315v2",
    "pdf_url": null
  },
  {
    "instance_id": "85c1d378b9f148e6895db9a496d9fc4a",
    "figure_id": "2205.14083v3-Figure4-1",
    "image_file": "2205.14083v3-Figure4-1.png",
    "caption": " Left: The change of the vanilla loss (exclude the trajectory loss) in each epoch. SAF does not affect the convergence of the training. Right: The change of sharpness, which is the measurement proposed by SAM with ρ = 0.05. SAF and MESA decrease the sharpness measure of SAM significantly.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most effective at reducing the sharpness measure of SAM?",
    "answer": "MESA.",
    "rationale": "The figure on the right shows that MESA decreases the sharpness measure of SAM significantly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.14083v3",
    "pdf_url": null
  },
  {
    "instance_id": "7357bf43946643a89cd67e51c7c5a717",
    "figure_id": "2005.09917v1-Figure4-1",
    "image_file": "2005.09917v1-Figure4-1.png",
    "caption": " The importance (within brackets) and regression predict curves with mean and variance learned by the random forest for each hyper-parameter. Importance is highly correlated to the curve steepnes. For the two most important parameters (epoch and layer), we can get a high rs within a small range.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which hyper-parameter has the highest importance?",
    "answer": "Epoch",
    "rationale": "The importance of each hyper-parameter is shown in brackets next to its name. The hyper-parameter with the highest importance is Epoch, with a value of 0.1429.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.09917v1",
    "pdf_url": null
  },
  {
    "instance_id": "1944fc394369442e95969c6af43ebc5f",
    "figure_id": "1911.12580v1-Figure4-1",
    "image_file": "1911.12580v1-Figure4-1.png",
    "caption": " Classification performance across various users’ age groups. All the models are trained on Age ∈ [20, 30) and tested on all the five groups.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best overall?",
    "answer": "SRDO",
    "rationale": "SRDO had the highest average AUC across all age groups and also the highest AUC in 3 of the 5 age groups.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.12580v1",
    "pdf_url": null
  },
  {
    "instance_id": "93ebdbfcbe4545e4b9a4ceb43b154c97",
    "figure_id": "2103.00083v5-Figure11-1",
    "image_file": "2103.00083v5-Figure11-1.png",
    "caption": " Comparing prediction interval lengths from DQA, conformalized DQA, and others at the nominal 0.8 level. The display is again as in Figure 7 (individual results on the left, averaged results on the right). Conformalized DQA inflates interval lengths compared to DQA (in order to achieve valid coverage), but not by a huge amount, making it still clearly favorable to the Average and Median aggregators, and comparable to QRA.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method consistently produced the shortest prediction interval lengths?",
    "answer": "FQRA",
    "rationale": "The bar chart on the right shows that FQRA has the shortest bars, indicating that it produced the shortest prediction interval lengths on average.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.00083v5",
    "pdf_url": null
  },
  {
    "instance_id": "579a2f5238d64e3e89c4d747e3b77246",
    "figure_id": "2302.06205v2-Figure11-1",
    "image_file": "2302.06205v2-Figure11-1.png",
    "caption": " 5-vs-5 scenario with Parameter sharing.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest win rate after 15,000 episodes?",
    "answer": "A2PPO",
    "rationale": "The figure shows the win rate of different algorithms over time. A2PPO has the highest win rate at the end of the training period.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.06205v2",
    "pdf_url": null
  },
  {
    "instance_id": "817f8cd58d644eeea84a11d339ec53f0",
    "figure_id": "2201.01221v2-Figure3-1",
    "image_file": "2201.01221v2-Figure3-1.png",
    "caption": " Performance comparison of COMA with different critics in multiple scenarios of SMAC.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which critic performs the best in the \"bane_vs_bane\" scenario?",
    "answer": "HC",
    "rationale": "The figure shows the mean test won percentage for each critic in each scenario. In the \"bane_vs_bane\" scenario, the HC critic has the highest mean test won percentage.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.01221v2",
    "pdf_url": null
  },
  {
    "instance_id": "cae2d72f8a6e4c70b74961c0372a8cff",
    "figure_id": "1907.08167v1-Figure8-1",
    "image_file": "1907.08167v1-Figure8-1.png",
    "caption": " Top-K accuracy of intent detection.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best when the top K prediction is 11?",
    "answer": "Word-CNN",
    "rationale": "The figure shows the top K accuracy of four different models. The Word-CNN model has the highest accuracy when the top K prediction is 11.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.08167v1",
    "pdf_url": null
  },
  {
    "instance_id": "e53ffaae02a14e648d62fd435ceef20f",
    "figure_id": "2303.09650v2-Figure2-1",
    "image_file": "2303.09650v2-Figure2-1.png",
    "caption": " Sparsity dynamics comparison between the ISS-P and IHT in the pruning stage. The proposed method allows a more active sparse pattern exploitation adapting to the optimization. We choose two representative layers from the SwinIR [25] backbone.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method leads to a more active sparse pattern exploitation adapting to the optimization?",
    "answer": "ISS-P",
    "rationale": "The figure shows that the ISS-P method has a higher percentage of sparse mask dynamic than the IHT method. This indicates that the ISS-P method is able to exploit sparse patterns more actively than the IHT method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.09650v2",
    "pdf_url": null
  },
  {
    "instance_id": "26899c3487144185863bf99258cb7ac7",
    "figure_id": "2208.10264v5-Figure19-1",
    "image_file": "2208.10264v5-Figure19-1.png",
    "caption": " All stage text (appended in step 2) for the different stages of the experiment.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "At what voltage level did the learner start to exhibit signs of distress?",
    "answer": "315 volts.",
    "rationale": "The figure shows that the learner started to pound on the walls of the room at 315 volts. This is a sign of distress, which suggests that the learner was experiencing discomfort at this voltage level.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.10264v5",
    "pdf_url": null
  },
  {
    "instance_id": "aaa87b7b8a944ceba28093ec55d4b10e",
    "figure_id": "2004.11883v3-Figure5-1",
    "image_file": "2004.11883v3-Figure5-1.png",
    "caption": " Visualization of where MoVie helps MCAN for different question types on VQA 2.0 val set. We compute the probability by assigning each question to MoVie based on similarity scores (see Sec. F for detailed explanations). The top contributed question types are counting related, confirming that state-of-the-art VQA models that perform global fusion are not ideally designed for counting, and the value of MoVie with local fusion. Best viewed on a computer screen with zoom.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which types of questions are the most difficult for VQA models that perform global fusion to answer?",
    "answer": "Counting-related questions.",
    "rationale": "The figure shows that the most frequently asked questions that MoVie helps MCAN with are counting-related questions. This suggests that VQA models that perform global fusion are not ideally designed for counting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.11883v3",
    "pdf_url": null
  },
  {
    "instance_id": "a24aad04b2594f0494c9e91816797f3f",
    "figure_id": "2211.11319v1-Figure9-1",
    "image_file": "2211.11319v1-Figure9-1.png",
    "caption": " Increasing the number of paths generally improves our caption consistency metrics. We find 64 to be sufficient to express and optimize SVGs that are coherent with the caption.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest R-Precision at 128 paths?",
    "answer": "VectorFusion (w/ SD+LIVE)",
    "rationale": "The red line with triangle markers represents the VectorFusion (w/ SD+LIVE) model, and it is the highest line at 128 paths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11319v1",
    "pdf_url": null
  },
  {
    "instance_id": "2e5905c90866435fb156c85e07852aad",
    "figure_id": "2303.03023v1-Figure3-1",
    "image_file": "2303.03023v1-Figure3-1.png",
    "caption": " (a, b) Instance- and class-conditionally generated samples using our CLEL in CIFAR-10. (c) Confusion matrix for the class-conditionally generated samples computed by an external classifier.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class was the model least confident in predicting?",
    "answer": "Frog",
    "rationale": "The confusion matrix shows the probabilities of the model predicting each class, given that the true class is known. The diagonal entries correspond to correct predictions, and the off-diagonal entries correspond to incorrect predictions. The lowest probability on the diagonal is 0.72, which corresponds to the frog class.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.03023v1",
    "pdf_url": null
  },
  {
    "instance_id": "44ee7d02bbab4d0db6ec8fe7d3742334",
    "figure_id": "2206.03674v3-Figure12-1",
    "image_file": "2206.03674v3-Figure12-1.png",
    "caption": " A set of visualization for a 2-joint manipulation task. The obstacles are randomly generated. (1) The 2-joint manipulation task shown in top-down workspace with 96× 96 resolution. This is used as the input to the Workspace Manipulation task. (2) The predicted configuration space in resolution 18 × 18 from a mapper module, which is jointly optimized with a planner network. (3) The ground truth configuration space from a handcraft algorithm in resolution 18× 18. This is used as input to the Configuration-space (C-space) Manipulation task and as target in the auxiliary loss for the Workspace Manipulation task (as done in SPT (Chaplot et al., 2021)). (4) The predicted policy (overlaid with C-space obstacle for visualization) from an end-to-end trained SymVIN model that uses a mapper to take the top-down workspace image and plans on a learned map. The red block is the goal position.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the resolution of the predicted configuration space from the mapper module?",
    "answer": "18x18",
    "rationale": "The caption states that the predicted configuration space from the mapper module is in resolution 18x18.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.03674v3",
    "pdf_url": null
  },
  {
    "instance_id": "0cf088e492dc4da49c02af0e6149cc52",
    "figure_id": "2208.05482v1-Figure4-1",
    "image_file": "2208.05482v1-Figure4-1.png",
    "caption": " Top-3 predictions of Graph2SMILES [43] and RetroDCVAE.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, Graph2SMILES or RetroDCVAE, is more likely to generate a plausible but incorrect retrosynthetic path?",
    "answer": "Graph2SMILES.",
    "rationale": "The figure shows that Graph2SMILES generated a plausible but incorrect retrosynthetic path for the product in rank 1, while RetroDCVAE did not generate any plausible but incorrect paths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.05482v1",
    "pdf_url": null
  },
  {
    "instance_id": "b0b10b89e9e74e418c60b6f4e1847dc3",
    "figure_id": "2110.06021v3-Figure15-1",
    "image_file": "2110.06021v3-Figure15-1.png",
    "caption": " Samples for Ornstein-Uhlenbeck process.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm most closely approximates the ground truth for the Ornstein-Uhlenbeck process?",
    "answer": "B-MAF",
    "rationale": "The plot shows the ground truth for the Ornstein-Uhlenbeck process in the top left corner. The B-MAF plot in the bottom right corner most closely resembles the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.06021v3",
    "pdf_url": null
  },
  {
    "instance_id": "551861c824a24659a1513fc7125284cc",
    "figure_id": "2306.04968v1-Figure3-1",
    "image_file": "2306.04968v1-Figure3-1.png",
    "caption": " Performance with different active labeled instances.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the FewRel and TACRED datasets?",
    "answer": "Our method performs the best on both datasets.",
    "rationale": "The figure shows the Macro-F1 score for different methods on the FewRel and TACRED datasets. The Macro-F1 score is a measure of how well a method performs overall. The higher the Macro-F1 score, the better the method performs. The figure shows that our method has the highest Macro-F1 score on both datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.04968v1",
    "pdf_url": null
  },
  {
    "instance_id": "20aa78925200435fb79957f9657b7d49",
    "figure_id": "2305.11101v1-Figure3-1",
    "image_file": "2305.11101v1-Figure3-1.png",
    "caption": " Visual comparison of our method against the previous state-of-the-art method Graphormer. XFormer performs slightly better than Graphormer with a large backbone. While with a small backbone, XFormer reconstructs human mesh more accurately than Graphormer. The 2D keypoints predicted by our keypoint decoder are also visualized.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate human mesh reconstruction?",
    "answer": "XFormer with a large backbone.",
    "rationale": "The figure shows that XFormer with a large backbone produces a more accurate human mesh reconstruction than Graphormer and XFormer with a small backbone. This can be seen in the second column of the figure, where the XFormer with a large backbone mesh is more similar to the input image than the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.11101v1",
    "pdf_url": null
  },
  {
    "instance_id": "741167b9c82c441f90be0ca896eb1c57",
    "figure_id": "1612.06915v2-Figure3-1",
    "image_file": "1612.06915v2-Figure3-1.png",
    "caption": " Value estimates for dissimilar strategies in Leduc hold’em",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which estimator has the highest standard deviation?",
    "answer": "chips",
    "rationale": "The table shows that the chips estimator has the highest standard deviation, with a value of 5.761.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1612.06915v2",
    "pdf_url": null
  },
  {
    "instance_id": "3d660b2dc0574dcd94d8aa1c1c25096c",
    "figure_id": "2302.07387v2-Figure10-1",
    "image_file": "2302.07387v2-Figure10-1.png",
    "caption": " The result comparison of LAVT [87], SeqTR [95] and PolyFormer on RefCOCOg test set. PolyFormer simultaneously predicts the bounding box and polygon vertices that forms the segmentation mask. LAVT is for referring image segmentation only. For SeqTR, we generate the bounding boxes and segmentation masks from the task-specific models as they perform better than the multi-task model.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods is able to predict both the bounding box and the segmentation mask of an object?",
    "answer": "PolyFormer",
    "rationale": "The caption states that \"PolyFormer simultaneously predicts the bounding box and polygon vertices that forms the segmentation mask.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.07387v2",
    "pdf_url": null
  },
  {
    "instance_id": "d63b4e3e410f46ed8fbb84db5abf573e",
    "figure_id": "2202.10687v1-Figure3-1",
    "image_file": "2202.10687v1-Figure3-1.png",
    "caption": " Examples of the mean frame. Row 1-2 and 3 denote examples of URFD and AIHub dataset, respectively. Examples of the last column are the failure cases.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which column in the figure shows examples of falling?",
    "answer": "The first column.",
    "rationale": "The figure shows two columns of images, with the first column labeled \"Falling\" and the second column labeled \"Non-falling\". The images in the first column show people falling, while the images in the second column show people who are not falling.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.10687v1",
    "pdf_url": null
  },
  {
    "instance_id": "9445be8e36324e5d852c515d6416323b",
    "figure_id": "1812.09851v2-Figure5-1",
    "image_file": "1812.09851v2-Figure5-1.png",
    "caption": " Valid displacements introduced in Lemmas 3.3, 3.4, 3.5.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which lemma introduces a valid displacement of v_i to v_i'?",
    "answer": "Lemma 3.3.",
    "rationale": "Panel B of the figure shows the valid displacement of v_i to v_i' as indicated by the blue arrow. The caption states that this displacement is introduced in Lemma 3.3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.09851v2",
    "pdf_url": null
  },
  {
    "instance_id": "d405d9a0718b462da0bf772ece77ba91",
    "figure_id": "2009.11896v1-Figure1-1",
    "image_file": "2009.11896v1-Figure1-1.png",
    "caption": " Our method retains context-relevant tokens from the observation text (shown in green) while pruning irrelevant tokens (shown in red). A second policy network re-trained on the pruned observations generalizes better by avoiding overfitting to unwanted tokens.",
    "figure_type": "Other.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which direction does the policy recommend going in?",
    "answer": "South.",
    "rationale": "The figure shows that the bootstrapped policy action is to go south.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.11896v1",
    "pdf_url": null
  },
  {
    "instance_id": "c60552c95513462b8a1677bc3b65875a",
    "figure_id": "1902.00113v3-Figure6-1",
    "image_file": "1902.00113v3-Figure6-1.png",
    "caption": " (a) Ablation study on PACS (↑). (b) Computational cost comparison on PACS (↓).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of accuracy on PACS?",
    "answer": "AGG",
    "rationale": "The figure shows the accuracy of different methods on the PACS dataset. AGG has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.00113v3",
    "pdf_url": null
  },
  {
    "instance_id": "268ec1342de0482f9265daa08b1eb20e",
    "figure_id": "2106.02299v1-Figure8-1",
    "image_file": "2106.02299v1-Figure8-1.png",
    "caption": " Visual comparison among different SR methods on the CUFED5 [33] testing set.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which SR method produced the sharpest and most detailed images?",
    "answer": "MASA (Ours)",
    "rationale": "The images produced by MASA (Ours) are the sharpest and most detailed, as evidenced by the clearer edges and textures in the zoomed-in regions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02299v1",
    "pdf_url": null
  },
  {
    "instance_id": "62535cb989a44ff3ad6cb41a7f692c6f",
    "figure_id": "2108.13934v2-Figure5-1",
    "image_file": "2108.13934v2-Figure5-1.png",
    "caption": " Performance as a function of entity frequency",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better for rare entities?",
    "answer": "KGI by head performs better for rare entities.",
    "rationale": "The figure shows that the macro accuracy of KGI by head is higher than that of BART by head for frequency deciles 1 and 2, which correspond to rare entities.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.13934v2",
    "pdf_url": null
  },
  {
    "instance_id": "6c7da4d3303c4e3ca8f0744d6aeb4b8f",
    "figure_id": "2010.15651v1-Figure7-1",
    "image_file": "2010.15651v1-Figure7-1.png",
    "caption": " Accuracy for evasion (transfer) attacks on Citeseer.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GCN method is most robust to evasion attacks on the Citeseer dataset?",
    "answer": "RGCN.",
    "rationale": "The figure shows the accuracy of different GCN methods as a function of the fraction of changed edges in the graph. RGCN has the highest accuracy for all fractions of changed edges, indicating that it is the most robust to evasion attacks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.15651v1",
    "pdf_url": null
  },
  {
    "instance_id": "1abdbdc7b6164b929c69e9276bc5dfff",
    "figure_id": "2010.07777v1-Figure4-1",
    "image_file": "2010.07777v1-Figure4-1.png",
    "caption": " EGTA for networked system control, with α = 0.1. Heatmap of average restraint percentage as a function of the regeneration rate for different MARL algorithms, from high (0.1) to low (0.03).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which MARL algorithm has the highest average restraint percentage when the regeneration rate is 0.088?",
    "answer": "IA2C",
    "rationale": "The heatmap shows that the average restraint percentage for IA2C is 0.54 when the regeneration rate is 0.088, which is the highest value among all the MARL algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.07777v1",
    "pdf_url": null
  },
  {
    "instance_id": "954a4584cbd94321a1231045370fe402",
    "figure_id": "2301.11300v3-Figure3-1",
    "image_file": "2301.11300v3-Figure3-1.png",
    "caption": " Correlation coefficients of various proxies vs. test accuracy on NASBench101 search space. ZiCo has significantly higher correlation scores than other proxies, except for Zen-score.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which proxy has the highest correlation with test accuracy according to Spearman's rank correlation coefficient?",
    "answer": "FLOPs",
    "rationale": "The bar chart shows the correlation coefficients between different proxies and test accuracy, as measured by Spearman's rank correlation coefficient and Kendall's tau. The blue bars represent Spearman's rank correlation coefficient, and the orange bars represent Kendall's tau. The height of each bar indicates the strength of the correlation. The FLOPs bar is the tallest blue bar, indicating that it has the highest correlation with test accuracy according to Spearman's rank correlation coefficient.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.11300v3",
    "pdf_url": null
  },
  {
    "instance_id": "95660439ec354387a329feb222cf80ce",
    "figure_id": "1905.07508v1-Figure1-1",
    "image_file": "1905.07508v1-Figure1-1.png",
    "caption": " Voting interface for cross-reference data from openbible.info.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which verse from the Bible is referenced in all three of the cross-references shown?",
    "answer": "Isaiah 25:8",
    "rationale": "The figure shows that Isaiah 25:8 is referenced in Revelation 21:4, 1 Corinthians 15:54, and Isaiah 35:10.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.07508v1",
    "pdf_url": null
  },
  {
    "instance_id": "92f94121a6574149be0bf1f88ecf35c3",
    "figure_id": "2110.04121v2-Figure9-1",
    "image_file": "2110.04121v2-Figure9-1.png",
    "caption": " Qualitative results for the unconditional generation using prior samples. For PolyMNIST (Subfigures (a) to (d)) and Translated-PolyMNIST (Subfigures (e) to (h)), we show 20 samples for each modality. For CUB, we show 100 generated images (Subfigures (i) to (l)) and 100 generated captions (Subfigures (m) to (p)) respectively. Best viewed zoomed and in color.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four models generated the most diverse and realistic samples for CUB?",
    "answer": "MoPoE-VAE.",
    "rationale": "Subfigures (i) to (l) show the generated images for each of the four models. The images generated by MoPoE-VAE appear to be more diverse and realistic than those generated by the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.04121v2",
    "pdf_url": null
  },
  {
    "instance_id": "975c64b192dc4073af7969799f4c2f3f",
    "figure_id": "2004.10102v2-Figure17-1",
    "image_file": "2004.10102v2-Figure17-1.png",
    "caption": " Examples of the reference alignment and the extracted patterns by each method in layer 1. Word pairs with a green frame shows the word with the highest weight or norm. The vertical axis represents the input source word in the decoder, and the pairs with a green frame are extracted as alignments in the AWI setting. Note that pairs that contain 〈/s〉 not extracted.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which word in the reference sentence has the highest attention weight according to the figure?",
    "answer": "\"We\"",
    "rationale": "The figure shows the attention weights for each word in the reference sentence. The word with the highest attention weight is indicated by a green frame. In this case, the word \"We\" has the highest attention weight.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.10102v2",
    "pdf_url": null
  },
  {
    "instance_id": "6cab169b23254958a3c4280e3b081fab",
    "figure_id": "2010.12721v1-Figure1-1",
    "image_file": "2010.12721v1-Figure1-1.png",
    "caption": " Reliability diagrams and ECE values before and after calibration with Temperature Scaling and PEP, for experiments described in Section 3.1 of the manuscript. From top to bottom: DenseNet121, InceptionV3, ResNet50, VGG16, and VGG19.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which calibration method reduces the ECE more, Temperature Scaling or PEP?",
    "answer": "Temperature Scaling",
    "rationale": "The ECE values for each model are lower after Temperature Scaling than after PEP. This can be seen in the plots by comparing the ECE values in the bottom right corner of each plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12721v1",
    "pdf_url": null
  },
  {
    "instance_id": "7ee0510b3db94d978bde394c0ddadc9c",
    "figure_id": "2001.04643v1-Figure7-1",
    "image_file": "2001.04643v1-Figure7-1.png",
    "caption": " Interpretable generative models enables disentanglement and extrapolation. Spectrograms of audio examples include dereverberation of a clip of solo violin playing (left), transfer of the extracted room response to new audio (center), and transposition below the range of training data (right).",
    "figure_type": "Other (Spectrograms)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three tasks shown in the figure is the most challenging for a machine learning model to perform?",
    "answer": "Transposition beyond training.",
    "rationale": "The figure shows that the model is able to dereverberate and transfer acoustics to new audio with good results. However, the model struggles to transpose audio below the range of training data. This suggests that transposition beyond training is the most challenging task for the model to perform.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.04643v1",
    "pdf_url": null
  },
  {
    "instance_id": "1f46cf224c3a43f095985c7437991c64",
    "figure_id": "2103.03583v2-Figure5-1",
    "image_file": "2103.03583v2-Figure5-1.png",
    "caption": " Result variation w.r.t. different layer number.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest precision@1 when the number of layers is 5?",
    "answer": "Quora",
    "rationale": "The figure shows the precision@1 for different datasets and different numbers of layers. The precision@1 for Quora is highest when the number of layers is 5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.03583v2",
    "pdf_url": null
  },
  {
    "instance_id": "37b4ef81fb65427fbaa3b985809d44f0",
    "figure_id": "2211.12316v2-Figure8-1",
    "image_file": "2211.12316v2-Figure8-1.png",
    "caption": " Distribution of SOP measure based on 50k samples of 4-layer Transformers and 1-layer LSTMs with width 64. Refer to Section B.1 for details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model tends to generate smaller SOP expressions?",
    "answer": "The LSTM model.",
    "rationale": "The LSTM distribution is shifted to the left compared to the Transformer distribution, which means that the LSTM model is more likely to generate smaller SOP expressions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.12316v2",
    "pdf_url": null
  },
  {
    "instance_id": "9fcc1e1a9678409b8f8763f994600aff",
    "figure_id": "2209.15292v1-Figure10-1",
    "image_file": "2209.15292v1-Figure10-1.png",
    "caption": " Sensitive Analysis of different C.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the performance of DPCML1 and DPCML2 compare when C is 5?",
    "answer": "DPCML1 performs better than DPCML2 when C is 5.",
    "rationale": "The figure shows that the P@5 for DPCML1 is higher than the P@5 for DPCML2 when C is 5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.15292v1",
    "pdf_url": null
  },
  {
    "instance_id": "574550313c46476d8375d2fa0a2a07d6",
    "figure_id": "2005.00782v4-Figure4-1",
    "image_file": "2005.00782v4-Figure4-1.png",
    "caption": " Performance of different transformer-based models on different settings of our data. BERT, RoBERTa, ERNIE, and BART are evaluated using masked word prediction and GPT2 is evaluated using sentence probability. Zero-shot performance is no better than random guessing. More data helps greatly for human-verified test set (10k) although noisy training hinders the improvement. Increasing data does not help at all for our human-curated set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Human-Verified set when trained on the Noisy 100k dataset?",
    "answer": "RoBERTa",
    "rationale": "The figure shows the average accuracy of different models on different settings of the data. The performance of RoBERTa on the Human-Verified set is the highest when trained on the Noisy 100k dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.00782v4",
    "pdf_url": null
  },
  {
    "instance_id": "a8e2d999dc9845a79a79d6ac6716004a",
    "figure_id": "2208.08302v1-Figure1-1",
    "image_file": "2208.08302v1-Figure1-1.png",
    "caption": " Schematic diagram of under-reaching and oversquashing in the topology-imbalance issue.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of error is represented by the dashed red arrow?",
    "answer": "Under-reaching.",
    "rationale": "The dashed red arrow points to a path that starts at a labeled node and ends at an unlabeled node, which is not included in the cluster. This is an example of under-reaching, where the cluster does not include all of the nodes that are similar to the labeled nodes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.08302v1",
    "pdf_url": null
  },
  {
    "instance_id": "5943e6ec9a174459af44f17f10ae225a",
    "figure_id": "1809.00898v1-Figure2-1",
    "image_file": "1809.00898v1-Figure2-1.png",
    "caption": " Overview of our method. Knowing a central fragment, we are looking for the correct arrangement to reassemble the image (a). We extract the feature of all the fragments (b) and we compare them to the features of the central fragment. We predict which fragments are part of the image (c). We retrieve the top eight fragments and we predict their relative position with respect to the central one. We turn the prediction into a graph (d). We then run a shortest path algorithm to reconstruct the image",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which step in the process involves comparing the features of the fragments to the features of the central fragment?",
    "answer": "The feature extraction step.",
    "rationale": "The figure shows that the feature extraction step is where the features of the fragments are extracted and compared to the features of the central fragment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.00898v1",
    "pdf_url": null
  },
  {
    "instance_id": "b7527f35890741ea821c1ce370e213ef",
    "figure_id": "1901.03396v1-Figure2-1",
    "image_file": "1901.03396v1-Figure2-1.png",
    "caption": " Median recovery error (MRE, see Eq. (5)) for 1800 test images on various GAN generators (PGGAN [16], MESCH [21] and DCGAN [25]) against to various distortions φ in latent recovery optimization (NNG) (see text for details).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three GAN generators performs best under additive white noise?",
    "answer": "DCGAN",
    "rationale": "The figure shows the median recovery error (MRE) for three GAN generators under different types of distortion. The lower the MRE, the better the generator performs. In the case of additive white noise, the DCGAN has the lowest MRE, indicating it performs best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.03396v1",
    "pdf_url": null
  },
  {
    "instance_id": "a3d753868bcf4151bba4bb1eaeace05c",
    "figure_id": "2210.14215v1-Figure13-1",
    "image_file": "2210.14215v1-Figure13-1.png",
    "caption": " Downstream performance of Algorithm Distillation with different values of random masking during training in 9x9 1 goal gridworld.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of random masking during training resulted in the highest downstream performance for Algorithm Distillation in the 9x9 1 goal gridworld?",
    "answer": "0.9",
    "rationale": "The figure shows that the curve corresponding to a random masking value of 0.9 is the highest among all the curves, indicating that it achieved the highest downstream performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.14215v1",
    "pdf_url": null
  },
  {
    "instance_id": "f6326c6db02248f3a55f591d3d6d6c69",
    "figure_id": "2012.02366v4-Figure9-1",
    "image_file": "2012.02366v4-Figure9-1.png",
    "caption": " Comparison of feature emphasis. With the feature attention filter and training data mining, our methods focus on the distinctive details of buildings, while avoiding confusing visual clues such as pedestrians, vegetation, or vehicles which are hard for feature repeatability.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods shown in the figure is best at avoiding confusing visual clues such as pedestrians, vegetation, or vehicles?",
    "answer": "Ours(V)",
    "rationale": "The figure shows that Ours(V) focuses on the distinctive details of buildings, while avoiding confusing visual clues such as pedestrians, vegetation, or vehicles. This is evident in the heatmaps, which show that Ours(V) places more emphasis on the buildings themselves and less emphasis on the surrounding objects.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.02366v4",
    "pdf_url": null
  },
  {
    "instance_id": "0fc319ceed4b432f8c3ae311a373ec4c",
    "figure_id": "2009.07526v2-Figure6-1",
    "image_file": "2009.07526v2-Figure6-1.png",
    "caption": " Visualization of scene graphs generated by SG-Transformer (blue) and SG-Transformer+CogTree (green). Compared with the ground-truth, the quality of predicted relationships are marked in three colors: red (false), blue (correct), purple (better).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is more accurate in predicting the relationship between the giraffe and the fence?",
    "answer": "SG-Transformer+CogTree",
    "rationale": "The image shows that the SG-Transformer model predicts that the giraffe is \"near\" the fence, while the SG-Transformer+CogTree model predicts that the giraffe is \"behind\" the fence. The ground truth shows that the giraffe is indeed behind the fence, so the SG-Transformer+CogTree model is more accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.07526v2",
    "pdf_url": null
  },
  {
    "instance_id": "65ca0214a31c439da54a6591d51e3691",
    "figure_id": "2002.10657v1-Figure1-1",
    "image_file": "2002.10657v1-Figure1-1.png",
    "caption": " Results of the experiment to reduce similarity by adding label noise (§2).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models achieved the highest training accuracy?",
    "answer": "The model with 0% corruption.",
    "rationale": "The training accuracy plot shows that the model with 0% corruption (blue line) has the highest accuracy at the end of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.10657v1",
    "pdf_url": null
  },
  {
    "instance_id": "3296f096a0a74a82bceccc06c356707e",
    "figure_id": "2310.02604v1-Figure5-1",
    "image_file": "2310.02604v1-Figure5-1.png",
    "caption": " Road map for the proof of Theorem 3.1",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two lemmas are used to prove that the convergence of  acts on the iterative process?",
    "answer": "Lemma B.2 and Lemma B.5.",
    "rationale": "The figure shows that Lemma B.2 is used to prove that the kernel space of  is the same as the intersection of the kernel space for . Lemma B.5 is used to prove that  acts on the iterative process. Therefore, both lemmas are used to prove that the convergence of  acts on the iterative process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.02604v1",
    "pdf_url": null
  },
  {
    "instance_id": "4bc115810a074fda837c1a26e3dcc786",
    "figure_id": "2002.11949v3-Figure12-1",
    "image_file": "2002.11949v3-Figure12-1.png",
    "caption": " MOTIFS† [71]: Recall@100 on Predicate Classification for the most frequent 35 predicates.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four models performed the best overall on the predicate classification task?",
    "answer": "MOTIFS†-GATE-TDE",
    "rationale": "The figure shows the recall@100 for each of the four models on the predicate classification task. The MOTIFS†-GATE-TDE model has the highest recall@100 for the most predicates, indicating that it performs the best overall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11949v3",
    "pdf_url": null
  },
  {
    "instance_id": "d60b4969bde3499c936d5e934d0bcbef",
    "figure_id": "2011.08900v1-Figure4-1",
    "image_file": "2011.08900v1-Figure4-1.png",
    "caption": " (a) Average CAM produced by the model trained from raw images with hand and background. (b) Average CAM produced by the model trained from the images only with hand. (c) Average of the actual hand masks. Overall, CAM by the hand-only model has its peak closer to the actual hand location.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the most accurate representation of the hand location?",
    "answer": "Image (c)",
    "rationale": "Image (c) shows the average of the actual hand masks, which is the ground truth for the hand location. The other images are CAMs, which are visualizations of the model's attention. While they are useful for understanding what the model is focusing on, they are not necessarily accurate representations of the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.08900v1",
    "pdf_url": null
  },
  {
    "instance_id": "44118fb57cd54c7b8ac57cc888634163",
    "figure_id": "2303.12803v2-Figure2-1",
    "image_file": "2303.12803v2-Figure2-1.png",
    "caption": " Performance comparison of PBT-MAP-ELITES with baselines on the basis of standard metrics from the QD literature for five environments from the QDAX suite (which is based on the BRAX engine). We benchmark two variants of PBT-MAP-ELITES, one where it is composed with SAC and one where it is composed with TD3. All methods are trained with a total budget of N = 1.5e8 environment timesteps. Experiments are repeated over 5 runs with different random seeds and the medians (resp. first and third quartile intervals) are depicted with full lines (resp. shaded areas).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of QD Score on the Ant-Uni environment?",
    "answer": "PBT-MAP-Elites (SAC)",
    "rationale": "The figure shows the QD Score for each algorithm on the Ant-Uni environment. PBT-MAP-Elites (SAC) has the highest QD Score, which means it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.12803v2",
    "pdf_url": null
  },
  {
    "instance_id": "5043fa297f3049308412533c27e10a97",
    "figure_id": "2109.14419v3-Figure8-1",
    "image_file": "2109.14419v3-Figure8-1.png",
    "caption": " Learning curves on a suite of Atari benchmark tasks for comparing two additional baselines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the Alien task?",
    "answer": "DB-ADP-C",
    "rationale": "The figure shows the learning curves for different algorithms on different Atari benchmark tasks. The learning curve for DB-ADP-C on the Alien task is the highest, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.14419v3",
    "pdf_url": null
  },
  {
    "instance_id": "dd667eb36f4a45e79611b4f09c974010",
    "figure_id": "2306.07684v3-Figure2-1",
    "image_file": "2306.07684v3-Figure2-1.png",
    "caption": " Convergence rate on quadratics of varying condition number. We fix the step k “ 20 for Lookahead and Lookaround, and fix the CM factor β “ 0.99.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most efficient for solving quadratic equations with a high condition number?",
    "answer": "Lookaround",
    "rationale": "The figure shows the convergence rate of different methods for solving quadratic equations as a function of the condition number. The Lookaround method has the highest convergence rate for high condition numbers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.07684v3",
    "pdf_url": null
  },
  {
    "instance_id": "3f2c9f31837b463aaf37f3443666e6de",
    "figure_id": "2102.13249v2-Figure1-1",
    "image_file": "2102.13249v2-Figure1-1.png",
    "caption": " Chess Notation",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which square on the chessboard is represented by the notation \"g5\"?",
    "answer": "The square in the 5th row and 7th column.",
    "rationale": "The figure shows the square naming convention for a chessboard. The rows are numbered from 1 to 8, and the columns are labeled from a to h. The notation \"g5\" therefore refers to the square in the 5th row and 7th column.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.13249v2",
    "pdf_url": null
  },
  {
    "instance_id": "f46f7f49f2324f3d95989dc90da503e0",
    "figure_id": "2112.06095v1-Figure11-1",
    "image_file": "2112.06095v1-Figure11-1.png",
    "caption": " End-to-end training time speedup of FPISA-A compared to the default SwitchML.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture benefited the most from FPISA-A in terms of training speedup when using 2 cores?",
    "answer": "LSTM.",
    "rationale": "The figure shows the end-to-end training speedup of FPISA-A compared to the default SwitchML for various model architectures. For the 2-core case, the LSTM model has the highest speedup of 56.3%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.06095v1",
    "pdf_url": null
  },
  {
    "instance_id": "71451c4733474b318417ad7f0d186c6a",
    "figure_id": "2110.11940v2-Figure17-1",
    "image_file": "2110.11940v2-Figure17-1.png",
    "caption": " We trained MLPs on the Covertype dataset, with a fixed 80:20 random split. Trained with ADAM, 50 ep., 1-cycle, using LRs determined automatically with LR-finder. Mean (bars: std dev) of n=5 weight inits.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activation function performs the best for the Covertype dataset with a 3 hidden layer MLP?",
    "answer": "ReLU",
    "rationale": "The figure shows that ReLU has the highest test accuracy for the Covertype dataset with a 3 hidden layer MLP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.11940v2",
    "pdf_url": null
  },
  {
    "instance_id": "ff604e8a9d924db3883ca8b742768e71",
    "figure_id": "2109.04014v1-Figure5-1",
    "image_file": "2109.04014v1-Figure5-1.png",
    "caption": " Highest-Score Strategy: Performance of EReader decreases when the knowledge number increase and the best is at 5. Highest-Frequency strategies: Performance of EReader increase when the knowledge number increase and the best is at 80.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which strategy is better when the number of retrieved knowledge is small (e.g., less than 10)?",
    "answer": "Highest-Score strategy.",
    "rationale": "The figure shows that the performance of the Highest-Score strategy is higher than the Highest-Frequency strategy when the number of retrieved knowledge is small (e.g., less than 10).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.04014v1",
    "pdf_url": null
  },
  {
    "instance_id": "093261312ae0499aae1fea6e078b9c1e",
    "figure_id": "2204.05514v1-Figure1-1",
    "image_file": "2204.05514v1-Figure1-1.png",
    "caption": " Diagnosticity vs time complexity for faithfulness metrics. The values are averages over all datasets and classification models. The faithfulness metrics near the top-right corner are more desirable than those near the bottom-left corner.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which faithfulness metric has the highest diagnosticity?",
    "answer": "COMP",
    "rationale": "The figure shows that COMP has the highest diagnosticity, as it is located highest on the y-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.05514v1",
    "pdf_url": null
  },
  {
    "instance_id": "41bbb684cd19479883d5d8a120d5d488",
    "figure_id": "1901.06085v1-Figure3-1",
    "image_file": "1901.06085v1-Figure3-1.png",
    "caption": " Quantification of algorithm inferences in Experiment 1 compared to human judgments. (a) Bayesian model averaging explains a high degree of variance in human judgments. (b) The maximum likelihood CTH captures some of the coarse grained aspects of the inferences but does not capture the uncertainty in people’s judgments.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model better captures the uncertainty in people's judgments?",
    "answer": "The Bayesian model averaging.",
    "rationale": "The Bayesian model averaging shows a wider spread of data points, indicating that it captures more of the variability in human judgments. The maximum likelihood CTH shows a narrower spread of data points, indicating that it captures less of the variability in human judgments.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.06085v1",
    "pdf_url": null
  },
  {
    "instance_id": "0aaac3b422ad4042ac8ccf80619ffd30",
    "figure_id": "2210.12400v1-Figure1-1",
    "image_file": "2210.12400v1-Figure1-1.png",
    "caption": " The architecture of Varifocal. We use a dependency parser to extract the different focal points, i.e. spans, then generate questions based on them. We rank the generated questions using a re-ranker and return the top n questions. The example in the figure was generated by our system. We show three highlighted focal points along with the (output) questions they led to.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three focal points highlighted in the figure?",
    "answer": "Miss Universe Guyana 2017, cocaine, and 11/15/17.",
    "rationale": "The figure shows how Varifocal works. The system uses a dependency parser to extract the different focal points, i.e. spans, then generates questions based on them. The example in the figure was generated by our system. We show three highlighted focal points along with the (output) questions they led to.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12400v1",
    "pdf_url": null
  },
  {
    "instance_id": "e73e6034b641467bb922fa2e4e11072a",
    "figure_id": "2302.00875v1-Figure9-1",
    "image_file": "2302.00875v1-Figure9-1.png",
    "caption": " GZSL performance evaluated on the AWA2 dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in the unseen setting?",
    "answer": "CLS&AAAM",
    "rationale": "The table on the right shows the performance of each method in the seen, unseen, and harmonic settings. CLS&AAAM has the highest accuracy in the unseen setting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.00875v1",
    "pdf_url": null
  },
  {
    "instance_id": "fb71ac6e21354a50bea5bc49a203526e",
    "figure_id": "2110.01543v1-Figure5-1",
    "image_file": "2110.01543v1-Figure5-1.png",
    "caption": " Test accuracy of ResNet18/WideResNet16-4 on CIFAR-10 and ResNeXt50/DenseNet121 on CIFAR-100.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performs the best on CIFAR-100 with DenseNet121?",
    "answer": "AdaSAM.",
    "rationale": "The plot shows that AdaSAM achieves the highest test accuracy among all optimizers on CIFAR-100 with DenseNet121.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.01543v1",
    "pdf_url": null
  },
  {
    "instance_id": "06e92e85fead4713bbcf4355dabffff9",
    "figure_id": "2111.05393v1-Figure10-1",
    "image_file": "2111.05393v1-Figure10-1.png",
    "caption": " The space-time-queried scene appearance prediction performance comparison between three DyMONs that are trained on three levels of scene-observer speed differences, i.e. DR-Lvl.1 ∼ 3, respectively. Left: Averaged MSE achieved by the three models on three DRoom testing sets, i.e. the testing sets of DR-Lvl.1 ∼ 3. Right: The performance of the three models on each of the three testing sets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed best on the DR-Lvl.3 test set?",
    "answer": "DR-Lvl.3",
    "rationale": "The right panel of the figure shows the performance of the three models on each of the three test sets. The DR-Lvl.3 model has the lowest MSE on the DR-Lvl.3 test set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.05393v1",
    "pdf_url": null
  },
  {
    "instance_id": "6533a2451f2140649b384091295d010d",
    "figure_id": "2211.14305v2-Figure4-1",
    "image_file": "2211.14305v2-Figure4-1.png",
    "caption": " Multi-scale control: Using the multi-scale inference (Equation (3)) allows fine-grained control over the input conditions. Given the same inputs (left), we can use different scales for each condition. In this example, if we put all the weight on the local scene (1), the generated image contains a horse with the correct color and posture, but not at the beach. Conversely, if we place all the weight on the global text (5), we get an image of a beach with no horse in it. The in-between results correspond to a mix of conditions — in (4) we get a gray donkey, in (2) the beach contains no water, and in (3) we get a brown horse at the beach on a sunny day.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the most accurate representation of the input conditions \"a sunny day at the beach\" and \"a brown horse\"?",
    "answer": "Image (3)",
    "rationale": "Image (3) shows a brown horse at the beach on a sunny day, which is the most accurate representation of the input conditions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.14305v2",
    "pdf_url": null
  },
  {
    "instance_id": "d458b1e456bc43f1bffae2ef48aee931",
    "figure_id": "2105.03363v3-Figure5-1",
    "image_file": "2105.03363v3-Figure5-1.png",
    "caption": " Model compounding errors and interactions numbers of different model usages.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model usage results in the lowest compounding error?",
    "answer": "All real opponents.",
    "rationale": "The plot on the right shows that the blue line, which represents all real opponents, has the lowest compounding error for all rollout lengths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.03363v3",
    "pdf_url": null
  },
  {
    "instance_id": "c4198f930aa7483491ac72dfc8edd0f2",
    "figure_id": "2110.07936v2-Figure7-1",
    "image_file": "2110.07936v2-Figure7-1.png",
    "caption": " An example on the summaries generated by different systems.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which system generated the most detailed summary?",
    "answer": "CSC(Multitask(SE))",
    "rationale": "The figure shows that CSC(Multitask(SE)) generated the most detailed summary, which includes the date, location, and event name.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.07936v2",
    "pdf_url": null
  },
  {
    "instance_id": "3b238bc58ef5461d8dd221dbdc590d14",
    "figure_id": "1811.03782v3-Figure3-1",
    "image_file": "1811.03782v3-Figure3-1.png",
    "caption": " Comparisons using various sampling patterns.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampling pattern resulted in the highest average PSNR for the Ours method?",
    "answer": "Gaussian Mask",
    "rationale": "The Ours method is represented by the red bars in the figure. The highest average PSNR for the Ours method is achieved with the Gaussian Mask.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.03782v3",
    "pdf_url": null
  },
  {
    "instance_id": "d0fd0f84e4ae44fab37d4ce60194426c",
    "figure_id": "2311.04943v2-Figure6-1",
    "image_file": "2311.04943v2-Figure6-1.png",
    "caption": " Top-1 vs. Latency of MathNAS over SOTA dynamic baselines on three devices.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest Top-1 accuracy on the Raspberry Pi?",
    "answer": "Ours (MathNAS-MB)",
    "rationale": "The figure shows the Top-1 accuracy of different methods on the Raspberry Pi. The red line, which represents Ours (MathNAS-MB), is the highest at all latency points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2311.04943v2",
    "pdf_url": null
  },
  {
    "instance_id": "5d0b75df754f4a43afc527462517ea8b",
    "figure_id": "2111.11704v2-Figure1-1",
    "image_file": "2111.11704v2-Figure1-1.png",
    "caption": " Point cloud reconstruction. We propose a novel neural architecture that jointly solves inherent shortcomings in raw point cloud, such as noise, sparsity, and incompleteness.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three inherent shortcomings of raw point clouds, as shown in the figure?",
    "answer": "Noise, sparsity, and incompleteness.",
    "rationale": "The figure shows three examples of raw point clouds, each with one of the shortcomings. The first example is a noisy point cloud of an airplane. The second example is a sparse point cloud of a chair. The third example is an incomplete point cloud of a kitchen.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.11704v2",
    "pdf_url": null
  },
  {
    "instance_id": "3ef25e53fadf4579a98c47555a1ef9e0",
    "figure_id": "1906.04356v2-Figure1-1",
    "image_file": "1906.04356v2-Figure1-1.png",
    "caption": " Empirical performance of exact computation, RAND, Med-dit and Correlated Sequential Halving. The error probability is the probability of not returning the correct medoid.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest error probability for a given number of pulls?",
    "answer": "The exact method.",
    "rationale": "The exact method is represented by the green dashed line in the figure, which is consistently below the other lines, indicating that it has the lowest error probability for any given number of pulls.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04356v2",
    "pdf_url": null
  },
  {
    "instance_id": "d20e799fa9294a3ca7a2e322cbba937a",
    "figure_id": "1911.03872v2-Figure1-1",
    "image_file": "1911.03872v2-Figure1-1.png",
    "caption": " Schematic extrapolation setting for d = 2.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the points are used to train the model?",
    "answer": "The blue points.",
    "rationale": "The legend on the right side of the figure indicates that the blue points are the training points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.03872v2",
    "pdf_url": null
  },
  {
    "instance_id": "375d5e1fa058437a8b136f47c8204c03",
    "figure_id": "2105.01879v1-Figure10-1",
    "image_file": "2105.01879v1-Figure10-1.png",
    "caption": " WordNet hierarchy. Super-classes of ImageNet-1k are based on the leaf nodes (in ellipses) except for misc. The super-class of misc contains 3 leaf nodes: abstract entity, matter, and location.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between \"living thing\" and \"organism\"?",
    "answer": "An organism is a type of living thing.",
    "rationale": "The figure shows that \"organism\" is a child node of \"living thing\", which means that it is a more specific type of living thing.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.01879v1",
    "pdf_url": null
  },
  {
    "instance_id": "e7e990d64c814a81acc56687bad8a969",
    "figure_id": "1903.00658v1-Figure4-1",
    "image_file": "1903.00658v1-Figure4-1.png",
    "caption": " Denoising experiment on a image of snacks.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the image with the highest quality?",
    "answer": "QCNN.",
    "rationale": "The QCNN image has the highest PSNR value (25.69dB), which is a measure of image quality. The higher the PSNR value, the better the image quality.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.00658v1",
    "pdf_url": null
  },
  {
    "instance_id": "92264f78955b43c18eea0d5612a7f816",
    "figure_id": "2310.19691v1-Figure2-1",
    "image_file": "2310.19691v1-Figure2-1.png",
    "caption": " DAGs for three causal contexts in which a counterfactually fair predictor is equivalent to a particular group fairness metric. Measurement error is represented with the unobserved true label Ỹ in a dashed circle, which is a parent of the observed label Y , and selection is represented with the selection status S in a rectangle, indicating an observed variable that is conditioned on in the data-generating process, which induces an association between its parents. Bidirectional arrows indicate that either variable could affect the other.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three causal contexts corresponds to demographic parity?",
    "answer": "Measurement error.",
    "rationale": "The figure shows that demographic parity corresponds to measurement error. This is because demographic parity requires that the proportion of positive predictions is the same for all groups, regardless of the true label. In the case of measurement error, the true label is unobserved, and the observed label is a noisy version of the true label. This can lead to a situation where the proportion of positive predictions is different for different groups, even if the true proportion of positive labels is the same.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.19691v1",
    "pdf_url": null
  },
  {
    "instance_id": "45db37ab141048c0bd691d70e4f4e86a",
    "figure_id": "1911.08706v2-Figure3-1",
    "image_file": "1911.08706v2-Figure3-1.png",
    "caption": " Win/Tie/Loss counts when comparing Online Style Inference to Multi-Task. Informal translations generated by OSI are annotated as more informal than Multi-Task, while formal translations are annotated as more formal. The OSI model also gets more instances that better preserve the meaning.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is more likely to produce a translation that is judged to be more formal than the original text?",
    "answer": "Multi-Task",
    "rationale": "The figure shows that the Multi-Task model produces more translations that are judged to be more formal than the original text. This is shown in panel (b) of the figure, where the bars for \"more formal\" and \"much more formal\" are higher for Multi-Task than for OSI.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.08706v2",
    "pdf_url": null
  },
  {
    "instance_id": "52af15f3a3164bb58b066c6e5c3c7b1a",
    "figure_id": "2103.11505v1-Figure6-1",
    "image_file": "2103.11505v1-Figure6-1.png",
    "caption": " Number of unsolved problems compared to the cumulative number of nodes expanded (left) or cumulative time (right) over the Bootstrap iterations. Lines correspond to the runs with the least remaining unsolved problems. All 5 training runs per algorithm lie within the colored areas.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the sokoban domain in terms of the number of unsolved problems?",
    "answer": "A",
    "rationale": "The A* algorithm has the lowest number of unsolved problems for all levels of nodes expanded on the sokoban domain. This can be seen in the top left plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.11505v1",
    "pdf_url": null
  },
  {
    "instance_id": "6ad15d91b4874b418c6a9100e718e9b1",
    "figure_id": "2207.09689v1-Figure4-1",
    "image_file": "2207.09689v1-Figure4-1.png",
    "caption": " Comparisons of visual results on UIEBD and RUIE datasets. (a) Original image. (b) Retinex. (c) GC. (d) Fusion. (e) IBLA. (f) Histogram-Prior. (g) Water-Net. (h) Ucolor. (i) PUIE-Net (MC). (j) PUIE-Net (MP).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the image with the most natural-looking colors?",
    "answer": "PUIE-Net (MP)",
    "rationale": "The image produced by PUIE-Net (MP) has the most natural-looking colors because it is the closest to the original image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.09689v1",
    "pdf_url": null
  },
  {
    "instance_id": "571a3f7fedf54df7ac9372bacd3120cf",
    "figure_id": "2111.07117v1-Figure11-1",
    "image_file": "2111.07117v1-Figure11-1.png",
    "caption": " Effect of T: as more observations are acquired, (Top) the spatial uncertainty reduces and the performance of novel-viewpoint prediction on observation (Bottom left) and segmentation (Bottom right) prediction boosts.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods performs the best in terms of reducing spatial uncertainty?",
    "answer": "CLE-Aug",
    "rationale": "The top plot shows the spatial uncertainty for each method as a function of the number of observations. CLE-Aug has the lowest spatial uncertainty for all values of T.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.07117v1",
    "pdf_url": null
  },
  {
    "instance_id": "6dbdcdb1ad8948d8a4961b36092fbc39",
    "figure_id": "2111.06849v1-Figure6-1",
    "image_file": "2111.06849v1-Figure6-1.png",
    "caption": " The overfitting and convergence status of APA compared to StyleGAN2 (SG2) on FFHQ [20] (256× 256). (a) The discriminator raw output logits of StyleGAN2 on the full (70k) or limited (7k) datasets. (b) The discriminator raw output logits of StyleGAN2 and APA on the limited (7k) dataset. (c) The training convergence shown by FID.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model appears to be overfitting the training data more quickly?",
    "answer": "APA",
    "rationale": "The discriminator outputs of APA in (b) show a much steeper increase than those of SG2 in (a) and (b), indicating that APA is learning the specific features of the training data more quickly than SG2. This can be a sign of overfitting, as the model may not generalize well to unseen data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.06849v1",
    "pdf_url": null
  },
  {
    "instance_id": "221cd3c1bfd5469580b2f07b70cdd1ba",
    "figure_id": "2105.13495v2-Figure7-1",
    "image_file": "2105.13495v2-Figure7-1.png",
    "caption": " Plot of the HCP-Task temporal attention Z(k) time averaged across all subjects.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which brain region shows the most consistent activation across all tasks?",
    "answer": "The motor cortex.",
    "rationale": "The figure shows that the motor cortex has a high level of activation across all tasks, as indicated by the bright red color in the heatmaps. This suggests that the motor cortex is involved in a wide range of cognitive functions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.13495v2",
    "pdf_url": null
  },
  {
    "instance_id": "096d68fa213e495d93db3848d13a6034",
    "figure_id": "2103.13620v1-Figure5-1",
    "image_file": "2103.13620v1-Figure5-1.png",
    "caption": " Comparison of activation-norm according to normalization methods in CP-ResNet(ch64).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of training on the activation-norm of a batch-normalized CP-ResNet(ch64) model?",
    "answer": "Training generally decreases the activation-norm.",
    "rationale": "The blue dashed line shows the activation-norm for a batch-normalized CP-ResNet(ch64) model with random initialization, while the blue solid line shows the activation-norm for the same model after training. The solid line is generally lower than the dashed line, indicating that training has decreased the activation-norm.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.13620v1",
    "pdf_url": null
  },
  {
    "instance_id": "50a569f51b064b29b88afff2f8b29e4e",
    "figure_id": "1909.03402v4-Figure6-1",
    "image_file": "1909.03402v4-Figure6-1.png",
    "caption": " Attention and feature map visualization of SA head1 and head4 of a trained SANet on PASCAL VOC dataset. For each head, the feature maps of main channel, attention channel, and output are demonstrated. (a) Raw image and its ground truth; the pixel group visualization of (b) blue point; (c) yellow point; and (d) magenta point.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the heads, SA Head1 or SA Head4, focuses more on the background information?",
    "answer": "SA Head4.",
    "rationale": "Comparing the \"Attn\" and \"Out\" columns of SA Head1 and SA Head4, it is evident that the attention and output of SA Head4 focus more on the background information, such as the road and the surrounding environment. In contrast, the attention and output of SA Head1 are more concentrated on the foreground object, which is the cow. This difference in focus suggests that SA Head4 might be more suitable for tasks that require understanding the context of the scene, while SA Head1 might be better suited for tasks that require focusing on the main object.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.03402v4",
    "pdf_url": null
  },
  {
    "instance_id": "3be62eeefa864334a3a5b60d085e8eb3",
    "figure_id": "2211.00937v1-Figure4-1",
    "image_file": "2211.00937v1-Figure4-1.png",
    "caption": " (a)∼(c) PSNR performance versus the CBR over the AWGN channel at SNR = 10dB. (d)∼(f) PSNR performance versus the CBR over the Rayleigh fast fading channel at SNR = 3dB.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which combination of algorithms and channels has the highest PSNR performance?",
    "answer": "BPG + LDPC over the AWGN channel at SNR = 10dB.",
    "rationale": "The PSNR performance is highest for this combination as seen in the plot (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.00937v1",
    "pdf_url": null
  },
  {
    "instance_id": "f3508558dd83488e8b9d74a47f728665",
    "figure_id": "1901.07687v3-Figure2-1",
    "image_file": "1901.07687v3-Figure2-1.png",
    "caption": " The cumulative loss for the face example with data samples coming from 20 different persons",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of minimizing the total loss?",
    "answer": "Follow the Leader",
    "rationale": "The black line in the figure represents the Follow the Leader algorithm, and it has the lowest total loss at each time step compared to the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.07687v3",
    "pdf_url": null
  },
  {
    "instance_id": "845df59da51043babbc82cdf3756764f",
    "figure_id": "2012.15421v1-Figure1-1",
    "image_file": "2012.15421v1-Figure1-1.png",
    "caption": " Framework for injecting verb knowledge into a pretrained Transformer encoder for event processing tasks. 1) Dedicated verb adapter parameters trained to recognise pairs of verbs from the same VerbNet (VN) class or FrameNet (FN) frame; 2) Fine-tuning for an event extraction task (e.g., event trigger identification and classification (UzZaman et al., 2013)): a) full fine-tuning – Transformer’s original parameters and verb adapters both fine-tuned for the task; b) task adapter (TA) fine-tuning – additional task adapter is mounted on top of verb adapter and tuned for the task. For simplicity, we show only a single transformer layer; verb- and task-adapters are used in all Transformer layers. Snowflakes denote frozen parameters in the respective training step.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two different ways to fine-tune the Transformer encoder for event extraction tasks?",
    "answer": "Full fine-tuning and task adapter fine-tuning.",
    "rationale": "The figure shows two different ways to fine-tune the Transformer encoder for event extraction tasks. In full fine-tuning, both the Transformer's original parameters and the verb adapters are fine-tuned for the task. In task adapter fine-tuning, an additional task adapter is mounted on top of the verb adapter and tuned for the task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.15421v1",
    "pdf_url": null
  },
  {
    "instance_id": "1525bd98ebd74281a8025cc77d501ede",
    "figure_id": "1810.00337v5-Figure12-1",
    "image_file": "1810.00337v5-Figure12-1.png",
    "caption": " The rewriting process that simplifies the expression ((v0− v1 + 12)/137 ∗ 137 + 137) ≤ min((v0− v1 + 149)/137 ∗ 137, v0− v1 + 13) to 136 ≤ (v0− v1 + 12)%137.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which step in the rewriting process involves the simplification of the expression `(v0− v1 + 12)/137 ∗ 137`?",
    "answer": " Step 1.",
    "rationale": " In Step 1, the expression `(v0− v1 + 12)/137 ∗ 137` is simplified to `v0− v1 + 12`. This can be seen by comparing the expressions in panels (a) and (b) of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.00337v5",
    "pdf_url": null
  },
  {
    "instance_id": "7b4685a4bf23434bb973137f4d9b96c5",
    "figure_id": "2305.19753v2-Figure34-1",
    "image_file": "2305.19753v2-Figure34-1.png",
    "caption": " In and out of distribution linear probing performance for VGG-19 trained on a 50-class subset of CIFAR-100. The shaded area depicts the tunnel, the red dashed line depicts the numerical rank, and the blue curve depicts linear probing accuracy (in and out of distribution) respectively. Out-of-distribution performance is computed on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the highest numerical rank in the in-distribution setting?",
    "answer": "Layer 10",
    "rationale": "The red dashed line in Figure (a) represents the numerical rank, and it peaks at layer 10.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19753v2",
    "pdf_url": null
  },
  {
    "instance_id": "c5b455487a7741059680d3ee829df0e2",
    "figure_id": "2310.04411v2-Figure14-1",
    "image_file": "2310.04411v2-Figure14-1.png",
    "caption": " Baird’s Counterexample.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of LayerNorm on the weight norm in Baird's counterexample?",
    "answer": "LayerNorm prevents the weight norm from exploding.",
    "rationale": "The figure shows that the weight norm for the model without LayerNorm explodes after about 1500 steps, while the weight norm for the model with LayerNorm remains relatively stable.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.04411v2",
    "pdf_url": null
  },
  {
    "instance_id": "b416f3f25afe48da95734a8b9d2bbabc",
    "figure_id": "1903.08811v1-Figure5-1",
    "image_file": "1903.08811v1-Figure5-1.png",
    "caption": " Box-plots of the performance of the different registration methods for longitudinal registration (green) and crosssubject registration (orange). Both AVSM and NiftyReg (LNCC) show high performance and small variance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which registration method has the highest median Dice score for cross-subject registration?",
    "answer": "NiftyReg (LNCC)",
    "rationale": "The orange box plot for NiftyReg (LNCC) has the highest median value among all the orange box plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.08811v1",
    "pdf_url": null
  },
  {
    "instance_id": "202ec5a47d774fd190f6dd1f5a724d49",
    "figure_id": "2202.00504v1-Figure11-1",
    "image_file": "2202.00504v1-Figure11-1.png",
    "caption": " Three pieces of cloth woven in different patterns show different dynamics.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three weaves is the most likely to wrinkle?",
    "answer": "The twill weave.",
    "rationale": "The twill weave has a diagonal pattern that creates more friction between the fibers, making it more likely to wrinkle than the other two weaves. The figure shows that the twill weave is more prone to bending and folding, which can lead to wrinkles.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.00504v1",
    "pdf_url": null
  },
  {
    "instance_id": "2a5ee4a9873a47d2a2594ceed8903a61",
    "figure_id": "2306.10759v3-Figure2-1",
    "image_file": "2306.10759v3-Figure2-1.png",
    "caption": " Scalability test of training time per epoch and GPU memory usage w.r.t. graph sizes (a.k.a. node numbers). NodeFormer suffers out-of-memory when # nodes reaches more than 30K.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models has the lowest GPU memory cost when the number of nodes is 10^4?",
    "answer": "SGFormer w/ Softmax",
    "rationale": "The plot on the right shows the GPU memory cost for each model as a function of the number of nodes. At 10^4 nodes, the line for SGFormer w/ Softmax is the lowest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.10759v3",
    "pdf_url": null
  },
  {
    "instance_id": "e0bcf6920b9c40c396788abed4ea2117",
    "figure_id": "2310.15342v2-Figure2-1",
    "image_file": "2310.15342v2-Figure2-1.png",
    "caption": " Illustration of the selection tensor decomposition",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the dimensions of the matrix U?",
    "answer": "m x d",
    "rationale": "The figure shows that the matrix U has m rows and d columns. This is also indicated by the label \"m x d\" above the matrix U.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.15342v2",
    "pdf_url": null
  },
  {
    "instance_id": "c0c062d5751e452b902a8ceaa453e424",
    "figure_id": "2105.14240v5-Figure2-1",
    "image_file": "2105.14240v5-Figure2-1.png",
    "caption": " Confusion matrix of robustness in the test set",
    "figure_type": "** Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which dataset has the highest accuracy? ",
    "answer": " MNIST. ",
    "rationale": " The confusion matrix for MNIST shows the highest values on the diagonal, indicating that the model is correctly classifying the images most of the time. The diagonal values represent the number of correctly classified images, while the off-diagonal values represent the number of misclassified images. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14240v5",
    "pdf_url": null
  },
  {
    "instance_id": "bb0116e217794457ae74ecab43e3c961",
    "figure_id": "2106.01282v2-Figure3-1",
    "image_file": "2106.01282v2-Figure3-1.png",
    "caption": " Bar chart showing the Gaussian cluster assignment of school classes over time. The height of each coloured bar represents the proportion of students, in that class and at that time, assigned to the corresponding Gaussian cluster, the total available height representing 100%. If the coloured bars do not sum to the full available height, the difference represents the proportion of inactive students. For legibility, only bars representing over 35% of the class are labelled with the cluster number.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "At what time on Day 1 was the proportion of inactive students in class 1A the highest?",
    "answer": "10:00",
    "rationale": "The figure shows that the total height of the colored bars for class 1A at 10:00 on Day 1 is less than the full available height. This means that some students were inactive at that time. The difference between the total height of the colored bars and the full available height represents the proportion of inactive students.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01282v2",
    "pdf_url": null
  },
  {
    "instance_id": "42de5b8da9a448a1ae2ae5ee7b7a6c22",
    "figure_id": "2010.00672v2-Figure1-1",
    "image_file": "2010.00672v2-Figure1-1.png",
    "caption": " Comparison of conventional XAI methods with SISE (our proposed) to demonstrate SISE’s ability to generate class discriminative explanations on a ResNet-50 model.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most effective in generating class discriminative explanations on a ResNet-50 model?",
    "answer": "SISE",
    "rationale": "The figure shows that SISE has the highest score for both the Horse and Person classes, indicating that it is the most effective method in generating class discriminative explanations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.00672v2",
    "pdf_url": null
  },
  {
    "instance_id": "28efe34adcbd4723a8ca6b074e8119a7",
    "figure_id": "2203.06474v2-Figure1-1",
    "image_file": "2203.06474v2-Figure1-1.png",
    "caption": " Amalgamated optimizer performance as measured by the best log validation loss and log training loss (lower is better) after 25 epochs; 95% confidence intervals are shown, and are estimated by a linear mixed effects model (Appendix D). In order to use a common y-axis, the validation loss is measured relative to the mean validation loss of the optimizer amalgamated from the large pool using optimal Choice amalgamation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer amalgamation strategy performs the best on the CIFAR-10 dataset?",
    "answer": "Choice (Large)",
    "rationale": "The figure shows that the Choice (Large) strategy has the lowest relative best log validation loss for the CIFAR-10 dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.06474v2",
    "pdf_url": null
  },
  {
    "instance_id": "76fbb30298bc4d1396eeba51398000ee",
    "figure_id": "2210.09520v6-Figure3-1",
    "image_file": "2210.09520v6-Figure3-1.png",
    "caption": " Result on dataset bias benchmarks. Left and center plots show the training domain and unseen domain performance of the zeroshot and fine-tuned baselines respectively. For both Colored MNIST (a) and Waterbirds (b), LADS is able to roughly match the unseen domain accuracy of zero-shot methods and the seen domain accuracy of fine-tuned methods, resulting in improved performance on the extended domain (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best on the extended domain for the Colored MNIST dataset?",
    "answer": "LADS",
    "rationale": "The rightmost plot in Figure (a) shows the performance of different methods on the extended domain for the Colored MNIST dataset. LADS has the highest accuracy of 87.54%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.09520v6",
    "pdf_url": null
  },
  {
    "instance_id": "25a75932b9a54ef794eddbd7b2bc0d3f",
    "figure_id": "2201.13052v1-Figure1-1",
    "image_file": "2201.13052v1-Figure1-1.png",
    "caption": " Left panel: rel-RMSE (17) as a function of CPU runtime for several IMC algorithms. Here X∗ has a condition number κ = 10. Right panel: runtime till convergence as a function of κ, where each point corresponds to the median of 50 independent realizations. In both panels, X∗ ∈ R1000×1000, A,B ∈ R20×20, r = 10 and oversampling ratio ρ = 1.5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the best performance in terms of both runtime and rel-RMSE?",
    "answer": "GNIMC.",
    "rationale": "The left panel of the figure shows that GNIMC has the fastest convergence rate and the lowest rel-RMSE. The right panel shows that GNIMC has the lowest runtime for all values of κ.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.13052v1",
    "pdf_url": null
  },
  {
    "instance_id": "1b814213d76c47918ff145e56aaab9b0",
    "figure_id": "2211.14503v1-Figure16-1",
    "image_file": "2211.14503v1-Figure16-1.png",
    "caption": " Rendering of the “statue” 3D scene SDF learned by the sinusoidal network from a point cloud.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the elephants and the women in the statue?",
    "answer": "The elephants are below the women in the statue.",
    "rationale": "The figure shows the elephants are located below the women on the statue.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.14503v1",
    "pdf_url": null
  },
  {
    "instance_id": "85128e5bf7a64a7c8721743de0b6bc23",
    "figure_id": "2012.05535v3-Figure2-1",
    "image_file": "2012.05535v3-Figure2-1.png",
    "caption": " The StyleGAN’s discriminator cannot distinguish the difference of high frequencies. The numbers are estimates of E[D(x)] by averaging 1,000 samples. Unless we change the spectrum over a large bandwidth, the outputs of the discriminator are roughly the same, which makes StyleGAN fail to reproduce spectral distribution.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the image when the high frequencies are multiplied by 0?",
    "answer": "The image becomes blurry.",
    "rationale": "The high frequencies in an image correspond to the details and edges. When these frequencies are multiplied by 0, the details and edges are removed, resulting in a blurry image. This is shown in the rightmost column of the figure, where the images have no high frequencies and are therefore blurry.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.05535v3",
    "pdf_url": null
  },
  {
    "instance_id": "837ab12b4d4e4f64bb84170d8774ccd7",
    "figure_id": "2111.13119v1-Figure18-1",
    "image_file": "2111.13119v1-Figure18-1.png",
    "caption": " Success rate of policies pre-trained on different combinations of state and change count rewards, with panoramic changes and random resets. States are always counted with egocentric views. C-BET (blue) outperforms other rewards, achieving the highest overall success rate and being the only transferring well to ObstructedMazes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which reward function performs the best on the DoorKey-8x8 environment?",
    "answer": "1 / √(N(s) + N(c))",
    "rationale": "The figure shows the success rate of different reward functions on different environments. The reward function with the highest success rate on the DoorKey-8x8 environment is 1 / √(N(s) + N(c)).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.13119v1",
    "pdf_url": null
  },
  {
    "instance_id": "36e33565c37f4ca8a69f616b579f36df",
    "figure_id": "2104.14403v2-Figure6-1",
    "image_file": "2104.14403v2-Figure6-1.png",
    "caption": " %Attr vs. test accuracy, more in App. B.4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most sensitive to changes in brightness?",
    "answer": "SmoothGrad",
    "rationale": "The SmoothGrad plot shows the largest change in test accuracy as the brightness changes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.14403v2",
    "pdf_url": null
  },
  {
    "instance_id": "b11acfc0d3b5439eb886a04b8408605f",
    "figure_id": "2103.12725v2-Figure6-1",
    "image_file": "2103.12725v2-Figure6-1.png",
    "caption": " Predictions and 90% confidence intervals for 8 held-out examples from the Cleveland Clinic Heart Disease dataset, and the observed outcomes, shown with the probabilities on the (a) absolute scale, and (b) logit scale.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which scale, absolute or logit, is better for visualizing the predictions and confidence intervals?",
    "answer": "It depends on the specific task.",
    "rationale": "The absolute scale is easier to interpret for most people, as it directly shows the probability of an event occurring. However, the logit scale can be helpful for visualizing predictions and confidence intervals when the probabilities are very close to 0 or 1. This is because the logit scale stretches out the ends of the probability range, making it easier to see small differences in probability.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.12725v2",
    "pdf_url": null
  },
  {
    "instance_id": "e499cacc2bf3482fa2fc63ea129ca7bd",
    "figure_id": "2110.10461v3-Figure13-1",
    "image_file": "2110.10461v3-Figure13-1.png",
    "caption": " Empirical CDFs of final test errors after training on larger-scale datasets for 50(a)/72(b) epochs from each of 100(a)/50(b) random initialisations of SGD hyperparameters. Notes as in Figure 9.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest test error on average for Fashion-MNIST using an MLP?",
    "answer": "Ours WD+HDLR+M",
    "rationale": "The empirical CDF of Ours WD+HDLR+M is the highest for all values of test error, meaning that it has the lowest test error on average.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.10461v3",
    "pdf_url": null
  },
  {
    "instance_id": "90334d184bf64d09bfd193ac7712e59b",
    "figure_id": "1911.00077v1-Figure1-1",
    "image_file": "1911.00077v1-Figure1-1.png",
    "caption": " Example real images from the CUB-200-2011 data set [9]. Upper row: Examples from the White Pelican class. Lower row: Examples from the Carolina Wren, Canada Warbler, Rufous Hummingbird, Blue Jay, and Red Headed Woodpecker classes in the order given.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which bird is the smallest in the image?",
    "answer": "The Rufous Hummingbird.",
    "rationale": "The Rufous Hummingbird is the smallest bird in the image. It is much smaller than the other birds, such as the pelican and the blue jay.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.00077v1",
    "pdf_url": null
  },
  {
    "instance_id": "5648cce770dd420782c1ad1573a4f68c",
    "figure_id": "1806.10758v3-Figure6-1",
    "image_file": "1806.10758v3-Figure6-1.png",
    "caption": " Certain transformations of the estimate can substantially improve accuracy of all estimators. Squaring alone provides small gains to the accuracy of all estimators, and is slightly better than a random guess. Left inset: The three base estimators that we consider (Gradients (GRAD), Integrated Gradients (IG) and Guided Backprop (GB)) perform worse than a random assignment of feature importance. At all fractions considered, a random assignment of importance degrades performance more than removing the pixels estimated to be most important by base methods. Right inset: Average test-set accuracy across 5 independent iterations for estimates that are squared before ranking and subsequent removal. When squared, base estimators perform slightly better than a random guess. However, this does not compare to the gains in accuracy of averaging a set of noisy estimates that are squared (SmoothGrad-Squared)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three base estimators performs the best when squared?",
    "answer": "GRAD",
    "rationale": "The figure shows that when squared, GRAD performs slightly better than a random guess, while the other two base estimators perform worse than a random guess.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.10758v3",
    "pdf_url": null
  },
  {
    "instance_id": "e98a4bcb94b84b3d9d860b6ca5497639",
    "figure_id": "2307.08286v2-Figure17-1",
    "image_file": "2307.08286v2-Figure17-1.png",
    "caption": " Comparison of Distcom, DistW , and DistH . The activation matching is used to obtain two modes that satisfy LLFC, θA and θB . The results are presented for different layers of MLP on the MNIST dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which distance metric generally leads to the lowest distance between the two modes that satisfy LLFC?",
    "answer": "Dist_h",
    "rationale": "The figure shows that the bars for Dist_h are generally shorter than the bars for the other two distance metrics, which indicates that Dist_h generally leads to a lower distance between the two modes that satisfy LLFC.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.08286v2",
    "pdf_url": null
  },
  {
    "instance_id": "a787ccf4edbf48c2a16dee4c21c408df",
    "figure_id": "2010.15011v3-Figure6-1",
    "image_file": "2010.15011v3-Figure6-1.png",
    "caption": " Additional simulation results. For each scenario we show a boxplot representing the RMSE values obtained over 50 repetitions using CleaneX (left box, orange), regression based method (middle box, blue) and KDE (right box, purple). The boxplots for the KDE method are not shown when all obtained RMSE values are higher that 0.07. The boxes extend from the lower to the upper quartile values, with a line at the median; whiskers show values at a distance of at most 1.5 IQR (interquartile range) from the lower and the upper quartiles; outliers are not shown.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best when the noise level is high and the data is not normally distributed?",
    "answer": "CleaneX",
    "rationale": "The boxplots show that CleaneX has the lowest RMSE values when the noise level is high (σ2 = 0.6 and σ2 = 0.9) and the data is not normally distributed (Y ~ u, X|Y ~ N^2 and Y ~ u, X|Y ~ u).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.15011v3",
    "pdf_url": null
  },
  {
    "instance_id": "e4ec73dc8e514cb7bf79ee5ad5be67bf",
    "figure_id": "1906.05370v1-Figure1-1",
    "image_file": "1906.05370v1-Figure1-1.png",
    "caption": " In NGE, several mutation operations are allowed. By using Policy Sharing, child species reuse weights from parents, even if the graphs are different. The same color indicates shared and reused weights. For better visualization, we only plot the sharing of propagation model (yellow curves).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which mutation operation is responsible for adding a new node to the graph?",
    "answer": "Add-Node",
    "rationale": "The figure shows four mutation operations: Add-Node, Add-Graph, Del-Graph, and Pert-Graph. The Add-Node operation is responsible for adding a new node to the graph. This can be seen in the bottom left corner of the figure, where a new node is added to the graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.05370v1",
    "pdf_url": null
  },
  {
    "instance_id": "9c655e8e71894699b8063eecd566a38e",
    "figure_id": "2012.06979v1-Figure56-1",
    "image_file": "2012.06979v1-Figure56-1.png",
    "caption": " Choices of ψ: MUSK (k = 20). Top: full experiment. Bottom: Zoom in.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four normalization methods (NORM1, NORM2, NORM-INF, RANDOM) results in the lowest mutual information gap for a given budget?",
    "answer": "NORM-INF.",
    "rationale": "The figure shows that the NORM-INF curve is consistently below the other curves, indicating that it has the lowest mutual information gap for a given budget.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.06979v1",
    "pdf_url": null
  },
  {
    "instance_id": "b1dede314a494784901acd1740aafb65",
    "figure_id": "2204.10546v1-Figure1-1",
    "image_file": "2204.10546v1-Figure1-1.png",
    "caption": " Pruning Method Comparison on MobilenetV1 VWW task. Parameters are reduced by 93% for our method (frozen tail), 68.9% for depth pruning (dense), and 20% for magnitude pruning if maximum accuracy drop is set to 0.65%.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pruning method is the most efficient in terms of accuracy and parameter reduction?",
    "answer": "The depth-aux (Ours) method.",
    "rationale": "The figure shows that the depth-aux method achieves the highest accuracy while also reducing the number of parameters by the largest amount.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.10546v1",
    "pdf_url": null
  },
  {
    "instance_id": "6f22dd48fddf486394ebdc727483ca5c",
    "figure_id": "1811.11829v2-Figure2-1",
    "image_file": "1811.11829v2-Figure2-1.png",
    "caption": " Learning with non-DC regularizers (right two) on different datasets for classification and regression.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization algorithm performs the best for the non-linear least square + l_0, real-sim task?",
    "answer": "SSDC-AdaGrad",
    "rationale": "The figure shows the convergence of different optimization algorithms on the non-linear least square + l_0, real-sim task. The SSDC-AdaGrad algorithm converges to the lowest objective function value, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.11829v2",
    "pdf_url": null
  },
  {
    "instance_id": "f425630f115a46b7928f8c1717c13fa6",
    "figure_id": "2210.00093v1-Figure4-1",
    "image_file": "2210.00093v1-Figure4-1.png",
    "caption": " Being agnostic and/or reactive to dynamic adaptation undermines efficiency while proactive scheduling minimizes makespan and maximizes efficiency.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which scheduling approach leads to the shortest makespan?",
    "answer": "Proactive scheduling.",
    "rationale": "The figure shows that proactive scheduling (d) results in the shortest makespan, which is the time it takes to complete all jobs. In this case, the makespan is 6 time units.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.00093v1",
    "pdf_url": null
  },
  {
    "instance_id": "9bab78fd040049219b02b3dfbaf1ea48",
    "figure_id": "2009.03300v3-Figure15-1",
    "image_file": "2009.03300v3-Figure15-1.png",
    "caption": " An Anatomy example.",
    "figure_type": "Other.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pharyngeal arches contribute to the formation of the hyoid bone?",
    "answer": "The first and second pharyngeal arches.",
    "rationale": "The hyoid bone is a U-shaped bone located in the neck, just below the mandible. It serves as an attachment point for several muscles involved in swallowing and speech. The hyoid bone is derived from the first and second pharyngeal arches during embryonic development.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.03300v3",
    "pdf_url": null
  },
  {
    "instance_id": "22927fdf03da4fe292327220f9d14796",
    "figure_id": "1902.03227v1-Figure5-1",
    "image_file": "1902.03227v1-Figure5-1.png",
    "caption": " Fragile recognition images (FRIs) in ImageNet. FRIs are generally less frequent for larger regions of the image. (a) Loose shrink FRIs, which indicate the general fragility of DNNs to a slight reduction in the visible region. (b) Strict shrink FRIs, which are the equivalent to human minimal images. Shift FRIs also follow this pattern (see Figure A.1).",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \nWhich type of FRIs is less frequent in general? Loose shrink or strict shrink? ",
    "answer": " Strict shrink FRIs are less frequent in general. ",
    "rationale": " The figure shows that the percentage of strict shrink FRIs is lower than the percentage of loose shrink FRIs for all DNNs and all values of P. This means that strict shrink FRIs are less frequent than loose shrink FRIs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.03227v1",
    "pdf_url": null
  },
  {
    "instance_id": "7cf8b59a650c4a58bb1aad9d5d05b7cf",
    "figure_id": "2007.14313v2-Figure3-1",
    "image_file": "2007.14313v2-Figure3-1.png",
    "caption": " LFR and RDF for sin(kπx) vs. 1/δ. Note that we normalize RDF in (b) by the maximal value of each curve for visualization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the LFR change with increasing k?",
    "answer": "The LFR increases with increasing k.",
    "rationale": "The figure shows that the LFR curves for different values of k are all increasing with increasing 1/δ. Additionally, the curves for larger values of k are higher than the curves for smaller values of k. This indicates that the LFR increases with increasing k.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.14313v2",
    "pdf_url": null
  },
  {
    "instance_id": "e56c9ae8fc4748348931a457346812f8",
    "figure_id": "2207.11152v1-Figure3-1",
    "image_file": "2207.11152v1-Figure3-1.png",
    "caption": " Results of grouping study",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in the low-priced group?",
    "answer": "HALOP",
    "rationale": "The figure shows that HALOP had the highest extra return in the low-priced group.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.11152v1",
    "pdf_url": null
  },
  {
    "instance_id": "32df4b72d0414977b30ff5ada3f20f67",
    "figure_id": "2212.09535v3-Figure5-1",
    "image_file": "2212.09535v3-Figure5-1.png",
    "caption": " Sentence retrieval accuracy for Russian before and after adaptation with MAD-X adapters and continued pretraining.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of sentence retrieval accuracy for Russian after adaptation?",
    "answer": "bloom-7b1",
    "rationale": "The figure shows that the blue line representing bloom-7b1 has the highest peak in terms of retrieval accuracy after adaptation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.09535v3",
    "pdf_url": null
  },
  {
    "instance_id": "dd00cfcce1c340f087a94ec5672ed3c6",
    "figure_id": "1909.07521v2-Figure2-1",
    "image_file": "1909.07521v2-Figure2-1.png",
    "caption": " Information about the semantic fragments considered in this paper, where the top four fragments test basic logic (Logic Fragments) and the last fragment covers monotonicity reasoning (Mono. Fragment).",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which fragment has the largest vocabulary size?",
    "answer": "SNLI+MNLI",
    "rationale": "The table shows that the SNLI+MNLI fragment has a vocabulary size of 101,110, which is larger than any other fragment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.07521v2",
    "pdf_url": null
  },
  {
    "instance_id": "8a73dd3618a74c318e37081f4b991fea",
    "figure_id": "2008.09777v4-Figure18-1",
    "image_file": "2008.09777v4-Figure18-1.png",
    "caption": " Scatter plots of the predicted performance against the true performance of different surrogate models on the test set in a Leave-One-Optimizer-Out setting.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which surrogate model has the highest correlation between predicted and true performance?",
    "answer": "XGB",
    "rationale": "The XGB plot shows the tightest clustering of points around the diagonal line, indicating a stronger correlation between predicted and true performance compared to the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.09777v4",
    "pdf_url": null
  },
  {
    "instance_id": "f68c9d986ab349d0bcc9440b83bb8bb8",
    "figure_id": "1805.11604v5-Figure12-1",
    "image_file": "1805.11604v5-Figure12-1.png",
    "caption": " Evaluation of deep linear networks trained with different `p normalization strategies. We observe that networks with any normalization strategy have improved performance and smoothness of the loss landscape over standard training. Details of the plots are the same as Figure 11 above.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which normalization strategy resulted in the smoothest loss landscape?",
    "answer": "BatchNorm",
    "rationale": "The figure shows the loss landscape for four different normalization strategies. The loss landscape for BatchNorm is the smoothest, which can be seen by the fact that it has the fewest peaks and valleys.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.11604v5",
    "pdf_url": null
  },
  {
    "instance_id": "a256631279794665929951646c8dc53a",
    "figure_id": "1908.07615v1-Figure2-1",
    "image_file": "1908.07615v1-Figure2-1.png",
    "caption": " Convergence of ILQR, regularized ILQR and accelerated regularized ILQR on the inverted pendulum (top) and two-link arm (bottom) control problems for an horizon τ = 100.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges the fastest?",
    "answer": "Accelerated Regularized ILQR",
    "rationale": "The plots show that the function values and gradient norm for Accelerated Regularized ILQR decrease to the smallest values in the least number of iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.07615v1",
    "pdf_url": null
  },
  {
    "instance_id": "9f13cf26a5a841eba430d9a65621734a",
    "figure_id": "1904.00993v2-Figure6-1",
    "image_file": "1904.00993v2-Figure6-1.png",
    "caption": " Top: original input from MatterPort3D [2] scene classification task. Bottom: our set of 12 overlapping views.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many views are in the original image?",
    "answer": "12.",
    "rationale": "The caption states that the bottom row of images shows the 12 overlapping views.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.00993v2",
    "pdf_url": null
  },
  {
    "instance_id": "3674053718e84923bc78a1f508cb52c8",
    "figure_id": "1812.06611v1-Figure7-1",
    "image_file": "1812.06611v1-Figure7-1.png",
    "caption": " The results of VGG-16 on ImageNet with four different speed-up configurations, as well as the comparison between theoretical and practical speed-up.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does the theoretical speed-up match the practical speed-up on the GPU?",
    "answer": "No.",
    "rationale": "The figure shows that the theoretical speed-up on the GPU is always higher than the practical speed-up. This is likely due to overhead associated with running on the GPU, such as data transfer time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.06611v1",
    "pdf_url": null
  },
  {
    "instance_id": "8a6e66d0dc774c1292b3693e65dd48df",
    "figure_id": "2104.03310v1-Figure5-1",
    "image_file": "2104.03310v1-Figure5-1.png",
    "caption": " Zero-centered gradient penalty values. We visualize the values of zero-centered gradient penalty (GP0) [48] during the training stage. The proposed regularization also constrains the values without explicitly minimizing the GP-0 loss.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method shows the most stable and lowest GP-0 values when trained on both the full dataset and 20% of the data?",
    "answer": "BigGAN + R_Lc (Ours)",
    "rationale": "The figure shows the GP-0 values for three different methods: BigGAN, BigGAN + GP-0, and BigGAN + R_Lc (Ours). We can see that the BigGAN + R_Lc (Ours) method has the most stable and lowest GP-0 values for both the full dataset and 20% of the data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.03310v1",
    "pdf_url": null
  },
  {
    "instance_id": "4225ddb0ad9c4650a821f06c8b728ae3",
    "figure_id": "2106.03058v3-Figure5-1",
    "image_file": "2106.03058v3-Figure5-1.png",
    "caption": " Comparison with GBP, PPRGo and ClusterGCN.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the -Amazon dataset?",
    "answer": "AGP",
    "rationale": "The figure shows the accuracy of different methods on the -Amazon dataset. AGP has the highest accuracy for most of the time range.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03058v3",
    "pdf_url": null
  },
  {
    "instance_id": "17b1b33509694bd4a1d7985b21c6b366",
    "figure_id": "2205.00943v2-Figure4-1",
    "image_file": "2205.00943v2-Figure4-1.png",
    "caption": " Average training hours on the Cartpole-Swingup task by different models.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model took the least amount of time to train on the Cartpole-Swingup task?",
    "answer": "SAC-Pixel.",
    "rationale": "The figure shows the average training hours for each model on the Cartpole-Swingup task. The bar for SAC-Pixel is the shortest, indicating that it took the least amount of time to train.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.00943v2",
    "pdf_url": null
  },
  {
    "instance_id": "30a0fda6ca5d4340b6ad0c82b3fa8cfe",
    "figure_id": "1811.00225v3-Figure3-1",
    "image_file": "1811.00225v3-Figure3-1.png",
    "caption": " SVCCA score between representations at each epoch and from the final trained LM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which representation achieves the highest SVCCA score?",
    "answer": "h2",
    "rationale": "The figure shows that the red line, which represents h2, is the highest of the three lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.00225v3",
    "pdf_url": null
  },
  {
    "instance_id": "34f8eea23de34e70be61d35026001732",
    "figure_id": "2211.12051v3-Figure5-1",
    "image_file": "2211.12051v3-Figure5-1.png",
    "caption": " Visual comparisons of various methods for color gaussian image denoising. The noisy image is corrupted by additive white gaussian noise (AWGN) with noise level σ = 50.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most effective at removing noise from the image?",
    "answer": "ADFNet",
    "rationale": "The ADFNet image is the clearest and has the least amount of noise. The other methods, while they do remove some noise, still leave the image looking somewhat blurry or distorted.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.12051v3",
    "pdf_url": null
  },
  {
    "instance_id": "848721cc4a564e72a12d50745861c6ec",
    "figure_id": "2012.09456v2-Figure2-1",
    "image_file": "2012.09456v2-Figure2-1.png",
    "caption": " The estimated Q values for our methods SM2, and comparison methods. The mean and a standard deviation are shown across 5 independent runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest estimated Q value for Catcher-PLE?",
    "answer": "SM2",
    "rationale": "The plot shows the estimated Q values for different methods on the Catcher-PLE environment. The SM2 line is the highest for this environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.09456v2",
    "pdf_url": null
  },
  {
    "instance_id": "43d298a257b146adb97854c352739ee0",
    "figure_id": "2003.01515v1-Figure6-1",
    "image_file": "2003.01515v1-Figure6-1.png",
    "caption": " Relative improvement on commercial objectives (30% traffic).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which objective saw the most relative improvement on January 14th?",
    "answer": "Objective 2",
    "rationale": "The blue line, which represents the relative improvement on Objective 2, is higher than the brown line, which represents the relative improvement on Objective 1, on January 14th.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.01515v1",
    "pdf_url": null
  },
  {
    "instance_id": "8ac76cfd6b5946b4affaf0efb064631d",
    "figure_id": "2203.06026v3-Figure12-1",
    "image_file": "2203.06026v3-Figure12-1.png",
    "caption": " Mean images and heatmaps of regions that are the most important for FID with StyleGAN2 images in FFHQ that get classified to some class, e.g., “lipstick”. The heatmaps highlight the regions of Top-1 classes that are typically located outside the face area.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What regions of the face are most important for FID with StyleGAN2 images in FFHQ that get classified to some class, e.g., “lipstick”?",
    "answer": "The regions outside the face area.",
    "rationale": "The heatmaps in the figure show that the regions outside the face area are the most important for FID with StyleGAN2 images in FFHQ that get classified to some class, e.g., “lipstick”. This suggests that the classifier is relying on features outside the face area to make its predictions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.06026v3",
    "pdf_url": null
  },
  {
    "instance_id": "843a780ab9be43dea0a4891d2436fae2",
    "figure_id": "2104.08691v2-Figure4-1",
    "image_file": "2104.08691v2-Figure4-1.png",
    "caption": " Parameter usage of various adaptation techniques, fixing architecture to T5.1.1 and prompt/prefix length to 1–100 tokens (bands show mean and stddev). Model Tuning: All parameters are task-specific. Prefix Tuning: Activations are tuned in the prefix of each layer, requiring 0.1–1% task-specific parameters for inference, but more are used for training. WARP: Task parameters are reduced to under 0.1% by only tuning input and output layers. Prompt Tuning: Only prompt embeddings are tuned, reaching under 0.01% for most model sizes. Prompt Design: Only a sequence of prompt IDs (500–2000 tokens) is required.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which adaptation technique uses the least number of task-specific parameters?",
    "answer": "Prompt Design.",
    "rationale": "The figure shows that Prompt Design requires only a sequence of prompt IDs (500-2000 tokens), which is significantly less than the other adaptation techniques.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.08691v2",
    "pdf_url": null
  },
  {
    "instance_id": "b49e2c3e07b542eca5f0c25b930ddbfb",
    "figure_id": "2106.02938v4-Figure6-1",
    "image_file": "2106.02938v4-Figure6-1.png",
    "caption": " First column: Change of predicted probabilities when removing features. The decoupling error is included in the legend. Last three columns: waterfall plots of feature importance from Variational Index, Shapley and Banzhaf.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature is the most important for predicting income based on the Variational Index, Shapley and Banzhaf methods?",
    "answer": "Capital Gain.",
    "rationale": "The waterfall plots show that the feature with the largest positive impact on the predicted probability is Capital Gain for all three methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02938v4",
    "pdf_url": null
  },
  {
    "instance_id": "e71ebad4168f4f6fad6f9912f422970b",
    "figure_id": "1905.12480v1-Figure3-1",
    "image_file": "1905.12480v1-Figure3-1.png",
    "caption": " Effectiveness of word- and review-level attentions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Yelp14 dataset?",
    "answer": "The word-level attention model.",
    "rationale": "The figure shows the mean squared error (MSE) for different models on three different datasets. The lower the MSE, the better the model performs. On the Yelp14 dataset, the word-level attention model has the lowest MSE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12480v1",
    "pdf_url": null
  },
  {
    "instance_id": "c0f2c50ddb6d42c8865e9d9222942b41",
    "figure_id": "2308.13133v1-Figure6-1",
    "image_file": "2308.13133v1-Figure6-1.png",
    "caption": " Visual quality comparisons on CVO dataset. Two small objects with large motions are emphasized with red boxes. More results can be found in the supplementary.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most accurate in tracking the small object with large motions?",
    "answer": "Acc+GMA (ours)",
    "rationale": "The figure shows that Acc+GMA (ours) is the most accurate in tracking the small object with large motions, as it is the closest to the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.13133v1",
    "pdf_url": null
  },
  {
    "instance_id": "12a8e67a4ff8427eb9bccb110b5973d7",
    "figure_id": "2302.00849v1-Figure4-1",
    "image_file": "2302.00849v1-Figure4-1.png",
    "caption": " Classification results for CIFAR-10 dataset with various network architectures with combinations of (h, β) chosen such that the effective learning rate h (1−β) remains same. In all of the experiments, external regularization like weight-decay, l.r scheduler, dropout,label-smoothing are kept off (except Batch-normalization). The results have been averaged over 3 random seeds having different initializations. (SGD+M) has a) higher test accuracy for increasing β than (SGD) confirming Remark 5.4 b) Less variance for test accuracy confirming Remark 5.3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture achieved the highest test accuracy for increasing beta?",
    "answer": "WideresNet-16-8",
    "rationale": "The figure shows that the test accuracy for WideresNet-16-8 is higher than the other network architectures for increasing beta.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.00849v1",
    "pdf_url": null
  },
  {
    "instance_id": "3d605f345ce04c98b495895a3d192f55",
    "figure_id": "2007.15220v1-Figure1-1",
    "image_file": "2007.15220v1-Figure1-1.png",
    "caption": " Hardness Reduction from Label Cover to L∞-margin Halfspace Learning.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the probability that the oracle O will output a sample with label +1?",
    "answer": "0.75",
    "rationale": "The oracle O will output a sample with label +1 in three cases: \n\n1. With probability 0.25, it outputs the sample 2γ∗⋅e with label +1. \n\n2. With probability 0.25, it outputs the sample eT×Σv−(1k2γ∗)e with label +1. \n\n3. With probability 0.25×(1−q), it outputs the labeled sample (|S|k+2γ∗)⋅e−es|Π|, with label +1. \n\nTherefore, the total probability that the oracle O will output a sample with label +1 is 0.25 + 0.25 + 0.25 × (1 − q) = 0.75.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.15220v1",
    "pdf_url": null
  },
  {
    "instance_id": "39dbbc90909e4c99b2b01341208cf90b",
    "figure_id": "2103.00112v3-Figure2-1",
    "image_file": "2103.00112v3-Figure2-1.png",
    "caption": " Performance comparison of the representative visual backbone networks on ImageNet.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest accuracy on ImageNet with the fewest parameters?",
    "answer": "ResNet.",
    "rationale": "The figure shows the accuracy of different models on ImageNet as a function of the number of parameters (a) and the number of FLOPS (b). ResNet achieves the highest accuracy with the fewest parameters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.00112v3",
    "pdf_url": null
  },
  {
    "instance_id": "0d61c6db25f24d649f749d75fb59cb3a",
    "figure_id": "2106.04399v2-Figure5-1",
    "image_file": "2106.04399v2-Figure5-1.png",
    "caption": " Number of diverse Bemis-Murcko scaffolds found above reward threshold T as a function of the number of molecules seen. Left, T = 7.5. Right, T = 8.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm found the most diverse Bemis-Murcko scaffolds with a reward threshold of 8?",
    "answer": "GFlowNet",
    "rationale": "The figure shows the number of diverse Bemis-Murcko scaffolds found by each algorithm as a function of the number of molecules seen. The right panel shows the results for a reward threshold of 8. GFlowNet is the blue line, and it is the highest line in the right panel, indicating that it found the most diverse Bemis-Murcko scaffolds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04399v2",
    "pdf_url": null
  },
  {
    "instance_id": "cfc913590c284342824d53444a1fc6bb",
    "figure_id": "2211.16386v1-Figure9-1",
    "image_file": "2211.16386v1-Figure9-1.png",
    "caption": " NeRF-Synthetic scenes. We show a random view for each scene in the dataset, comparing ground truth with our VQ-DVGO, VQ-Plenoxels, VQ-TensoRF.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, VQ-DVGO, VQ-Plenoxels, or VQ-TensorRF, produces images that are most similar to the ground truth?",
    "answer": "VQ-Plenoxels.",
    "rationale": "The images produced by VQ-Plenoxels are visually very similar to the ground truth images, while the images produced by the other two methods have some noticeable differences.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.16386v1",
    "pdf_url": null
  },
  {
    "instance_id": "ca1cd684c7674acfac0b51e83c8808ec",
    "figure_id": "1905.04270v1-Figure5-1",
    "image_file": "1905.04270v1-Figure5-1.png",
    "caption": " Different models’ loss visualizations: model with higher robustness demonstrates more smooth and stable geometry.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is the most robust?",
    "answer": "The MinMax Training Model.",
    "rationale": "The caption states that \"model with higher robustness demonstrates more smooth and stable geometry.\" The MinMax Training Model has the smoothest and most stable geometry, as shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.04270v1",
    "pdf_url": null
  },
  {
    "instance_id": "f52687a6d14c486790e811abed0b3236",
    "figure_id": "2205.01588v1-Figure3-1",
    "image_file": "2205.01588v1-Figure3-1.png",
    "caption": " The screenshot of the SparCAssist user interface",
    "figure_type": "Screenshot of a user interface.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the sentiment of the review of the movie To End All Wars, as predicted by the model in the SparCAssist user interface?",
    "answer": "Positive.",
    "rationale": "The sentiment of the review is predicted to be positive because the model in the SparCAssist user interface displays a happy face emoji next to the prediction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.01588v1",
    "pdf_url": null
  },
  {
    "instance_id": "1a1e7a1d562145cb9150c5cf9536072c",
    "figure_id": "2109.07319v2-Figure1-1",
    "image_file": "2109.07319v2-Figure1-1.png",
    "caption": " INCEPTIONXML(+) (Ours) hits the sweet spot in terms of performance on the P@1 metric, training time, model size and inference times.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best P@1 score and the shortest training time?",
    "answer": "Giga-FLOPS",
    "rationale": "The figure shows that Giga-FLOPS has the highest P@1 score and the shortest training time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.07319v2",
    "pdf_url": null
  },
  {
    "instance_id": "d4d8efeb53274bacb22edd4a0627753e",
    "figure_id": "2004.03548v2-Figure1-1",
    "image_file": "2004.03548v2-Figure1-1.png",
    "caption": " Visual tempo variation of intra- and inter-class. The action examples above show that people tend to act at different tempos even for the same action. The plot below shows different action categories sorted by their variances of visual tempos. Specifically Somersaulting has the largest variance in the visual tempo of its instances while Shearing sheep has the smallest variance. Details of variation measurements can be found in the experiment section.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which action category has the most variation in visual tempo?",
    "answer": "Somersaulting.",
    "rationale": "The plot shows that Somersaulting has the highest variance in visual tempo, as indicated by the height of the bar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.03548v2",
    "pdf_url": null
  },
  {
    "instance_id": "765bafd143ee4ccea0f54ef0914dc32e",
    "figure_id": "2203.15712v2-Figure4-1",
    "image_file": "2203.15712v2-Figure4-1.png",
    "caption": " N -way 1-shot FS-CS performance comparison of four methods by varying N from 1 to 5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods has the lowest classification error rate for N=3?",
    "answer": "PANet",
    "rationale": "The left-hand plot shows the classification error rate for each of the four methods. For N=3, the blue bar representing PANet is the shortest, indicating the lowest error rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.15712v2",
    "pdf_url": null
  },
  {
    "instance_id": "79c92a4c010346ffa6ecc77836cefce8",
    "figure_id": "2003.00651v1-Figure6-1",
    "image_file": "2003.00651v1-Figure6-1.png",
    "caption": " Qualitative comparison of the proposed model with other state-of-the-art methods. Obviously, saliency maps generated by our approach are more accurate and much close to the ground truth in various challenging scenarios.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods in the figure produced the most accurate saliency maps?",
    "answer": "Ours",
    "rationale": "The saliency maps generated by \"Ours\" are visually the most similar to the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.00651v1",
    "pdf_url": null
  },
  {
    "instance_id": "49521a12f1d942ba8eb91af8860d576e",
    "figure_id": "2111.05393v1-Figure17-1",
    "image_file": "2111.05393v1-Figure17-1.png",
    "caption": " Qualitative comparisons of DyMON and GSWM on DR0-|fv|. Top: reconstruction performance. Bottom: segmentation performance (we observe that DyMON outperforms GSWM in segmenting scenes).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods (GT, DyMON, or GSWM) is most accurate in segmenting the scene?",
    "answer": "DyMON",
    "rationale": "The bottom row of the figure shows the segmentation performance of each method. DyMON's segmentation is the most accurate, as it correctly identifies the different objects in the scene.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.05393v1",
    "pdf_url": null
  },
  {
    "instance_id": "d410083ea89345898be241ee38034012",
    "figure_id": "2005.12210v1-Figure3-1",
    "image_file": "2005.12210v1-Figure3-1.png",
    "caption": " Performance comparison when some % of the reviews are randomly masked.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the Instant Video dataset when 0% of the reviews are kept?",
    "answer": "D-CoNN",
    "rationale": "The figure shows the performance of different methods on the Instant Video dataset when different percentages of the reviews are kept. When 0% of the reviews are kept, D-CoNN has the lowest MSE, indicating the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.12210v1",
    "pdf_url": null
  },
  {
    "instance_id": "e046203a1fce4ff48a1a00009eafb2b4",
    "figure_id": "2209.15145v1-Figure6-1",
    "image_file": "2209.15145v1-Figure6-1.png",
    "caption": " Per-group coverage of BatchMVP (left) and BatchGCP (right) on a representative run.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the most groups that meet the target coverage?",
    "answer": "BatchMVP",
    "rationale": "The figure shows that BatchMVP has more groups with bars above the target coverage line than BatchGCP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.15145v1",
    "pdf_url": null
  },
  {
    "instance_id": "1066773f9f234a7eab608e0a9de61100",
    "figure_id": "2004.01174v1-Figure1-1",
    "image_file": "2004.01174v1-Figure1-1.png",
    "caption": " The events of Watching a sad movie, Eating popcorn, and Crying, may highly co-occur in a hypothetical corpus. What distinguishes valid event pair inferences (event pairs linked in a commensense scenario; noted by checkmarks above) versus invalid inferences (noted by a ‘X’)?",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following event pairs is most likely to co-occur in a commonsense scenario based on the figure?\na) Watching a sad movie and crying\nb) Eating popcorn and crying\nc) Watching a sad movie and eating popcorn",
    "answer": "a) Watching a sad movie and crying",
    "rationale": "The figure shows that watching a sad movie and crying are both valid event pairs, meaning they are likely to co-occur in a commonsense scenario. Eating popcorn and crying are not shown to be a valid event pair, as the figure indicates that this is an invalid inference.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.01174v1",
    "pdf_url": null
  },
  {
    "instance_id": "b0844037c46146a7b4756a43e96577ba",
    "figure_id": "2107.00833v1-Figure6-1",
    "image_file": "2107.00833v1-Figure6-1.png",
    "caption": " The distribution of average lifts (a notion of agency) over users. Colors indicate different user action spaces for LibFM (left) and KNN (right) on ML-1M.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model (LibFM or KNN) and user action space combination has the highest average user lift?",
    "answer": "LibFM with the \"Next K\" user action space.",
    "rationale": "The violin plot for \"Next K\" in the LibFM figure shows a higher distribution of average lifts compared to the other two user action spaces. The KNN figure shows lower average lifts for all user action spaces.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.00833v1",
    "pdf_url": null
  },
  {
    "instance_id": "355191ed813d4f76abba09f1b65acf83",
    "figure_id": "2012.14905v4-Figure19-1",
    "image_file": "2012.14905v4-Figure19-1.png",
    "caption": " Short horizon bias.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the difference in performance between VSML and MetaRNN on the MNIST dataset?",
    "answer": "VSML outperforms MetaRNN on the MNIST dataset.",
    "rationale": "The figure shows the cumulative accuracy of different algorithms on the MNIST dataset. VSML achieves a higher cumulative accuracy than MetaRNN for all values of \"Total examples seen.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.14905v4",
    "pdf_url": null
  },
  {
    "instance_id": "555bf09acef0441d9c94c860705818d1",
    "figure_id": "2307.13226v2-Figure4-1",
    "image_file": "2307.13226v2-Figure4-1.png",
    "caption": " A toy example to illustrate the TensoRF-CP with global decomposition in (left) axis-aligned and (right) nonaxis-aligned situations. The bottom shows the grid values. In axis-aligned case, only 1 component is needed to represent the scene (vector bases recover grid values by outer product). In non-axis-aligned case, however, 3 components are needed because the rank of matrix changes from 1 to 3 after scene rotation. While our design with local low-rank tensors can alleviate this issue.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many components are needed to represent the scene in the non-axis-aligned case?",
    "answer": "3",
    "rationale": "The caption states that \"In non-axis-aligned case, however, 3 components are needed because the rank of matrix changes from 1 to 3 after scene rotation.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.13226v2",
    "pdf_url": null
  },
  {
    "instance_id": "f89049fca73f4f1eb84a2e74622f5e84",
    "figure_id": "1811.11304v2-Figure1-1",
    "image_file": "1811.11304v2-Figure1-1.png",
    "caption": " A universal perturbation made using a subset of ImageNet and the VGG-16 architecture. When added to the validation images, their labels usually change. The perturbation was generated using the proposed algorithm 2. Perturbation pixel values lie in [−10, 10] (i.e. ε = 10).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the labels of the validation images when the universal perturbation is added to them?",
    "answer": "Their labels usually change.",
    "rationale": "The caption states that \"When added to the validation images, their labels usually change.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.11304v2",
    "pdf_url": null
  },
  {
    "instance_id": "fa2d023ee76c4a869f217e37ce3e44cd",
    "figure_id": "2005.04551v1-Figure9-1",
    "image_file": "2005.04551v1-Figure9-1.png",
    "caption": " Visualizations of the matching results along the epipolar line in an easy case in Human3.6M [13]. We here use E.T. as a shorthand for epipolar transformer. The compared features are (a) deep features learned through the epipolar transformer (deep features with E.T., denoted in red), (b) deep feature learned by ResNet-50[12] without epipolar transformer (deep features w/o E.T., denoted in yellow), and (c) RGB features (denoted in blue). Green dot on the reference view is the selected joint, and the green dot on the source view is the corresponding point offered by the groundtruth.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature performs the best in this example?",
    "answer": "Deep feature with E.T.",
    "rationale": "The figure shows the matching results along the epipolar line for three different features: deep feature with E.T., deep feature w/o E.T., and RGB. The green dot on the reference view is the selected joint, and the green dot on the source view is the corresponding point offered by the groundtruth. The deep feature with E.T. has the highest peak at the green dot on the source view, indicating that it performs the best in this example.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.04551v1",
    "pdf_url": null
  },
  {
    "instance_id": "85d9a80056424020b275cd8f7b898a6e",
    "figure_id": "2211.06401v1-Figure2-1",
    "image_file": "2211.06401v1-Figure2-1.png",
    "caption": " Distribution of 15 most frequently appearing emojis in our dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which emoji appears most frequently in the dataset?",
    "answer": "Folded hands emoji",
    "rationale": "The folded hands emoji has the highest bar in the bar chart, which means it has the highest percentage of occurrence among the top 15 emojis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.06401v1",
    "pdf_url": null
  },
  {
    "instance_id": "dbae15c3994d4c4f9c313bc2e8d73db2",
    "figure_id": "1902.02530v1-Figure2-1",
    "image_file": "1902.02530v1-Figure2-1.png",
    "caption": " Noise and despeckled Flevoland images. We trained supervised model with L = 4, which is same as the number of look of image.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the best noise reduction?",
    "answer": "DoPAMINEEB_AFT",
    "rationale": "The DoPAMINEEB_AFT image has the least amount of noise and the most detail. This is evident in the zoomed-in areas, where the individual fields can be seen more clearly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.02530v1",
    "pdf_url": null
  },
  {
    "instance_id": "51c6b7ee908247849250778e6b3ab77b",
    "figure_id": "1811.07484v3-Figure5-1",
    "image_file": "1811.07484v3-Figure5-1.png",
    "caption": " The comparison of attention maps from different VGG19 [33] layers. Ours has less attention shift than Grad-CAM [9]. In the marked areas, ours attends to the target objects, i.e. bird, while Grad-CAM [9] tends to highlight the background pixels.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attention map has less attention shift than Grad-CAM?",
    "answer": "Ours has less attention shift than Grad-CAM.",
    "rationale": "The caption states that \"Ours has less attention shift than Grad-CAM.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.07484v3",
    "pdf_url": null
  },
  {
    "instance_id": "345beb7406b141c5bcfa23e6dffc9f31",
    "figure_id": "2109.15044v1-Figure5-1",
    "image_file": "2109.15044v1-Figure5-1.png",
    "caption": " Selected samples for Turbulent Flows dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model produced the most realistic samples?",
    "answer": "The real samples are the most realistic.",
    "rationale": "The real samples are actual images of turbulent flows, while the other samples are generated by different models. The models are trained to produce images that are as realistic as possible, but they are not perfect.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.15044v1",
    "pdf_url": null
  },
  {
    "instance_id": "faf7579dcea4423d86d01549ad694916",
    "figure_id": "2309.15701v2-Figure5-1",
    "image_file": "2309.15701v2-Figure5-1.png",
    "caption": " Top-10 word frequencies in N-best hypotheses and ground-truth transcription of CHiME-4 test set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the top 3 most frequent words in the N-best hypotheses?",
    "answer": "\"the\", \"and\", \"to\"",
    "rationale": "The bar chart on the left shows the word frequency of the N-best hypotheses. The top 3 most frequent words are \"the\", \"and\", and \"to\", as their bars are the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.15701v2",
    "pdf_url": null
  },
  {
    "instance_id": "943eb1a86819493f995b0e7bdf26ce7f",
    "figure_id": "2207.11770v1-Figure5-1",
    "image_file": "2207.11770v1-Figure5-1.png",
    "caption": " Visual comparison using 15s training clip for different training iterations.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the sharpest image after 40k iterations?",
    "answer": "Ours (Fine-tuned)",
    "rationale": "The images in the rightmost column are the sharpest and most realistic, especially after 40k iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.11770v1",
    "pdf_url": null
  },
  {
    "instance_id": "da4ea94c31524a1caa552af1110e46fa",
    "figure_id": "2010.15327v2-FigureD.3-1",
    "image_file": "2010.15327v2-FigureD.3-1.png",
    "caption": "Figure D.3: Block structure emerges in shallower networks when trained on less data (CIFAR-100). We plot CKA similarity heatmaps as we increase network depth (going right along each row) and also decrease the size of training data (down each column). Similar to the observation made in Figure 2, as a result of the increased model capacity (with respect to the task) from smaller dataset size, smaller (shallower) models now also exhibit the block structure.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network exhibits the block structure most prominently in all three data regimes?",
    "answer": "ResNet-224 1x",
    "rationale": "The block structure is most visible in the heatmaps for ResNet-224 1x, regardless of the amount of training data used. This is evident by the presence of bright squares along the diagonal of the heatmaps, which indicate high CKA similarity between layers within the same block.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.15327v2",
    "pdf_url": null
  },
  {
    "instance_id": "bba021b19530445eb2394c2314301cbf",
    "figure_id": "2109.01583v1-Figure2-1",
    "image_file": "2109.01583v1-Figure2-1.png",
    "caption": " Metric trend per epoch on MTOP w/ or w/o instance relabeling strategy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does instance relabeling help to improve the exact match accuracy of the model?",
    "answer": "Yes.",
    "rationale": "The figure shows the exact match accuracy of the model with and without instance relabeling. The accuracy of the model with relabeling is higher than the accuracy of the model without relabeling.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.01583v1",
    "pdf_url": null
  },
  {
    "instance_id": "a184bf02a80b4199b49789a38add2299",
    "figure_id": "2302.09815v1-Figure1-1",
    "image_file": "2302.09815v1-Figure1-1.png",
    "caption": " The proof diagram for Theorems 1-4.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which Lemmas are used to prove Theorem 4?",
    "answer": "Lemmas 6, 7, and 8.",
    "rationale": "The figure shows that Theorem 4 is supported by Lemmas 6, 7, and 8. The arrows pointing from the Lemmas to the Theorem indicate that the Lemmas are used to prove the Theorem.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.09815v1",
    "pdf_url": null
  },
  {
    "instance_id": "3dcc002a295c48b4acf2fd3bdc79266f",
    "figure_id": "2102.07764v2-Figure2-1",
    "image_file": "2102.07764v2-Figure2-1.png",
    "caption": " High level schematics of the ESM-integrated network architectures ESMN-RGB and ESMN, as well as other baseline architectures used in the experiments: Mono, LSTM and NTM.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the network architectures depicted in the figure utilizes depth information in addition to RGB images?",
    "answer": "ESMN-RGB",
    "rationale": "The figure shows that ESMN-RGB takes both RGB and depth images as input, while ESMN only takes RGB images as input.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.07764v2",
    "pdf_url": null
  },
  {
    "instance_id": "9266fafc3f4a4cfdb0c53e693301289d",
    "figure_id": "1905.01240v1-Figure13-1",
    "image_file": "1905.01240v1-Figure13-1.png",
    "caption": " Results for box pushing tasks with quadruped. Starting from left, first: move one box to one of 2 targets with go to another. Second: move one box to 1 target. Third: move one box to one of 2 targets. Forth: move one box to one of 3 targets. The legends denote additional to the proprioception, information passed to the default policy (except baseline, where we do not use default policy).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the tasks was the most difficult for the quadruped to learn?",
    "answer": "Move one box to one of 3 targets.",
    "rationale": "The figure shows that the \"Target+box\" condition, which corresponds to the task of moving one box to one of 3 targets, took the longest for the quadruped to learn. This is evident from the fact that the curve for this condition rises more slowly than the curves for the other conditions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.01240v1",
    "pdf_url": null
  },
  {
    "instance_id": "44b97948baaa4a6eb607ece96c641cf5",
    "figure_id": "2107.11646v2-Figure8-1",
    "image_file": "2107.11646v2-Figure8-1.png",
    "caption": " Quantitative Evaluation. The figures demonstrate the 2D PCK of the re-projected hand pose on HIU-data, IOU PCK of the re-projected hand mask on HIU-data, 2D PCK of the re-projected hand pose on CMU, 3D per-vertex PCK on FreiHAND, respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the HIU-data set?",
    "answer": "Ours (AUC=0.794)",
    "rationale": "The 2D PCK on HIU-Data plot shows that our method has the highest AUC value of 0.794, which indicates the best performance among the compared methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.11646v2",
    "pdf_url": null
  },
  {
    "instance_id": "8ec3b8c1704947aa902c117d3dbfbca6",
    "figure_id": "2302.11831v1-Figure8-1",
    "image_file": "2302.11831v1-Figure8-1.png",
    "caption": " Distribution analysis of the UHD-LL dataset. Pixel intensity histograms of images for low-noise (green) and normal-clear (blue) from the training and test partitions are shown in (a) and (b), suggesting their consistent pixel intensity distributions. (c) The whole dataset covers a wide range of SNR distribution ranging from 1 to 30 and the SNR values center at the range of [0,10], indicating the challenging noise levels.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has a wider range of SNR distribution?",
    "answer": "The whole UHD-LL dataset.",
    "rationale": "Figure (c) shows the SNR distribution for the whole UHD-LL dataset, which ranges from 1 to 30. In contrast, Figures (a) and (b) show the intensity distributions for the training and test partitions of the dataset, which are much narrower.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.11831v1",
    "pdf_url": null
  },
  {
    "instance_id": "9f7ef6ca0c1141be973b041fb8cb7ca7",
    "figure_id": "2201.09635v4-Figure2-1",
    "image_file": "2201.09635v4-Figure2-1.png",
    "caption": " Learning curves of SAGA and baselines on all environments. Each curve and its shaded region represent average episode reward (for Ant Gather) or average success rate (for the rest; see the appendix) and 95% confidence interval respectively, averaged over 10 independent trials. We find that SAGA performs well across all tasks. It is worth noting that SAGA learns rapidly; on the complex navigation tasks it normally requires only less than three million environment steps to achieve good performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the Ant Gather task?",
    "answer": "SAGA",
    "rationale": "The figure shows the learning curves of different algorithms on the Ant Gather task. The y-axis shows the reward, and the x-axis shows the number of time steps. The SAGA curve is the highest, indicating that it achieves the highest reward.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.09635v4",
    "pdf_url": null
  },
  {
    "instance_id": "2a12ad0dae3242819592388b18df1c0a",
    "figure_id": "2211.16928v2-Figure7-1",
    "image_file": "2211.16928v2-Figure7-1.png",
    "caption": " 4× visual comparison on real-world SR competition benchmarks (NTIRE2020 Track2, LR images of which come straight from the smartphone camera).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm produced the most realistic results in the 4x visual comparison on real-world SR competition benchmarks?",
    "answer": "KDSR5_GAN",
    "rationale": "The figure shows that the KDSR5_GAN algorithm produced the most realistic results in the 4x visual comparison on real-world SR competition benchmarks. This is because the KDSR5_GAN images are the most similar to the original images, and they have the least amount of artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.16928v2",
    "pdf_url": null
  },
  {
    "instance_id": "23d7d5f9fba54e0aab269b7d9aa73cad",
    "figure_id": "2005.04114v4-Figure5-1",
    "image_file": "2005.04114v4-Figure5-1.png",
    "caption": " Evaluation for negation. We show the accuracy difference with BERT w/ Mean pooling.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best when there are two negation words in the sentence?",
    "answer": "SentiBERT w/o Attention to Children",
    "rationale": "The plot shows the delta accuracy of different models with respect to the number of negation words in the sentence. The SentiBERT w/o Attention to Children model has the highest delta accuracy when there are two negation words in the sentence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.04114v4",
    "pdf_url": null
  },
  {
    "instance_id": "3abcaee33b0b4647b555b9286b0b6922",
    "figure_id": "2308.13133v1-Figure7-1",
    "image_file": "2308.13133v1-Figure7-1.png",
    "caption": " Average EPE ↓ (ALL) of long-range flows from the compared methods in different estimation ranges.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of average end-point error for long-range flows?",
    "answer": "AccFlow",
    "rationale": "The figure shows the average end-point error (AEPE) for different methods and estimation ranges. AccFlow has the lowest AEPE for all estimation ranges, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.13133v1",
    "pdf_url": null
  },
  {
    "instance_id": "107604643caa4a4f83f65215f840a774",
    "figure_id": "2210.08909v1-Figure5-1",
    "image_file": "2210.08909v1-Figure5-1.png",
    "caption": " OOD SC curves in the training process.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods performs the best in terms of OOD SC?",
    "answer": "DKT",
    "rationale": "The figure shows that DKT has the highest OOD SC values throughout the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.08909v1",
    "pdf_url": null
  },
  {
    "instance_id": "0b5b2221367745888579f4cfecdf301c",
    "figure_id": "1811.07662v2-Figure5-1",
    "image_file": "1811.07662v2-Figure5-1.png",
    "caption": " Examples of recalls for a single object category. T1∼T10 denotes CGO results with different numbers of selected objects. GT denotes the statistical results of the ground truth caption labels. B denotes the results of the base model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object category has the highest recall for the base model?",
    "answer": "Elephant",
    "rationale": "The bar chart shows the recall for each object category for different models. The base model is represented by the blue bar. The blue bar for the elephant category is the tallest, indicating that the base model has the highest recall for this category.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.07662v2",
    "pdf_url": null
  },
  {
    "instance_id": "a49961f4481a438985f0698e4bce564e",
    "figure_id": "2106.04174v1-Figure4-1",
    "image_file": "2106.04174v1-Figure4-1.png",
    "caption": " The Key Attribute Tree generated by HIF+KATXGB for D-A1 dataset.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature is more important for predicting whether a paper is relevant to a query, the author or the title?",
    "answer": "The author is more important.",
    "rationale": "The figure shows that the L2 distance between the authors of a paper and the query is used to split the data into two groups, while the L2 distance between the title of a paper and the query is used to further split one of those groups. This indicates that the author is a more important feature for predicting relevance than the title.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04174v1",
    "pdf_url": null
  },
  {
    "instance_id": "5b2d1e13523a44f48ea9fb5271b6d946",
    "figure_id": "2201.09079v1-Figure1-1",
    "image_file": "2201.09079v1-Figure1-1.png",
    "caption": " Graphical illustration of the recovered normal vectors of S by (left) the proposed DPCPPSGM approach and (right) methods that use spectral initialization and impose orthogonality constraints . Initial vectors b01, b 0 2, b 0 3 are randomly initialized and are non-orthogonal in (left) and spectrally initialized (hence orthogonal) in (right). Note that in (left) rank(B̂∗) (where B̂∗ = [b∗1, b ∗ 2, b ∗ 3]) equals to the true codimension c = 2 of S and span(B∗) ≡ S⊥ while in (right) B∗ is orthogonal hence rank(B∗) = 3 with b∗2 ∈ S.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is able to recover the normal vectors of S with the correct codimension?",
    "answer": "The proposed DPCPPSGM approach.",
    "rationale": "The caption states that \"in (left) rank(B̂∗) (where B̂∗ = [b∗1, b ∗ 2, b ∗ 3]) equals to the true codimension c = 2 of S and span(B∗) ≡ S⊥ while in (right) B∗ is orthogonal hence rank(B∗) = 3 with b∗2 ∈ S.\" This means that the DPCPPSGM approach (left) correctly recovers the normal vectors with the correct codimension, while the methods that use spectral initialization and impose orthogonality constraints (right) do not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.09079v1",
    "pdf_url": null
  },
  {
    "instance_id": "6aa712e7fc28403eb69f50794b0e28bc",
    "figure_id": "2102.10769v3-Figure6-1",
    "image_file": "2102.10769v3-Figure6-1.png",
    "caption": " Learning curves tracking the running maximum averaged across seeds comparing MobILE against BC, BC-O, ILPO, GAIL and GAIFO. MobILE tends to reach expert performance consistently and in a more sample efficient manner.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods shown in the figure reaches expert performance the fastest?",
    "answer": "MobILE",
    "rationale": "The figure shows learning curves for several methods, with MobILE shown in red. The learning curves show how the performance of each method improves as the number of online samples increases. MobILE reaches expert performance (indicated by the horizontal lines) faster than any of the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.10769v3",
    "pdf_url": null
  },
  {
    "instance_id": "3cdf208ea3e645eca3e76e3a2348a0d4",
    "figure_id": "2310.18884v1-Figure5-1",
    "image_file": "2310.18884v1-Figure5-1.png",
    "caption": " The testing performance varying decay rate λ. More results are given in Appendix C.7.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest accuracy?",
    "answer": "Cora",
    "rationale": "The figure shows that the accuracy for Cora is higher than the accuracy for Pubmed and Squirrel.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.18884v1",
    "pdf_url": null
  },
  {
    "instance_id": "ff68a889b198451aaaac1784427ec782",
    "figure_id": "2005.09669v2-Figure10-1",
    "image_file": "2005.09669v2-Figure10-1.png",
    "caption": "Fig 10. Second stage of the experiment. In this stage, we treat the 1000 samples from the first stage of the experiment as burn-in and look at the performance of the next 1000 samples. Left: We plot the logarithm of the norm of the running mean versus iteration. Right: We again display the corresponding samples.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four algorithms, NLA, MLA, ULA, or TULA, appears to converge to the stationary distribution the fastest?",
    "answer": "TULA.",
    "rationale": "The left plot shows the logarithm of the norm of the running mean versus iteration for each algorithm. TULA has the lowest value at the end of the iterations, which indicates that it converges to the stationary distribution the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.09669v2",
    "pdf_url": null
  },
  {
    "instance_id": "c984f6538b9e472abab42b912bcf2fa2",
    "figure_id": "1905.03684v3-Figure1-1",
    "image_file": "1905.03684v3-Figure1-1.png",
    "caption": " Let h1, h2, h3 denote the 1st, 2nd, and 3rd blocks of a 16-layer WideResNet and Ji the Jacobian of the output w.r.t layer i. In log-scale we plot a histogram of the 100 largest values on the training set of ∑3 i=1 ‖hi‖‖Ji‖/γ for a WideResNet trained with and without Batchnorm on CIFAR10, where γ is the example’s margin.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method resulted in a higher proportion of large values for the sum of the product of the layer norms and Jacobian norms?",
    "answer": "Batchnorm",
    "rationale": "The blue bars, which represent the Batchnorm training method, are generally higher than the orange bars, which represent the No BN training method. This indicates that the Batchnorm training method resulted in a higher proportion of large values for the sum of the product of the layer norms and Jacobian norms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.03684v3",
    "pdf_url": null
  },
  {
    "instance_id": "3f01c92fdde442c4b55efb7423256e23",
    "figure_id": "2212.10879v1-Figure4-1",
    "image_file": "2212.10879v1-Figure4-1.png",
    "caption": " Left: Hierarchical clustering based on crosslinguistic syntactic difference derived from mBERT. Right: The gold phylogenetic tree from Glottolog (Hammarström et al., 2021). IE stands for the IndoEuropean family.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language family is most closely related to the Indo-Iranian family according to the hierarchical clustering based on crosslinguistic syntactic difference derived from mBERT?",
    "answer": "The Balto-Slavic family.",
    "rationale": "The figure shows that the Balto-Slavic and Indo-Iranian families are both branches of the Indo-European family, and they are the two closest branches to each other.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10879v1",
    "pdf_url": null
  },
  {
    "instance_id": "ebf14b2c33734d528af5707084160c5a",
    "figure_id": "1906.02059v1-Figure1-1",
    "image_file": "1906.02059v1-Figure1-1.png",
    "caption": " Attention over words (colored words) and facts (vertical heat bars) as produced by HAN.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the predicted violation in this case?",
    "answer": "Article 3",
    "rationale": "The predicted violation is listed at the top of the document.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02059v1",
    "pdf_url": null
  },
  {
    "instance_id": "bf80a1c89a1140779c7bdd02d84df414",
    "figure_id": "1911.09879v2-Figure5-1",
    "image_file": "1911.09879v2-Figure5-1.png",
    "caption": " Average ROC curves for VAR datasets",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best for 30% sparsity and T = 500?",
    "answer": "cMLP",
    "rationale": "The ROC curve for cMLP is the closest to the top left corner, which indicates that it has the highest true positive rate (TPR) for any given false positive rate (FPR).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.09879v2",
    "pdf_url": null
  },
  {
    "instance_id": "7415bdbe48ab45c99d3d6364c5050755",
    "figure_id": "2104.01136v2-Figure1-1",
    "image_file": "2104.01136v2-Figure1-1.png",
    "caption": " Speed-accuracy operating points for convolutional and visual transformers. Left plots: on 1 CPU core, Right: on 1 GPU. LeViT is a stack of transformer blocks, with pooling steps to reduce the resolution of the activation maps as in classical convolutional architectures.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best trade-off between speed and accuracy on a CPU?",
    "answer": "LeViT-384.",
    "rationale": "The figure shows the speed-accuracy trade-off for different models on a CPU and a GPU. LeViT-384 achieves the highest accuracy (around 83%) while still maintaining a relatively high speed (around 10 images/s) on a CPU.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.01136v2",
    "pdf_url": null
  },
  {
    "instance_id": "7fdcc790508e4fd8bfc455aee650cfc6",
    "figure_id": "2109.07726v2-Figure2-1",
    "image_file": "2109.07726v2-Figure2-1.png",
    "caption": " A visualization of the cosine distance matrix of the hyperbolic sentence “I’ve drowned myself trying to help you”.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which word in the sentence is most similar to the word \"drowned\"?",
    "answer": "\"myself\"",
    "rationale": "The cosine distance between \"drowned\" and \"myself\" is the smallest (0.00), indicating that the two words are the most similar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.07726v2",
    "pdf_url": null
  },
  {
    "instance_id": "41ada583dacd4f898db1ae2614b23ab2",
    "figure_id": "2012.05400v1-Figure5-1",
    "image_file": "2012.05400v1-Figure5-1.png",
    "caption": " In two cross-domain datasets, KITTI to Cityscapes and Cityscapes to Foggy Cityscapes, the ratio of true positives and false positives to the entire ground truth are counted in different confidence intervals. The confidence of target domain data is directly predicted by the pre-trained model from source domain. False negatives (<0.0), which are difficult to box out even when the confidence threshold is set to zero, are found to dominate in the noisy labels.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has a higher rate of false negatives?",
    "answer": "Cityscapes to Foggy Cityscapes",
    "rationale": "The red bars in the figure represent the false negative rate. The red bar in the Cityscapes to Foggy Cityscapes plot is much higher than the red bar in the KITTI to Cityscapes plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.05400v1",
    "pdf_url": null
  },
  {
    "instance_id": "a6144ddefe1c4794b7e4d764816f4de8",
    "figure_id": "2302.07317v3-Figure9-1",
    "image_file": "2302.07317v3-Figure9-1.png",
    "caption": " VOC",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest Mean Average Precision (MAP) at 10,000 labels?",
    "answer": "TAILOR Div.",
    "rationale": "The plot in (a) shows the MAP for different algorithms as a function of the number of labels. TAILOR Div. has the highest MAP at 10,000 labels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.07317v3",
    "pdf_url": null
  },
  {
    "instance_id": "5f356d4bd4cc4f12a96bbeeb3704fa1d",
    "figure_id": "2210.01883v2-Figure2-1",
    "image_file": "2210.01883v2-Figure2-1.png",
    "caption": " Samples from the positive-pair Markov chain for MNIST k-pixel-subsampling augmentations at three strengths (k = 10, 20, 50). At each step we condition on an augmentation at (middle row) to sample an uncorrupted example zt (top row), then sample at+1 from zt so that (at, at+1) is a positive pair. Below, we plot the five slowest-varying eigenfunctions f1, . . . , f5 at each step of the chain, labeled with their eigenvalues λ1, . . . , λ5. Weaker augmentations lead to slower mixing, smoother eigenfunctions, and eigenvalues closer to 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the strength of the augmentations affect the mixing of the positive-pair Markov chain?",
    "answer": "Weaker augmentations lead to slower mixing.",
    "rationale": "The figure shows that the eigenvalues of the slowest-varying eigenfunctions are closer to 1 for weaker augmentations. This indicates that the chain is mixing more slowly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.01883v2",
    "pdf_url": null
  },
  {
    "instance_id": "0575ef60162f4fb9a2e71949579b9fe0",
    "figure_id": "2212.10229v4-Figure32-1",
    "image_file": "2212.10229v4-Figure32-1.png",
    "caption": " Few-shot training results for Dogs with 10 shots.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods generated the most realistic images?",
    "answer": "Pretrained and Real",
    "rationale": "The images in the Pretrained and Real rows look the most realistic, while the images in the other rows look more cartoonish or distorted.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10229v4",
    "pdf_url": null
  },
  {
    "instance_id": "05c3bda405e3435ba597233abe7107c4",
    "figure_id": "1909.09569v3-Figure16-1",
    "image_file": "1909.09569v3-Figure16-1.png",
    "caption": " Test loss curves of NASNet and its variants on CIFAR-10 suring training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which NASNet variant has the lowest test loss on CIFAR-10?",
    "answer": "C12nasnet",
    "rationale": "The figure shows the test loss curves of NASNet and its variants on CIFAR-10 during training. C12nasnet has the lowest test loss at the end of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.09569v3",
    "pdf_url": null
  },
  {
    "instance_id": "e167a39863f547b79a3474a7ea3294be",
    "figure_id": "1912.01196v3-Figure8-1",
    "image_file": "1912.01196v3-Figure8-1.png",
    "caption": " Effect of number of stacks and scale factor.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which combination of scale and number of stacks yields the best results according to the PSNR and SSIM metrics?",
    "answer": "7S stacks and 2x scale factor.",
    "rationale": "The table shows that for a 2x scale factor, using 7S stacks results in the highest PSNR and SSIM values (16.42 and 0.600 respectively).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.01196v3",
    "pdf_url": null
  },
  {
    "instance_id": "b0ed5a43f8104f0eaf89a6211e33d2d2",
    "figure_id": "2105.13495v2-Figure4-1",
    "image_file": "2105.13495v2-Figure4-1.png",
    "caption": " Analysis of spatio-temporal attention for working memory task of the task decoding experiment. (a) Plot of average temporal attention matrix Z(k) time across subjects. (b) Proportion of statistically significant regions within the 7 ICNs from the spatial attention GLM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which brain region showed the most significant difference in spatial attention between the different tasks?",
    "answer": "The DMN region in Layer 1.",
    "rationale": "The bar plot in (b) shows the proportion of statistically significant regions within each ICN for each layer. The DMN bar in Layer 1 is the tallest, indicating that this region showed the most significant difference in spatial attention between the different tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.13495v2",
    "pdf_url": null
  },
  {
    "instance_id": "16f5fa6b59ee4f53b9b4533426a25986",
    "figure_id": "2304.00101v1-Figure6-1",
    "image_file": "2304.00101v1-Figure6-1.png",
    "caption": " Limitation. Accuracy (%) vs. speed (ms) comparison with different methods on balanced CIFAR-100. SuperDisco has little impact on the performance of balanced datasets at the expense of increased inference time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest accuracy on the balanced CIFAR-100 dataset?",
    "answer": "Meta-SuperDisco",
    "rationale": "The plot shows the accuracy and inference time for three different methods: ResNet, SuperDisco, and Meta-SuperDisco. The x-axis shows the inference time in milliseconds, and the y-axis shows the accuracy in percentage. The data points for each method are represented by different colored triangles. The highest point on the plot is the red triangle, which represents Meta-SuperDisco. This indicates that Meta-SuperDisco achieves the highest accuracy of 70.6% on the balanced CIFAR-100 dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.00101v1",
    "pdf_url": null
  },
  {
    "instance_id": "48ac35920e434134aa367272c1bbf89c",
    "figure_id": "2305.06984v3-Figure4-1",
    "image_file": "2305.06984v3-Figure4-1.png",
    "caption": " Percentage of high-level failure modes for each evaluation method on NQ-OPEN.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which evaluation method has the highest percentage of symbolic equation errors?",
    "answer": "GPT4-eval",
    "rationale": "The figure shows that GPT4-eval has the highest percentage of symbolic equation errors, at approximately 50%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.06984v3",
    "pdf_url": null
  },
  {
    "instance_id": "6462bba5146d4ac2947bedc99c4e477f",
    "figure_id": "1911.07959v2-Figure7-1",
    "image_file": "1911.07959v2-Figure7-1.png",
    "caption": " Qualitative results of failure cases for each tracker under eight challenge factors. Best viewed in color.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tracker is most likely to fail when the target object is occluded by other objects?",
    "answer": "SiamRPN++",
    "rationale": "The figure shows qualitative results of failure cases for each tracker under eight challenge factors. One of the challenge factors is occlusion (OCC). The figure shows that SiamRPN++ fails to track the target object when it is occluded by other objects.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.07959v2",
    "pdf_url": null
  },
  {
    "instance_id": "5beafae56a90426cbdca4fa6a6896070",
    "figure_id": "2201.08024v1-Figure4-1",
    "image_file": "2201.08024v1-Figure4-1.png",
    "caption": " Impact of unclicked samples’ size.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better in terms of AUC when the number of clicks and unclicks are equal?",
    "answer": "AUCCtcvr",
    "rationale": "When the ratio of clicks to unclicks is 1:1, the AUCCtcvr line is higher than the D-AUCCtcvr line in both the Click-Adaptive Teacher and Base Student plots. This indicates that AUCCtcvr performs better in terms of AUC when the number of clicks and unclicks are equal.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.08024v1",
    "pdf_url": null
  },
  {
    "instance_id": "ac036f998f8746a185277222c1d03d5c",
    "figure_id": "2008.00325v3-Figure2-1",
    "image_file": "2008.00325v3-Figure2-1.png",
    "caption": " Google-News Results showing runtime (y-axis) as more of the dataset is sampled (x-axis).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two implementations of UMAP is faster for larger datasets?",
    "answer": "cuML UMAP is faster for larger datasets.",
    "rationale": "The figure shows that the runtime of UMAP-learn increases more rapidly than that of cuML UMAP as the number of data samples increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.00325v3",
    "pdf_url": null
  },
  {
    "instance_id": "6e5a8b66f6fd46f7bfc9647da66baa0e",
    "figure_id": "2211.13975v3-Figure4-1",
    "image_file": "2211.13975v3-Figure4-1.png",
    "caption": " The results of final client sampling counts on FashionMNIST-YMF-0.9 and Cifar10-LN-0.5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which client sampling method resulted in the most uniform distribution of client IDs in FashionMNIST-YMF-0.9?",
    "answer": "Uniform",
    "rationale": "The figure shows the distribution of client IDs for each client sampling method. The Uniform method has the most even distribution of client IDs, as indicated by the relatively flat histogram.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.13975v3",
    "pdf_url": null
  },
  {
    "instance_id": "4bc4791cdd6040c687b65113f771822a",
    "figure_id": "2211.10772v4-Figure5-1",
    "image_file": "2211.10772v4-Figure5-1.png",
    "caption": " Visualizations on Total-Text, IC15 and CTW1500.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the images shows a restaurant?",
    "answer": "The image on the far right.",
    "rationale": "The image on the far right shows a sign that says \"Welcome to Colyton Restaurant.\" The other images show a brewery and a restaurant, but they are not signs for restaurants.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.10772v4",
    "pdf_url": null
  },
  {
    "instance_id": "6138dae23638461c94c572e6dfc47bea",
    "figure_id": "2209.06129v1-Figure6-1",
    "image_file": "2209.06129v1-Figure6-1.png",
    "caption": " Comparison of the averaged rewards on real-world datasets using Hier-LinUCB, LinUCB and ConUCB. The averaged reward of Hier-LinUCB grows faster than ConUCB on Yelp and LastFM, while slightly slower than ConUCB on MovieLens at the early stage. This is likely due to the large number of key-terms for MovieLens.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the Yelp dataset?",
    "answer": "Hier-LinUCB",
    "rationale": "The plot for the Yelp dataset shows that Hier-LinUCB has the highest averaged reward, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.06129v1",
    "pdf_url": null
  },
  {
    "instance_id": "1b6e2cbc2c184982a074b6fc60003be8",
    "figure_id": "2112.08619v3-Figure7-1",
    "image_file": "2112.08619v3-Figure7-1.png",
    "caption": " First question from pre-generated question. The workers are given the first question",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Where is this place?",
    "answer": "This place is the FIA Formula Two Championship.",
    "rationale": "The image shows a racing track and cars racing. The caption says \"FIA_Formula_Two_Championship.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.08619v3",
    "pdf_url": null
  },
  {
    "instance_id": "1fa1c9a7ec29463ca33a418e0b11a158",
    "figure_id": "1905.06817v1-Figure6-1",
    "image_file": "1905.06817v1-Figure6-1.png",
    "caption": " Robustness of RingNet to varying lighting conditions. Images from the MultiPIE dataset [14].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does RingNet perform under different lighting conditions?",
    "answer": "RingNet is robust to varying lighting conditions.",
    "rationale": "The figure shows that RingNet can accurately reconstruct a 3D face model from images taken under different lighting conditions. The top row of the figure shows three input images of the same person taken under different lighting conditions. The bottom row of the figure shows the corresponding 3D face models reconstructed by RingNet. The reconstructed models are all very similar, despite the differences in the input images. This suggests that RingNet is able to handle variations in lighting well.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.06817v1",
    "pdf_url": null
  },
  {
    "instance_id": "f54dbbb9e3f14ddfa555ba4517678eff",
    "figure_id": "2110.04429v2-Figure3-1",
    "image_file": "2110.04429v2-Figure3-1.png",
    "caption": " Learning curves of SCDL and other baselines about F1 score vs. training iterations on CoNLL03.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest F1 score?",
    "answer": "SCDL (Ours)",
    "rationale": "The figure shows the F1 score of each model over the course of training. The SCDL (Ours) model has the highest F1 score at the end of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.04429v2",
    "pdf_url": null
  },
  {
    "instance_id": "9b3ef87b91a2401d8fff468c7463a47c",
    "figure_id": "2001.08779v1-Figure8-1",
    "image_file": "2001.08779v1-Figure8-1.png",
    "caption": " Perceptual Realism Plot for human survey (section 4.4). The blue and red dots represent the threshold and the number of people fooled for each question respectively. Here every question has different number of responses and hence the threshold for each question is varying. Also, we are only providing the plot for 50 of 100 questions involved in the survey.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which question had the highest number of people fooled?",
    "answer": "Question 47.",
    "rationale": "The red line in the figure represents the number of people fooled for each question. The highest point on the red line corresponds to question 47.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.08779v1",
    "pdf_url": null
  },
  {
    "instance_id": "a7d7baabafa8445b949706ec5c3ad746",
    "figure_id": "2211.05955v2-Figure2-1",
    "image_file": "2211.05955v2-Figure2-1.png",
    "caption": " Distributions of entity types in each language.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the highest percentage of entities that are classified as \"Person\"?",
    "answer": "Hindi",
    "rationale": "The figure shows that the \"Person\" category is represented by the light blue bar. The light blue bar for Hindi is the tallest, indicating that Hindi has the highest percentage of entities that are classified as \"Person\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.05955v2",
    "pdf_url": null
  },
  {
    "instance_id": "805daac8fa4c40f787f05bbba5486805",
    "figure_id": "2306.10315v1-Figure7-1",
    "image_file": "2306.10315v1-Figure7-1.png",
    "caption": " The ratio of the test dialogue history where its distance between history and (history, golden response) is smaller than the one between history and (history, random response). Larger numbers denote better results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed better on MW0Z?",
    "answer": "FutureTOD",
    "rationale": "The bar for FutureTOD is the tallest in the MW0Z plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.10315v1",
    "pdf_url": null
  },
  {
    "instance_id": "9a1ae14471d44faca05d757283a57dfc",
    "figure_id": "2105.02091v2-Figure2-1",
    "image_file": "2105.02091v2-Figure2-1.png",
    "caption": " Sankey plots showing the distribution of ground-truth (left) and inferred (right) demographic traits for five algorithms. The algorithms tend to mis-classify minorities as Whites. DeepFace tends to mis-classify Women as Men.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is the most accurate at inferring race/ethnicity?",
    "answer": "EthCNN",
    "rationale": "The Sankey plots for EthCNN show that the inferred distribution of race/ethnicity is closest to the ground-truth distribution. This is especially true for White and Black.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.02091v2",
    "pdf_url": null
  },
  {
    "instance_id": "d1b99882f5b24d8db443f2668e70e0f5",
    "figure_id": "2110.09419v2-Figure14-1",
    "image_file": "2110.09419v2-Figure14-1.png",
    "caption": " Sort-of-CLEVR Retrieval Activation. Activation statistics against the different types of questions in the dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of question has the highest activation for Retrieval 1?",
    "answer": "Unary questions.",
    "rationale": "The color of the square in the top left corner of the heatmap is the darkest, indicating the highest activation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.09419v2",
    "pdf_url": null
  },
  {
    "instance_id": "51ff91f27e7d4d888822b1bb141e3169",
    "figure_id": "2206.03753v2-Figure6-1",
    "image_file": "2206.03753v2-Figure6-1.png",
    "caption": " User Study. This donut chart highlights the user preferences collected through the user study. A majority of the participants preferred the results produced by the proposed method.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method was preferred by the most participants in the user study?",
    "answer": "Ours",
    "rationale": "The donut chart shows that 41.1% of participants preferred the results produced by the \"Ours\" method, which is the largest percentage of any of the methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.03753v2",
    "pdf_url": null
  },
  {
    "instance_id": "92eb51cf3f5d4dfeaba0cf665daab32b",
    "figure_id": "2303.17548v1-Figure3-1",
    "image_file": "2303.17548v1-Figure3-1.png",
    "caption": " Group representativeness scoresRG m of LMs as a function of political ideology and income: A higher score (lighter) indicates that, on average across dataset questions, the LMs opinion distribution is more similar to that of survey respondents from the specified group (i.e.,RG m(Q) is larger). The coloring is normalized by column to highlight the groups a given model (column) is most/least aligned to. We find that the demographic groups with the highest representativeness shift from base LM (moderate to conservative with low income) to the RLHF trained ones (liberal and high income). Other demographic categories are shown in Appendix 8.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which demographic groups are most represented by the RLHF trained language models?",
    "answer": "Liberal and high income.",
    "rationale": "The figure shows that the demographic groups with the highest representativeness shift from base LM (moderate to conservative with low income) to the RLHF trained ones (liberal and high income).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.17548v1",
    "pdf_url": null
  },
  {
    "instance_id": "ce2dcc13763a42569dbae0cf91a2f6ef",
    "figure_id": "2210.00107v2-Figure9-1",
    "image_file": "2210.00107v2-Figure9-1.png",
    "caption": " Original version and augmentations of French horn images with their class predictions (top rows), along with the corresponding COCOA attributions (red for higher values and blue for lower values) with each original image as the corpus and random images as the foil (bottom rows). Cutout and rotation are not included in SimCLR training.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image augmentation technique is most likely to cause the model to misclassify the image?",
    "answer": "Cutout.",
    "rationale": "The cutout technique removes a random rectangular region from the image. This can be seen in the fourth column of the figure, where a portion of the French horn has been removed. This makes it more difficult for the model to identify the object in the image, and can lead to misclassification.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.00107v2",
    "pdf_url": null
  },
  {
    "instance_id": "ea330f65b9cc4d488e6c6472bec28af3",
    "figure_id": "2310.20695v1-Figure8-1",
    "image_file": "2310.20695v1-Figure8-1.png",
    "caption": " Example images of the used datasets. Our proposed HAP is pre-trained on (a) LUPerson, and evaluated on a broad range of tasks and datasets, including (b-c) person ReID, (d-f) 2D pose estimation, (g-i) text-to-image person ReID, (j) 3D pose and shape estimation, (k-m) pedestrian attribute recognition. These datasets encompass diverse environments with varying image quality, where persons are varied by scales, occlusions, orientations, truncation, clothing, appearances, and postures, demonstrating the the great generalization ability of HAP in human-centric perception.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset is used for pre-training the HAP model?",
    "answer": "LUPerson",
    "rationale": "The caption states that \"Our proposed HAP is pre-trained on (a) LUPerson\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.20695v1",
    "pdf_url": null
  },
  {
    "instance_id": "ced37ca4afc94823989b6e3864e27076",
    "figure_id": "2007.15960v4-Figure4-1",
    "image_file": "2007.15960v4-Figure4-1.png",
    "caption": " Visualizations (t-SNE projection) of sentence embeddings output by HICTL (left) and XLM-R (right). We collect 10 sets of samples from WMT’14-19, each of them contains 100 parallel sentences distributed in 5 languages (i.e., English, French, German, Russian, and Spanish). Each set is identified by a color and different languages marked by different shapes. We can see that a set of sentences under the same meaning are clustered more densely for HICTL than XLM-R, which reveals the strong capability of HICTL on learning universal representations across different languages.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language is represented by the green dots in the left panel of the figure?",
    "answer": "German",
    "rationale": "The legend in the left panel of the figure shows that green dots represent German sentences.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.15960v4",
    "pdf_url": null
  },
  {
    "instance_id": "4b5619ad336b4799b1c24e17c9158d75",
    "figure_id": "1810.11702v8-Figure4-1",
    "image_file": "1810.11702v8-Figure4-1.png",
    "caption": " Win rate at test time across StarCraft II scenarios: 2 Stalkers & 3 Zealots [left], 3 Marines [middle] and 8 Marines [right]. Plots show means and their standard errors with [number of runs].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieved the highest win rate in the 8 Marines scenario?",
    "answer": "MACKRL",
    "rationale": "The black line in the rightmost plot represents MACKRL, and it is consistently above the other lines, indicating that it achieved the highest win rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.11702v8",
    "pdf_url": null
  },
  {
    "instance_id": "37d64b004c594b6c984a73b515a6cbda",
    "figure_id": "2106.10891v3-Figure4-1",
    "image_file": "2106.10891v3-Figure4-1.png",
    "caption": " Results of sensitivity analysis on CIFAR-10 with different η.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of η resulted in the highest test accuracy?",
    "answer": "0.1",
    "rationale": "The figure shows that the curve corresponding to η = 0.1 is the highest among all the curves, indicating that it achieved the highest test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.10891v3",
    "pdf_url": null
  },
  {
    "instance_id": "4ecc81411fde4041a21ebd261daabdf0",
    "figure_id": "2106.05763v3-Figure14-1",
    "image_file": "2106.05763v3-Figure14-1.png",
    "caption": " Average test set (a) CI and (b) RAE on synthetic data with K = 3 clusters achieved by VaDeSC models with different numbers of mixture components. Error bars correspond to standard deviations across 5 independent simulations. The CI plot (left) exhibits a pronounced ‘elbow’ at the true number of clusters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the optimal number of clusters for the VaDeSC model on synthetic data with K = 3 clusters?",
    "answer": "The optimal number of clusters is 3.",
    "rationale": "The CI plot shows a pronounced 'elbow' at 3 clusters, indicating that this is the optimal number of clusters. The RAE plot also shows a decrease in error as the number of clusters increases to 3, and then a slight increase as the number of clusters increases further.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05763v3",
    "pdf_url": null
  },
  {
    "instance_id": "4e7e94eef3544dd3917a055f5a2bad55",
    "figure_id": "1908.04725v2-Figure8-1",
    "image_file": "1908.04725v2-Figure8-1.png",
    "caption": " Impact of number of parameters on reconstruction error.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture has the highest reconstruction error?",
    "answer": "6-layer AN",
    "rationale": "The table shows the reconstruction error for different network architectures. The 6-layer AN has the highest reconstruction error of 1.35.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.04725v2",
    "pdf_url": null
  },
  {
    "instance_id": "24b120a922bb4775a5984dd1cd0069a3",
    "figure_id": "2006.03647v2-Figure17-1",
    "image_file": "2006.03647v2-Figure17-1.png",
    "caption": " Performance in Deployment-Efficient RL experiments with different reward function of HalfCheetah.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in the CheetahRun environment with a batch size of 200,000 rewards?",
    "answer": "SAC(online)",
    "rationale": "The figure shows the cumulative rewards for different algorithms in the CheetahRun environment. The SAC(online) algorithm has the highest cumulative reward at the end of the experiment for both batch sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.03647v2",
    "pdf_url": null
  },
  {
    "instance_id": "5e1b06146b774b0bbfa2b40ca1044751",
    "figure_id": "2301.12860v1-Figure4-1",
    "image_file": "2301.12860v1-Figure4-1.png",
    "caption": " ImageNet-21k ResNet152x1 latency in terms of FLOPs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods (DET, HET, HET-H, HET-XL) is the most computationally efficient?",
    "answer": "HET-XL",
    "rationale": "The figure shows the latency of different methods in terms of FLOPs. HET-XL has the lowest latency, which means it is the most computationally efficient.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.12860v1",
    "pdf_url": null
  },
  {
    "instance_id": "3697f1652e114af58820a49da611f929",
    "figure_id": "2305.11392v1-Figure1-1",
    "image_file": "2305.11392v1-Figure1-1.png",
    "caption": " F-score (%) vs Inference Speed (FPS) on FUNSD test set. Triangles indicate the methods focus on document understanding, and the red triangle is our method. Diamonds mean that we utilize the efficient transformers in our framework, and the FPS is computed without image embeddings. The red diamond shows our method.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest F-score?",
    "answer": "Ours_Tr",
    "rationale": "The figure shows that Ours_Tr has the highest F-score, which is indicated by the red diamond.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.11392v1",
    "pdf_url": null
  },
  {
    "instance_id": "d9a2bc560c7a4122bda2248bee18e5a2",
    "figure_id": "2106.05763v3-Figure4-1",
    "image_file": "2106.05763v3-Figure4-1.png",
    "caption": " Cluster-specific Kaplan–Meier (KM) curves (top) and t-SNE visualisation of latent representations (bottom), coloured according to survival times (yellow and blue correspond to higher and lower survival times, respectively), learnt by different models (b-e) from one replicate of the survMNIST dataset. Panel (a) shows KM curves of the ground truth clusters. Plots were generated using 10,000 data points randomly sampled from the training set; similar results were observed on the test set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is most successful in separating clusters with different survival times?",
    "answer": "VaDeSC.",
    "rationale": "The t-SNE visualizations show how well each model separates clusters with different survival times. In the VaDeSC visualization, the clusters are clearly separated, with yellow points (high survival times) on one side and blue points (low survival times) on the other. This indicates that VaDeSC is most successful in separating clusters with different survival times.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05763v3",
    "pdf_url": null
  },
  {
    "instance_id": "9d56c620498d40e2992abf98bfa3f416",
    "figure_id": "1906.01044v2-Figure4-1",
    "image_file": "1906.01044v2-Figure4-1.png",
    "caption": " Reconstructed and Generated Images",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset is most likely to be used for training a model to recognize handwritten digits?",
    "answer": "The MNIST dataset.",
    "rationale": "The MNIST dataset contains images of handwritten digits, while the other datasets contain images of clothing, faces, chairs, and cars.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.01044v2",
    "pdf_url": null
  },
  {
    "instance_id": "30cc825fc14a403a810f58ce48c33fc2",
    "figure_id": "1902.08336v1-Figure4-1",
    "image_file": "1902.08336v1-Figure4-1.png",
    "caption": " Model capacity and training set size’s influences on accuracy and robust accuracy. In each subfigure, the top row contains accuracy and robust accuracy measured on training set, the bottom row contains results measured on test set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method results in the highest accuracy on the MNIST dataset when the training set size is full?",
    "answer": "Standard training",
    "rationale": "The plot in the bottom left corner of Figure (c) shows that the standard training method (blue line) has the highest accuracy on the test set when the training set size is full.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.08336v1",
    "pdf_url": null
  },
  {
    "instance_id": "a3d29ef4f47347acaf83a298f9220dd2",
    "figure_id": "1904.01480v1-Figure4-1",
    "image_file": "1904.01480v1-Figure4-1.png",
    "caption": " Semantic-conditioned batch normalization (SCBN) with (a) sentence-level cues that consists of a one-hidden-layer MLP to extract modulation parameters from the sentence feature vector; and (b) word-level cues that uses VSE module to fuse the visual features and word features. Note that the illustration only takes γc as the example and the implementation for βc is alike.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two different types of cues that can be used with SCBN?",
    "answer": "Sentence-level cues and word-level cues.",
    "rationale": "The figure shows two different ways to implement SCBN, one with sentence-level cues (a) and one with word-level cues (b).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.01480v1",
    "pdf_url": null
  },
  {
    "instance_id": "fb0de967cfaf426685a3f9cb79e1cb63",
    "figure_id": "1901.06543v2-Figure4-1",
    "image_file": "1901.06543v2-Figure4-1.png",
    "caption": " Dialect classification results on the validation set provided by the KRR classifier based on the presence bits string kernel with n-grams in the range 5-8. Results are reported for various λ values from 10−3 to 10−6. Best viewed in color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of lambda gives the best performance for 6-grams?",
    "answer": "10^-4",
    "rationale": "The bar corresponding to lambda = 10^-4 is the highest for 6-grams.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.06543v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd7d93e979f64d9dbb646f6889159bb9",
    "figure_id": "2302.09579v1-Figure9-1",
    "image_file": "2302.09579v1-Figure9-1.png",
    "caption": " Posterior probability of the readout models at each timestep on ImageNet. Pre-trained methods are ResNet50 + SimCLR, ResNet50 + BYOL, ViT/B16 + DINO and ViT/B16 + MAE",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pre-trained method has the highest posterior probability for the Linear readout model?",
    "answer": "ViT/B16-MAE",
    "rationale": "The blue line in the figure represents the posterior probability of the Linear readout model. The ViT/B16-MAE plot shows the blue line peaking at a value of 1, which is the highest value observed for the Linear readout model across all pre-trained methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.09579v1",
    "pdf_url": null
  },
  {
    "instance_id": "6cba9890228a4efe85be4147be1ab47e",
    "figure_id": "2012.06932v1-Figure6-1",
    "image_file": "2012.06932v1-Figure6-1.png",
    "caption": " Experiments of warm starting optimizations using the result of HPO on another dataset. The directions of knowledge transfer are the opposite of those in the main paper.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest validation error in both (a) and (b)?",
    "answer": "WS-CMA-ES",
    "rationale": "In both (a) and (b), the red line representing WS-CMA-ES has the lowest validation error among all methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.06932v1",
    "pdf_url": null
  },
  {
    "instance_id": "291af6473f7c480b8cfdf27a2d72c0be",
    "figure_id": "2210.15144v2-Figure5-1",
    "image_file": "2210.15144v2-Figure5-1.png",
    "caption": " Probabilities of RoBERTa (A, D), MentalRoBERTa (B, E), and ClinicalLongformer (C, F) for predicting male, female, and unspecified-gender words. Each subplot shows prompts for three health action phases: Diagnosis, Intention, and Action (see 3.1 for definition). RoBERTa (A) and MentalRoBERTa (B) predict female subjects with consistently higher likelihood than male subjects in mental-health-related (MH) prompts for all three action phases (**). These gender disparities are significantly larger in MH prompts (A–C) than in non-mental-health-related (non-MH) prompts (***, D–F), and the disparity increases for later health action phases. ClinicalLongformer (C, F), trained on clinical notes instead of web texts, reverses the trend and predicts male subjects with significantly higher probability across all categories (**) and most commonly generates unspecified-gender subjects. (***: p < .001, **: p < .01, *:p < .05)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is most likely to predict a female subject for a mental health-related prompt in the Diagnosis phase?",
    "answer": "RoBERTa",
    "rationale": "The figure shows that RoBERTa predicts female subjects with a higher probability than male subjects for all three health action phases (Diagnosis, Intention, and Action) in mental-health-related prompts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.15144v2",
    "pdf_url": null
  },
  {
    "instance_id": "8e1ad459caa5476987945ea36ce32463",
    "figure_id": "2303.07067v1-Figure2-1",
    "image_file": "2303.07067v1-Figure2-1.png",
    "caption": " Statistics of the data from 482 COVID-19 positive users and 2, 478 negative users.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which age group had the most users in the study?",
    "answer": "The 20-29 age group.",
    "rationale": "The bar for the 20-29 age group in Figure (a) is the tallest, indicating that this age group had the most users.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.07067v1",
    "pdf_url": null
  },
  {
    "instance_id": "bace0098c1fe4e0bad5f17e4fc658382",
    "figure_id": "2103.10022v1-Figure3-1",
    "image_file": "2103.10022v1-Figure3-1.png",
    "caption": " Qualitative comparison results on two test images of CelebA-HQ. For each group, from top to bottom, from left to right, the pictures are: incomplete image, results of CA [36], GC [37], CSA [16], SF [23], FE [15], results of PIC [41] (with blue box), results of UCTGAN [40] (with green box), and results of our method (with red box).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most realistic results for the man's face?",
    "answer": "Our method (with red box).",
    "rationale": "The results of our method are shown in the red box. They are the most realistic because they preserve the man's natural features, such as his wrinkles and skin texture.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.10022v1",
    "pdf_url": null
  },
  {
    "instance_id": "f0d41b02b59d42aaad36ab1c14294a06",
    "figure_id": "2304.14610v2-Figure7-1",
    "image_file": "2304.14610v2-Figure7-1.png",
    "caption": " Results of the human subjective survey. The colorchanging from warm to cool represents the deterioration of image quality; The y-axis represents the number of images in each rating index.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm produced the images with the highest average quality rating?",
    "answer": "The \"Ours\" algorithm.",
    "rationale": "The figure shows the results of a human subjective survey on the quality of images produced by different algorithms. The y-axis represents the number of images in each rating index, with the color changing from warm to cool representing the deterioration of image quality. The \"Ours\" algorithm has the highest proportion of images in the highest rating category (5) and the lowest proportion of images in the lowest rating category (1).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.14610v2",
    "pdf_url": null
  },
  {
    "instance_id": "1310236fffc246ab8f972988e9769fba",
    "figure_id": "2001.01568v3-Figure6-1",
    "image_file": "2001.01568v3-Figure6-1.png",
    "caption": " Ablation Study.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best performance in terms of both loss and PSNR?",
    "answer": "GMM + Attention.",
    "rationale": "The loss curves in (a) and (c) show that GMM + Attention has the lowest validation loss for both N=128 and N=192. The RD points in (b) and (d) show that GMM + Attention achieves the highest PSNR for both N=128 and N=192.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.01568v3",
    "pdf_url": null
  },
  {
    "instance_id": "30456aaeae6a4919bf21f0c2ed4cb968",
    "figure_id": "2311.01075v1-Figure3-1",
    "image_file": "2311.01075v1-Figure3-1.png",
    "caption": " Training curves of different methods on MT10 and MT50 benchmarks, each curve is averaged over 8 seeds(Shaded areas: the standard deviation over 8 seeds). Our approach consistently outperforms baselines in all environments, whether on asymptotic performance or sample efficiency, and the superiority is more obvious on MT10-Mixed and MT50-Mixed. TE = Task Encoder.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest asymptotic performance on the MT10-Fixed benchmark?",
    "answer": "CMTAP (ours)",
    "rationale": "The figure shows that the CMTAP (ours) curve is the highest among all the methods on the MT10-Fixed benchmark. This means that CMTAP (ours) achieves the highest success rate after training for a long time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2311.01075v1",
    "pdf_url": null
  },
  {
    "instance_id": "a2f26640b0304545be791abca3e2feea",
    "figure_id": "2010.11545v1-Figure2-1",
    "image_file": "2010.11545v1-Figure2-1.png",
    "caption": " Rainbow MNIST results. Top: Accuracy over all tasks; Bottom: Performance statistics. Here, Average Ranking (AR) is calculated by first rank all methods for each dataset, from higher to lower. Each method receive a score corresponds to its rank, e.g. rank one receives one point. The scores for each method are then averaged to form the reported AR. Lower AR is better.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best on average across all tasks?",
    "answer": "OSMIL",
    "rationale": "The table at the bottom of the figure shows the average ranking (AR) for each method. OSMIL has the lowest AR, indicating that it performed the best on average.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.11545v1",
    "pdf_url": null
  },
  {
    "instance_id": "6a82ce88756b410399b1314442396242",
    "figure_id": "2204.13305v1-Figure11-1",
    "image_file": "2204.13305v1-Figure11-1.png",
    "caption": " Reduction of 3-SAT-instance C1 = {a, b, c}, C2 = {¬a,¬b}, C3 = {¬a, c}, C4 = {b,¬c}, to an instance (F, S) of VerPCAF",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the sets A, V, and H?",
    "answer": "A is the union of V, V-bar, and H.",
    "rationale": "The figure shows that A is defined as the union of V, V-bar, and H.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.13305v1",
    "pdf_url": null
  },
  {
    "instance_id": "55920f9a0557465f8e5eedfe73083e98",
    "figure_id": "1911.07959v2-Figure4-1",
    "image_file": "1911.07959v2-Figure4-1.png",
    "caption": " Numbers of sequences for different challenge factors in TracKlinic. There are at least 30 videos for each challenge factor, indicating that our diagnosis analysis is statistically meaningful.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which challenge factor has the highest number of videos?",
    "answer": "OCC",
    "rationale": "The figure shows a bar chart of the number of videos for each challenge factor. The OCC bar is the tallest, indicating that it has the highest number of videos.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.07959v2",
    "pdf_url": null
  },
  {
    "instance_id": "749923d4b9734f6c91afe02c349e5b9e",
    "figure_id": "2301.09091v3-Figure2-1",
    "image_file": "2301.09091v3-Figure2-1.png",
    "caption": " Comparison of the 3D geometry extracted by marching cubes. (a) GIRAFFE-HD exhibits broken 3D shapes, (b) StyleNeRF has jaggy surfaces, and (c) EG3D has hair sticking to the wall. Unlike other models, (d) our model produces high-quality foreground geometry that is separated from the background.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models shown in the figure produces the most accurate 3D geometry?",
    "answer": "Our model",
    "rationale": "The figure shows that the 3D geometry extracted by our model is of higher quality than the geometry extracted by the other models. Specifically, our model produces foreground geometry that is separated from the background, while the other models exhibit broken shapes, jaggy surfaces, and hair sticking to the wall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.09091v3",
    "pdf_url": null
  },
  {
    "instance_id": "81dc898f8c5c41c58e6ad07fb36ba764",
    "figure_id": "1812.02713v1-Figure19-1",
    "image_file": "1812.02713v1-Figure19-1.png",
    "caption": " Template visualization (2/3). We present the templates for storage furniture, faucet, clock, bed, knife (cutting instrument), and trash can. The lamp template is shown in the main paper. The And-nodes are drawn in solid lines and Or-nodes in dash lines.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following parts is a part of a cutting instrument?",
    "answer": "Guard",
    "rationale": "The figure shows a schematic of a cutting instrument, and the guard is shown as a part of the cutting instrument.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.02713v1",
    "pdf_url": null
  },
  {
    "instance_id": "9480c84388d64099bfea243e0c0ae96e",
    "figure_id": "2301.08881v2-Figure1-1",
    "image_file": "2301.08881v2-Figure1-1.png",
    "caption": " An example of the SOTA model Picard (Scholak et al., 2021) against DB, NLQ, SQL perturbations on the database WTA. Picard predicts a correct SQL on pre-perturbation data but fails on post-perturbation data. The blue and gray areas highlight the modification on input and the errors of predicted SQLs respectively.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which perturbation caused Picard to predict the age of the winner instead of the number of winners?",
    "answer": "SQL Perturbation.",
    "rationale": "The figure shows that Picard predicts the age of the winner instead of the number of winners when the SQL query is perturbed. This is indicated by the blue highlight in the SQL query and the gray highlight in the predicted SQL.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.08881v2",
    "pdf_url": null
  },
  {
    "instance_id": "806ab5d9c9ba4fde8806df8094a1139d",
    "figure_id": "2009.13891v3-Figure6-1",
    "image_file": "2009.13891v3-Figure6-1.png",
    "caption": " Meta-RL tasks: left-to-right: humanoid, ant, half cheetah, and walker.",
    "figure_type": "** schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the Meta-RL tasks is the most complex?",
    "answer": " The humanoid task is the most complex. ",
    "rationale": " The humanoid task has the most degrees of freedom and requires the agent to learn to walk upright on two legs. This is a more challenging task than the other tasks, which involve simpler forms of locomotion. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.13891v3",
    "pdf_url": null
  },
  {
    "instance_id": "bb58c7222e7445758793000835490627",
    "figure_id": "2203.09711v1-Figure6-1",
    "image_file": "2203.09711v1-Figure6-1.png",
    "caption": " Scatter plots and regression lines of different models predicted scores versus FED-overall human evaluations. Overlapped points are represented darker.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best at predicting FED-overall human evaluations?",
    "answer": "DREAM",
    "rationale": "The DREAM model has the highest R^2 value, which indicates that it is the best at predicting FED-overall human evaluations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.09711v1",
    "pdf_url": null
  },
  {
    "instance_id": "4f7224e193e841028bfcf1db57f3d320",
    "figure_id": "2102.01748v1-Figure1-1",
    "image_file": "2102.01748v1-Figure1-1.png",
    "caption": " The implementable “plug-in” lower confidence bound estimators zt and gt.",
    "figure_type": "Table.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three different settings considered in the table?",
    "answer": "Non-stationary, Stationary, and ∞-Horizon.",
    "rationale": "The table shows three different settings for the lower confidence bound estimators zt and gt.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.01748v1",
    "pdf_url": null
  },
  {
    "instance_id": "2f13e4f25b3349b18d38ad2864471ff6",
    "figure_id": "1806.02248v2-Figure4-1",
    "image_file": "1806.02248v2-Figure4-1.png",
    "caption": " The n-step regret of TopRank (red), CascadeKL-UCB (blue), and BatchRank (gray) in two click models. The results are averaged over 60 queries and 10 runs per query. The error bars are the standard errors of our regret estimates.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of regret?",
    "answer": "CascadeKL-UCB.",
    "rationale": "The figure shows the n-step regret of three algorithms: TopRank, CascadeKL-UCB, and BatchRank. The regret of CascadeKL-UCB is lower than the regret of the other two algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.02248v2",
    "pdf_url": null
  },
  {
    "instance_id": "59b30e585ad34f2d9dc366e666018f7a",
    "figure_id": "1911.12760v2-Figure4-1",
    "image_file": "1911.12760v2-Figure4-1.png",
    "caption": " Perceptual Results of naturalness for low, medium, high and aggregated emotional intensities. We plot MUSHRA responses of listeners as box-plots for each intensity. The number in white and location of the green triangle represents the mean of the listeners response for that system.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best in terms of naturalness for low emotional intensity?",
    "answer": "Rec",
    "rationale": "The box plot for Rec has the highest median and mean values for low emotional intensity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.12760v2",
    "pdf_url": null
  },
  {
    "instance_id": "838291a9ad754adcbe25e4cd2bb1a997",
    "figure_id": "1908.08185v1-Figure6-1",
    "image_file": "1908.08185v1-Figure6-1.png",
    "caption": " Evaluation of 3D point cloud density. Left: The density histogram of reconstructed points. Right: Visualization of its spacial distribution.",
    "figure_type": "plot and photographs",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced a denser point cloud, \"Ours\" or \"Method [30]\"?",
    "answer": "\"Ours\"",
    "rationale": "The right side of the figure shows two visualizations of the point cloud density, one for each method. The color bar indicates that redder colors represent denser regions of the point cloud. The \"Ours\" visualization is much redder than the \"Method [30]\" visualization, indicating that it has a higher density of points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.08185v1",
    "pdf_url": null
  },
  {
    "instance_id": "de3fa56627e54f4383b6702a659084d2",
    "figure_id": "1808.00020v6-Figure9-1",
    "image_file": "1808.00020v6-Figure9-1.png",
    "caption": " FID curves with 5 discriminators. acGAN presented earlier convergence and reached lower FID values.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method had the lowest FID score at the end of the training process?",
    "answer": "acGAN",
    "rationale": "The plot shows the FID score for three different methods over time. The acGAN line is the lowest at the end of the training process, indicating that it achieved the best results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.00020v6",
    "pdf_url": null
  },
  {
    "instance_id": "2d8dbbfe3251433882533eedfedbabf3",
    "figure_id": "2206.11775v1-Figure3-1",
    "image_file": "2206.11775v1-Figure3-1.png",
    "caption": " The top left, top right and bottom left plots show the curves of label permutation recovery under different design settings by using different estimation methods. The bottom right plot shows the permutation recovery curves under different p.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which estimation method performs best in the Sparse Case when p = 25?",
    "answer": "MLE",
    "rationale": "The bottom right plot shows the permutation recovery curves under different p. The MLE curve is the highest for p = 25, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.11775v1",
    "pdf_url": null
  },
  {
    "instance_id": "10bec0e4616d4fbaa3711fd6bb8f663a",
    "figure_id": "2309.14700v1-Figure7-1",
    "image_file": "2309.14700v1-Figure7-1.png",
    "caption": " Attack success rates (%) of various defense methods on the adversarial examples generated by TIM, DIM, DEM, Admix, SSA, and SIA under ensemble model setting. The adversarial examples are generated on the eight models simultaneously.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which defense method is the most effective against adversarial examples generated by SIA?",
    "answer": "NRP",
    "rationale": "The figure shows the attack success rates of various defense methods against adversarial examples generated by six different methods, including SIA. The NRP defense method has the lowest attack success rate against SIA-generated adversarial examples, indicating that it is the most effective defense method against this type of attack.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.14700v1",
    "pdf_url": null
  },
  {
    "instance_id": "50c28722c12c4bd690f12d78510bac0d",
    "figure_id": "2108.06098v3-Figure5-1",
    "image_file": "2108.06098v3-Figure5-1.png",
    "caption": " Average test accuracy over ten local models trained by each algorithm. (a) 100% of local training data on FEMNIST are used with the non-IID setting, which mimics enough local data to train and evaluates each local model on their own data. (b) 20% of local training data on FEMNIST are used with the non-IID setting, which mimics insufficient local data to train local models. (c) 100% of local training data on MNIST are used with the highly-skew non-IID setting, where each client has at most two classes. The error bars denote 95% confidence intervals obtained by 5 repeats.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in Scenario 2?",
    "answer": "pFedPara",
    "rationale": "In Scenario 2, pFedPara achieved the highest accuracy of 99.5%, which is significantly higher than the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.06098v3",
    "pdf_url": null
  },
  {
    "instance_id": "e17b06804d6544718ddfcf66f2ff7551",
    "figure_id": "2201.01251v3-Figure19-1",
    "image_file": "2201.01251v3-Figure19-1.png",
    "caption": " Performance profiles based on score distributions for the stochastic games listed in Table 2, following Agarwal et al. (2021). Shaded regions show pointwise 95% confidence bands based on percentile bootstrap with stratified sampling.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm consistently performs the best across all games and normalized scores?",
    "answer": "DRRN.",
    "rationale": "The figure shows the fraction of runs with a score greater than a certain threshold for different algorithms. The DRRN line is consistently above the other lines, which indicates that it performs the best across all games and normalized scores.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.01251v3",
    "pdf_url": null
  },
  {
    "instance_id": "b86d6480484948d08948c6a4f2df0471",
    "figure_id": "2206.03126v1-Figure6-1",
    "image_file": "2206.03126v1-Figure6-1.png",
    "caption": " BLEU scores by increasing the number of transformers blocks. ‘X’ Transformer blocks implies in total ‘X’ encoder self-attention, ‘X’ decoder selfattention and ‘X’ decoder cross-attention layers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performs best when the number of transformer blocks is 12?",
    "answer": "SGD: POST-LN",
    "rationale": "The plot shows that the BLEU score for SGD: POST-LN is the highest when the number of transformer blocks is 12.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.03126v1",
    "pdf_url": null
  },
  {
    "instance_id": "bee8a8491222404d8e07d975d20902f6",
    "figure_id": "2010.14407v2-Figure6-1",
    "image_file": "2010.14407v2-Figure6-1.png",
    "caption": " Noise improves generalization across the OOD2 scenarios and less so for the OOD1 scenarios as seen from the transfer scores. Top row: Spearman rank correlation coefficients between transfer metrics and presence of noise in the input.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which OOD scenario shows the most significant improvement in generalization when noise is added to the input?",
    "answer": "OOD2-A",
    "rationale": "The violin plots for OOD2-A show a clear shift to the right when noise is added to the input, indicating an increase in the transfer score. This shift is more pronounced than in the other OOD scenarios.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.14407v2",
    "pdf_url": null
  },
  {
    "instance_id": "a857927e6d5949bb99cce974a1ee962d",
    "figure_id": "1811.09845v3-Figure12-1",
    "image_file": "1811.09845v3-Figure12-1.png",
    "caption": " Random selection of examples generated by our Aux model for the CoDraw dataset.",
    "figure_type": "\"other\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which objects are located in the middle of the scene?",
    "answer": "A rocket, a tree, and a boy.",
    "rationale": "The rocket is located in the middle of the scene, above the tree. The tree is located in the middle of the scene, below the rocket. The boy is located in the middle of the scene, to the right of the tree.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.09845v3",
    "pdf_url": null
  },
  {
    "instance_id": "3cb7c0d2253f45f6a05c7bae4f996414",
    "figure_id": "2011.12102v1-Figure8-1",
    "image_file": "2011.12102v1-Figure8-1.png",
    "caption": " Demonstration of the scores for the three latent fluents and lifestyle. For Day 5, the user has a healthy lifestyle: sedentary work, food and motion are well-distributed, the scores for hunger, thirst and fatigue are low and the score for lifestyle is high. For Day 4, the user participates in excessive sedentary work, and the score for lifestyle is low.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which day had a healthier lifestyle, Day 4 or Day 5?",
    "answer": " Day 5 had a healthier lifestyle than Day 4.",
    "rationale": " The figure shows that the lifestyle score for Day 5 was higher than the lifestyle score for Day 4. This is because the user participated in more sedentary work on Day 4 than on Day 5. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.12102v1",
    "pdf_url": null
  },
  {
    "instance_id": "92504f4caf93418090cdbc6f3cbab683",
    "figure_id": "2011.08277v2-Figure8-1",
    "image_file": "2011.08277v2-Figure8-1.png",
    "caption": " The dataset collection interface for WAY. These are the interfaces that the Observer and Locator workers used on Amazon Mechanical Turk.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two roles that participants in the WAY dataset collection task could be assigned?",
    "answer": "Locator and Observer.",
    "rationale": "The figure shows two different interfaces, one for the Locator and one for the Observer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.08277v2",
    "pdf_url": null
  },
  {
    "instance_id": "fd1cd77859f0429abcf657242d990c25",
    "figure_id": "2205.10408v1-Figure2-1",
    "image_file": "2205.10408v1-Figure2-1.png",
    "caption": " State forecasts at τ = 7. The univariate forecast is compared against three multivariate forecasts where the M , G & TRoB features make up the covariate set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which state's forecast is most accurate, based on the comparison of the true value and the predicted values?",
    "answer": "Washington",
    "rationale": "The true value line in Washington is closely followed by all of the predicted values, indicating that the forecasts are accurate. In contrast, the true value lines in the other states are not as closely followed by the predicted values, indicating that the forecasts are less accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.10408v1",
    "pdf_url": null
  },
  {
    "instance_id": "9d55fdd557ba40f7ad1180648bd46864",
    "figure_id": "2104.06751v2-Figure6-1",
    "image_file": "2104.06751v2-Figure6-1.png",
    "caption": " Distribution of confidence scores of rules mined by different rule mining methods on the training set of WD15K.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which rule mining method produced the most rules with a confidence score of 0.8 or higher?",
    "answer": " AMIE+",
    "rationale": " The figure shows the distribution of confidence scores for rules mined by different methods. The AMIE+ plot has the highest bar at the 0.8 confidence score bin, indicating that it produced the most rules with a confidence score of 0.8 or higher.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.06751v2",
    "pdf_url": null
  },
  {
    "instance_id": "9e02055044474fd99fc32ac8423f45d8",
    "figure_id": "2106.00085v3-Figure7-1",
    "image_file": "2106.00085v3-Figure7-1.png",
    "caption": " Permutation distributions for the difference in means between length, stopword, and symbol distributions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature has the most similar permutation distributions across all models?",
    "answer": "Length",
    "rationale": "The permutation distributions for length are all centered around 0 and have similar shapes, indicating that the difference in means between the length distributions for each model is not statistically significant.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.00085v3",
    "pdf_url": null
  },
  {
    "instance_id": "a39eb9b16d574f7aabc44c9cf023dc9b",
    "figure_id": "2104.05847v1-Figure3-1",
    "image_file": "2104.05847v1-Figure3-1.png",
    "caption": " Training loss surfaces of traditional training vs TAT on MNLI.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method results in a loss surface with fewer local minima?",
    "answer": "TAT",
    "rationale": "The loss surface of TAT has fewer peaks and valleys than the loss surface of traditional training. This suggests that TAT is less likely to get stuck in local minima.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.05847v1",
    "pdf_url": null
  },
  {
    "instance_id": "9a3089c29140494cb5e62ee77afb4c9c",
    "figure_id": "2305.19475v1-Figure6-1",
    "image_file": "2305.19475v1-Figure6-1.png",
    "caption": " An ℓ-community instance to show Price of Fairness (GF) and incompatibility between GF and other fairness constraints when k is odd.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the value of ℓ in the figure?",
    "answer": "ℓ - 1",
    "rationale": "The figure shows a chain of nodes, with one red node in the middle. The caption states that this is an ℓ-community instance. The length of the chain is ℓ - 1, as indicated by the label below the chain.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19475v1",
    "pdf_url": null
  },
  {
    "instance_id": "666d20a64ba3459f822e5ccc8030a2a4",
    "figure_id": "2006.13843v2-Figure3-1",
    "image_file": "2006.13843v2-Figure3-1.png",
    "caption": " Comparison between BN-SLIM(k-MAX) and k-MAX over 94 data sets",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the difference in the number of data sets between BN-SLIM(k-MAX) and k-MAX for tw 5?",
    "answer": "17",
    "rationale": "The figure shows that BN-SLIM(k-MAX) has 74 data sets and k-MAX has 11 data sets for tw 5. The difference between these two numbers is 17.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.13843v2",
    "pdf_url": null
  },
  {
    "instance_id": "8c72044c4ff2404f8edba7e856d2ea3a",
    "figure_id": "2003.04808v1-Figure2-1",
    "image_file": "2003.04808v1-Figure2-1.png",
    "caption": " BERT LARGE on SQuAD2.0: vulnerability to noisy attacks on held out data for differently sized attack spaces (parameter η) and different beam search depth (perturbation radius ρ).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which perturbation type is more vulnerable to noisy attacks?",
    "answer": "Part of Speech perturbations",
    "rationale": "The plot shows that the undersensitivity error rate for Part of Speech perturbations is higher than for Named Entity perturbations. This indicates that Part of Speech perturbations are more vulnerable to noisy attacks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.04808v1",
    "pdf_url": null
  },
  {
    "instance_id": "7ddffaf0b30b4c40bcfe86f30f6c5caa",
    "figure_id": "2306.12230v2-Figure26-1",
    "image_file": "2306.12230v2-Figure26-1.png",
    "caption": " The validation accuracy versus the training epoch for different pruning criteria and densities on the ResNet-56 (CIFAR10 dataset).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pruning criterion leads to the highest validation accuracy at a density of 0.05?",
    "answer": "𝐶𝑀𝑎𝑔𝑛𝑖𝑡𝑢𝑑𝑒",
    "rationale": "The figure shows that at a density of 0.05, the 𝐶𝑀𝑎𝑔𝑛𝑖𝑡𝑢𝑑𝑒  curve is the highest among all the other curves.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.12230v2",
    "pdf_url": null
  },
  {
    "instance_id": "a4e7fb6584e6413fb03b04784cda4743",
    "figure_id": "2302.06752v1-Figure7-1",
    "image_file": "2302.06752v1-Figure7-1.png",
    "caption": " Random forest classifier with heterogeneous bias (SQF data). Scores F ∗ (green), F (ST ) (orange), and Ftheo(S T ) (blue) vs. proportion of low probability stops k.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which curve in the plot represents the theoretical F-score for the affected subset?",
    "answer": "The blue curve.",
    "rationale": "The caption of the figure states that the blue curve represents the theoretical F-score for the affected subset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.06752v1",
    "pdf_url": null
  },
  {
    "instance_id": "6232571f67b84cd295a25e35e83c3eec",
    "figure_id": "2209.07366v1-Figure5-1",
    "image_file": "2209.07366v1-Figure5-1.png",
    "caption": " This figure contains a qualitative comparison between our method (3DMM-RF), MoFaNeRF [95] and HeadNeRF [33]. From left to right, the columns include the input image, MoFaNeRF’s fitted prediction, HeadNeRF’s prediction, 3DMM-RF’s prediction, a novel view rendered by HeadNeRF and 3DMM-RF and ours under diffuse albedo.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method appears to produce the most accurate novel view of the face?",
    "answer": "3DMM-RF",
    "rationale": "The novel views rendered by each method are shown in columns (f) and (g). Comparing these columns, it can be seen that the novel view generated by 3DMM-RF appears to be more accurate and realistic than the one generated by HeadNeRF.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.07366v1",
    "pdf_url": null
  },
  {
    "instance_id": "8d70bd15b06b46699b8c5ae2e2ea1853",
    "figure_id": "2303.01239v2-Figure6-1",
    "image_file": "2303.01239v2-Figure6-1.png",
    "caption": " Qualitative results on VQA v2 validation set. The Prediction is generated by the VL-T5 tuned with the proposed MixPHM. GT denotes the annotated answer and the corresponding score. We visualize the top-down attention [1] of images and mark the taskrelevant tokens of questions for the first and second highest attention scores.",
    "figure_type": "Photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the person in the air doing?",
    "answer": "Snowboarding.",
    "rationale": "The figure shows a person in the air with a snowboard.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.01239v2",
    "pdf_url": null
  },
  {
    "instance_id": "2b5d936e063b4899a817b95afead2c6c",
    "figure_id": "2207.03578v5-Figure2-1",
    "image_file": "2207.03578v5-Figure2-1.png",
    "caption": " A bird’s eye view of a compiler toolchain, exemplified with LLVM. The unoptimized version (-O0) is shown here for illustration. In practice we used the size-optimized version (-Oz) of the IR as boxed, which does the compile time optimization of computing the addition of 26 and 16.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stage of the compiler toolchain performs language-specific optimizations?",
    "answer": "The language front-end.",
    "rationale": "The figure shows that the language front-end performs tasks such as lexing, parsing, and semantic analysis, which are all language-specific.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.03578v5",
    "pdf_url": null
  },
  {
    "instance_id": "4ed2c7087df543bbac8c663bd2e08a0a",
    "figure_id": "2312.01473v1-Figure6-1",
    "image_file": "2312.01473v1-Figure6-1.png",
    "caption": " Snapshots from free play with RaIR + CEE-US. We showcase snapshots of highest RaIR values, equivalent to lowest entropy, from exemplary rollouts at different iterations of free play. Following the regularity objective, stacks and alignments are generated.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the robot doing in the image?",
    "answer": "The robot is stacking blocks.",
    "rationale": "The image shows the robot at different stages of stacking blocks. In each iteration, the robot picks up a block and places it on top of another block, until it has created a stack of blocks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2312.01473v1",
    "pdf_url": null
  },
  {
    "instance_id": "7823c8981d2140d3a0768e4018e04dd0",
    "figure_id": "2305.19753v2-Figure6-1",
    "image_file": "2305.19753v2-Figure6-1.png",
    "caption": " The representations rank for deeper layers collapse early in training. The curves present the evolution of representations’ numerical rank over the first 75 training steps for all layers of the VGG-19 trained on CIFAR-10. We present a more detailed tunnel development analysis in Appendix G.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer of the VGG-19 network has the highest numerical rank at the beginning of training?",
    "answer": "The first layer.",
    "rationale": "The figure shows that the numerical rank of the first layer is the highest at the beginning of training, and it decreases as training progresses.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19753v2",
    "pdf_url": null
  },
  {
    "instance_id": "924808dfe1f04c7f9b7c962d34b14425",
    "figure_id": "2006.05752v1-Figure3-1",
    "image_file": "2006.05752v1-Figure3-1.png",
    "caption": " MNIST logistic regression training results for AMB and FMB operating in the hub-andspoke topology.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization method converges faster, AMB or FMB?",
    "answer": "FMB converges faster than AMB.",
    "rationale": "The figure shows the cost of the optimization function over time for both AMB and FMB. The cost of FMB decreases faster than the cost of AMB, indicating that FMB converges faster.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.05752v1",
    "pdf_url": null
  },
  {
    "instance_id": "c361575a2bc145bdb612d4cec5971b6d",
    "figure_id": "2302.00873v3-Figure2-1",
    "image_file": "2302.00873v3-Figure2-1.png",
    "caption": " Box-plot of single-variable distribution for company equity graph dataset. Each box-plot actually represents the conditional distribution 𝑃 (𝑋 |𝑌,O), where the Yaxis is the value of attributes after the log function.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Do listed companies have higher actual capital than unlisted companies?",
    "answer": "Yes.",
    "rationale": "The box plot for actual capital shows that the median value for listed companies is higher than the median value for unlisted companies. This suggests that listed companies tend to have higher actual capital than unlisted companies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.00873v3",
    "pdf_url": null
  },
  {
    "instance_id": "3f86d4772f4f4966ae2229881f5175ff",
    "figure_id": "2312.00548v1-Figure22-1",
    "image_file": "2312.00548v1-Figure22-1.png",
    "caption": " The learning curves for the simpler IL tasks in the target domain",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best on the IPShort-to-color task?",
    "answer": "TPIL",
    "rationale": "The TPIL curve is the highest and has the least variance, indicating that it achieved the highest mean episodic return with the least amount of variation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2312.00548v1",
    "pdf_url": null
  },
  {
    "instance_id": "25148cfb45ab4601852a8f76134c6538",
    "figure_id": "1803.03635v5-Figure2-1",
    "image_file": "1803.03635v5-Figure2-1.png",
    "caption": " Architectures tested in this paper. Convolutions are 3x3. Lenet is from LeCun et al. (1998). Conv-2/4/6 are variants of VGG (Simonyan & Zisserman, 2014). Resnet-18 is from He et al. (2016). VGG-19 for CIFAR10 is adapted from Liu et al. (2019). Initializations are Gaussian Glorot (Glorot & Bengio, 2010). Brackets denote residual connections around layers.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture has the highest number of convolutional layers?",
    "answer": "VGG-19",
    "rationale": "The figure shows that VGG-19 has 4 convolutional layers, while the other networks have 2 or 3 convolutional layers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1803.03635v5",
    "pdf_url": null
  },
  {
    "instance_id": "c53cb7bef7af4ad2ae13767e0d3c3856",
    "figure_id": "2011.07660v1-Figure5-1",
    "image_file": "2011.07660v1-Figure5-1.png",
    "caption": " The frequency distribution of the 25 most common words in the dataset. Stopwords and target object words have been removed.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which word is the most common in the dataset, excluding stopwords and target object words?",
    "answer": "\"turn\"",
    "rationale": "The figure shows the frequency distribution of the 25 most common words in the dataset. The word \"turn\" has the largest slice of the pie, indicating that it is the most frequent word.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.07660v1",
    "pdf_url": null
  },
  {
    "instance_id": "250e928e03674a878e8db9bb1aa0b9bd",
    "figure_id": "2004.01316v2-Figure6-1",
    "image_file": "2004.01316v2-Figure6-1.png",
    "caption": " Discovered influence by our model of Asian cities on global trends of 6 fashion styles. The width of the connection is relative to the influence weight of that city in relation to other influencer of the same style.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which city has the strongest influence on style S1?",
    "answer": "Shanghai",
    "rationale": "The width of the connection between Shanghai and S1 is the widest among all connections to S1. This indicates that Shanghai has the strongest influence on this style.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.01316v2",
    "pdf_url": null
  },
  {
    "instance_id": "674cb390932e4b1d9238d0edecccc288",
    "figure_id": "2310.01926v1-Figure24-1",
    "image_file": "2310.01926v1-Figure24-1.png",
    "caption": " Tracking results on the sequence b250fb0c-01a1b8d3 of the BDD100K validation set in the adaptation setting SHIFT → BDD100K. We analyze 5 consecutive frames centered around the frame #114 at time t̂ and spaced by k=0.2 seconds. We visualize the No Adap. baseline (top row) and DARTH (bottom row). On each row, green boxes represent correctly tracked objects, and orange boxes represent false negatives. We omit false positive boxes and ID switches for ease of visualization.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better at tracking objects, No Adap. or DARTH?",
    "answer": "DARTH performs better at tracking objects.",
    "rationale": "In the figure, the green boxes represent correctly tracked objects, and the orange boxes represent false negatives. DARTH has more green boxes and fewer orange boxes than No Adap., indicating that it is more accurate at tracking objects.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.01926v1",
    "pdf_url": null
  },
  {
    "instance_id": "96887882b8c24cdeb141c10d0b7398c2",
    "figure_id": "1811.12833v2-Figure3-1",
    "image_file": "1811.12833v2-Figure3-1.png",
    "caption": " Qualitative results in the GTA5→Cityscapes set-up. Column (a) shows a input image and the corresponding semantic segmentation ground-truth. Column (b), (c) and (d) show segmentation results (bottom) along with prediction entropy maps produced by different approaches (top). Best viewed in colors.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods produces the most accurate segmentation results?",
    "answer": "AdvEnt",
    "rationale": "The segmentation results in column (d) are the most similar to the ground-truth in column (a). This suggests that the AdvEnt method is the most accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.12833v2",
    "pdf_url": null
  },
  {
    "instance_id": "343025968dfd4f1294ab4a1397311ad5",
    "figure_id": "2004.05304v1-Figure10-1",
    "image_file": "2004.05304v1-Figure10-1.png",
    "caption": " Performance (mIoU) of ERFNet using different loss terms of IntRA-KD on ApolloScape-test. Here, “High” denotes the mimicking of high-level features (block 5) and “Middle” denotes the mimicking of middle-level features (block 3) of the teacher model. Lµi denotes the variant where only the i-th order moment is deployed for inter-region affinity distillation. Here, “40.4” is the performance of ERFNet without distillation and “43.2” is the performance of ERFNet-IntRA-KD that considers Lµ1 + Lµ2 + Lµ3 and La. The number besides each bar is performance gain brought by each loss term compared with ERFNet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the performance gain of using the High+Middle loss term compared to using no distillation at all?",
    "answer": "The performance gain is 1.9 mIoU.",
    "rationale": "The figure shows that the performance of ERFNet without distillation is 40.4 mIoU, and the performance of ERFNet-IntRA-KD that considers Lµ1 + Lµ2 + Lµ3 and La (High+Middle) is 43.2 mIoU. The difference between these two values is 1.9 mIoU.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.05304v1",
    "pdf_url": null
  },
  {
    "instance_id": "678a53a6f19c44a092947d01bd817b51",
    "figure_id": "2110.10031v2-Figure1-1",
    "image_file": "2110.10031v2-Figure1-1.png",
    "caption": " i-Blurry-N -M split. N% of classes are partitioned into the disjoint set and the rest into the BlurryM set where M denotes the blurry level (Aljundi et al., 2019c). To form the i-BlurryN -M task splits, we draw training samples from a uniform distribution from the ‘disjoint’ or the ‘BlurryM ’ set (Aljundi et al., 2019c). The ‘blurry’ classes always appear over the tasks while disjoint classes gradually appear.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which classes are more likely to appear in Task @ t+1 than in Task @ t?",
    "answer": "The classes in the \"N% Disjoint\" set.",
    "rationale": "The figure shows that the classes in the \"N% Disjoint\" set only appear at fixed tasks, while the classes in the \"BlurryM\" set always appear across tasks. This means that the classes in the \"N% Disjoint\" set are more likely to appear in Task @ t+1 than in Task @ t.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.10031v2",
    "pdf_url": null
  },
  {
    "instance_id": "211fd362380547509d6d6ecd3f6a99bc",
    "figure_id": "2201.10830v1-Figure6-1",
    "image_file": "2201.10830v1-Figure6-1.png",
    "caption": " Errors of depth estimation. We show the errors of depth estimation as a function of the depth (x-axis) for the baseline model (left) and our full model (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between depth and depth error?",
    "answer": "The depth error increases as the depth increases.",
    "rationale": "The figure shows that the points are clustered around a line that slopes upward, indicating that the depth error increases as the depth increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.10830v1",
    "pdf_url": null
  },
  {
    "instance_id": "ceed79cb8d9c4e15ae35b575baa5ae37",
    "figure_id": "2007.08489v2-Figure9-1",
    "image_file": "2007.08489v2-Figure9-1.png",
    "caption": " AP of instance segmentation and object detection models with backbones initialized with ε-robust models before training. Robust backbones generally lead to better AP, and the best robust backbone always outperforms the standard-trained backbone for every task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does the best robust backbone always outperform the standard-trained backbone for every task?",
    "answer": "Yes.",
    "rationale": "The figure shows that the best robust backbone (the one with the highest AP) always outperforms the standard-trained backbone (the one with the lowest AP) for both instance segmentation and object detection tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.08489v2",
    "pdf_url": null
  },
  {
    "instance_id": "d96d3594122c475a8d3bff582ead6fec",
    "figure_id": "1912.03263v3-Figure6-1",
    "image_file": "1912.03263v3-Figure6-1.png",
    "caption": " Distal Adversarials. Confidently classified images generated from noise, such that: p(y = “car”|x) > .9.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces images that are most similar to the original images?",
    "answer": "JEM",
    "rationale": "The JEM images are the only ones that are clearly recognizable as cars. The CNN and ADV images are much more distorted and difficult to interpret.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.03263v3",
    "pdf_url": null
  },
  {
    "instance_id": "b695d01b0a394800a7c0837dd856c56d",
    "figure_id": "2107.01850v2-Figure21-1",
    "image_file": "2107.01850v2-Figure21-1.png",
    "caption": " Larger Barabási–Albert graphs with 1000 nodes (excluding coloring which takes more than 80 extra interventions). (a). and (b). S = 1; (c). |I∗| = 100.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which intervention method is the most effective at reducing the relative extra rate for a sparsity constraint of 5?",
    "answer": "CliqueTree",
    "rationale": "In Figure (c), the CliqueTree method has the lowest relative extra rate for a sparsity constraint of 5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.01850v2",
    "pdf_url": null
  },
  {
    "instance_id": "db54b1196d4f4cb5bd3b328f1711f69d",
    "figure_id": "2302.07221v3-Figure6-1",
    "image_file": "2302.07221v3-Figure6-1.png",
    "caption": " Toy example in Trillos et al. [45, Section 5.2], Case 4-(i). Best viewed in color.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the three points x1, x2, and x3?",
    "answer": "The three points form an equilateral triangle.",
    "rationale": "The figure shows three circles of equal radius centered at x1, x2, and x3. The circles intersect at the vertices of an equilateral triangle.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.07221v3",
    "pdf_url": null
  },
  {
    "instance_id": "927602402c084ec487f9a71c6b2aff6c",
    "figure_id": "2111.04193v2-Figure4-1",
    "image_file": "2111.04193v2-Figure4-1.png",
    "caption": " Analysis of interactions in terms of length of source sentences provided to the model (a, c) and rewritten spans in the generated text (b, d). In each boxplot, the box indicates the interquartile range with the median values marked by the red line. Length is measured in terms of number of characters. We see that the model is more effective when given longer source context sentences (a) and generating smaller rewritten spans of text in the target sentences (b). Skilled writers find the model to be more effective (Table 8) because they play to the model’s strengths by writing longer context sentences (c) and requesting shorter spans to be rewritten in them (d).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which group of writers, skilled or novice, tended to write longer source context sentences?",
    "answer": "Skilled writers.",
    "rationale": "The box plot in panel (c) shows that the median source length for skilled writers is higher than the median source length for novice writers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.04193v2",
    "pdf_url": null
  },
  {
    "instance_id": "cbbe0b9cd04b4793af21bbc99aafe333",
    "figure_id": "2012.10880v1-Figure3-1",
    "image_file": "2012.10880v1-Figure3-1.png",
    "caption": " IoU histogram of different methods, CSP and the proposed W3Net, in which shows the W3Net generates more high quality proposals with IoU > 0.5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method generates more high-quality proposals with IoU > 0.5?",
    "answer": "W3Net",
    "rationale": "The figure shows that the W3Net histogram has more bars to the right of the 0.5 IoU mark than the CSP histogram. This indicates that W3Net generates more high-quality proposals with IoU > 0.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.10880v1",
    "pdf_url": null
  },
  {
    "instance_id": "6c6c98c543e04bf5853d9a4405420bcb",
    "figure_id": "2005.04107v1-Figure9-1",
    "image_file": "2005.04107v1-Figure9-1.png",
    "caption": " Results of the experiment with synthetic functions.We compare the sequential line search (SLS) [Koyama et al. 2017], the sequential plane search (SPS) using random plane construction, and our SPS using Bayesian optimization-based plane construction. We run 50 trials for each condition. Each plot shows the mean value with the colored regiong showing the standard deviation. Vertical axes represent optimality gaps (lower is better).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of optimality gap on the Isotropic Gaussian 5D function?",
    "answer": "Our SPS method performs best.",
    "rationale": "The figure shows that the green line, which represents our SPS method, has the lowest optimality gap for all iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.04107v1",
    "pdf_url": null
  },
  {
    "instance_id": "bbec940939264026949829fc6a9463ca",
    "figure_id": "2101.07600v1-Figure13-1",
    "image_file": "2101.07600v1-Figure13-1.png",
    "caption": " The adjacency matrix of the GC summary graph for the model given by Equation 14.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many causes are there in the model?",
    "answer": "4",
    "rationale": "The adjacency matrix shows the connections between causes and effects. Each row represents a cause, and each column represents an effect. The number of rows in the matrix is equal to the number of causes. In this case, there are 4 rows, so there are 4 causes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.07600v1",
    "pdf_url": null
  },
  {
    "instance_id": "c2edf350c0944c83a9c01b3c82702d8c",
    "figure_id": "2103.01050v1-Figure1-1",
    "image_file": "2103.01050v1-Figure1-1.png",
    "caption": " (a) shows the suspicious appearance of camouflages generated by previous work (i.e., UPC [19]). (b) is the painted car that commonly exists in the physical world. (c) shows the adversarial example (classified as pop bottle) generated by existing work (i.e., CAMOU [48]) and its corresponding attention map. (d) shows the adversarial example (classified as Shih-Tzu) generated by our DAS and its distracted attention map.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the images shows an adversarial example generated by DAS?",
    "answer": "(d)",
    "rationale": "The caption states that (d) shows the adversarial example generated by DAS and its distracted attention map.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.01050v1",
    "pdf_url": null
  },
  {
    "instance_id": "fb0debf779864c4ea0002792676e5498",
    "figure_id": "2004.13590v2-Figure3-1",
    "image_file": "2004.13590v2-Figure3-1.png",
    "caption": " Distribution of sentences containing different numbers of (golden) triggers of three datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest percentage of sentences with no triggers?",
    "answer": "ACE 2005.",
    "rationale": "The figure shows that the blue bar, which represents ACE 2005, is the highest for the category of 0 triggers per sentence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.13590v2",
    "pdf_url": null
  },
  {
    "instance_id": "3f1d4e82df3543ce9be9ab03306efd3f",
    "figure_id": "2106.02248v1-Figure4-1",
    "image_file": "2106.02248v1-Figure4-1.png",
    "caption": " Accuracy of dangling entity detection.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest accuracy for detecting dangling entities in EN-FR translation?",
    "answer": "BR",
    "rationale": "The figure shows the accuracy of three models (NNC, MR, and BR) for detecting dangling entities in six different language pairs. The bar for BR in EN-FR is the highest, indicating that BR has the highest accuracy for detecting dangling entities in EN-FR translation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02248v1",
    "pdf_url": null
  },
  {
    "instance_id": "9c646893fbb24c569c051ba799e8b52a",
    "figure_id": "2307.10200v1-Figure4-1",
    "image_file": "2307.10200v1-Figure4-1.png",
    "caption": " Gender inequality using text entailment. For a given unpleasant verb, a negative value indicates that a female has played the role of a victim more often than a male.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following verbs is most likely to have a female victim according to the entailment ratio gap?",
    "answer": "Rape",
    "rationale": "The figure shows the entailment ratio gap for a variety of unpleasant verbs. A negative value indicates that a female has played the role of a victim more often than a male. The verb \"rape\" has the largest negative entailment ratio gap, which means that it is most likely to have a female victim.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.10200v1",
    "pdf_url": null
  },
  {
    "instance_id": "8b230fac9b4640aea8fb64e11dc23005",
    "figure_id": "1811.06094v1-Figure11-1",
    "image_file": "1811.06094v1-Figure11-1.png",
    "caption": " The distribution of the scales (ρ2 i τ 2) for each of the rows of the target factor loading matrix W. The title of the plot is the type of sensor: accelerometer (Acc), electrocardiogram (EGC), gyroscope (Gyro), and magnetometer (Mag). X, Y, and Z indicate the axis of the measurement. Note that the y-axis is not shared amongst the plots and the magnetometer measurements are highly peaked near zero. Increasing values of a pruning threshold will cause an increasing number of the magnetometer sensors coefficients to go to zero. A reasonably chosen threshold may also result in pruning the Acc Chest X-axis sensor.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which sensor type is most likely to be pruned first when increasing the pruning threshold?",
    "answer": "The magnetometer sensors.",
    "rationale": "The figure shows the distribution of the scales for each sensor. The magnetometer sensors have a very sharp peak near zero, indicating that most of the values are close to zero. This means that even a small increase in the pruning threshold will cause many of the magnetometer sensors coefficients to go to zero.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.06094v1",
    "pdf_url": null
  },
  {
    "instance_id": "b8159f7180cf4449b372499bfa6e90f6",
    "figure_id": "2009.13272v1-Figure3-1",
    "image_file": "2009.13272v1-Figure3-1.png",
    "caption": " Model performance on limited amount of training data. The error bars indicate the standard deviation over 4 random trials. Ours-o is our model with its original labels. Ours-n is our model with numeric labels.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best when trained on only 25% of the data?",
    "answer": "Ours-n.",
    "rationale": "In Figure (a), the blue line representing Ours-n is higher than the other lines at the 25% mark on the x-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.13272v1",
    "pdf_url": null
  },
  {
    "instance_id": "e1ad78bc03354c5489bc61cb81e83501",
    "figure_id": "2012.14072v1-Figure7-1",
    "image_file": "2012.14072v1-Figure7-1.png",
    "caption": " Human evaluation results of DQN, ACL-DQN(A), ACL-DQN(B), and ACL-DQN(C), the number of test dialogues indicated on each bar.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method had the highest success rate in the human evaluation?",
    "answer": "ACL-DQN(C)",
    "rationale": "The figure shows the success rate of different methods, and ACL-DQN(C) has the highest bar, indicating it has the highest success rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.14072v1",
    "pdf_url": null
  },
  {
    "instance_id": "326f0a6e3c1648db9c97427a1b29cbf5",
    "figure_id": "2011.05429v1-Figure6-1",
    "image_file": "2011.05429v1-Figure6-1.png",
    "caption": " Diagnosing Mislabelled Training Examples. The Figure shows two training inputs along with feature attributions for each method. The correct label row corresponds to feature attributions derived from a model with the correct label in the training set. The incorrect-label row shows feature attributions derived from a model with the wrong label in the training set. We see that the attributions under both settings are visually similar.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which methods appear to be the most sensitive to changes in the training label?",
    "answer": "Grad, SGrad, SGradSQ, and VGrad.",
    "rationale": "These methods show the most significant differences in the feature attributions between the correct and incorrect label rows. For example, in the first row, the Grad, SGrad, SGradSQ, and VGrad methods highlight the bird's body and head in the correct label case, while they highlight the bird's wings and tail in the incorrect label case.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.05429v1",
    "pdf_url": null
  },
  {
    "instance_id": "95f23e91f52d4a888931f5a4fdaa5d19",
    "figure_id": "2106.02514v2-Figure11-1",
    "image_file": "2106.02514v2-Figure11-1.png",
    "caption": " More results from PA. From left to right, references, targets, PATN [56], PN-GAN [37], PoseWarp [3], MR-Net [47], Taming [14], our iLAT.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method generates the most realistic images?",
    "answer": "iLAT.",
    "rationale": "The images generated by iLAT are the most similar to the reference images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02514v2",
    "pdf_url": null
  },
  {
    "instance_id": "a9f3e50238bb48bea00930d450b3d74b",
    "figure_id": "2209.14345v1-FigureA-3-1",
    "image_file": "2209.14345v1-FigureA-3-1.png",
    "caption": "Figure A-3: We compare the effect of pre-training with different combinations of the components of the Audio Barlow Twins audio augmentation (AA) module. Results are shown both evaluated on FSD50K (blue) and the average score on the 5 HEAR-L tasks (red).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pre-training combination achieved the highest FSD50K score?",
    "answer": "Mixup + RRC + RLF.",
    "rationale": "The blue line in the figure represents the FSD50K score. The highest point on the blue line is achieved with the pre-training combination Mixup + RRC + RLF.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.14345v1",
    "pdf_url": null
  },
  {
    "instance_id": "d441562dd9ae45e2b16b4f3bb70cb556",
    "figure_id": "2206.05682v1-Figure2-1",
    "image_file": "2206.05682v1-Figure2-1.png",
    "caption": " MI-AL performance",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which active learning strategy performs best on the 20NewsGroup dataset?",
    "answer": "ADMIL-P-F",
    "rationale": "The plot shows that ADMIL-P-F achieves the highest mAP on the 20NewsGroup dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.05682v1",
    "pdf_url": null
  },
  {
    "instance_id": "8b75b242b290426d86381d629f457462",
    "figure_id": "2212.10132v2-Figure5-1",
    "image_file": "2212.10132v2-Figure5-1.png",
    "caption": " BD rate saving (%) of our CACD and CAFT methods on the Tecnick dataset. We use [30] without the auto-regressive context model as our baseline method.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest BD rate saving on the Tecnick dataset?",
    "answer": "Baseline + CACD + CAFT (Ours)",
    "rationale": "The figure shows that the Baseline + CACD + CAFT (Ours) method consistently achieves a higher BD rate saving than the other two methods at all PSNR levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10132v2",
    "pdf_url": null
  },
  {
    "instance_id": "c6d4073e75784ea3bdc1301779e625a3",
    "figure_id": "2111.01587v1-FigureA.5-1",
    "image_file": "2111.01587v1-FigureA.5-1.png",
    "caption": "Figure A.5: Training performance on Procgen. See Figure 3 in the main text for corresponding results on the test set. Left: the effect of planning. Right: the effect of self-supervision. Reporting median performance over 3 seeds (5 seeds for MZ and MZ+Recon), with shaded regions indicating min/max seeds.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of mean normalized score?",
    "answer": "MZ+SPR.",
    "rationale": "The figure shows the mean normalized score for different methods on the Procgen benchmark. The MZ+SPR method has the highest mean normalized score, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.01587v1",
    "pdf_url": null
  },
  {
    "instance_id": "b542389635c04b8a83048d8389c44550",
    "figure_id": "2006.16913v3-Figure24-1",
    "image_file": "2006.16913v3-Figure24-1.png",
    "caption": " (h): Illustration of Code Mutation for Karel task Diagonal from the Intro to Programming with Karel course by CodeHS.com [22]; We present the Karel variant of Fig. 4 here.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the meaning of the symbol ∧ in the figure?",
    "answer": "The symbol ∧ represents the logical AND operator.",
    "rationale": "The figure shows a set of logical constraints that define the possible actions that can be taken in a Karel task. The AND operator is used to combine multiple constraints, indicating that all of the constraints must be satisfied for the action sequence to be valid.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.16913v3",
    "pdf_url": null
  },
  {
    "instance_id": "bfea5e4ec6cb4a14a9b2ef5f4f528aa9",
    "figure_id": "1905.06649v1-Figure6-1",
    "image_file": "1905.06649v1-Figure6-1.png",
    "caption": " F1-score (all entities condition) of the three models, per mention type, and token frequency of each mention type.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on common nouns?",
    "answer": "biLSTM",
    "rationale": "The figure shows that biLSTM has the highest F1-score for common nouns.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.06649v1",
    "pdf_url": null
  },
  {
    "instance_id": "dfb94e876f1442338f8c45e0874e5d74",
    "figure_id": "2105.14781v1-Figure3-1",
    "image_file": "2105.14781v1-Figure3-1.png",
    "caption": " The before-attack (a) and after-attack accuracy (b) of methods with different sample sizes on SCT. The after-attack accuracy of Pro-A, CGA and Self-Talk is below 5.0%, and thus omitted in (b).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most robust to attacks?",
    "answer": "SEQA.",
    "rationale": "The figure shows that SEQA has the highest accuracy both before and after the attack, and its accuracy does not decrease as much as the other methods after the attack.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14781v1",
    "pdf_url": null
  },
  {
    "instance_id": "12bbf175ecdf42568b475354531e0474",
    "figure_id": "2203.08411v2-Figure7-1",
    "image_file": "2203.08411v2-Figure7-1.png",
    "caption": " Model Size vs. Entity Extraction F1 Score on CORD benchmark. The proposed FormNets significantly outperform other recent approaches – FormNetA2 achieves higher F1 score (97.10%) while using a 2.5x smaller model and 7.1x less pre-training data than DocFormer (96.99%; Appalaraju et al., 2021). FormNet-A3 obtains the highest 97.28% F1 score.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best performance?",
    "answer": "FormNet-A3 has the best performance.",
    "rationale": "The figure shows that FormNet-A3 achieves the highest F1 score of 97.28%, which is higher than all other models shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.08411v2",
    "pdf_url": null
  },
  {
    "instance_id": "59ad5c004627456d9bd03e3fe5028b4d",
    "figure_id": "1908.11216v3-Figure5-1",
    "image_file": "1908.11216v3-Figure5-1.png",
    "caption": " Strategy 1, σ = 5",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which lambda weight increases the fastest in the first 10 epochs?",
    "answer": "λToken",
    "rationale": "The plot shows the lambda weights for token, sentence, and text as a function of the number of epochs. The λToken line increases the most rapidly in the first 10 epochs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.11216v3",
    "pdf_url": null
  },
  {
    "instance_id": "310370feb2a842a195ceb822b9ba0799",
    "figure_id": "2304.15004v2-Figure7-1",
    "image_file": "2304.15004v2-Figure7-1.png",
    "caption": " Induced emergent reconstruction ability in shallow nonlinear autoencoders. (A) A published emergent ability at the BIG-Bench Periodic Elements task [28]. (B) Shallow nonlinear autoencoders trained on CIFAR100 [19] display smoothly decreasing mean squared reconstruction error. (C) Using a newly defined Reconstructionc metric (Eqn. 2) induces an unpredictable change.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which figure shows the results of the published emergent ability at the BIG-Bench Periodic Elements task?",
    "answer": "Figure A",
    "rationale": "Figure A shows the normalized score for the published emergent ability as a function of the number of language model effective parameters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.15004v2",
    "pdf_url": null
  },
  {
    "instance_id": "ff97b1d848934fd68e99d7a4d294c174",
    "figure_id": "2205.12467v1-Figure6-1",
    "image_file": "2205.12467v1-Figure6-1.png",
    "caption": " NER-based metrics vs NLI-Acc",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which NER-based metric has the highest NLI-Acc score when the NER-based metric is at its lowest value?",
    "answer": "MI",
    "rationale": "The figure shows that the MI line is at its highest point when the NER-based metric is at its lowest value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.12467v1",
    "pdf_url": null
  },
  {
    "instance_id": "3d4999ee4b1b4e46817ad46a615667f3",
    "figure_id": "2212.12393v3-Figure4-1",
    "image_file": "2212.12393v3-Figure4-1.png",
    "caption": " Inference time for a single input x for different nr. of digits. The left plot uses a log scale, and the right plot a linear scale. DSL stands for DeepStochLog. We use the GM variant of DPLA*.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods is the fastest for a single input x?",
    "answer": "DeepProbLog",
    "rationale": "The figure shows the inference time for different methods, with DeepProbLog being the fastest for all values of N.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.12393v3",
    "pdf_url": null
  },
  {
    "instance_id": "55de2073374b49d182a42f622ccd85d1",
    "figure_id": "2106.15482v2-Figure7-1",
    "image_file": "2106.15482v2-Figure7-1.png",
    "caption": " Test error vs. an estimated upper bound over 10 clients on CIFAR-10 with varying degrees of a training set data size on ResNet-18 (left) and MobileNetV2 (right). Each dot represents a combination of client and data size. In parenthesis - the average difference between the empirical and the test error.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better when the training set data size is 25%?",
    "answer": "MobileNetV2",
    "rationale": "The average difference between the empirical and the test error for MobileNetV2 at 25% training set data size is 0.21, while for ResNet-18 it is 0.24. A lower difference indicates better performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15482v2",
    "pdf_url": null
  },
  {
    "instance_id": "a0d76dbd64514a38b0ce5ce04ffb5893",
    "figure_id": "2107.00644v2-Figure14-1",
    "image_file": "2107.00644v2-Figure14-1.png",
    "caption": " DistractingCS. Episode return as a function of randomization intensity, for each of the 5 tasks from DMControl-GB (using ConvNets). Mean of 5 seeds. We find that the difficulty of DistractingCS varies greatly between tasks, but SVEA consistently outperforms DrQ in terms of generalization across all intensities and tasks, except for Ball in cup, catch at the highest intensity.",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which algorithm performs better in the Walker, walk task at low intensity levels?",
    "answer": " DrQ",
    "rationale": " The figure shows that the blue line (DrQ) is higher than the other two lines (SVEA with and without convolutional layers) at low intensity levels for the Walker, walk task. This indicates that DrQ achieves a higher episode return than SVEA at these intensity levels.\n\n**Figure type:** Plot",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.00644v2",
    "pdf_url": null
  },
  {
    "instance_id": "681ad19b2a8f45cb88b859020ecaf0cb",
    "figure_id": "2110.07584v2-Figure4-1",
    "image_file": "2110.07584v2-Figure4-1.png",
    "caption": " Comparison of different methods on inverted velocity maps of FlatFault (top) and CurvedFault (bottom). For FlatFault, our UPFWI-48K reveals more accurate details at layer boundaries and the slope of the fault in deep region. For CurvedFault, our UPFWI reconstructs the geological anomalies on the surface that best match the ground truth.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method best reconstructs the geological anomalies on the surface of the CurvedFault model?",
    "answer": "UPFWI-48K",
    "rationale": "The figure shows the inverted velocity maps of the CurvedFault model for different methods. The ground truth is shown in the leftmost column. The UPFWI-48K method (rightmost column) produces a velocity map that most closely matches the ground truth, including the geological anomalies on the surface.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.07584v2",
    "pdf_url": null
  },
  {
    "instance_id": "1b010c9637a24949bad6e825361f7f21",
    "figure_id": "1911.12861v2-Figure19-1",
    "image_file": "1911.12861v2-Figure19-1.png",
    "caption": " Visual comparison of semantic image synthesis results on the CelebAMask-HQ dataset. We compare Pix2PixHD, SPADE, and our method.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most realistic results?",
    "answer": "Our method.",
    "rationale": "The images produced by our method are the most similar to the ground truth images. This can be seen in the details of the faces, such as the eyes, nose, and mouth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.12861v2",
    "pdf_url": null
  },
  {
    "instance_id": "6eec27e17e064466b7c7f099fb3f0d0d",
    "figure_id": "1711.06614v2-Figure1-1",
    "image_file": "1711.06614v2-Figure1-1.png",
    "caption": " Frequency of reports equal to x for different P(x).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which P(x) value is most likely to be reported by the mechanism?",
    "answer": "0.3",
    "rationale": "The figure shows the frequency of reports equal to x for different P(x) values. The highest frequency is observed for P(x) = 0.3, indicating that this value is most likely to be reported by the mechanism.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.06614v2",
    "pdf_url": null
  },
  {
    "instance_id": "e99d050b259d48a4a3ccb6639565e69d",
    "figure_id": "2208.01018v3-Figure1-1",
    "image_file": "2208.01018v3-Figure1-1.png",
    "caption": " The five language pairs for each evaluation task/dataset for which we observe the largest performance improvement with Babel-FT mBERT. Each language pair score is obtained using the best layer for the specific dataset-model combination as reported in Table 4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language pair in the Tatoeba dataset shows the largest performance improvement with Babel-FT mBERT?",
    "answer": "ile-eng",
    "rationale": "The figure shows the accuracy for each language pair in the Tatoeba dataset for both vanilla mBERT and Babel-FT mBERT. The ile-eng language pair has the largest difference in accuracy between the two models, indicating the largest performance improvement.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.01018v3",
    "pdf_url": null
  },
  {
    "instance_id": "5d5c377fd95742609ecb6068e0fb297a",
    "figure_id": "1806.03915v3-Figure9-1",
    "image_file": "1806.03915v3-Figure9-1.png",
    "caption": " The samples from the IXI dataset held by four agents.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many brains are shown in the image?",
    "answer": "There are four brains shown in the image.",
    "rationale": "The image shows four different brains, each in a different position. The brains are all different sizes and shapes, which suggests that they belong to different individuals.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.03915v3",
    "pdf_url": null
  },
  {
    "instance_id": "1ae4dad0b3734a348731721fae6743ee",
    "figure_id": "2204.01469v2-Figure4-1",
    "image_file": "2204.01469v2-Figure4-1.png",
    "caption": " The heatmaps display the p-values calculated between pairs of estimators for mean absolute bias (MAB) and mean squared error (MSE) for Experiment 1. More purple values mean the estimator on the y-axis (Estimator 2) is better than the estimator on the x-axis (Estimator 1). Comparisons tend to become non-significant as N increases, since all the estimators gradually converge to the true entropy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which estimator performs better in terms of MSE when K = 1000 and N = 10000?",
    "answer": "MLE",
    "rationale": "The heatmap for MSE shows that when K = 1000 and N = 10000, the p-value for MLE vs. all other estimators is very close to 0 (dark purple), indicating that MLE is significantly better than all other estimators in terms of MSE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.01469v2",
    "pdf_url": null
  },
  {
    "instance_id": "b9476836217d4fbeb67ec84b05092544",
    "figure_id": "2210.02077v1-Figure6-1",
    "image_file": "2210.02077v1-Figure6-1.png",
    "caption": " Example of a dataset sampled with the procedure outlined in Appendix E.1. The only difference from the dataset in the experiment is the dimensionality and number of clusters which was minimized for visualization purposes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which cluster has the highest variance?",
    "answer": "Cluster 4",
    "rationale": "The variance of a cluster can be estimated by the size of the ellipse that surrounds it. In this case, cluster 4 has the largest ellipse, indicating that it has the highest variance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.02077v1",
    "pdf_url": null
  },
  {
    "instance_id": "a1aace260cfd48a8b0661f7907c3d40b",
    "figure_id": "2012.05292v1-Figure9-1",
    "image_file": "2012.05292v1-Figure9-1.png",
    "caption": " Exploration trajectories. Each row shows different exploration trajectories, going from blue to red, for a single environment. White represents traversable regions, while gray represents untraversable areas.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which environment has the most complex layout?",
    "answer": " Environment 2",
    "rationale": " Environment 2 has the most intricate and winding path, with many turns and changes in direction. This is evident from the exploration trajectories, which show the robot navigating through a series of narrow corridors and open spaces.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.05292v1",
    "pdf_url": null
  },
  {
    "instance_id": "7e1a390d6e5f41e4a0cfad3bbcb02ebc",
    "figure_id": "1911.05142v3-Figure1-1",
    "image_file": "1911.05142v3-Figure1-1.png",
    "caption": " Regret and Compensation for UCB, ε-Greedy and Thompson Sampling without reward drift.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest regret and compensation?",
    "answer": "Thompson Sampling",
    "rationale": "The figure shows that the regret and compensation for Thompson Sampling are lower than those for UCB and ε-Greedy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.05142v3",
    "pdf_url": null
  },
  {
    "instance_id": "b780b0f963e542038431f33def829165",
    "figure_id": "2012.14395v2-Figure2-1",
    "image_file": "2012.14395v2-Figure2-1.png",
    "caption": " (Best viewed in color) Variance in attribution robustness metrics, Top-k intersection score and Kendall’s correlation, for proposed method and RAR over all test samples on different datasets. Blue=Top-k intersection score; Green=Kendall’s correlation. Our method achieves significant improvement on both metrics over RAR for all datasets, while the variance is fairly similar to variance in RAR",
    "figure_type": "Table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest Top-k intersection score and Kendall's correlation on the Fashion-MNIST dataset?",
    "answer": "Our method.",
    "rationale": "The table in (b) shows that our method achieves a Top-k intersection score of 81.50% and a Kendall's correlation of 0.7216, which are both higher than the scores achieved by the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.14395v2",
    "pdf_url": null
  },
  {
    "instance_id": "0caebcd4a8254d40940e5ff6527a8104",
    "figure_id": "2112.03860v4-Figure4-1",
    "image_file": "2112.03860v4-Figure4-1.png",
    "caption": " Comparison of compressive sensing MRI inversion results (Accl=8x, SNR=20 dB).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which reconstruction method appears to be the most accurate based on visual inspection?",
    "answer": "G layers",
    "rationale": "The G layers reconstruction appears to be the most visually similar to the true image. It has the highest PSNR and SSIM values, which are metrics used to quantify the quality of an image reconstruction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.03860v4",
    "pdf_url": null
  },
  {
    "instance_id": "9f714f22047648b2b89cbc1cdee3c8ad",
    "figure_id": "2104.03493v1-Figure6-1",
    "image_file": "2104.03493v1-Figure6-1.png",
    "caption": " 3D face verification on LFW [22].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has better performance for 3D face verification on LFW, Ours(R) or Shang20?",
    "answer": "Ours(R) has better performance.",
    "rationale": "The figure shows that the ROC curve for Ours(R) is higher than the ROC curve for Shang20. This means that for a given False Acceptance Rate (FAR), Ours(R) has a higher True Acceptance Rate (TAR).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.03493v1",
    "pdf_url": null
  },
  {
    "instance_id": "1037561392164d2f9c7df3b92d6031e3",
    "figure_id": "2102.04130v3-Figure23-1",
    "image_file": "2102.04130v3-Figure23-1.png",
    "caption": " Man-woman share by ethnicity for all GPT-2 occupations with greater than 140 = n ∗ 0.25% mentions, making up 82% of returned valid responses.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which occupation has the largest proportion of White men?",
    "answer": "Construction worker",
    "rationale": "The figure shows the proportion of men and women of different ethnicities in various occupations. The bar for construction workers is the tallest and mostly red, indicating that the majority of construction workers are White men.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.04130v3",
    "pdf_url": null
  },
  {
    "instance_id": "82a7c8dc00a2408aa32f0bbec8d2bed6",
    "figure_id": "2002.06487v2-Figure2-1",
    "image_file": "2002.06487v2-Figure2-1.png",
    "caption": " Comparison of three algorithms using the simple MDP in Figure 1 with different values of µ, and thus different expected rewards. For µ = +0.1, shown in (a), the optimal ε-greedy policy is to take the Left action with 95% probability. For µ = −0.1, shown in in (b), the optimal policy is to take the Left action with 5% probability. The reported distance is the absolute difference between the probability of taking the Left action under the learned policy compared to the optimal ε-greedy policy. All results were averaged over 5, 000 runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better in the case of overestimation?",
    "answer": "Double Q-learning",
    "rationale": "In the case of overestimation (µ = +0.1), Double Q-learning has a lower distance to the optimal policy than the other algorithms, as shown in Figure (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.06487v2",
    "pdf_url": null
  },
  {
    "instance_id": "01a39b297bce44ccbd05090819aeb16b",
    "figure_id": "2105.14781v1-Figure5-1",
    "image_file": "2105.14781v1-Figure5-1.png",
    "caption": " The before-attack (a) and after-attack accuracy (b) of methods with different sample sizes on COPA. The after-attack accuracy of Pro-A, CGA and Self-Talk is below 10.0%, and thus omitted in (b).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest before-attack accuracy when the sample size is 50?",
    "answer": "SEQQA",
    "rationale": "The figure shows the before-attack accuracy of different methods for different sample sizes. When the sample size is 50, the SEQQA method has the highest accuracy, which is about 75%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14781v1",
    "pdf_url": null
  },
  {
    "instance_id": "d57f699c04e040d680777040a408b7ba",
    "figure_id": "2004.11660v2-Figure4-1",
    "image_file": "2004.11660v2-Figure4-1.png",
    "caption": " Face images generated by our DiscoFaceGAN. As shown in the figures, the variations of identity, expression, pose and illumination are highly disentangled, and we can precisely control expression, illumination and pose.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What factors are randomly varied in the DiscoFaceGAN model?",
    "answer": "Identity and other factors.",
    "rationale": "The figure shows that the inputs to the DiscoFaceGAN model are \"Random identities\" and \"Random other factors\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.11660v2",
    "pdf_url": null
  },
  {
    "instance_id": "37c3c8ed3da94eb4923887aa16b41552",
    "figure_id": "1806.10779v5-Figure7-1",
    "image_file": "1806.10779v5-Figure7-1.png",
    "caption": " Selected operations of each SN layer in ResNet50. There are 53 SN layers. (a,b) show the importance weights for µ and σ of (8, 32), while (c,d) show those of (8, 2). The y-axis represents the importance weights that sum to 1, while the x-axis shows different residual blocks of ResNet50. The SN layers in different places are highlighted differently. For example, the SN layers follow the 3 × 3 conv layers are outlined by shaded color, those in the shortcuts are marked with ‘ ’, while those follow the 1 × 1 conv layers are in flat color. The first SN layer follows a 7 × 7 conv layer. We see that SN learns distinct importance weights for different normalization methods as well as µ and σ, adapting to different batch sizes, places, and depths of a deep network.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which normalization method has the highest importance weight for σ in the residual block res3 with a batch size of 8 and a channel size of 32?",
    "answer": " 3x3 LN σ",
    "rationale": " The figure shows the importance weights for different normalization methods for both µ and σ. In the subfigure (b), we can see that for the residual block res3 with a batch size of 8 and a channel size of 32, the bar for 3x3 LN σ is the highest, indicating that it has the highest importance weight. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.10779v5",
    "pdf_url": null
  },
  {
    "instance_id": "502b6c913a884c95a8998d0d8db808a8",
    "figure_id": "2308.13266v3-Figure6-1",
    "image_file": "2308.13266v3-Figure6-1.png",
    "caption": " Qualitative results of MITS on VOT, compared with SOTA SOT methods MixFormer [13] and OSTrack [98].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method appears to be the most accurate in terms of object detection?",
    "answer": "MITS",
    "rationale": "The figure shows the results of different object detection methods on a video sequence. The ground truth (GT) is shown in the first row, and the results of the different methods are shown in the following rows. MITS appears to be the most accurate, as its bounding boxes are closest to the ground truth bounding boxes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.13266v3",
    "pdf_url": null
  },
  {
    "instance_id": "6070e14192d74561baf139e2d3f81325",
    "figure_id": "2104.08801v2-Figure6-1",
    "image_file": "2104.08801v2-Figure6-1.png",
    "caption": " Test set Confusion matrix of Out-of-domain (OOD) and In-domain classes for classifier probability threshold of 0.8.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of the in-domain samples were correctly classified as in-domain?",
    "answer": "92%",
    "rationale": "The confusion matrix shows that 92 out of the 140 in-domain samples were correctly classified as in-domain. This is calculated by dividing the number of correctly classified in-domain samples (92) by the total number of in-domain samples (140) and multiplying by 100.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.08801v2",
    "pdf_url": null
  },
  {
    "instance_id": "dfb956eafa37486e804bb09e29e73498",
    "figure_id": "2004.02097v1-Figure4-1",
    "image_file": "2004.02097v1-Figure4-1.png",
    "caption": " Example of 3D image registration on OASIS dataset. Left panel: axial, coronal, and sagittal view of source and target images. Right panel: deformed images and determinant of Jacobian of the transformations by FLASH, Quicksilver, Voxelmorph, and DeepFLASH.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method in the image best preserves the shape of the source image?",
    "answer": "DeepFLASH.",
    "rationale": "The deformed image generated by DeepFLASH is visually closest to the target image, and the determinant of the Jacobian is also closest to 1, indicating that there is less distortion in the deformation process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.02097v1",
    "pdf_url": null
  },
  {
    "instance_id": "dd32d3f2192b43d5b1d29ff93c70984e",
    "figure_id": "2201.06820v2-Figure4-1",
    "image_file": "2201.06820v2-Figure4-1.png",
    "caption": " Impact of the shard number on the unlearning efficiency and performance on Yelp2018 and Movielens-1m datasets. The bar chart shows the recommendation performance and the line chart shows the unlearning time cost.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest Recall@20 on the Yelp2018 dataset?",
    "answer": "LightGCN",
    "rationale": "The bar chart shows the Recall@20 for each method on the Yelp2018 dataset. The LightGCN bar is the tallest, indicating that it has the highest Recall@20.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.06820v2",
    "pdf_url": null
  },
  {
    "instance_id": "2103ae9152fe46f8b166d91173aff941",
    "figure_id": "2010.06659v1-Figure3-1",
    "image_file": "2010.06659v1-Figure3-1.png",
    "caption": " The WW spotter.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the input to the DNN?",
    "answer": "LFBE features",
    "rationale": "The figure shows a schematic of the WW spotter. The first step is to extract LFBE features from the input data. These features are then fed into a DNN, which outputs WW posteriors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.06659v1",
    "pdf_url": null
  },
  {
    "instance_id": "b8d483e0e4e44b68b27c8c0aaf15e57c",
    "figure_id": "2303.06682v2-Figure5-1",
    "image_file": "2303.06682v2-Figure5-1.png",
    "caption": " The results of noisy HSI super-resolution by different methods on HSI Cloth (scale factor=×2, σ=0.1).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best at denoising and super-resolving the HSI Cloth image?",
    "answer": "DDS2M",
    "rationale": "The DDS2M method achieved the highest PSNR of 25.485, indicating that it produced the image closest to the original.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.06682v2",
    "pdf_url": null
  },
  {
    "instance_id": "4241cc2c090e4758be1c55fe75f7e10c",
    "figure_id": "1709.06560v3-Figure16-1",
    "image_file": "1709.06560v3-Figure16-1.png",
    "caption": " TRPO Policy and Value Network activation",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activation function performs the best on the HalfCheetah-v1 environment?",
    "answer": "ReLU.",
    "rationale": "The plot on the left shows the average return for the HalfCheetah-v1 environment with different activation functions. The ReLU line is consistently above the other lines, indicating that it achieves the highest average return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1709.06560v3",
    "pdf_url": null
  },
  {
    "instance_id": "aee43df35ac44d37a071761fab83c7c3",
    "figure_id": "2101.12457v1-Figure5-1",
    "image_file": "2101.12457v1-Figure5-1.png",
    "caption": " The CSR performance by changing the value of RAR balancing factor _.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the most consistent performance across different values of λ?",
    "answer": "MovieLens-1M",
    "rationale": "The plot for MovieLens-1M shows that the performance of all three metrics (P@10, N@10, and R@10) remains relatively stable as λ varies. This suggests that the model's performance is less sensitive to changes in the RAR balancing factor for this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.12457v1",
    "pdf_url": null
  },
  {
    "instance_id": "4656eb8934cb4fac8616b5909ac5f916",
    "figure_id": "2309.06891v1-Figure3-1",
    "image_file": "2309.06891v1-Figure3-1.png",
    "caption": " Image classification on ImageNet-20. Supervised training of ResNet-18 for 100 epochs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which group of methods achieved the highest top-1 accuracy?",
    "answer": "Group 4",
    "rationale": "The figure shows the top-1 accuracy for different image classification methods on ImageNet-20. The methods are grouped into four groups, and the top-1 accuracy for each group is shown as a bar. The bar for group 4 is the highest, indicating that the methods in group 4 achieved the highest top-1 accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.06891v1",
    "pdf_url": null
  },
  {
    "instance_id": "0a68ba65de5a4debaa1a02492fa2bec4",
    "figure_id": "1907.09150v2-Figure2-1",
    "image_file": "1907.09150v2-Figure2-1.png",
    "caption": " Performance comparison of our algorithms with the C-SAGA on the MDP with S = 10. The performance is measured in terms of the number of oracle calls to achieve a certain objective gap.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the lowest objective gap with the fewest oracle calls?",
    "answer": "SVRPDA-I.",
    "rationale": "The plot shows that SVRPDA-I achieves a lower objective gap than both SVRPDA-II and C-SAGA with fewer oracle calls.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.09150v2",
    "pdf_url": null
  },
  {
    "instance_id": "392c22a04351439b85554d0850f4c935",
    "figure_id": "2204.03375v1-Figure2-1",
    "image_file": "2204.03375v1-Figure2-1.png",
    "caption": " Data format for human evaluation",
    "figure_type": "Table.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many people are going to the Rajmahal restaurant?",
    "answer": "Two.",
    "rationale": "The user specifies that they need a table for two people at 19:45 on Tuesday.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.03375v1",
    "pdf_url": null
  },
  {
    "instance_id": "8d34a4face354176be06619fb5141831",
    "figure_id": "1912.01196v3-Figure4-1",
    "image_file": "1912.01196v3-Figure4-1.png",
    "caption": " Detailed architecture of the proposed super resolving network (SRNet) (Green-block in Fig. 2). Four main residual networks are designed to perform as a large encoder-decoder scheme. RNet-A is used to update the hidden state while RNet-B and RNet-D act as an encoder and decoder respectively to map the hidden state as a super resolved intensity output (In+m).",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the network is responsible for mapping the hidden state as a super-resolved intensity output?",
    "answer": "RNet-B and RNet-D.",
    "rationale": "The passage states that \"RNet-B and RNet-D act as an encoder and decoder respectively to map the hidden state as a super resolved intensity output (In+m).\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.01196v3",
    "pdf_url": null
  },
  {
    "instance_id": "bca7939f40dc49e09c9470ae004f7db8",
    "figure_id": "2004.09197v1-Figure11-1",
    "image_file": "2004.09197v1-Figure11-1.png",
    "caption": " Our zero-shot generalized LSM model is more robust to occlusion than OFL [74] for video segmentation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more robust to occlusion for video segmentation?",
    "answer": "LSM-Zero",
    "rationale": "The figure shows that the LSM-Zero method is able to segment the bus more accurately than the OFL method, even when the bus is partially occluded by other objects.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.09197v1",
    "pdf_url": null
  },
  {
    "instance_id": "b47aacf1221f4ace957a1999ac8d759e",
    "figure_id": "1905.08232v2-Figure8-1",
    "image_file": "1905.08232v2-Figure8-1.png",
    "caption": " When the number of training data per-class is very limited (right bars), adversarially robust transfer learning [Transferred] is better overall. However, as the number of training data increases (left bars), fine-tuning with adversarial examples of the target domain [Fine-tuned with AT] results in an overall better performing model. Adversarially robust transfer learning is 3× faster than finetuning with adversarial examples of the target domain.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better when the number of training data points per class is very limited?",
    "answer": "Adversarially robust transfer learning.",
    "rationale": "The figure shows that when the number of training data points per class is very limited (right bars), the \"Transferred\" bars are higher than the \"Fine-tuned with AT\" bars. This means that adversarially robust transfer learning performs better than fine-tuning with adversarial examples of the target domain in this case.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.08232v2",
    "pdf_url": null
  },
  {
    "instance_id": "7791b6fa0d1140fba165fc14006e88ce",
    "figure_id": "1911.08669v1-Figure1-1",
    "image_file": "1911.08669v1-Figure1-1.png",
    "caption": " Average social welfare",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest average social welfare?",
    "answer": "IS-8RD",
    "rationale": "The figure shows the average social welfare for each algorithm. The IS-8RD line is the highest, indicating that it has the highest average social welfare.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.08669v1",
    "pdf_url": null
  },
  {
    "instance_id": "815b288fd0b0418085306ad1358819ff",
    "figure_id": "2202.04348v1-Figure2-1",
    "image_file": "2202.04348v1-Figure2-1.png",
    "caption": " Main simulation results of ECE, ECEsweep and MVCE. The bin numbers of ECE and MVCE are set as 32 in this experiment. The bin number of ECEsweep is fixed and determined by itself.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, ECE, ECE_sweep, or MVCE, has the lowest bias at the end of the simulation?",
    "answer": "MVCE",
    "rationale": "The plot shows the bias of each method as a function of the number of samples. At the end of the simulation (around 6000 samples), the red line representing MVCE is the lowest of the three lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.04348v1",
    "pdf_url": null
  },
  {
    "instance_id": "b29e9501c39f408aa220295becf18a58",
    "figure_id": "1809.07122v1-Figure4-1",
    "image_file": "1809.07122v1-Figure4-1.png",
    "caption": " Training loss and test accuracy of ResNet34 models w.r.t. number of effective passes on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which lambda value gives the highest test accuracy for ResNet34 on CIFAR-10?",
    "answer": "2e-5",
    "rationale": "The figure shows the test accuracy of ResNet34 models for different lambda values. The line corresponding to lambda = 2e-5 has the highest test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.07122v1",
    "pdf_url": null
  },
  {
    "instance_id": "71be3a0ee13e4c0590140a8df034080e",
    "figure_id": "2208.09106v2-Figure6-1",
    "image_file": "2208.09106v2-Figure6-1.png",
    "caption": " Our constrained, pessimistic, risk-sensitive method (CRiSP; yellow) accumulates larger positive rewards (left column) at the prescribed cost levels (right column) than other methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest positive rewards while staying within the prescribed cost levels?",
    "answer": "CRiSP",
    "rationale": "The figure shows the mean episode reward and mean episode cost for different methods on three different tasks. CRiSP (yellow) consistently achieves higher rewards than other methods while staying within the prescribed cost levels (indicated by the dashed red line).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.09106v2",
    "pdf_url": null
  },
  {
    "instance_id": "c244feec738f45038af40edb9a7ce97e",
    "figure_id": "2007.02095v1-Figure3-1",
    "image_file": "2007.02095v1-Figure3-1.png",
    "caption": " The recommendationdiversity on cold-start phase.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which recommendation algorithm performs best on the MovieLens dataset with a cutoff of 5?",
    "answer": "MF",
    "rationale": "The figure shows the performance of different recommendation algorithms on the MovieLens and Netflix datasets, with different cutoff values. The MF algorithm achieves the highest α-NDCG@5 score on the MovieLens dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.02095v1",
    "pdf_url": null
  },
  {
    "instance_id": "31db9dc95d9a436aab8382b8970496db",
    "figure_id": "2001.08614v2-Figure12-1",
    "image_file": "2001.08614v2-Figure12-1.png",
    "caption": " Empath [14] topics most strongly (anti-)associated with citation events (cf. Sec. 6.2 for description). Reference text not studied for hover event (Sec. 6.3) because unlikely to be visible to user before hovering.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which topic is most strongly associated with a user clicking on a reference, as opposed to not clicking on a reference?",
    "answer": "\"wedding\"",
    "rationale": "The figure shows the Empath topics that are most strongly associated with different citation events. In the plot for \"Click event (sentence text)\" in panel (a), \"wedding\" has the highest score for clicked references, meaning it is the topic most strongly associated with clicking on a reference.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.08614v2",
    "pdf_url": null
  },
  {
    "instance_id": "109bb6afbd174d7ca7f0408a3cc1d774",
    "figure_id": "1810.12366v1-Figure7-1",
    "image_file": "1810.12366v1-Figure7-1.png",
    "caption": " We show a word cloud of all the comments left by subjects after completing the tasks across all settings. From the frequency of positive comments about the tasks, it appears that subjects were enthusiastic to familiarize themselves with Vicki.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the overall sentiment of the comments left by subjects after completing the tasks across all settings?",
    "answer": "The overall sentiment of the comments is positive.",
    "rationale": "The word cloud shows that many of the words are positive, such as \"fun,\" \"interesting,\" \"accurate,\" \"right,\" \"thanks,\" \"correct,\" and \"like.\" This suggests that subjects were generally pleased with the tasks and enjoyed interacting with Vicki.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.12366v1",
    "pdf_url": null
  },
  {
    "instance_id": "43341a04a7b345589092de00bbc535ee",
    "figure_id": "2012.07988v1-Figure3-1",
    "image_file": "2012.07988v1-Figure3-1.png",
    "caption": " ROC curves of different models on the OCT dataset with “overall” being the anomlous class.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the OCT dataset with \"overall\" being the anomalous class?",
    "answer": "Skip-GANomaly(en)",
    "rationale": "The ROC curve for Skip-GANomaly(en) is closest to the top left corner of the plot, which indicates that it has the highest true positive rate and the lowest false positive rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.07988v1",
    "pdf_url": null
  },
  {
    "instance_id": "3b9f8c2e269b42a7bc94c1ddb85706ec",
    "figure_id": "2209.10529v1-Figure3-1",
    "image_file": "2209.10529v1-Figure3-1.png",
    "caption": " Visualisation of augmented samples.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which augmentation technique is most likely to be used to train a model to be robust to occlusion?",
    "answer": "Synthetic occlusion (keypoints).",
    "rationale": "This technique occludes parts of the image with other objects, such as the TV and the car in the example. This helps the model learn to recognize objects even when they are partially occluded.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.10529v1",
    "pdf_url": null
  },
  {
    "instance_id": "bd859e5d88294e489b346d6dd49f8805",
    "figure_id": "2007.08949v2-Figure4-1",
    "image_file": "2007.08949v2-Figure4-1.png",
    "caption": " NLL/RMSE for 100 test tasks for the cart-pole, pendubot and cart-double-pole with observed task parameters as task-descriptors. Across all environments, PAML performs significantly better than the baselines UNI and LHS.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods performs the best for the cart-double-pole environment?",
    "answer": "PAML",
    "rationale": "The figure shows the NLL and RMSE for the cart-double-pole environment for the different methods. The PAML method has the lowest NLL and RMSE, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.08949v2",
    "pdf_url": null
  },
  {
    "instance_id": "b1048783ae2d4782ae97d79932358d95",
    "figure_id": "1903.12529v1-Figure3-1",
    "image_file": "1903.12529v1-Figure3-1.png",
    "caption": " The performance comparison of different methods for scale factor 4 on image “102061” with motion kernel (first row) and image “189080” with disk kernel (second row). The blur kernel is shown on the upper-left corner of the super-resolved image by RCAN.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on image \"102061\" with motion kernel?",
    "answer": "DPSR (ours)",
    "rationale": "The PSNR and SSIM values are shown below each image. The higher the PSNR and SSIM values, the better the image quality. For image \"102061\" with motion kernel, DPSR (ours) has the highest PSNR and SSIM values of 23.75 and 0.739, respectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.12529v1",
    "pdf_url": null
  },
  {
    "instance_id": "704f03627c9f4478be0765be89354141",
    "figure_id": "2303.13752v1-Figure4-1",
    "image_file": "2303.13752v1-Figure4-1.png",
    "caption": " Accuracy and forgetting at each incremental step on CCH5000, averaged across three runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest accuracy when there are 8 classes and 1 new class is added per step?",
    "answer": "Our method",
    "rationale": "The plot on the left in (a) shows the accuracy of each method for different numbers of classes when 1 new class is added per step. The purple line, which represents our method, is the highest when there are 8 classes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.13752v1",
    "pdf_url": null
  },
  {
    "instance_id": "0d1e7ad2d2e74d20a050c2ca6f1fe16b",
    "figure_id": "2111.01692v2-Figure4-1",
    "image_file": "2111.01692v2-Figure4-1.png",
    "caption": " Analysis of auditory evoked fields (AEF) of one representative subject using eLORETA, MCE, thin and full Dugh. As it can be seen, thin and full Dugh can correctly localize bilateral auditory activity to Heschl’s gyrus, which is the characteristic location of the primary auditory cortex, with as few as 5 trials. In this challenging setting, all competing methods show inferior performance.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods shown in the figure is most accurate in localizing auditory activity in the brain?",
    "answer": "Thin and full Dugh.",
    "rationale": "The figure shows that both thin and full Dugh methods correctly localize bilateral auditory activity to Heschl’s gyrus, which is the characteristic location of the primary auditory cortex, with as few as 5 trials. In contrast, the other methods shown in the figure (eLORETA and MCE) show inferior performance in this challenging setting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.01692v2",
    "pdf_url": null
  },
  {
    "instance_id": "3bcfebfd5ab742559fbb0a6fdccb24db",
    "figure_id": "1905.12006v1-Figure7-1",
    "image_file": "1905.12006v1-Figure7-1.png",
    "caption": " (a) The precondition (top) and positive effect (bottom) for the DownLadder operator, which states that in order to execute the option, the agent must be standing above the ladder. The option results in the agent standing on the ground below it. The black spaces refer to unchanged low-level state variables. (b) Three problem-space partitions for the DownLadder operator. Each of the circled partitions is assigned a unique label and combined with the portable rule in (a) to produce a grounded operator. (c) The PDDL representation of the operator specified in (a).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the precondition for the DownLadder operator?",
    "answer": "The agent must be standing above the ladder.",
    "rationale": "The precondition is shown in the top image of (a). The agent is standing on top of the ladder.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12006v1",
    "pdf_url": null
  },
  {
    "instance_id": "2a9e29a324ee4e07844bd35ed5f497cf",
    "figure_id": "1810.03264v1-Figure12-1",
    "image_file": "1810.03264v1-Figure12-1.png",
    "caption": " The number of batches to reach test loss 130 by Variational Autoencoders (VAEs) on 1 worker, under staleness 0 to 16. We consider VAEs with depth 1, 2, and 3 (the number of layers in the encoder and the decoder networks). The numbers of batches are normalized by s = 0 for each VAE depth, respectively. Configurations that do not converge to the desired test loss are omitted, such as Adam optimization for VAE with depth 3 and s = 16.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization algorithm is the most efficient for training VAEs with depth 3?",
    "answer": "SGD with momentum.",
    "rationale": "The figure shows that SGD with momentum requires the least number of batches to reach test loss 130 for all depths, including depth 3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.03264v1",
    "pdf_url": null
  },
  {
    "instance_id": "134efe63c65740128d01169432fda7fa",
    "figure_id": "2106.07504v3-Figure1-1",
    "image_file": "2106.07504v3-Figure1-1.png",
    "caption": " Fidelity-unfairness trade-off of fairwashing attacks for equalized odds (∆EOdds), equal opportunity (∆EOpp), predictive equality (∆PE) and statistical parity (∆SP) metrics on Adult Income and COMPAS datasets, using logistic regression as explanation models. Vertical lines denote the unfairness of the black-box models. Results are averaged over 10 fairwashing attacks. The standard deviations are shown as shaded regions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric shows the greatest fidelity-unfairness trade-off for the XGBoost model on the COMPAS dataset?",
    "answer": "Statistical Parity (SP)",
    "rationale": "The figure shows the fidelity-unfairness trade-off for different metrics and models. For the XGBoost model on the COMPAS dataset, the SP curve has the largest area under the curve, indicating the greatest trade-off.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07504v3",
    "pdf_url": null
  },
  {
    "instance_id": "9c0c747a7a1f4976a6110865a5d5b818",
    "figure_id": "1902.08654v2-Figure4-1",
    "image_file": "1902.08654v2-Figure4-1.png",
    "caption": " Calibrated human judgments of conversational aspects for the baselines and best controlled models. Note: In Figure 3 and here, the Specificity and Question controlled models both include Repetition control, but Question control doesn’t include Specificity control, or vice versa. See Table 8 for exact numbers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the controlled models performs the best in terms of avoiding repetition?",
    "answer": "The Repetition-controlled (WD) model.",
    "rationale": "The figure shows the performance of different models on various conversational aspects, including avoiding repetition. The Repetition-controlled (WD) model has the highest score for avoiding repetition, indicating that it performs the best in this regard.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.08654v2",
    "pdf_url": null
  },
  {
    "instance_id": "603db5cf6f2d46769c52a080d438e3d2",
    "figure_id": "1905.13399v2-Figure8-1",
    "image_file": "1905.13399v2-Figure8-1.png",
    "caption": " Left: the adversarial perturbation generator’s estimate on the optimal noise emission timing (shown by color; dark represents later time, light represents earlier time) of the first noise segment at each time point. Each row represents one attack trial and 64 trials are shown in total. Right: the mean prediction error over time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which noise segment is the most effective at causing the model to make mistakes?",
    "answer": "The first noise segment.",
    "rationale": "The right panel of the figure shows that the error rate is highest for the first noise segment, and decreases for subsequent segments. This suggests that the first noise segment is the most effective at causing the model to make mistakes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.13399v2",
    "pdf_url": null
  },
  {
    "instance_id": "7070bbf9107442dc90798eb481e8c09d",
    "figure_id": "2109.12286v1-Figure5-1",
    "image_file": "2109.12286v1-Figure5-1.png",
    "caption": " (a) Convergence error ‖w − w∗‖2 + ‖θ − θ∗‖2 where (θ∗, w∗) = (0, 0) is the equilibrium. (b) The return R(θ) of the actor. The Stackelberg update eliminates cycling and hence, converges more directly to the equilibrium as can be seen in (a), whereas the individual gradient update oscillates significantly. Regularization helps to speed up convergence.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three algorithms converges most quickly to the equilibrium?",
    "answer": "Stackelberg w/ reg",
    "rationale": "The figure shows that the Stackelberg w/ reg algorithm has the smallest error after a given number of iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.12286v1",
    "pdf_url": null
  },
  {
    "instance_id": "032abf7c50a04f3480bedb6e33af6d3b",
    "figure_id": "2311.05067v2-Figure10-1",
    "image_file": "2311.05067v2-Figure10-1.png",
    "caption": " The success rate for the 6 AntMaze tasks. Every method except Oracle has no access to the reward function. Our method consistently performs well and nearly matches the oracle performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the umaze-diverse task?",
    "answer": "The Oracle method.",
    "rationale": "The figure shows the success rate for the 6 AntMaze tasks. The Oracle method is shown as a black dashed line and consistently has the highest success rate across all tasks, including the umaze-diverse task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2311.05067v2",
    "pdf_url": null
  },
  {
    "instance_id": "ae838bcf21dd4ddab754858877f5485d",
    "figure_id": "1905.03304v1-Figure3-1",
    "image_file": "1905.03304v1-Figure3-1.png",
    "caption": " Top left: input. Top right: result of ICP with random initialization. Bottom left: initial transformation provided by DCP. Bottom right: result of ICP initialized with DCP. Using a good initial transformation provided by DCP, ICP converges to the global optimum.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods produces the best results for aligning the point clouds?",
    "answer": "DCP+ICP",
    "rationale": "The figure shows that DCP+ICP produces the best results for aligning the point clouds. This is because DCP provides a good initial transformation for ICP, which helps ICP converge to the global optimum.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.03304v1",
    "pdf_url": null
  },
  {
    "instance_id": "a1b2618d743d42679053c615ee26b447",
    "figure_id": "1909.05122v1-Figure5-1",
    "image_file": "1909.05122v1-Figure5-1.png",
    "caption": " Sample complexity requirements. We let d = 5000, σ = 1 and w? S = 1S . The plot on the left computes the log2 error ratio for our method (stopping time chosen by cross-validation) and the lasso (λ chosen optimally using knowledge of w?). The plot on the right computes ‖wt 1Sc‖∞ for optimally chosen t.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better in terms of error, gradient descent or lasso?",
    "answer": "Gradient descent.",
    "rationale": "The plot on the left shows the log2 error ratio for gradient descent and lasso. Gradient descent has a lower error ratio, which means it performs better in terms of error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.05122v1",
    "pdf_url": null
  },
  {
    "instance_id": "b84dfa4f42a448fbbd6ed8c68d379396",
    "figure_id": "2110.14432v5-Figure9-1",
    "image_file": "2110.14432v5-Figure9-1.png",
    "caption": " Synthesized labels on half-moon data with 1-hot constraint. (a) ground truth labels. (b,c,d) synthesized labels by LAST. The purple line is the current classifier and the green dotted line is the optimal classifier.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the synthesized labels is closest to the ground truth labels?",
    "answer": "(b)",
    "rationale": "The ground truth labels show two distinct clusters of data points, one blue and one orange. The synthesized labels in (b) also show two distinct clusters of data points, one blue and one orange. The synthesized labels in (c) and (d) show more overlap between the two clusters, which is not as close to the ground truth labels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14432v5",
    "pdf_url": null
  },
  {
    "instance_id": "7353aa92470a4ed3ad166bc0667d3fd8",
    "figure_id": "2210.09396v1-Figure18-1",
    "image_file": "2210.09396v1-Figure18-1.png",
    "caption": " Average marginal effects of LIWC psycholinguistic lexical category lyrical features on listener affective responses, controlling for musical features and listener demographics. With the intent to reduce noise at the extremities, x-axis limits are capped at their 95% quantile values. Arranged in alphabetical order, standard errors are shown; valence in red, arousal in blue (Part 4/4).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which LIWC psycholinguistic lexical category has the strongest positive effect on listener valence?",
    "answer": "See.",
    "rationale": "The plot for See shows the largest positive average marginal effect for valence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.09396v1",
    "pdf_url": null
  },
  {
    "instance_id": "afd7f46bf4b7485cbc9ac719b9b5ac65",
    "figure_id": "1905.10307v4-Figure4-1",
    "image_file": "1905.10307v4-Figure4-1.png",
    "caption": " Multi-task curriculum training. The target tasks are three column patterns (AAB, ABA, and ABB) and the sole curriculum task is the ‘between’ relation. The green line indicates the reusability of the learned representations. The PrediNet out-performs all four of the baselines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models benefited the most from pre-training on the \"between\" relation task?",
    "answer": "PrediNet.",
    "rationale": "The PrediNet curve (dashed orange line) is consistently above all other curves in all of the subplots. This indicates that it achieved the highest test accuracy across all models and all batches.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10307v4",
    "pdf_url": null
  },
  {
    "instance_id": "28c85f2e0496406a8d6140777220535c",
    "figure_id": "1812.09755v1-Figure5-1",
    "image_file": "1812.09755v1-Figure5-1.png",
    "caption": " Average steps taken to complete an episode of StarCraft Explore10 Medic 50×50 task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most efficient in terms of the number of steps taken to complete an episode?",
    "answer": "CommNet",
    "rationale": "The plot shows that CommNet consistently takes the least number of steps to complete an episode compared to the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.09755v1",
    "pdf_url": null
  },
  {
    "instance_id": "6988f0cd06da4f7a928ddebdf18c4587",
    "figure_id": "1806.03863v2-Figure10-1",
    "image_file": "1806.03863v2-Figure10-1.png",
    "caption": " Performance/efficiency trade-off introduced by depth-parallelising models with multi-rate clocks on 4 GPUs. Note that the baseline models use multi-rate clocks hence the smaller speedups compared to tables 3 and 4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best trade-off between performance and efficiency?",
    "answer": "3 Parallel Par-Inception action.",
    "rationale": "The figure shows that 3 Parallel Par-Inception action has the highest frame per second throughput with a relatively low drop in performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.03863v2",
    "pdf_url": null
  },
  {
    "instance_id": "eb4d6b2bcc104d7bb263070610933516",
    "figure_id": "2005.09755v1-Figure6-1",
    "image_file": "2005.09755v1-Figure6-1.png",
    "caption": " The proportions of underdemanded pairs matched over the course of the simulation, by profile and algorithm. The “PRIORITIZED” algorithm matches using the original profile weights, while the “LINEAR PRIORITIZED” algorithm matches using the alternative weights given above.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm resulted in a higher proportion of underdemanded pairs matched for the 1-YRH profile?",
    "answer": "The PRIORITIZED algorithm.",
    "rationale": "The boxplot for the PRIORITIZED algorithm for the 1-YRH profile is higher than the boxplot for the LINEAR PRIORITIZED algorithm, indicating that a higher proportion of underdemanded pairs were matched using the PRIORITIZED algorithm.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.09755v1",
    "pdf_url": null
  },
  {
    "instance_id": "55fb738e86ce4aff85810676387a21b4",
    "figure_id": "2011.07635v1-Figure3-1",
    "image_file": "2011.07635v1-Figure3-1.png",
    "caption": " Plots showing the probability distribution of each child bandit of the HM-Bandit model on the QG task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three child bandits is the most stable in terms of probability distribution?",
    "answer": "QAP Child Bandit",
    "rationale": "The QAP Child Bandit has the least amount of fluctuation in its probability distribution compared to the other two child bandits. This can be seen in the plot for the QAP Child Bandit, where the probability distribution is relatively smooth and does not vary as much as the other two plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.07635v1",
    "pdf_url": null
  },
  {
    "instance_id": "3765177e5ed24794946dc38192e469cc",
    "figure_id": "2305.04835v3-Figure9-1",
    "image_file": "2305.04835v3-Figure9-1.png",
    "caption": " Average exact-match accuracy and structural accuracy with fictional words.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the accuracy of text-davinci-002 in predicting fictional words with structural accuracy?",
    "answer": "65.9%",
    "rationale": "The green bar in the middle of the plot shows the structural accuracy for fictional words for the model text-davinci-002. The top of the green bar is at about 65.9% on the y-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.04835v3",
    "pdf_url": null
  },
  {
    "instance_id": "e021fd2fbeaf438d9591c5a8d9bcbecb",
    "figure_id": "1805.07709v2-Figure5-1",
    "image_file": "1805.07709v2-Figure5-1.png",
    "caption": " Denoising results of an image from BSD68 with noise level 35.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the denoising methods produced the image with the highest PSNR?",
    "answer": "DURR.",
    "rationale": "The PSNR values are shown in the caption for each of the denoised images. DURR has the highest PSNR of 27.42dB.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.07709v2",
    "pdf_url": null
  },
  {
    "instance_id": "24aa4b19c21342ddbbe3139c87e1bffd",
    "figure_id": "1805.07043v1-Figure3-1",
    "image_file": "1805.07043v1-Figure3-1.png",
    "caption": " The outputs of the ReLU gates in GTRU.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which word in the sentence \"Average to good Thai food but terrible delivery\" has the highest activation value for the \"food\" gate?",
    "answer": "\"food\"",
    "rationale": "The figure shows the outputs of the ReLU gates in GTRU for the sentence \"Average to good Thai food but terrible delivery.\" The \"food\" gate is activated for words that are related to food. The word \"food\" has the highest activation value for the \"food\" gate, which is shown by the darkest square in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.07043v1",
    "pdf_url": null
  },
  {
    "instance_id": "20854c0f6e484ffe9df6a0cd2e703d2b",
    "figure_id": "1810.13243v1-Figure13-1",
    "image_file": "1810.13243v1-Figure13-1.png",
    "caption": " Validation Loss Surface (log scale) for points on the plane defined by {w30, w70, w30−70} including projections of iterates on this plane",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which point on the validation loss surface has the lowest loss value?",
    "answer": "w30",
    "rationale": "The validation loss surface shows the loss values for different points in the parameter space. The point with the lowest loss value is the one that is closest to the center of the innermost contour line. In this case, that point is w30.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.13243v1",
    "pdf_url": null
  },
  {
    "instance_id": "27d1cb924617466eb7486d89a6b223fb",
    "figure_id": "2112.13635v1-Figure1-1",
    "image_file": "2112.13635v1-Figure1-1.png",
    "caption": " (a) Conventional body representation generally used in top-down methods such as Mask-RCNN (He et al. 2017) and Rmpe (Fang et al. 2017) as well as bottomup methods such as CMU-pose (Cao et al. 2017) and AE (Newell, Huang, and Deng 2017). (b) Center-to-joint body representation proposed by CenterNet (Zhou, Wang, and Krähenbühl 2019). (c) Hierarchical structured body representation introduced by SPM (Nie et al. 2019). (d) An adaptive point set representation proposed by our method. (e) Inference time (s) versus precision (AP). Our method achieves the best speed-accuracy trade-offs compared with the representative bottom-up and single-stage methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest accuracy?",
    "answer": "Ours-HRNet.",
    "rationale": "The plot in the figure shows the accuracy and inference time for different methods. Ours-HRNet has the highest accuracy, as shown by the red dot at the top right of the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.13635v1",
    "pdf_url": null
  },
  {
    "instance_id": "50519de0097040db95dcec62eaa7ce1a",
    "figure_id": "2111.05496v2-Figure4-1",
    "image_file": "2111.05496v2-Figure4-1.png",
    "caption": " An equivalence to Figure 3 emphasizing the growth of the input dimension at each block.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the input dimension change at each block in the figure?",
    "answer": "The input dimension grows at each block.",
    "rationale": "The figure shows that the input to each block is the output of the previous block, and the output of each block is larger than the input. This means that the input dimension must be growing at each block.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.05496v2",
    "pdf_url": null
  },
  {
    "instance_id": "7fd90265293549d9bb3659f1b9571b79",
    "figure_id": "2209.10811v2-Figure2-1",
    "image_file": "2209.10811v2-Figure2-1.png",
    "caption": " Lowering distortion on the uninterest region. Inversion results of pSp [24], Restyle [3], and ours. An overlapped obstacle (i.e., hand) on the facial region precludes clean inversion. Firstly, pSp shows high distortion on the eyes and generates unrealistic facial shapes on the obstacle region. Restyle tries to reconstruct the obstacle region, but the reconstructed image shows artifacts on the nose and chin. On the contrary, our model shows the lowest distortion among the existing models, while maintaining high perceptual quality as shown above.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the least distortion in the uninterest region?",
    "answer": "Interestyle.",
    "rationale": "The figure shows the results of three different methods for inverting a face image: pSp, Restyle, and Interestyle. The original image has a hand covering part of the face, which is considered the uninterest region. pSp produces high distortion in the eyes and unrealistic facial shapes in the obstacle region. Restyle tries to reconstruct the obstacle region, but the reconstructed image shows artifacts on the nose and chin. Interestyle, on the other hand, shows the lowest distortion among the existing models, while maintaining high perceptual quality.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.10811v2",
    "pdf_url": null
  },
  {
    "instance_id": "0abbc8872a66431fad84dbc95c67d65c",
    "figure_id": "2110.00519v1-Figure4-1",
    "image_file": "2110.00519v1-Figure4-1.png",
    "caption": " A positive correlation between learned embedding magnitude and concept frequency confirms our motivating intuition: more frequent concepts have larger magnitudes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does the figure show a positive or negative correlation between embedding magnitude and concept frequency?",
    "answer": "Positive",
    "rationale": "The figure shows a scatter plot of embedding magnitude vs. concept frequency. The data points are clustered around a line that slopes upward from left to right, indicating a positive correlation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.00519v1",
    "pdf_url": null
  },
  {
    "instance_id": "92920ceff9854de9bcafae736f328f47",
    "figure_id": "2307.09619v1-Figure3-1",
    "image_file": "2307.09619v1-Figure3-1.png",
    "caption": " Fitting a log-normal distribution to the per-group sizes of the new text datasets we introduce: we show a Q-Q plot of the log quantiles of the per-group data sizes vs. those of a Gaussian distribution.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Do the per-group data sizes of the new text datasets appear to follow a log-normal distribution?",
    "answer": "Yes.",
    "rationale": "The Q-Q plots show that the log quantiles of the per-group data sizes are very close to the quantiles of a Gaussian distribution. This suggests that the per-group data sizes are well-approximated by a log-normal distribution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.09619v1",
    "pdf_url": null
  },
  {
    "instance_id": "82e7738ef14445ddb6b66aab7dc58607",
    "figure_id": "2011.14381v1-Figure9-1",
    "image_file": "2011.14381v1-Figure9-1.png",
    "caption": " Ablation study of goal-conditioned attention policy on Visual Rearrange with two objects (left) and out-of-distribution testing on Visual Rearrange with one object (right). We compare variants of the attention policy with only goal-conditional and only goal-unconditional attention heads, plus an alternative approach to aggregate sets of vector representations in the form of DeepSets (Zaheer et al., 2017). Our results demonstrate that both types of attention heads are necessary to achieve the best results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which variant of the attention policy performs best in the 2-object environment?",
    "answer": "SMORL",
    "rationale": "The figure on the left shows the average object distance for different variants of the attention policy in a 2-object environment. The SMORL line is consistently lower than the other lines, indicating that it has the lowest average object distance and therefore performs best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.14381v1",
    "pdf_url": null
  },
  {
    "instance_id": "5c47d1c4dac346d4ac883bcaf95cff26",
    "figure_id": "2305.15555v2-Figure4-1",
    "image_file": "2305.15555v2-Figure4-1.png",
    "caption": " Percentage improvement of the average performance after adding plasticity injection across all 57 Atari games. We take the maximum score among the agents with plasticity injection after 25M, 50M, and 100M steps to roughly estimate the improvement as if plasticity injection was applied at a proper timestep and to demonstrate what the performance could have been if plasticity loss was mitigated. Table 1 later presents a categorization of environments into buckets where the agent does and does not benefit from injection. Learning curves corresponding to each environment are available in Appendix A.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which Atari game benefited the most from plasticity injection?",
    "answer": "Years",
    "rationale": "The figure shows the percentage improvement of the average performance after adding plasticity injection across all 57 Atari games. The game with the highest percentage improvement is Years, which has a bar that reaches almost 50%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.15555v2",
    "pdf_url": null
  },
  {
    "instance_id": "d0bb8ac229cf4a4c92edf813771d3205",
    "figure_id": "2205.10186v3-Figure4-1",
    "image_file": "2205.10186v3-Figure4-1.png",
    "caption": " Performance across the 10 runs ±1 standard deviation. The x-axis represents the number of iterations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization algorithm appears to be the most efficient for the Branin problem?",
    "answer": "QB-MGP",
    "rationale": "The Branin plot shows that QB-MGP achieves the lowest NLL and RMSE values within the shortest number of iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.10186v3",
    "pdf_url": null
  },
  {
    "instance_id": "133b38d1a11a4e5c817904970cd4fe45",
    "figure_id": "2207.04242v2-Figure3-1",
    "image_file": "2207.04242v2-Figure3-1.png",
    "caption": " Illustrations of four comparison methods i.e., (a) Basic convolutional layers, (b) MLP-Mixer [17], (c) ConvMLP [18], (d) CrossMLP module [15] and (e) our parallelConvMLP. The symbols ⊕ and c© denote the element-wise addition and channel-wise concatenation.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four comparison methods utilizes both spatial and channel information in parallel?",
    "answer": "The proposed Parallel-ConvMLP module.",
    "rationale": "Figure (e) shows that the Parallel-ConvMLP module first processes the input through a series of convolutional layers. The output of these layers is then split into two branches: one branch applies a channel-wise MLP, while the other branch applies a spatial MLP. The outputs of these two branches are then concatenated and fed into a final convolutional layer. This parallel processing allows the module to extract both spatial and channel information from the input data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.04242v2",
    "pdf_url": null
  },
  {
    "instance_id": "b5bb4a30ef01460686dbf17e9c1e8c1e",
    "figure_id": "2110.15174v1-Figure21-1",
    "image_file": "2110.15174v1-Figure21-1.png",
    "caption": " Comparison of gradient norm on Cora dataset. The curve stops early at the largest training accuracy iteration.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the smallest gradient norm?",
    "answer": "GCNII (L=2)",
    "rationale": "The plot in the top left corner shows the gradient norms for each model. The GCNII (L=2) model has the lowest gradient norm throughout the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.15174v1",
    "pdf_url": null
  },
  {
    "instance_id": "52896a74db3747cfa74015cb200973ae",
    "figure_id": "1904.09288v1-Figure13-1",
    "image_file": "1904.09288v1-Figure13-1.png",
    "caption": " Comparison of the per-class breakdown frameAP at IoU threshold 0.5 on AVA.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class has the highest frameAP at IoU threshold 0.5 on AVA?",
    "answer": "\"person jumping\"",
    "rationale": "The figure shows a bar plot of the frameAP for each class at IoU threshold 0.5 on AVA. The height of each bar represents the frameAP for the corresponding class. The class with the highest bar is \"person jumping\", which indicates that it has the highest frameAP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.09288v1",
    "pdf_url": null
  },
  {
    "instance_id": "5af500ae72334f69a468902b2d6301eb",
    "figure_id": "2210.15748v2-Figure2-1",
    "image_file": "2210.15748v2-Figure2-1.png",
    "caption": " Query time for DESSERT vs. brute force on 1000 random sets of m glove vectors with the y-axis as a log scale. Lower is better.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the best query time performance?",
    "answer": "DESSERT.",
    "rationale": "The figure shows that DESSERT has the lowest query time for all values of m.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.15748v2",
    "pdf_url": null
  },
  {
    "instance_id": "ccd60dc20d0e41eca7ccab21f3c9c380",
    "figure_id": "2006.04139v2-FigureD.7-1",
    "image_file": "2006.04139v2-FigureD.7-1.png",
    "caption": "Figure D.7. Visual comparison of different SR methods on Urban100 [11] dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which SR method produces the sharpest and most realistic images?",
    "answer": "TTSR(Ours)",
    "rationale": "The figure shows that TTSR(Ours) produces images that are closest to the ground truth (GT) images. The other methods produce images that are either blurry or have artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04139v2",
    "pdf_url": null
  },
  {
    "instance_id": "a6b96d9db24e4b5f8e52ca3baeed2cad",
    "figure_id": "1904.03498v1-Figure5-1",
    "image_file": "1904.03498v1-Figure5-1.png",
    "caption": " ReLPV block STFT volume search. LP-mC3D3 network with STFT volume of 3× 3× 3 performs the best.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four networks achieves the highest clip accuracy?",
    "answer": "LP-mC3D3",
    "rationale": "The figure shows the clip accuracy of four networks over 16 epochs. The LP-mC3D3 network achieves the highest clip accuracy of approximately 54%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03498v1",
    "pdf_url": null
  },
  {
    "instance_id": "be354f6917fb4c2c9a78f799d3eb7832",
    "figure_id": "2112.03631v1-Figure16-1",
    "image_file": "2112.03631v1-Figure16-1.png",
    "caption": " Comparison with state-of-the-art methods. The first three rows: frontal face, the middle three rows: different expressions and poses, the last three rows: object occlusion (glasses or hair). From left to right, Target, Reference, BeautyGAN, PSGAN, SCGAN, Semantic, Ours. The value of each spatial position of semantic is obtained by the different weighted sum of the reference image according to semantic correspondence. In all cases, SSAT produces more accurate transfer results, especially for eye shadow and blush.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most accurate transfer results for eye shadow and blush?",
    "answer": "SSAT",
    "rationale": "The caption states that \"In all cases, SSAT produces more accurate transfer results, especially for eye shadow and blush.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.03631v1",
    "pdf_url": null
  },
  {
    "instance_id": "ba25fb6b7cef4dc381660ac87f8de97f",
    "figure_id": "1806.02985v3-Figure3-1",
    "image_file": "1806.02985v3-Figure3-1.png",
    "caption": " Illustrations of the value functions obtained by CTGP, CTKF, GPTD, and DTKF for time intervals ∆t = 20.0 and ∆t = 1.0. The policy is obtained by RL based on the cross-entropy method. CT approaches are not affected by the choice of ∆t.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods is most affected by the choice of ∆t?",
    "answer": "GPTD.",
    "rationale": "The figure shows that the value functions obtained by GPTD are more spread out for ∆t = 20.0 than for ∆t = 1.0, while the value functions obtained by the other methods are relatively similar for both values of ∆t. This suggests that GPTD is more sensitive to the choice of ∆t than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.02985v3",
    "pdf_url": null
  },
  {
    "instance_id": "2c672d0b508f4c1593f60fa968575205",
    "figure_id": "1901.11084v2-Figure3-1",
    "image_file": "1901.11084v2-Figure3-1.png",
    "caption": " Cartpole with Adam optimizer (left) and Acrobot (right) with deep networks. Learning rate parameter in parentheses.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which agent performed the best on the Acrobot task?",
    "answer": "S51 with PMF (0.01)",
    "rationale": "The right plot shows the average reward for each agent on the Acrobot task. The S51 with PMF (0.01) agent has the highest average reward, which means it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.11084v2",
    "pdf_url": null
  },
  {
    "instance_id": "47342c34a38d405294b980840eb8457e",
    "figure_id": "2203.14139v1-FigureA.1-1",
    "image_file": "2203.14139v1-FigureA.1-1.png",
    "caption": "Figure A.1: MDL probing compression across layers for source and target domain detection for LCC(en) dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best at source domain detection?",
    "answer": "RoBERTa",
    "rationale": "The plot shows that RoBERTa has the highest MDL probing compression for source domain detection across all layers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.14139v1",
    "pdf_url": null
  },
  {
    "instance_id": "5dd04ae969254a4c89765a8596c7d3e2",
    "figure_id": "1910.00057v3-Figure1-1",
    "image_file": "1910.00057v3-Figure1-1.png",
    "caption": " Number of optimization problems solved—i.e., loop iterations of Algorithm 1—before arriving at the optimal solution for different score functions. Each dot represents an instance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which score function performs better in terms of convergence for the Fannie Mae model?",
    "answer": "Vanilla.",
    "rationale": "The plot shows that Vanilla consistently converges in fewer iterations than the other two score functions for the Fannie Mae model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.00057v3",
    "pdf_url": null
  },
  {
    "instance_id": "9d75a97f884d41f68b16e22b4ed29945",
    "figure_id": "1812.04677v1-Figure1-1",
    "image_file": "1812.04677v1-Figure1-1.png",
    "caption": " Recall, precision, and average precision of InfoPath and DST on predicting the time-varying networks generated per day. The DST model is trained unsupervisedly on separate cascades using basic and enhanced features. The upper row uses graph-structured cascades from the ICWSM 2011 dataset. The lower row uses the subset of cascades with tree structures.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest average precision on graph data?",
    "answer": "Enhanced DST",
    "rationale": "The plot in (c) shows the average precision for each method on graph data. The Enhanced DST method (red line) has the highest average precision, followed by Basic DST (blue line) and InfoPath (green line).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.04677v1",
    "pdf_url": null
  },
  {
    "instance_id": "21f41c8adaa140a1a4510aa25bc7d580",
    "figure_id": "2202.04557v2-Figure9-1",
    "image_file": "2202.04557v2-Figure9-1.png",
    "caption": " Achieved perplexity on the WikiText dataset using transformer models with varying similarity functions across a range of learning rates. All successful similarity functions achieved similar results although the absolute value and Euclidean distance similarity functions appeared more sensitive to choices of the β hyperparameter.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which similarity function appears to be the least sensitive to the choice of beta hyperparameter?",
    "answer": "q*k",
    "rationale": "The plot for q*k shows the least variation in perplexity across different values of beta. This suggests that the q*k similarity function is the least sensitive to the choice of beta hyperparameter.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.04557v2",
    "pdf_url": null
  },
  {
    "instance_id": "c574212bb795477eb5578c76e6c443f7",
    "figure_id": "1904.07457v1-Figure5-1",
    "image_file": "1904.07457v1-Figure5-1.png",
    "caption": " (Best viewed magnified.) Image inpainting using the deep image prior. The posterior mean using SGLD (Panel (d)) achieves higher PSNR values and has fewer artifacts than SGD variants. See the Appendix for more comprisons.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate inpainting results according to the caption?",
    "answer": "SGLD mean",
    "rationale": "The caption states that the posterior mean using SGLD (Panel (d)) achieves higher PSNR values and has fewer artifacts than SGD variants.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.07457v1",
    "pdf_url": null
  },
  {
    "instance_id": "2154492f0ed04fc8bd79c5a91d7b9e29",
    "figure_id": "2210.12696v1-Figure3-1",
    "image_file": "2210.12696v1-Figure3-1.png",
    "caption": " Comparing encoded concepts of base models with their SST fine-tuned versions. X-axis = base model, Y-axis = fine-tuned model. Each cell in the matrix represents a percentage (aligned concepts/total concepts in a layer) between the base and fine-tuned models. Darker color means higher percentage. Detailed plots with actual overlap values are provided in the Appendix.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest percentage of aligned concepts between the base and fine-tuned versions?",
    "answer": "BERT.",
    "rationale": "The darkest color in the figure is in the top left corner, which corresponds to the BERT model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12696v1",
    "pdf_url": null
  },
  {
    "instance_id": "b75339d721a34adfb5b6a67f23c26372",
    "figure_id": "1711.08792v1-Figure2-1",
    "image_file": "1711.08792v1-Figure2-1.png",
    "caption": " A sample intrusion detection question. Here, ‘visual’ is the intruder word.",
    "figure_type": "\"other\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which word in the list is the odd one out?",
    "answer": "\"visual\"",
    "rationale": "All other words in the list are related to family relationships, while \"visual\" is not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.08792v1",
    "pdf_url": null
  },
  {
    "instance_id": "5e945d7d165c485ba4964de7b5252906",
    "figure_id": "2106.03097v5-Figure12-1",
    "image_file": "2106.03097v5-Figure12-1.png",
    "caption": " Local and global learning curves of FL methods. The accuracy of the local model is evaluated on: (Local Train) - the local private trains samples, and (Personalized): the test samples from the same label distribution with the local client",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which FL method performs the best on CIFAR-10 with sharding s = 2?",
    "answer": "FedAvg.",
    "rationale": "The figure shows that FedAvg has the highest local train accuracy and personalized accuracy on CIFAR-10 with sharding s = 2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03097v5",
    "pdf_url": null
  },
  {
    "instance_id": "d1c8e123390e456d99e649ac50d99ceb",
    "figure_id": "2002.07418v2-Figure4-1",
    "image_file": "2002.07418v2-Figure4-1.png",
    "caption": " Membership functions used in CartPole.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " How many membership functions are used for each variable in the CartPole environment?",
    "answer": " Three.",
    "rationale": " The figure shows the membership functions for each of the four variables in the CartPole environment: PoleAngle, PoleVelocityAtTip, CartPosition, and CartVelocity. Each variable has three membership functions, labeled NE (negative), PO (positive), and SM (small).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.07418v2",
    "pdf_url": null
  },
  {
    "instance_id": "44f24be5a09c4343b2cf016f77fae249",
    "figure_id": "2006.05400v2-Figure4-1",
    "image_file": "2006.05400v2-Figure4-1.png",
    "caption": " Minimal surface property: using SALD (middle) and SAL (right) with the input unsigned distance function of a curve with a missing part (left) leads to a solution (black line, middle and right) with approximately minimal length in the missing part area. Note that the SALD solution also preserves sharp features of the original shape, better than SAL.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods preserves sharp features of the original shape better?",
    "answer": "SALD",
    "rationale": "The caption states that \"the SALD solution also preserves sharp features of the original shape, better than SAL.\" This is evident in the middle and right images, where the SALD solution (middle) more closely resembles the original shape (left) than the SAL solution (right).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.05400v2",
    "pdf_url": null
  },
  {
    "instance_id": "00baa4a7b25a45ecb5b3ec2632d38e9f",
    "figure_id": "2102.07091v1-Figure4-1",
    "image_file": "2102.07091v1-Figure4-1.png",
    "caption": " Numerical results of DRDGD, DRGTA, DSA on MNIST data set. y-axis: log-scale ds(x̄k, x ∗).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges the fastest for both ring graphs with 20 and 40 nodes?",
    "answer": "DRDGD with t=1 and beta hat = 0.1",
    "rationale": "The figure shows the convergence of different algorithms on the MNIST data set for two different ring graphs. The y-axis shows the log-scale distance between the current iterate and the optimal solution. The algorithm that converges the fastest is the one whose curve reaches the smallest value of the y-axis in the fewest number of iterations. For both ring graphs, DRDGD with t=1 and beta hat = 0.1 converges the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.07091v1",
    "pdf_url": null
  },
  {
    "instance_id": "09dc11b3d9124a4680a479573b25139e",
    "figure_id": "1902.00175v1-Figure4-1",
    "image_file": "1902.00175v1-Figure4-1.png",
    "caption": " Evaluating performance of different methods on dating documents with and without time mentions. Please see Section 7.3 for details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on dating documents with time mentions?",
    "answer": "NeuralDater",
    "rationale": "The figure shows the accuracy of different methods on dating documents with and without time mentions. NeuralDater has the highest accuracy on dating documents with time mentions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.00175v1",
    "pdf_url": null
  },
  {
    "instance_id": "bdb0b5b3893c44239d64b4f4c1ef1b7a",
    "figure_id": "2010.02830v1-Figure4-1",
    "image_file": "2010.02830v1-Figure4-1.png",
    "caption": " QA performance comparison between PROVER and RuleTakers with models trained on DU0, DU1, DU2 and DU3 and tested on DU5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on DU1, RT or PR?",
    "answer": "PR",
    "rationale": "The plot shows that the accuracy of PR on DU1 is 73.7%, while the accuracy of RT on DU1 is 63.5%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02830v1",
    "pdf_url": null
  },
  {
    "instance_id": "7f3d943dd0eb46d38cf11247063a1e29",
    "figure_id": "1909.07623v1-Figure5-1",
    "image_file": "1909.07623v1-Figure5-1.png",
    "caption": " Optical flow refinement incorporating the raw ToF depth measurements greatly refines flow quality.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the raw ToF depth measurements?",
    "answer": "The third image from the left in the top row.",
    "rationale": "The image is labeled \"ToF depth\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.07623v1",
    "pdf_url": null
  },
  {
    "instance_id": "3f08efffb10742faacb492a7417aee21",
    "figure_id": "2006.10726v3-Figure1-1",
    "image_file": "2006.10726v3-Figure1-1.png",
    "caption": " Predictions with lower entropy have lower error rates on corrupted CIFAR-100-C. Certainty can serve as supervision during testing.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the error rate change as the entropy increases?",
    "answer": "The error rate increases as the entropy increases.",
    "rationale": "The figure shows a bar plot of the error rate (%) vs. the entropy. The bars increase in height from left to right, indicating that the error rate increases as the entropy increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.10726v3",
    "pdf_url": null
  },
  {
    "instance_id": "a13a7c73d87842be9edf8b1a654a8bab",
    "figure_id": "1804.02958v3-Figure15-1",
    "image_file": "1804.02958v3-Figure15-1.png",
    "caption": " Notice that our model produces much better sky and grass textures than [34], and also preserves the texture of the light tower more faithfully.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image is the original photograph?",
    "answer": "The leftmost image is the original photograph.",
    "rationale": "The caption states that the leftmost image is the original photograph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.02958v3",
    "pdf_url": null
  },
  {
    "instance_id": "723f61ad52814140a7b0e388cafde5a9",
    "figure_id": "2205.11027v3-Figure7-1",
    "image_file": "2205.11027v3-Figure7-1.png",
    "caption": " Quantitative experiments of Theorem 1 on the D4RL MuJoCo-medium datasets. The red star-shaped dots are the interpolated data and the circle dots are the extrapolated data. The color of the dots represents ‖Qθ(x)−Qθ(ProjD(x))‖ values in (a) and g(x) values in (b), respectively. The darker the color, the smaller the corresponding value. In (a), the yellow dash line is the empirical upper bound of ‖Qθ(x)−Qθ(ProjD(x))‖.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \nDoes the figure show that the empirical upper bound of ‖Qθ(x)−Qθ(ProjD(x))‖ is a good approximation of the actual value of ‖Qθ(x)−Qθ(ProjD(x))‖? ",
    "answer": " \nNo. ",
    "rationale": " \nThe figure shows that the empirical upper bound of ‖Qθ(x)−Qθ(ProjD(x))‖ is often much larger than the actual value of ‖Qθ(x)−Qθ(ProjD(x))‖. This is because the empirical upper bound is based on the worst-case scenario, while the actual value of ‖Qθ(x)−Qθ(ProjD(x))‖ can be much smaller for many data points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.11027v3",
    "pdf_url": null
  },
  {
    "instance_id": "59be7f456b604088913efb52e2b6def1",
    "figure_id": "2004.14065v2-Figure1-1",
    "image_file": "2004.14065v2-Figure1-1.png",
    "caption": " An example from our dataset, with fixed grammatical gender. Red (italic) stands for masculine, cyan (normal) stands for feminine.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What grammatical gender is the word \"victime\" in French?",
    "answer": "Feminine",
    "rationale": "The word \"victime\" is highlighted in cyan, which according to the caption indicates that it is feminine.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14065v2",
    "pdf_url": null
  },
  {
    "instance_id": "6b668a03a851496d96d1c88b95471d63",
    "figure_id": "2111.07393v1-Figure6-1",
    "image_file": "2111.07393v1-Figure6-1.png",
    "caption": " Entity translation accuracy aggregated over different entity sets for Ukrainian.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pre-training method consistently leads to the best entity translation accuracy?",
    "answer": "DEEP + MT",
    "rationale": "The figure shows that the DEEP + MT line is consistently above the other lines, regardless of the number of fine-tuning steps or the entity set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.07393v1",
    "pdf_url": null
  },
  {
    "instance_id": "b84483f5f06242a7a6d2851bda2603fa",
    "figure_id": "2012.01988v6-Figure2-1",
    "image_file": "2012.01988v6-Figure2-1.png",
    "caption": " Ensembles work well in the large computation regime and cascades show benefits in all computation regimes. These cascades are directly converted from ensembles without optimizing the choice of models (see Sec. 4). Black dots represent single models. Ensembles: Ensembles are more cost-effective than large single models, e.g., EfficientNet-B5/B6/B7 and ResNet-152/200. Cascades: Converting ensembles to cascades significantly reduces the FLOPs without hurting the full ensemble accuracy (each star is on the left of a square).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture is the most efficient in terms of accuracy per FLOP?",
    "answer": "MobileNetV2",
    "rationale": "The figure shows the accuracy and FLOPs for different model architectures. MobileNetV2 achieves the highest accuracy for the lowest number of FLOPs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.01988v6",
    "pdf_url": null
  },
  {
    "instance_id": "bfbc3940ba9949b2a746a655f057f6e7",
    "figure_id": "1905.05702v2-Figure6-1",
    "image_file": "1905.05702v2-Figure6-1.png",
    "caption": " Training timing on three DE EN runs. Markers show validation checkpoints for one of the runs.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two activation functions, softmax or 1.5-entmax, converges to a higher validation accuracy faster?",
    "answer": "1.5-entmax.",
    "rationale": "The figure shows that the validation accuracy for 1.5-entmax is higher than the validation accuracy for softmax at all time points. This indicates that 1.5-entmax converges to a higher validation accuracy faster.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.05702v2",
    "pdf_url": null
  },
  {
    "instance_id": "a92ce7abf2bc417ebed774604f542120",
    "figure_id": "2302.04599v3-Figure4-1",
    "image_file": "2302.04599v3-Figure4-1.png",
    "caption": " Changing the Source Node: In this case, the source node is P4 (a professor) and we obtain a different, although intuitive partitioning: P5 is a colleague of P4, {P1,P2,P6,P7,P8} are P4’s students, P3 is a student that P4 is not teaching and {B1,B2} are the academic books of the department. Note how it is possible to extract these abstract concepts, even though the only explicit information we initially provided was that entities are either books or people.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between P4 and P3?",
    "answer": "P3 is a student that P4 is not teaching.",
    "rationale": "The figure shows that P4 is a professor and P3 is a student. However, there is no direct connection between P4 and P3, indicating that P4 is not teaching P3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.04599v3",
    "pdf_url": null
  },
  {
    "instance_id": "7389f9b244074dc79931a6b546631f02",
    "figure_id": "1908.03631v1-Figure9-1",
    "image_file": "1908.03631v1-Figure9-1.png",
    "caption": " Additional ablation study. For both datasets, we analyze the independent and combined effects of our skip functions (DispSkip) and the conditional entropy on top of the single-image hyperprior model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the Cityscapes dataset in terms of MS-SSIM?",
    "answer": "DispSkip + CEO + Hyp",
    "rationale": "The figure shows that the DispSkip + CEO + Hyp method has the highest MS-SSIM values for all bitrates on the Cityscapes dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.03631v1",
    "pdf_url": null
  },
  {
    "instance_id": "5076cc3e5b6d4e89a4b7225ea4acc12d",
    "figure_id": "2105.06669v3-Figure4-1",
    "image_file": "2105.06669v3-Figure4-1.png",
    "caption": ": Four partitions of a rectangular land into three axis-aligned rectangles. Each of these partitions can be a 1-out-of-3 maximin partition of an agent. The white space between the green rectangles has thickness s.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the maximum number of agents that can be accommodated in the rectangular land with a 1-out-of-3 maximin partition?",
    "answer": "3",
    "rationale": "Each of the four partitions shown in the figure can accommodate 3 agents, with each agent getting one of the three axis-aligned rectangles.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.06669v3",
    "pdf_url": null
  },
  {
    "instance_id": "298cfc03352f4441983deda9fb728d26",
    "figure_id": "2303.02444v1-Figure6-1",
    "image_file": "2303.02444v1-Figure6-1.png",
    "caption": " AUROC (top) and AUPR (bottom) for OOD detection using ViTs trained on CIFAR100.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best AUROC performance on the CIFAR100 to CIFAR10 transfer task?",
    "answer": "SGPAE",
    "rationale": "The bar for SGPAE is the highest in the top row of plots, which corresponds to the AUROC performance on the CIFAR100 to CIFAR10 transfer task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.02444v1",
    "pdf_url": null
  },
  {
    "instance_id": "87dd58ba3ef0419cbef0d77ca1b46a8d",
    "figure_id": "2206.06878v1-Figure6-1",
    "image_file": "2206.06878v1-Figure6-1.png",
    "caption": " The clustered temporal multivariate distribution of four variables across all time stages (4 × 12 dimensional). For example, one observation of temperature within cluster 1 at time stage 1 are used in removing prediction uncertainties of humidity in cluster 2 at time stage 3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which cluster and time stage combination has the highest value?",
    "answer": "Cluster 1 at time stage 1.",
    "rationale": "The figure shows a heatmap of the values, with the color red indicating the highest values. The darkest red square is located in the top left corner, which corresponds to cluster 1 and time stage 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.06878v1",
    "pdf_url": null
  },
  {
    "instance_id": "ffb59b69cfbc44bfac9b981b956693d9",
    "figure_id": "1909.00700v3-Figure2-1",
    "image_file": "1909.00700v3-Figure2-1.png",
    "caption": " Architecture and Pipeline of TTFNet. Features are extracted by a backbone network and then up-sampled to 1/4 resolution of the original image. Then, the features are used for localization and regression tasks. For localization, the network can produce higher activations near the object center. For regression, all samples inside the Gaussian-area of the object can directly predict the distance to four sides of the box.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two tasks that the network performs after extracting features from the image?",
    "answer": "Localization and regression.",
    "rationale": "The figure shows that the features extracted by the backbone network are used for two tasks: localization and regression. Localization is used to identify the center of the object, while regression is used to predict the distance to the four sides of the bounding box.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.00700v3",
    "pdf_url": null
  },
  {
    "instance_id": "52fe65124ccf4689ad4505cac857aa6e",
    "figure_id": "2211.00918v1-Figure14-1",
    "image_file": "2211.00918v1-Figure14-1.png",
    "caption": " Qualitative results on the effectiveness of adapters on a vector art.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three images has the highest PSNR/BPP?",
    "answer": "The \"Ours\" image has the highest PSNR/BPP.",
    "rationale": "The PSNR/BPP values are shown below each image. The \"Ours\" image has a PSNR/BPP of 35.4, which is higher than the other two images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.00918v1",
    "pdf_url": null
  },
  {
    "instance_id": "4ef0fcae0a9043589b3cd29ead88ad76",
    "figure_id": "2010.06897v2-Figure5-1",
    "image_file": "2010.06897v2-Figure5-1.png",
    "caption": " a) Comparison between all methods, shown as recall@N averaged over the 5 target domains (Average Recall@N). The base CNN encoder is denoted in brackets: (A)lexNet and (R)esNet18. b) Results of experiments with AdAGeo with 0-shots, 1-shot, 5-shots, 50-shots and all-shots. With easier domains (Snow, Rain, Overcast) AdAGeo shows a good improvement in accuracy with just 1 target domain image, while with more challenging domains (Sun, Night) AdAGeo requires a higher amount of images to perform significant improvements.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on average across all domains when using all available target images?",
    "answer": "AdaGeo (R)",
    "rationale": "The figure shows that AdaGeo (R) has the highest Recall@1 when using all available target images, as shown in subfigure (b).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.06897v2",
    "pdf_url": null
  },
  {
    "instance_id": "de27100a6d0343f49c467ac3c3c0f0e9",
    "figure_id": "2010.06709v1-Figure2-1",
    "image_file": "2010.06709v1-Figure2-1.png",
    "caption": " (a)-(c) Cumulative regrets for three LDP algorithms; (d)-(f) Cumulative regrets (and standard variance) for non-private versions on heavy-tailed data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the non-private synthetic data with a kSE kernel?",
    "answer": "MoMA-GP-UCB",
    "rationale": "The figure shows the cumulative regret for each algorithm on the non-private synthetic data with a kSE kernel. MoMA-GP-UCB has the lowest cumulative regret, indicating it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.06709v1",
    "pdf_url": null
  },
  {
    "instance_id": "2b223413193d46c08952c8fb553ea596",
    "figure_id": "1807.02234v1-Figure1-1",
    "image_file": "1807.02234v1-Figure1-1.png",
    "caption": " Regression coefficient recovery performance for different corruption ratios.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best for a corruption ratio of 0.2 in Figure (a)?",
    "answer": "DSPL.",
    "rationale": "The DSPL curve is the lowest at a corruption ratio of 0.2 in Figure (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1807.02234v1",
    "pdf_url": null
  },
  {
    "instance_id": "620c33b526724e29bd691d93a003bb61",
    "figure_id": "2111.10952v2-Figure2-1",
    "image_file": "2111.10952v2-Figure2-1.png",
    "caption": " EXMIX task sizes in log scale. The dashed line is the 3× 105 sampling rate cap.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the maximum task size in the EXMIX dataset?",
    "answer": "Approximately 3 × 10^5.",
    "rationale": "The dashed line in the figure represents the 3 × 10^5 sampling rate cap, and the bars in the figure represent the task sizes. The tallest bar in the figure is just below the dashed line, indicating that the maximum task size is approximately 3 × 10^5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.10952v2",
    "pdf_url": null
  },
  {
    "instance_id": "e4051bd32d0b41c1ad43e4b7676f79e0",
    "figure_id": "2201.05863v1-Figure2-1",
    "image_file": "2201.05863v1-Figure2-1.png",
    "caption": " Performance gains from curriculum learning",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on clean data?",
    "answer": "ConvMixer †",
    "rationale": "The figure shows the accuracy of different models on clean and noisy data. The ConvMixer † model has the highest accuracy on clean data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.05863v1",
    "pdf_url": null
  },
  {
    "instance_id": "272f3e7e7d534cc6b3ff2330438e886c",
    "figure_id": "1911.12125v2-Figure7-1",
    "image_file": "1911.12125v2-Figure7-1.png",
    "caption": " The comparison between symmetric processing and asymmetric processing.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which type of processing achieves better mAP on both datasets? ",
    "answer": " Asymmetric processing. ",
    "rationale": " The figure shows that the blue line, which represents asymmetric processing, is consistently higher than the orange line, which represents symmetric processing, for both the MS-COCO and NUS-WIDE datasets. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.12125v2",
    "pdf_url": null
  },
  {
    "instance_id": "53318632c87c4f6b8bd1882ae13bf43a",
    "figure_id": "2003.06209v1-Figure3-1",
    "image_file": "2003.06209v1-Figure3-1.png",
    "caption": " The relationship between the ratio of word overlapping and relative improvement measured by F1 scores",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which domain exhibits the largest relative improvement in F1 scores as the ratio of overlapping vocabularies with SNIL increases?",
    "answer": "Phones",
    "rationale": "The plot shows the relative improvement in F1 scores as a function of the ratio of overlapping vocabularies with SNIL for different domains. The data point for \"Phones\" is the highest on the plot, indicating that this domain experiences the largest relative improvement in F1 scores.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.06209v1",
    "pdf_url": null
  },
  {
    "instance_id": "16f5e5f010ba4ad5a6d5065fc28abee1",
    "figure_id": "2211.08794v4-Figure6-1",
    "image_file": "2211.08794v4-Figure6-1.png",
    "caption": " Results on different types of autoencoders in MVCR. IMDb results can be found in §A.7.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of autoencoder performed best on MNLI and WikiAnn?",
    "answer": "HAE",
    "rationale": "The figure shows the distribution of results for each type of autoencoder on the MNLI and WikiAnn datasets. The orange stars indicate the median value for each distribution. On both datasets, the HAE has the highest median value, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.08794v4",
    "pdf_url": null
  },
  {
    "instance_id": "6e581c3cd9454c8c82c26f993b56962e",
    "figure_id": "2210.15809v2-Figure5-1",
    "image_file": "2210.15809v2-Figure5-1.png",
    "caption": " Performance comparison between our proposed method (CCS) and other baselines on CIFAR10, CIFAR100, and ImageNet-1k. The pruning rate is the fraction of examples removed from the original datasets. The evaluation results show that our method achieves better performance than all other baselines at high pruning rates (e.g., 70%, 80%, 90%) and comparable performance at low pruning rates (e.g., 30%, 50%). We also present detailed numerical numbers in Appendix D.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the best performance at a data pruning rate of 80% on CIFAR100?",
    "answer": "CCS (Ours)",
    "rationale": "The plot shows that the CCS (Ours) method has the highest test accuracy at a data pruning rate of 80% on CIFAR100.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.15809v2",
    "pdf_url": null
  },
  {
    "instance_id": "c6327f76e3de4fb6880b4611bd67dbc5",
    "figure_id": "1905.10714v1-Figure1-1",
    "image_file": "1905.10714v1-Figure1-1.png",
    "caption": " A toy example of Weighted Graph Model",
    "figure_type": "** Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \n\nIn the figure, which nodes are directly connected to node w5? ",
    "answer": " \n\nw4, w1, and w2.",
    "rationale": " \n\nThe figure shows a weighted graph, with nodes represented by circles and edges represented by lines. The edges are labeled with weights, which indicate the strength of the connection between the two nodes. The nodes that are directly connected to node w5 are the nodes that have an edge connecting them to w5. These nodes are w4, w1, and w2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10714v1",
    "pdf_url": null
  },
  {
    "instance_id": "1ae86f3a2d2a46b5865c8bf5a4c05c36",
    "figure_id": "1811.09496v2-Figure6-1",
    "image_file": "1811.09496v2-Figure6-1.png",
    "caption": " Mainmetrics for Random Forests using only errorbased features (RF63), adding raw satellite images (RF126), time of day (RF127) and coordinates (RF129).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four models has the highest accuracy?",
    "answer": "RF 129",
    "rationale": "The figure shows the accuracy of four different models, RF 63, RF 126, RF 127, and RF 129. The blue bars represent the accuracy of each model. The bar for RF 129 is the highest, indicating that it has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.09496v2",
    "pdf_url": null
  },
  {
    "instance_id": "fbfa76c507784ff8a431287771642853",
    "figure_id": "2111.12073v1-Figure3-1",
    "image_file": "2111.12073v1-Figure3-1.png",
    "caption": " Distribution of the movement between the start and end of the predictions on different datasets. X-axis shows the moving distance. Ours is the most similar to the ground truth while the others intend to predict smaller movement. The model is only trained on CMU-Mocap.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model most closely resembles the ground truth in terms of predicted movement?",
    "answer": "Ours",
    "rationale": "The figure shows the distribution of the movement between the start and end of the predictions on different datasets. The x-axis shows the moving distance. The curve for \"Ours\" is the most similar to the curve for \"GT\" (ground truth), indicating that our model is the most accurate in terms of predicting movement.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.12073v1",
    "pdf_url": null
  },
  {
    "instance_id": "21ba1ab68b41498fb18e98daa8fd5b75",
    "figure_id": "2103.08115v1-Figure1-1",
    "image_file": "2103.08115v1-Figure1-1.png",
    "caption": " An example of two-view KB. Regular metarelations and hierarchical meta-relations are denoted as orange and black dashed lines respectively in the ontology view.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between \"Barack Obama\" and \"Columbia University\"?",
    "answer": "Barack Obama graduated from Columbia University.",
    "rationale": "The figure shows a directed arrow from \"Barack Obama\" to \"Columbia University\" labeled \"graduated_from\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.08115v1",
    "pdf_url": null
  },
  {
    "instance_id": "d01cfb1afca24a3a9074d69f770db514",
    "figure_id": "2304.08971v1-Figure5-1",
    "image_file": "2304.08971v1-Figure5-1.png",
    "caption": " The computation cost, memory footprint, and the number of surfels grow according to input frames measured on a three rooms scene.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stage of the pipeline is the most computationally expensive?",
    "answer": "Rasterization.",
    "rationale": "The \"Time(Rasterization)\" line in the figure shows that rasterization takes the longest amount of time compared to the other stages.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.08971v1",
    "pdf_url": null
  },
  {
    "instance_id": "e6b6435fa9714dfda3087fb0402837be",
    "figure_id": "2109.02575v1-Figure7-1",
    "image_file": "2109.02575v1-Figure7-1.png",
    "caption": " Distribution over abstract templates by sampling method, for (a) a sample of size 5K, and (b) a sample of size 120K. The y-axis is in log-scale. The x-axis enumerates abstract templates sorted by probability, i.e each point is a template.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampling method produces the most diverse set of abstract templates?",
    "answer": "The CMaxEnt + UAT sampling method.",
    "rationale": "The figure shows that the CMaxEnt + UAT method produces a distribution over abstract templates that is more spread out than the other methods. This means that the CMaxEnt + UAT method is more likely to generate a diverse set of templates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.02575v1",
    "pdf_url": null
  },
  {
    "instance_id": "4e060315a9b84c7d8d61e37fa67c6d17",
    "figure_id": "2010.06189v3-Figure1-1",
    "image_file": "2010.06189v3-Figure1-1.png",
    "caption": " X-FACTR contains 23 languages, for which the data availability varies dramatically. Prompts get instantiated to produce grammatical sentences with different numbers of mask tokens and are used to obtain predictions for [Y]. In this Spanish example, the verb gerund “fundar” to found is rendered as “fundada” to agree in gender and number with the subject “Bloomberg L.P.”. The final prediction is in bold.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the most articles in Wikipedia according to the figure?",
    "answer": "English",
    "rationale": "The figure shows the number of articles in Wikipedia for different languages. The bar for English is the tallest, indicating that it has the most articles.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.06189v3",
    "pdf_url": null
  },
  {
    "instance_id": "c713e3015b0e4630aed66da8769aaa91",
    "figure_id": "2104.14403v2-Figure20-1",
    "image_file": "2104.14403v2-Figure20-1.png",
    "caption": " Additional attention visualizations on the CN (left) and NC (right) datasets. Orange/blue/green bars represent corr. articles/ non-corr. articles/other words.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of articles are most frequently attended to in the CN dataset?",
    "answer": "Correlated articles",
    "rationale": "The orange bars in the figure represent correlated articles. The left side of the figure shows the CN dataset, and the orange bars are taller than the blue and green bars, indicating that correlated articles are more frequently attended to.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.14403v2",
    "pdf_url": null
  },
  {
    "instance_id": "2826dce62d794697a96cdd5bf2fbcbd6",
    "figure_id": "2306.04948v1-Figure3-1",
    "image_file": "2306.04948v1-Figure3-1.png",
    "caption": " The matchup matrix of women’s singles.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many matches did each player play?",
    "answer": "Each player played 9 matches.",
    "rationale": "The figure shows a matchup matrix of women's singles. The number of matches played by each player is equal to the number of rows (or columns) in the matrix.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.04948v1",
    "pdf_url": null
  },
  {
    "instance_id": "1a8abe0f544e43beb173b44b02eaf167",
    "figure_id": "2106.04379v3-Figure5-1",
    "image_file": "2106.04379v3-Figure5-1.png",
    "caption": " An MDP and a Markov abstraction that is not a KI abstraction.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the probability of transitioning from state X1 to state X2?",
    "answer": "0.5",
    "rationale": "The arrow from X1 to X2 is labeled with the number 0.5, which indicates the probability of transitioning between these two states.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04379v3",
    "pdf_url": null
  },
  {
    "instance_id": "db6bfd338038433eadd44dcf5a0853bf",
    "figure_id": "2206.04551v1-Figure4-1",
    "image_file": "2206.04551v1-Figure4-1.png",
    "caption": " The average prediction errors of dynamics models on training environments during training process (over three times). Specifically, the x axis is the training timesteps and y axis is the log value of average prediction prediction errors. More figures are given at Appendix A.8.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the Halfcheetah environment?",
    "answer": "Our model.",
    "rationale": "The figure shows that the prediction error of our model is consistently lower than the other two models across all timesteps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.04551v1",
    "pdf_url": null
  },
  {
    "instance_id": "2a379aa1bf7d4ac487d179243eeb890b",
    "figure_id": "2301.11494v3-Figure16-1",
    "image_file": "2301.11494v3-Figure16-1.png",
    "caption": " Baseline comparison of velocity inference on a synthetic video. Our method recovers the underlying velocity field with the highest accuracy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method appears to be the most accurate in reconstructing the underlying velocity field?",
    "answer": "The \"Ours\" method.",
    "rationale": "The figure shows the ground truth velocity field, as well as the velocity fields reconstructed by several different methods. The \"Ours\" method appears to be the most similar to the ground truth, with the other methods showing more artifacts and deviations from the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.11494v3",
    "pdf_url": null
  },
  {
    "instance_id": "c59a8fc2728d40d8bda6a9698ceb2120",
    "figure_id": "2106.05012v3-Figure3-1",
    "image_file": "2106.05012v3-Figure3-1.png",
    "caption": " Tsitsiklis counterexample.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms shown in the figure converges the fastest?",
    "answer": "BBO",
    "rationale": "The figure shows the convergence of four different algorithms on the Tsitsiklis counterexample. BBO is the only algorithm that reaches a small MSE value in a small number of training steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05012v3",
    "pdf_url": null
  },
  {
    "instance_id": "ab7a180b1d6c491e977690ad2d18cfea",
    "figure_id": "2010.13974v4-Figure2-1",
    "image_file": "2010.13974v4-Figure2-1.png",
    "caption": " (a) Validation of Theorem 1: All points should be close to the diagonal line or to its right. (b) Support for orthogonal keys: Min. RHS value of Eq.(7) for all keys are either positive (MNIST) or close to zero (CelebA). (c,d) Statistics of µ and Σ for two sample user-end models for MNIST and CelebA. Small µ and small diag(Σ) suggest good match of Gφ to the perturbed data distributions. (e-h) Distinguishability, attributability, perturbation length, and orthogonality of 100 StyleGAN user-end models on FFHQ and 100 DCGAN user-end models on MNIST or CelebA, respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the best support for orthogonal keys?",
    "answer": "MNIST",
    "rationale": "Subfigure (b) shows that the minimum RHS value of Eq. (7) is positive for all keys in the MNIST dataset, which indicates good support for orthogonal keys. In contrast, the minimum RHS value for CelebA is close to zero, indicating weaker support for orthogonal keys.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.13974v4",
    "pdf_url": null
  },
  {
    "instance_id": "c5bce5a70b024c78931e53f12502ad55",
    "figure_id": "2301.12274v1-Figure5-1",
    "image_file": "2301.12274v1-Figure5-1.png",
    "caption": " Top row: approximations obtained by comparing the best conductance set found by each method against the conductance lower bound computed byHCM. Bottom row: runtimes in seconds. The runtimes for IPM and CE do not include the time it takes to compute the HCM lower bound. We plot mean over 5 runs of HCM; shaded region indicates standard deviation.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three algorithms has the fastest runtime for all four datasets?",
    "answer": "HCM.",
    "rationale": "The bottom row of the figure shows the runtimes of the three algorithms for each dataset. The blue line, which represents HCM, is always below the other two lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.12274v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee83d7c95529412e904a63c2adebb8e9",
    "figure_id": "1911.11322v1-Figure5-1",
    "image_file": "1911.11322v1-Figure5-1.png",
    "caption": " An original subgraph from Citeseer, and the corresponding subgraphs generated by various methods.",
    "figure_type": "** Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method generated a subgraph that is most similar to the original subgraph? ",
    "answer": " TVGA. ",
    "rationale": " The subgraphs generated by the other methods are more clustered and have fewer edges than the original subgraph. The subgraph generated by TVGA, on the other hand, is more similar to the original subgraph in terms of its overall structure and the number of edges.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.11322v1",
    "pdf_url": null
  },
  {
    "instance_id": "5ab7fe6544d9439ebe1c500bafd57a77",
    "figure_id": "2302.12247v4-Figure8-1",
    "image_file": "2302.12247v4-Figure8-1.png",
    "caption": " Overview of CVX estimator. Appropriate histogramming of each Xi or clustering in preprocessed feature space is first performed to constrain |Xi|. By rewriting the entropy objective as a KL divergence (with auxiliary variables) which is recognized as convex, this allows us to use conic solvers such as SCS [75], ECOS [27], and MOSEK [6] in CVXPY to solve for q∗ and obtain PID values.",
    "figure_type": "** Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What are the two types of data that are used as input to the CVX estimator?",
    "answer": " Text and images.",
    "rationale": " The figure shows two inputs to the CVX estimator, one labeled \"Text\" and the other labeled \"Image.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.12247v4",
    "pdf_url": null
  },
  {
    "instance_id": "601070aa22574a098e05e2b1c50e9d15",
    "figure_id": "1910.12757v3-Figure2-1",
    "image_file": "1910.12757v3-Figure2-1.png",
    "caption": " System Latency Comparison",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which system has the highest latency when the basket size is 100 items?",
    "answer": "Annoy",
    "rationale": "The figure shows that the Annoy line is the highest at a basket size of 100 items.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.12757v3",
    "pdf_url": null
  },
  {
    "instance_id": "13feae02b8b148138d4747a9d40ae7f6",
    "figure_id": "2303.04381v1-Figure1-1",
    "image_file": "2303.04381v1-Figure1-1.png",
    "caption": " Quantitative results of reversing GPT-2 and GPT-J on toxic outputs. We plot the average success rate on all outputs (bold) and average normalized success rate (dotted) on 1, 2, and 3-token toxic outputs from CivilComments across 5 random runs of each optimizer.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method had the highest success rate for reversing GPT-J on 1-token toxic outputs?",
    "answer": "GBDA",
    "rationale": "The plot shows that the GBDA line is higher than the other two lines for 1-token toxic outputs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.04381v1",
    "pdf_url": null
  },
  {
    "instance_id": "f5bb4609a5fa4288915b2cf47c5e46cc",
    "figure_id": "2306.02282v1-Figure4-1",
    "image_file": "2306.02282v1-Figure4-1.png",
    "caption": " Number of papers in different disciplines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which discipline has the most papers published?",
    "answer": "Biology",
    "rationale": "The figure shows the number of papers published in different disciplines, and Biology has the tallest bar, indicating it has the most papers published.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.02282v1",
    "pdf_url": null
  },
  {
    "instance_id": "87de174f31d84690b228da0e79f2435f",
    "figure_id": "2004.04908v2-Figure3-1",
    "image_file": "2004.04908v2-Figure3-1.png",
    "caption": " Distributions of human annotations and model outputs on the test data (90 responses).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the most similarly to human annotators?",
    "answer": "RUBER",
    "rationale": "The distribution of RUBER scores most closely resembles the distribution of human scores.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.04908v2",
    "pdf_url": null
  },
  {
    "instance_id": "81dd52aa8d064cb0879ee4ff52513c6a",
    "figure_id": "2110.13957v4-Figure4-1",
    "image_file": "2110.13957v4-Figure4-1.png",
    "caption": " Fairness metrics evaluated on link prediction task on Pokec-n with node2vec as the embedding model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which embedding model performs the best in terms of link prediction accuracy?",
    "answer": "node2vec",
    "rationale": "The figure shows the link prediction accuracy for different embedding models. The link prediction accuracy is measured by the NDCG@10 metric. The higher the NDCG@10, the better the link prediction accuracy. In the figure, the node2vec embedding model has the highest NDCG@10 for all fairness metrics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.13957v4",
    "pdf_url": null
  },
  {
    "instance_id": "b4049a3fa0584c5c8a99bab8df73978f",
    "figure_id": "1906.03329v2-Figure5-1",
    "image_file": "1906.03329v2-Figure5-1.png",
    "caption": " Computation times for the logistic (5a) and Poisson (5b) regression experiments. Plots show the median KL divergence (estimated using the Laplace approximation [62] and normalized by the value for the prior) across 10 trials, with 25th and 75th percentiles shown by shaded areas. From top to bottom, (5a) shows the results for logistic regression on synthetic, chemical reactivities, and phishing websites data, while (5b) shows the results for Poisson regression on synthetic, bike trips, and airport delays data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest for the logistic regression experiment on the synthetic data?",
    "answer": "GIGA (Optimal)",
    "rationale": "The plot shows that the GIGA (Optimal) method has the lowest CPU time for a given KL divergence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.03329v2",
    "pdf_url": null
  },
  {
    "instance_id": "4fa4fbd8461440bd860193b7f8e00cdd",
    "figure_id": "1907.11461v3-Figure6-1",
    "image_file": "1907.11461v3-Figure6-1.png",
    "caption": " The attack action’s Q-values of ASN and vanilla under different circumstances.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which scenario does ASN outperform Vanilla in terms of Q-value when the agent is within the attack range?",
    "answer": "Scenario 1",
    "rationale": "In scenario 1, the Q-value of ASN is higher than that of Vanilla when the agent is within the attack range (indicated by the red dashed line). This suggests that ASN is more likely to choose the attack action in this situation than Vanilla.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.11461v3",
    "pdf_url": null
  },
  {
    "instance_id": "07ee1f8a5ded418a98225bdb09210886",
    "figure_id": "2212.07339v1-Figure10-1",
    "image_file": "2212.07339v1-Figure10-1.png",
    "caption": " Qualitative comparison on the VideoLQ dataset for ×4 video SR. Compared against other methods, FastRealVSR can better remove the noise around texts and buildings, with clearer edges. (Zoom-in for best view)",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method is able to best remove the noise around texts and buildings, with clearer edges?",
    "answer": " FastRealVSR",
    "rationale": " The figure shows that FastRealVSR is able to better remove the noise around texts and buildings, with clearer edges, compared to other methods. This can be seen in the zoomed-in images of the text on the tram and the buildings in the background.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.07339v1",
    "pdf_url": null
  },
  {
    "instance_id": "a0ea6c95cb124257afd314b278bbbf01",
    "figure_id": "2302.00965v2-Figure11-1",
    "image_file": "2302.00965v2-Figure11-1.png",
    "caption": " Different data augmentation strategies for discriminators on Finger-Spin with 10 expert trajectories over 5 random seeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which data augmentation strategy appears to perform the best for discriminators on Finger-Spin?",
    "answer": "PatchAIL-Bonus",
    "rationale": "The PatchAIL-Bonus plot shows the highest return for all frames and appears to be the most stable across the 5 random seeds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.00965v2",
    "pdf_url": null
  },
  {
    "instance_id": "7c59fc3a091d4e3aa61a76ffb1cf9e17",
    "figure_id": "2303.10971v1-Figure10-1",
    "image_file": "2303.10971v1-Figure10-1.png",
    "caption": " Mean geodesic errors for point cloud matching in different noise magnitudes. Our method achieves more robust point cloud matching based on deep feature similarity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves more robust point cloud matching based on deep feature similarity?",
    "answer": "Ours.",
    "rationale": "The figure shows that the \"Ours\" method has a lower mean geodesic error than the \"w.o. E_align, E_nce\" method for all noise magnitudes. This indicates that the \"Ours\" method is more robust to noise.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.10971v1",
    "pdf_url": null
  },
  {
    "instance_id": "3a7a32b8eea24b62a4d744fdb44af322",
    "figure_id": "2206.07459v2-Figure2-1",
    "image_file": "2206.07459v2-Figure2-1.png",
    "caption": " The reconstruction error distributions in different forms (CIFAR-10 as ID).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest reconstruction error in CLF (MD)?",
    "answer": "SVHN",
    "rationale": "The figure shows the reconstruction error distributions for different datasets in different forms. The reconstruction error is the difference between the original image and the reconstructed image. The higher the reconstruction error, the worse the reconstruction quality. In the figure, the SVHN dataset has the highest reconstruction error in CLF (MD), as indicated by the darkest red color in the corresponding heatmap.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.07459v2",
    "pdf_url": null
  },
  {
    "instance_id": "b11562f1ccde43eb8859c7966dfcf8d0",
    "figure_id": "2112.03497v2-Figure8-1",
    "image_file": "2112.03497v2-Figure8-1.png",
    "caption": " MasakhaNER Geographic Distributions (Part 1).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which language is spoken in the most countries?",
    "answer": "Hausa.",
    "rationale": "The figure shows that Hausa is spoken in six different countries, which is more than any other language shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.03497v2",
    "pdf_url": null
  },
  {
    "instance_id": "cb445285da6e422caaa3241d63928b8f",
    "figure_id": "1906.02715v2-Figure8-1",
    "image_file": "1906.02715v2-Figure8-1.png",
    "caption": " Additional examples of BERT parse trees. In each pair, at left is a drawing of the abstract tree; at right is a PCA view of the embeddings. Colors are the same as in Figure 7.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following sentences is most likely to have been generated by BERT?\n\n(a) \"A spokesman for the guild said the union's lawyers are reviewing the suit.\"\n(b) \"This trial is expected to last five weeks.\"\n(c) \"In July, the Environmental Protection Agency imposed a gradual ban on virtually all uses of asbestos.\"\n(d) \"He succeeds Terrence D. Daniels, formerly a W.R. Grace vice chairman, who resigned.\"",
    "answer": "(a)",
    "rationale": "The figure shows the parse trees for four different sentences. The parse tree for sentence (a) is the most complex, with the most branches and nodes. This suggests that sentence (a) is the most difficult for BERT to generate, and therefore the most likely to have been generated by BERT.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02715v2",
    "pdf_url": null
  },
  {
    "instance_id": "0cd4ddd1b0ae498db780da6e88fb7b8b",
    "figure_id": "2301.13012v1-Figure7-1",
    "image_file": "2301.13012v1-Figure7-1.png",
    "caption": " Average top-10 confidences of a CIFAR-10 classifier (WideResNet-40-2) tested on CIFAR-100 (OOD).",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class of objects is the classifier most confident in predicting?",
    "answer": "Airplane.",
    "rationale": "The bar chart for the \"Airplane\" class has the highest average top-10 confidences, which indicates that the classifier is most confident in predicting this class of objects.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.13012v1",
    "pdf_url": null
  },
  {
    "instance_id": "930a74d3b3e44233944950b1be6343b6",
    "figure_id": "2212.06836v1-Figure4-1",
    "image_file": "2212.06836v1-Figure4-1.png",
    "caption": " The reward distribution of the top 10 sensitive features on PEDec and EHR.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the top 10 features has the highest reward distribution for PEDec?",
    "answer": "Feature 4857",
    "rationale": "The figure shows the reward distribution for the top 10 features for both PEDec and EHR. For PEDec, the bar for feature 4857 is the tallest, indicating that it has the highest reward distribution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.06836v1",
    "pdf_url": null
  },
  {
    "instance_id": "d8f44de58d8241a4a89af0fa2412bd1a",
    "figure_id": "1910.04450v1-Figure3-1",
    "image_file": "1910.04450v1-Figure3-1.png",
    "caption": " Learning curves of success rate or average return in Ant Maze, Swimmer Maze and Ant Gather tasks. The curves are HAAR with skill annealing, HAAR without skill length annealing, SNN4HRL and TRPO, respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieved the highest success rate in the Ant Maze task?",
    "answer": "HAAR with skill annealing",
    "rationale": "The figure shows that the blue line, which represents HAAR with skill annealing, reaches the highest success rate in the Ant Maze task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.04450v1",
    "pdf_url": null
  },
  {
    "instance_id": "831c975efa454524b018c0b6657e5c79",
    "figure_id": "1903.11332v2-Figure3-1",
    "image_file": "1903.11332v2-Figure3-1.png",
    "caption": " An edge moves from position 1 to 10 at two different speeds. It creates a slope on the Standard Time Surface T (Top) and on the corresponding Speed Invariant Time Surface S (Bottom). S is identical for both speeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which edge moves faster, the one at position 1 or the one at position 8?",
    "answer": "The edge at position 8 moves faster.",
    "rationale": "The figure shows the time it takes for the edge to move from one position to another. The edge at position 8 takes less time to move than the edge at position 1, which means it is moving faster.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.11332v2",
    "pdf_url": null
  },
  {
    "instance_id": "ea845b90342c48e49d0b31a2f1e59fd7",
    "figure_id": "1908.09345v2-Figure1-1",
    "image_file": "1908.09345v2-Figure1-1.png",
    "caption": " A comparison of the analytical convergence rate for SVRG and SARAH. In both figures we set κ = 105 with L = 1, µ = 10−5, and the step sizes are selected as: (a) SVRG with η = 0.1/L; and (b) SARAH with η = 0.5/L.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm, SVRG or SARAH, has a faster convergence rate?",
    "answer": "SVRG has a faster convergence rate than SARAH.",
    "rationale": "The figure shows that the convergence rate of SVRG is faster than that of SARAH for all values of m/k. This can be seen by comparing the slopes of the curves in the two figures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.09345v2",
    "pdf_url": null
  },
  {
    "instance_id": "280b3bf3a3d74b27a5720ea5b56f6462",
    "figure_id": "2302.00392v1-Figure2-1",
    "image_file": "2302.00392v1-Figure2-1.png",
    "caption": " Comparing regret performance of BPE-Delay and GP-UCB-SDF.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has a lower regret in both (a) and (b)?",
    "answer": "GP-UCB-SDF",
    "rationale": "The regret of GP-UCB-SDF is lower than BPE-Delay in both (a) and (b). This can be seen by comparing the two lines in each plot. In (a), the red line (BPE-Delay) is always above the cyan line (GP-UCB-SDF). In (b), the red line is again above the cyan line, although the difference is smaller.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.00392v1",
    "pdf_url": null
  },
  {
    "instance_id": "9ebdaf3766974da9b44a5536ead3050e",
    "figure_id": "2305.15182v2-Figure1-1",
    "image_file": "2305.15182v2-Figure1-1.png",
    "caption": " Micro-F1 score and the number of trainable parameters of our method and SOTAs with dual encoders on Web Of Science dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest Micro-F1 score?",
    "answer": "HiTIN.",
    "rationale": "The figure shows the Micro-F1 score and the number of trainable parameters for different methods. HiTIN is the only method that is marked with a red star, which indicates that it achieves the highest Micro-F1 score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.15182v2",
    "pdf_url": null
  },
  {
    "instance_id": "57917609caee4ecca8b5dac65f1cb2f3",
    "figure_id": "2010.02903v1-Figure12-1",
    "image_file": "2010.02903v1-Figure12-1.png",
    "caption": " CALM (n-gram) learning Zork1. Results show the five independent training runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which learning rate resulted in the most consistent performance across the five training runs?",
    "answer": "zork1.25-1",
    "rationale": "The \"Last 100 Episode Scores\" plot shows that the zork1.25-1 learning rate resulted in the most consistent performance across the five training runs, with all five runs converging to a similar score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02903v1",
    "pdf_url": null
  },
  {
    "instance_id": "42dbcbd7b2d44fbbb89d59f2b344160d",
    "figure_id": "2211.12883v3-Figure1-1",
    "image_file": "2211.12883v3-Figure1-1.png",
    "caption": " Evaluation of background and foreground static bias. (a) Testing on IID HMDB51 [38] test videos. (b) Testing on SCUBA videos, constructed by replacing the video background with a synthetic sinusoidal stripe image. (c) Testing on videos with conflicting foreground cues, constructed by inserting a random static foreground into the SCUBA video.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the SCUBA videos?",
    "answer": "Swin-T + StillMix",
    "rationale": "The table on the right side of the figure shows the accuracy of each method on each type of video. For the SCUBA videos, Swin-T + StillMix has the highest accuracy of 0.4973.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.12883v3",
    "pdf_url": null
  },
  {
    "instance_id": "7f20f6fb255e454f9867bad78c3d04d7",
    "figure_id": "2207.10276v4-Figure3-1",
    "image_file": "2207.10276v4-Figure3-1.png",
    "caption": " Comparison of clean sample selection on CIFAR-10/100 dataset with 80% symmetric noise. The threshold of DivideMix and forget rate of JoCoR have been re-tuned to get the best F1 score.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of F1 score on CIFAR-10 with 80% symmetric noise?",
    "answer": "ProMix.",
    "rationale": "The figure shows the F1 score for each method on CIFAR-10 with 80% symmetric noise. ProMix has the highest F1 score of approximately 0.8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.10276v4",
    "pdf_url": null
  },
  {
    "instance_id": "3fdcd6b7a52e4210b8bdbfa56d3a45a6",
    "figure_id": "2210.12309v1-Figure4-1",
    "image_file": "2210.12309v1-Figure4-1.png",
    "caption": " Performance Comparison on MSRVTT.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on MSRVTT when trained on 2.4M HowTo100M training samples?",
    "answer": "PTC-PCFG",
    "rationale": "The figure shows the S-F1 scores of different models on MSRVTT, trained on different numbers of HowTo100M training samples. The PTC-PCFG model achieves the highest S-F1 score of 62.8% when trained on 2.4M HowTo100M training samples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12309v1",
    "pdf_url": null
  },
  {
    "instance_id": "f53abaf1f7084c8a8ef2e6d7ec58b5c6",
    "figure_id": "2310.18894v1-Figure1-1",
    "image_file": "2310.18894v1-Figure1-1.png",
    "caption": " Shape bias of our sparse CNNs versus standard CNNs and SOTA transformer-based networks in comparison to the shape bias of human subjects, as evaluated on benchmark dataset [10] across 16 classes. The red dotted line shows the frontier of transformer-based networks with the best shape bias. The greed dotted line shows that sparse CNNs push the frontier of the shape bias boundary toward humans.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest shape bias?",
    "answer": "CLIP_ViT-B(400M)",
    "rationale": "The figure shows the fraction of 'shape' decisions and the fraction of 'texture' decisions for different models. The model with the highest shape bias is the one with the highest fraction of 'shape' decisions and the lowest fraction of 'texture' decisions. In this case, CLIP_ViT-B(400M) has the highest shape bias.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.18894v1",
    "pdf_url": null
  },
  {
    "instance_id": "692e08765955405eb91e999b0a1b7e3c",
    "figure_id": "2011.07831v2-Figure4-1",
    "image_file": "2011.07831v2-Figure4-1.png",
    "caption": " Two randomly generated environments with the agent’s location coloured in green and the reward location coloured in yellow. Edge labels indicate the set of valid actions (0, 1, or 2) to transition along that arrow. Invalid actions are not visualised. The graph and the locations of the agent and reward are set randomly at the beginning of the experiment. If the agent reaches the reward location or did not reach it after six steps, both are randomly reset.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In the image, how many actions are possible for the agent in the left environment?",
    "answer": "3",
    "rationale": "The image shows the possible actions for the agent at each node. In the left environment, the agent can take actions 0, 1, and 2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.07831v2",
    "pdf_url": null
  },
  {
    "instance_id": "5f2a43b983ea4d358927bb7090463c4d",
    "figure_id": "2209.14491v3-Figure5-1",
    "image_file": "2209.14491v3-Figure5-1.png",
    "caption": " The human evaluation scores for both frequent and infrequent entities.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods performs the best for frequent entities?",
    "answer": "Re-Imagen",
    "rationale": "The figure shows that Re-Imagen has the highest scores for all frequent entities.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.14491v3",
    "pdf_url": null
  },
  {
    "instance_id": "3285f956a6a8444b8816ee4d954eff39",
    "figure_id": "2207.09078v2-Figure2-1",
    "image_file": "2207.09078v2-Figure2-1.png",
    "caption": " High-level skeleton of the ILASR architecture",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which component of the ILASR architecture is responsible for updating the model with the aggregated gradients?",
    "answer": "Incremental Model Update",
    "rationale": "The figure shows that the Gradient Aggregator sends the aggregated gradients to the Incremental Model Update component, which is responsible for updating the model with the new gradients.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.09078v2",
    "pdf_url": null
  },
  {
    "instance_id": "1a8d24f0cac942ccbaaf9bf41c1fb718",
    "figure_id": "2008.01342v2-Figure2-1",
    "image_file": "2008.01342v2-Figure2-1.png",
    "caption": " Training loss curves for SimCLR, GIM and LoCo",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the ImageNet dataset?",
    "answer": "SimCLR",
    "rationale": "The SimCLR model has the lowest training loss at the end of training, indicating that it performs better than the other models on the ImageNet dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.01342v2",
    "pdf_url": null
  },
  {
    "instance_id": "6288cff3d23a442082b5cfb9bcd16b41",
    "figure_id": "2308.16905v1-Figure2-1",
    "image_file": "2308.16905v1-Figure2-1.png",
    "caption": " We present ground truth HOI sequences (left), object motions (middle), and objects relative to the contacts after coordinate transformations (right). Our key insight is to inject coordinate transformations into a diffusion model, as the relative motion shows simpler patterns that are easier to predict, e.g., rotating around a fixed axis (top) or being almost stationary (bottom).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the purpose of the coordinate transformations?",
    "answer": "To simplify the motion patterns of the objects.",
    "rationale": "The figure shows that the relative motion of the objects after the coordinate transformations is simpler than the original motion. For example, the top row shows that the objects are rotating around a fixed axis, while the bottom row shows that the objects are almost stationary. This makes it easier for the diffusion model to predict the motion of the objects.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.16905v1",
    "pdf_url": null
  },
  {
    "instance_id": "bfee268e9e7c4469b58b200463e23a83",
    "figure_id": "2304.04952v1-Figure5-1",
    "image_file": "2304.04952v1-Figure5-1.png",
    "caption": " Median SRCC versus Epochs on the LIVEC and KonIQ testing datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest median SRCC on the KonIQ dataset?",
    "answer": "DEIQT",
    "rationale": "The figure shows the median SRCC for different models on the KonIQ dataset. The DEIQT model has the highest median SRCC of all the models shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.04952v1",
    "pdf_url": null
  },
  {
    "instance_id": "00deb5337bce4b4fb6e6bfda8660210f",
    "figure_id": "2012.02374v1-Figure10-1",
    "image_file": "2012.02374v1-Figure10-1.png",
    "caption": " Comparing the FID score distributions of Star-GAN v2 [9] and CIT-GAN for each synthetically generated PA domain. A lower FID is better.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GAN generated more realistic printed eyes?",
    "answer": "Star-GAN v2",
    "rationale": "The figure shows the FID score distributions for Star-GAN v2 and CIT-GAN for each synthetically generated PA domain. A lower FID score indicates that the generated images are more realistic. The figure shows that Star-GAN v2 has a lower FID score for printed eyes than CIT-GAN, which indicates that Star-GAN v2 generated more realistic printed eyes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.02374v1",
    "pdf_url": null
  },
  {
    "instance_id": "2ad03551a78f4be8b006cf0ed5fdb8b9",
    "figure_id": "1912.00426v2-Figure6-1",
    "image_file": "1912.00426v2-Figure6-1.png",
    "caption": " Time cost of area queries with various query sizes",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach has the lowest time cost for all query sizes?",
    "answer": "Our approach.",
    "rationale": "The green bars, which represent our approach, are consistently shorter than the black bars, which represent the traditional approach.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.00426v2",
    "pdf_url": null
  },
  {
    "instance_id": "9cf93c33f0664dc08b397c25035440d6",
    "figure_id": "2101.06521v3-Figure2-1",
    "image_file": "2101.06521v3-Figure2-1.png",
    "caption": " The four tasks we evaluate on. From left to right: 7-DOF PUSHER, 7-DOF REACHER, GOALTASK, and KICKBALL. The first two tasks simulate a one-armed PR2 robot environment while the last two are in the SOCIALROBOT environment. The final picture shows a closeup of the PIONEER2DX robot used in SOCIALROBOT.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task simulates a one-armed PR2 robot environment?",
    "answer": "7-DOF PUSHER and 7-DOF REACHER",
    "rationale": "The first two images in the figure show a one-armed PR2 robot environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.06521v3",
    "pdf_url": null
  },
  {
    "instance_id": "864f301ecbb14c248268c4233b7a0b6b",
    "figure_id": "1908.10192v3-Figure8-1",
    "image_file": "1908.10192v3-Figure8-1.png",
    "caption": " (Left:) Top 5 images, closest to corresponding centroid. “Rare” classes. (Top row): Gros-Horloge in Rouen, (middle row): Jan Breydel and Pieter de Coninck Monument in Bruges, (bottom row): Esplanade des Invalides in Paris.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following images is most likely to be from the \"Rare\" classes?",
    "answer": "The image of the Gros-Horloge in Rouen.",
    "rationale": "The caption states that the top 5 images are from the \"Rare\" classes, and the first image in the top row is the Gros-Horloge in Rouen.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.10192v3",
    "pdf_url": null
  },
  {
    "instance_id": "68218c991fae40a69399b7cc7abb5fb3",
    "figure_id": "1809.01337v1-Figure8-1",
    "image_file": "1809.01337v1-Figure8-1.png",
    "caption": " TEMPO - Human Language (HL). Localized moments using the temporal word “after”. The blue line shows the context considered when localizing the moment, and the correctly predicted moment is highlighted in green.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following events occurs first in the video? \nA. The cat sits down.\nB. The kid adjusts her shoe. \nC. The camera zooms out from the dancers.",
    "answer": "A. The cat sits down.",
    "rationale": "The image shows three examples of how the model uses the word \"after\" to localize moments in a video. In the first example, the cat sits down and then looks up. In the second example, the kid adjusts her shoe and then places her foot on her knee. In the third example, the camera zooms out from the dancers and then zooms back in. Therefore, the event that occurs first in the video is the cat sitting down.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.01337v1",
    "pdf_url": null
  },
  {
    "instance_id": "ebd09ac5115e4e6694eafb5ccf7e35eb",
    "figure_id": "2201.01251v3-Figure18-1",
    "image_file": "2201.01251v3-Figure18-1.png",
    "caption": " Performance profiles based on score distributions for the deterministic games listed in Table 1, following Agarwal et al. (2021). Shaded regions show pointwise 95% confidence bands based on percentile bootstrap with stratified sampling.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best across all normalized score thresholds?",
    "answer": "DRRN.",
    "rationale": "The DRRN line is consistently higher than all other lines in the plot, indicating that it has a higher fraction of runs with scores above each threshold.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.01251v3",
    "pdf_url": null
  },
  {
    "instance_id": "ec004c960ea84bbfa91c2c82c8940691",
    "figure_id": "2011.03173v2-Figure1-1",
    "image_file": "2011.03173v2-Figure1-1.png",
    "caption": " Fair risk minimization problem when there are two groups. Recall R is a set of risk profiles and F is the set of risk profiles that satisfy risk parity. The horizontal and vertical coordinates of the risk profiles represent the risk of the model on the majority and minority subpopulations. In the left plot, we see the empirical risk minimization (ERM) optimal point R̃ and the fair risk minimization (FRM) optimal point R̃F . In the center plot, we see that FRM can both improve and harm performance in the target domain (as long as the assumptions of 3.2 are satisfied). The green dotted line separates the P ∗A’s that lead to worse and improved performance in the target domain: if P ∗A falls below the green line, then FRM harms performance in the target domain. In the right plots, we reproduce this effect in a simulation. As the fraction of samples from the minority group decreases in the target domain, there is a point beyond which enforcing fairness harms accuracy (in the target domain). We refer to Appendix C for the simulation details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the accuracy of the model when the fraction of samples from the minority group decreases in the target domain?",
    "answer": "The accuracy of the model decreases.",
    "rationale": "The right plot shows that as the fraction of samples from the minority group decreases in the target domain, the accuracy of the model decreases. This is because the model is trained to be fair on the majority group, and as the minority group becomes smaller, the model is less able to generalize to the minority group.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.03173v2",
    "pdf_url": null
  },
  {
    "instance_id": "9fe546217f314f7b9faa884beeb07851",
    "figure_id": "1905.11545v4-Figure1-1",
    "image_file": "1905.11545v4-Figure1-1.png",
    "caption": " (Left) Approximating a quadratic function via a max-affine function. (Middle-left) Bregman divergence approximation from every 2-d sample point to the specific point A in the data, as x varies around the circle. x to the specific point A in the data (Middle-right) Switches the roles of x and A (recall the BD is asymmetric) (Right) distances from points x to A using a Mahalanobis distance learned via linear metric learning (ITML). When this BD is used to define a Bregman divergence, points within a given class have a small learned divergence, leading to clustering, k-nn, and ranking performance of 98%+ (see experimental results for details).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following figures shows the Bregman divergence approximation from every 2-d sample point to the specific point A in the data, as x varies around the circle?",
    "answer": "The middle-left figure.",
    "rationale": "The middle-left figure shows the Bregman divergence approximation from every 2-d sample point to the specific point A in the data, as x varies around the circle. This is evident from the caption, which states that the middle-left figure shows \"Bregman divergence approximation from every 2-d sample point to the specific point A in the data, as x varies around the circle.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11545v4",
    "pdf_url": null
  },
  {
    "instance_id": "6235086f129a465195b40b6e49f1e162",
    "figure_id": "2203.06063v2-Figure9-1",
    "image_file": "2203.06063v2-Figure9-1.png",
    "caption": " Sentence-level prediction accuracy of direct assessment metrics with the Linear, BTL, and BTLLogistic models averaged across the 7 WMT datasets",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on average across all 7 WMT datasets for sentence-level prediction accuracy?",
    "answer": "The BTL model.",
    "rationale": "The figure shows the average accuracy of three different models (Linear, BTL, and BTL-Logistic) across all 7 WMT datasets. The BTL model has the highest average accuracy of the three models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.06063v2",
    "pdf_url": null
  },
  {
    "instance_id": "674c2b02e31642ab812b1775706e64f9",
    "figure_id": "2302.03292v2-Figure2-1",
    "image_file": "2302.03292v2-Figure2-1.png",
    "caption": " The 6-class grasp type taxonomy simplified from [8].",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which grasp type is most appropriate for holding a pen?",
    "answer": "Precision Pad",
    "rationale": "The figure shows that the Precision Pad grasp type is used for holding small objects, such as a pen, with the thumb and fingers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.03292v2",
    "pdf_url": null
  },
  {
    "instance_id": "bb26cf5d407845e6b0d9eba074c4b4c4",
    "figure_id": "2106.07176v5-Figure3-1",
    "image_file": "2106.07176v5-Figure3-1.png",
    "caption": " Curve of GLUE Score for SAS-Small model and its competitors with respect to the pre-training steps on the GLUE dev set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the GLUE dev set when pre-trained with 250,000 steps?",
    "answer": "SAS-Small",
    "rationale": "The figure shows that the GLUE score for SAS-Small is highest at 250,000 pre-training steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07176v5",
    "pdf_url": null
  },
  {
    "instance_id": "9fc70a91566148da88af40aed2e4d1bf",
    "figure_id": "2008.11048v1-Figure5-1",
    "image_file": "2008.11048v1-Figure5-1.png",
    "caption": " Visual comparison of different algorithms. Each row represents one image and corresponding saliency maps. Each column represents the predictions of one method. Apparently, our method is good at dealing with cluttered background and producing more accurate and clear saliency maps.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best at detecting salient objects in cluttered backgrounds?",
    "answer": "Our method.",
    "rationale": "The figure shows that our method produces more accurate and clear saliency maps, even in images with cluttered backgrounds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.11048v1",
    "pdf_url": null
  },
  {
    "instance_id": "4308bfa0080c428cafa48dc760ba3c18",
    "figure_id": "1811.04110v2-Figure5-1",
    "image_file": "1811.04110v2-Figure5-1.png",
    "caption": " LENET++ RESPONSES TO CIFAR IMAGES. This figure shows responses of a network trained to classify MNIST digits and reject Latin letters when exposed to samples from CIFAR dataset, which are very different from both the classes of interest (MNIST) and the background classes (Letters).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods shown in the figure has the highest correct classification rate for a given false positive rate?",
    "answer": "ObjectSphere",
    "rationale": "The OpenSet Recognition Curve in Figure (d) shows that the ObjectSphere method has the highest correct classification rate for a given false positive rate. This means that ObjectSphere is the most effective method at correctly classifying images while also rejecting unknown classes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.04110v2",
    "pdf_url": null
  },
  {
    "instance_id": "05364dc820d048ffbe325becc5878c50",
    "figure_id": "2307.02484v6-Figure8-1",
    "image_file": "2307.02484v6-Figure8-1.png",
    "caption": " Ablation study on the step size δ. The greater the step size the smaller the length search space is.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which step size appears to be the best choice for Hopper?",
    "answer": "4",
    "rationale": "The plot shows the return for different step sizes, and the step size of 4 has the highest return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.02484v6",
    "pdf_url": null
  },
  {
    "instance_id": "3f68249cddd64ba29258bf7f43d6e598",
    "figure_id": "2010.09865v1-Figure3-1",
    "image_file": "2010.09865v1-Figure3-1.png",
    "caption": " Histogram of predicted TCP scores for CE confidence network [18] (left) and our proposed IAD constrained confidence network (right) on CIFAR-10 test set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model, CE or IAD, has a higher proportion of correct predictions with a predicted TCP score above 0.8?",
    "answer": "IAD",
    "rationale": "The IAD histogram shows a higher proportion of correct predictions in the 0.8-1.0 bin than the CE histogram. This indicates that the IAD model is more confident in its correct predictions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.09865v1",
    "pdf_url": null
  },
  {
    "instance_id": "a982b012b4054ab9ad447d2c56bc7862",
    "figure_id": "2109.09011v2-Figure23-1",
    "image_file": "2109.09011v2-Figure23-1.png",
    "caption": " Molecules obtained by the model during an optimization phase (left side), together with their SAS (right side).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which molecule has the highest SAS?",
    "answer": "Molecule 9.",
    "rationale": "The plot on the right side of the figure shows the obtained SAS for each molecule. Molecule 9 has the highest obtained SAS, which is approximately 4.1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.09011v2",
    "pdf_url": null
  },
  {
    "instance_id": "964054ea58bc4545addd92d1d8cef5a8",
    "figure_id": "2310.18593v1-Figure2-1",
    "image_file": "2310.18593v1-Figure2-1.png",
    "caption": " Synthetic Example: Vanilla NPM v.s. Mean-matched NPM v.s. FNPM (ours).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which NPM method results in the most overlap between the two groups?",
    "answer": "Vanilla NPM",
    "rationale": "The figure shows that the Vanilla NPM method has the most overlap between the two groups, as the two groups are clustered together in the center of the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.18593v1",
    "pdf_url": null
  },
  {
    "instance_id": "d8298564135b4f608efcefce5eae6bc6",
    "figure_id": "1904.06019v3-Figure4-1",
    "image_file": "1904.06019v3-Figure4-1.png",
    "caption": " Empirical coverages and median lengths from conformal prediction, on the airfoil data set, with no covariate shift.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the shortest prediction intervals?",
    "answer": "Logistic regression.",
    "rationale": "The figure shows the distribution of prediction interval lengths for each method. The logistic regression distribution is shifted to the left compared to the other two distributions, indicating that it produced shorter prediction intervals.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.06019v3",
    "pdf_url": null
  },
  {
    "instance_id": "42926f938bdf48248aa522c03fcd16dc",
    "figure_id": "2106.05387v2-Figure14-1",
    "image_file": "2106.05387v2-Figure14-1.png",
    "caption": " Activation maps for First TextWorld Problems Cooking Task showing the region of interest when using the imagination based model (AttnGAN) for selecting the next action. We include both the generated images and its attention plot for clarity.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the region of interest when using the imagination based model (AttnGAN) for selecting the next action \"put wet checkered socks on BBQ\"?",
    "answer": "The region of interest is the BBQ.",
    "rationale": "The figure shows the attention plots for the different actions. The attention plot for the action \"put wet checkered socks on BBQ\" shows that the model is focusing on the BBQ.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05387v2",
    "pdf_url": null
  },
  {
    "instance_id": "b01250f193be43ba8ed232dd20b0b437",
    "figure_id": "2106.01862v2-Figure7-1",
    "image_file": "2106.01862v2-Figure7-1.png",
    "caption": " Qualitative results of FireNet and FireFlowNet variants on a sequence from MVSEC [55].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the FireNet variants produces the most accurate results on this sequence?",
    "answer": "LIF-FireFlowNet",
    "rationale": "The LIF-FireFlowNet variant produces the most accurate results because it most closely matches the ground truth image. This can be seen in the way that the LIF-FireFlowNet variant correctly identifies the person in the scene, as well as the objects in the background. The other FireNet variants either miss some of the objects in the scene or produce results that are less accurate than the LIF-FireFlowNet variant.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01862v2",
    "pdf_url": null
  },
  {
    "instance_id": "dafda4740bf64f15b010928cedeb2cdc",
    "figure_id": "1910.14268v4-Figure2-1",
    "image_file": "1910.14268v4-Figure2-1.png",
    "caption": " (a) Pruning attacks in descending order, (b) Pruning attacks in ascending order.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pruning strategy, ascending or descending, results in a faster decrease in model accuracy?",
    "answer": "Descending.",
    "rationale": "In Figure (a), which shows the results of pruning in descending order, the model accuracy drops off much more quickly than in Figure (b), which shows the results of pruning in ascending order. This suggests that pruning in descending order is more effective at reducing model accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.14268v4",
    "pdf_url": null
  },
  {
    "instance_id": "9456f054e64d48ddbff0f8cc0d327db3",
    "figure_id": "1901.05127v1-Figure1-1",
    "image_file": "1901.05127v1-Figure1-1.png",
    "caption": " Existing methods are unable to coordinate spatial distribution of visual attention between content image and stylized image, misleading the stylized rendering on diverse content regions in an indiscriminate way, resulting in insufficient stylization (Style-Swap), unexpected patterns (AdaIN, StrokePyramid) or distorted attention regions (WCT, Avatar-Net). Due to attention mechanism and multi-stroke fusion strategy, our method can achieve faithful transfer of style and attention consistency with content image simultaneously.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the existing methods is most likely to produce unexpected patterns in the stylized image?",
    "answer": "AdaIN and StrokePyramid.",
    "rationale": "The caption states that \"Existing methods are unable to coordinate spatial distribution of visual attention between content image and stylized image, misleading the stylized rendering on diverse content regions in an indiscriminate way, resulting in insufficient stylization (Style-Swap), unexpected patterns (AdaIN, StrokePyramid) or distorted attention regions (WCT, Avatar-Net).\" The figure shows examples of stylized images produced by each of the existing methods, and the images produced by AdaIN and StrokePyramid clearly show unexpected patterns.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.05127v1",
    "pdf_url": null
  },
  {
    "instance_id": "aeb56a035a1844f996c70ddf00eaeaad",
    "figure_id": "2303.11934v1-Figure16-1",
    "image_file": "2303.11934v1-Figure16-1.png",
    "caption": " Empirical Stale Momenta Seen During Training. (a) We take all neurons that are dead during a batch and record the mean L2 magnitude that its weights experience during the update. Adam (teal) has the largest updates to dead neurons. Note that these magnitudes are still 5-10x smaller than the average for alive neurons. SGDM (purple) also updates dead neurons but to a lesser degree. SGD (yellow) and RMSProp (orange) are not visible because they do not apply any updates to dead neurons. We start our x-axis at training step 300k because this is when there are enough dead neurons for their gradient updates to be meaningfully calculated. (b) We track all gradient updates applied to ten random neurons during the course of training and pick a representative to display here. We track gradient updates starting right as the GABA switch occurs around epoch 100 and all training steps from this point on are shown along the x-axis. RMSProp (orange) has the highest variance and magnitude in gradient updates followed by Adam (teal).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer has the largest updates to dead neurons?",
    "answer": "Adam",
    "rationale": "In panel (a), the teal line representing Adam has the highest L2 gradient magnitude for dead neurons. This indicates that Adam applies the largest updates to dead neurons compared to the other optimizers shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.11934v1",
    "pdf_url": null
  },
  {
    "instance_id": "82458493877f4d6db5b0eac5d7237cfa",
    "figure_id": "1809.02736v1-Figure3-1",
    "image_file": "1809.02736v1-Figure3-1.png",
    "caption": " When evaluated using MS-SSIM (RGB) on Kodak, our combined approach has better RD performance than all previous methods when optimized for MS-SSIM. When optimized for MSE, our method still provides better MS-SSIM scores than all of the standard codecs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best in terms of MS-SSIM on Kodak?",
    "answer": "Our method, optimized for MS-SSIM.",
    "rationale": "The plot shows that our method (green line) has the highest MS-SSIM score for all bitrates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.02736v1",
    "pdf_url": null
  },
  {
    "instance_id": "e0362910a134416aac153dc85a9cc9a4",
    "figure_id": "2306.06779v1-Figure10-1",
    "image_file": "2306.06779v1-Figure10-1.png",
    "caption": " The results of UCB with preference feedback (i.e., UCB-Preference) with comparison to UCB and Co-UCB. The first row is based on XLMR, and the second row is based on SpanBERT. Experiments are run three times with random seeds, and the average results are reported.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the SQuAD dataset with the SpanBERT model?",
    "answer": "UCB-Preference",
    "rationale": "The figure shows the performance of three methods on four datasets. The first row shows the results for the XLMR model, and the second row shows the results for the SpanBERT model. The height of the bars represents the performance of each method. For the SQuAD dataset with the SpanBERT model, the UCB-Preference method has the highest bar, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.06779v1",
    "pdf_url": null
  },
  {
    "instance_id": "aaa59514ac4840e69d6567a6064f2560",
    "figure_id": "2109.06085v2-Figure1-1",
    "image_file": "2109.06085v2-Figure1-1.png",
    "caption": " Performance comparisons on TACoS in terms of R@1, IoU@0.5 and Query Per Second (the number of queries that are retrieved each second during inference). Marker sizes are proportional to the model size. Our GTR-H is 4.9% better than 2D-TAN (Zhang et al., 2020b) with 5 times faster speed.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of R@1, IoU@0.5 and Query Per Second?",
    "answer": "GTR-H",
    "rationale": "The figure shows that GTR-H has the highest R@1, IoU@0.5 and Query Per Second values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.06085v2",
    "pdf_url": null
  },
  {
    "instance_id": "87cd4a93f3464b2c893b5a3e8f4684dc",
    "figure_id": "1805.08322v4-Figure5-1",
    "image_file": "1805.08322v4-Figure5-1.png",
    "caption": " User study results",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four options (RR, RD, LR, or GR) had the highest median gain in the German user study?",
    "answer": "RD",
    "rationale": "The box plot for RD in the German user study (Figure (a)) has the highest median value, as indicated by the horizontal red line within the box.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.08322v4",
    "pdf_url": null
  },
  {
    "instance_id": "9c5de511d76e46128a823baa36e47aa1",
    "figure_id": "2105.02091v2-Figure3-1",
    "image_file": "2105.02091v2-Figure3-1.png",
    "caption": " Population subgroups in our Chess players, Entrepreneurs, and Equestrians datasets.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which population subgroup is most represented in the EQ dataset?",
    "answer": "White Men",
    "rationale": "The bar for White Men in the EQ dataset is the longest, indicating that they are the most represented subgroup.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.02091v2",
    "pdf_url": null
  },
  {
    "instance_id": "36c9714275274e2baf84c301618af565",
    "figure_id": "2201.10787v1-Figure7-1",
    "image_file": "2201.10787v1-Figure7-1.png",
    "caption": " KL and entropy of ql(z) after VMI attack on CelebA and ChestX-ray at different layers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset exhibits a larger difference in KL-divergence between the Gaussian and Flow models?",
    "answer": "CelebA",
    "rationale": "The KL-divergence plots show that the difference between the Gaussian and Flow models is larger for CelebA than for ChestX-ray8. This is evident from the larger gap between the two lines in the CelebA plots compared to the ChestX-ray8 plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.10787v1",
    "pdf_url": null
  },
  {
    "instance_id": "08a94ea6db5343dd9b3c50d640c6d6ff",
    "figure_id": "1711.09541v1-Figure2-1",
    "image_file": "1711.09541v1-Figure2-1.png",
    "caption": " Preliminary study results. We reproduce the real evolving process of the network and restart SVD using the true error. Three measurements used in baselines are calculated between two consecutive restart time points: the number of edge changes, the time interval and the change of reconstruction loss. No obvious pattern is observed, suggesting that no simple correlation between the baselines and the true error can be induced. See Section 3.1 for details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the most consistent increase in the number of edges over time?",
    "answer": "Facebook.",
    "rationale": "The plot for the Facebook dataset shows a steady increase in the number of edges with each restart, while the other datasets show more fluctuations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.09541v1",
    "pdf_url": null
  },
  {
    "instance_id": "6e4a71e6454b415d8763f25d9bc8d667",
    "figure_id": "2001.09382v2-Figure2-1",
    "image_file": "2001.09382v2-Figure2-1.png",
    "caption": " Molecules generated in property optimization and constrained property optimization tasks. (a) Molecules with high penalized logP scores. (b) Molecules with high QED scores. (c) Two pairs of molecules in constrained property optimization for penalized logP with similarity 0.71(top) and 0.64(bottom).",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization method resulted in molecules with the highest similarity?",
    "answer": "Constrained property optimization.",
    "rationale": "The caption states that the two pairs of molecules in (c) were generated using constrained property optimization, and their similarity scores are 0.71 and 0.64, which are higher than the similarity scores of the molecules generated using the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.09382v2",
    "pdf_url": null
  },
  {
    "instance_id": "354341e998ae4ca4bf435d6ff837295e",
    "figure_id": "2004.11795v2-Figure3-1",
    "image_file": "2004.11795v2-Figure3-1.png",
    "caption": " Inference-speed of different models, compared with lattice LSTM ♣. ♣ denotes non-batchparallel version, and ♠ indicates the model is run in 16 batch size parallelly. For model LR-CNN, we do not get its batch-parallel version.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is the fastest?",
    "answer": "FLAT♠",
    "rationale": "The figure shows the relative speed of different models compared to LatticeLSTM♣. The FLAT♠ model has the highest bar, which means it is the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.11795v2",
    "pdf_url": null
  },
  {
    "instance_id": "99fdd963eb0f4f0681b0e71e4dd0f396",
    "figure_id": "2211.05588v2-Figure1-1",
    "image_file": "2211.05588v2-Figure1-1.png",
    "caption": " We address the task of text based Video Question Answering, incorporating VideoText (VideoText is the textual content embedded in the videos) information (bottom right). We propose a new dataset of News Videos along with QA annotations grounded on video text, and explore VQA models that jointly reason over temporal and text based information.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following tasks requires both temporal and text-based reasoning?",
    "answer": "VideoText aware Video QA",
    "rationale": "The figure shows four different types of visual question answering (VQA) tasks. The x-axis represents whether the task requires scene text reasoning, and the y-axis represents whether the task requires temporal reasoning. The four tasks are: VQA (top left), Scene-Text VQA (top right), VideoQA (bottom left), and VideoText aware Video QA (bottom right). \n\nOnly VideoText aware Video QA requires both scene text reasoning and temporal reasoning.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.05588v2",
    "pdf_url": null
  },
  {
    "instance_id": "6eb3659f946b4ff8a0e4ff909a264bfc",
    "figure_id": "2310.16412v1-Figure11-1",
    "image_file": "2310.16412v1-Figure11-1.png",
    "caption": " Convergence of loss values of FlatMatch and FixMatch.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function decreases faster in the first 200 epochs, supervised or unsupervised?",
    "answer": "Supervised loss.",
    "rationale": "The supervised loss curves for both FixMatch and FlatMatch decrease faster than the unsupervised loss curves in the first 200 epochs. This can be seen in the first and second plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.16412v1",
    "pdf_url": null
  },
  {
    "instance_id": "19636eab7e0a441fa4eeaf30a59fba68",
    "figure_id": "2211.11853v2-Figure5-1",
    "image_file": "2211.11853v2-Figure5-1.png",
    "caption": " Synthetic data results learning C, λ1 and λ2. On the top row, we show the node classification accuracy, and in the bottom row we show the learned values of λ1 and λ2 for L-CAT. In the two left-most figures, we show how the results vary with the noise level q for ‖µ‖ = 0.1 and ‖µ‖ = 4.3. In the two right-most figures, we show how the results vary with the norm of the means ‖µ‖ for q = 0.1 and q = 0.3. We use two vertical lines to present the classification threshold stated in Thm. 1 (solid line) and Cor. 2 (dashed line).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following statements is true about the classification accuracy of L-CAT?\n\nA. It increases as the noise level increases.\nB. It decreases as the noise level increases.\nC. It is independent of the noise level.\nD. It is not shown in the figure.",
    "answer": "B. It decreases as the noise level increases.",
    "rationale": "The top row of the figure shows the node classification accuracy for L-CAT and other methods. The x-axis of the plots in the top row represents the noise level q. We can see that as q increases, the accuracy of L-CAT decreases. This is true for both values of ‖µ‖ shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11853v2",
    "pdf_url": null
  },
  {
    "instance_id": "54a8f1d3daed48e0992a3579a266ec05",
    "figure_id": "1906.07647v2-Figure12-1",
    "image_file": "1906.07647v2-Figure12-1.png",
    "caption": " Confusion matrix of our UCCsegment model for ucc predictions on our segmentation dataset.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the accuracy of the UCCsegment model for ucc1 predictions?",
    "answer": "74%",
    "rationale": "The confusion matrix shows that 74% of the ucc1 predictions were correct.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.07647v2",
    "pdf_url": null
  },
  {
    "instance_id": "c82f21aae91b439cb52142509b718fc2",
    "figure_id": "2206.06177v1-Figure1-1",
    "image_file": "2206.06177v1-Figure1-1.png",
    "caption": " The realistic noise matrix in the EuroSAT dataset [11] based on the CLIP model. The label noise is significantly unbalanced among different categories, where the accuracy of 6-th category (“Pasture”) is only 0.01 while the accuracy of 8-th category (“Residential”) is up to 0.95.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which land cover category has the highest predicted accuracy based on the CLIP model?",
    "answer": "Residential",
    "rationale": "The heatmap shows the predicted accuracy for each land cover category. The brighter the color, the higher the accuracy. The Residential category has the brightest color, indicating the highest predicted accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.06177v1",
    "pdf_url": null
  },
  {
    "instance_id": "d80c6d3d9b8c41eab7e97e025a7fe2ec",
    "figure_id": "1909.13155v1-Figure4-1",
    "image_file": "1909.13155v1-Figure4-1.png",
    "caption": " Top-down, the rows correspond to ground truth sequence of actions (pour oil, crack egg, fry egg, put egg2plate) and our action segmentations with neighbor-window size of 20 on the sample video P03 cam01 P03 friedegg from Breakfast dataset using LCDF, LDF and LF, respectively. The background frames are marked in white. The result for LCDF is the best.",
    "figure_type": "schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most successful at correctly segmenting the actions in the video?",
    "answer": "LCDF.",
    "rationale": "The figure shows the ground truth sequence of actions in the video, as well as the action segmentations produced by three different methods: LCDF, LDF, and LF. The LCDF method produces the most accurate segmentation, as it correctly identifies all of the actions in the video.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.13155v1",
    "pdf_url": null
  },
  {
    "instance_id": "d83d0f3cb03a4ca48ca48bd6474e139b",
    "figure_id": "2303.17589v2-Figure2-1",
    "image_file": "2303.17589v2-Figure2-1.png",
    "caption": " DNNs with frozen ImageNet-Polarity (IN-Polarity) learn more quickly and with less data in image classification tasks. A) Experiments on Fashion-MNIST image classification dataset. From left to right: 1) Statistical efficiency: Frozen-Nets with IN-Polarity (red) always learn with fewer samples than Fluid-Net (blue); the majority of the gain is contributed by the knowledge transferred from the initial polarity configuration (pink vs. blue); Frozen-Net RAND-Polarity (green) never performs worse than Fluid-Net (blue). 2) Frozen-Net IN-Polarity always has a higher chance of reaching 80% validation accuracy than Fluid-Net; Frozen-Net RAND-Polarity has comparable and sometimes a higher chance of reaching 80% validation accuracy than Fluid-Net. 3) Computational efficiency: Frozen-Net IN-Polarity always takes less time than Fluid-Net to reach 80% validation accuracy; again, effective knowledge transfer from the preset polarity pattern is the major contributing factor (pink vs. blue); Frozen-Net RAND-Polarity takes a similar number of computational iterations as Fluid-Net. B) Same as A except experiments were on the CIFAR-10 dataset, and the validation accuracy threshold is at 50%. Gray lines in the first column correspond to the validation accuracy thresholds used to plot the next two columns. For a comprehensive view of performance at different thresholds, see Supplementary Figure C.2. For statistical significance of the difference, see Figure 3. Both datasets: n=20 trials, 100 epochs, lr=0.001. No data augmentation was performed.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest success rate for reaching 80% validation accuracy on Fashion-MNIST?",
    "answer": "Freeze IN-Polarity",
    "rationale": "The second column of the figure shows the success rate for reaching 80% validation accuracy on Fashion-MNIST. The red line, which represents Freeze IN-Polarity, is always above the other lines, indicating that it has the highest success rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.17589v2",
    "pdf_url": null
  },
  {
    "instance_id": "131884181ebf4728afadbfa4cb1cd6e0",
    "figure_id": "1910.14673v1-Figure4-1",
    "image_file": "1910.14673v1-Figure4-1.png",
    "caption": " Reconstructions errors over the number of progressive GAN training iterations. (a) MSSIM on CelebA; (b) MSE on CelebA; (c) MSSIM on LSUN; (d) MSE on LSUN; (e) MSSIM on CelebA-HQ; (f) MSE on CelebA-HQ. Ground Truth Masked Image GD+single z GD+multi z AIS",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method achieves the best reconstruction performance on CelebA-HQ?",
    "answer": "AIS.",
    "rationale": "The figure shows the reconstruction errors of different training methods on CelebA-HQ. The AIS method has the lowest reconstruction error, as indicated by the blue line in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.14673v1",
    "pdf_url": null
  },
  {
    "instance_id": "3c71dc768ae94db29c329335fcafebe9",
    "figure_id": "2206.06243v4-Figure9-1",
    "image_file": "2206.06243v4-Figure9-1.png",
    "caption": " Histogram showing the distribution of remaining length of stay (MIMIC vs. AUMC). The buckets are the following: one bucket for less than one day, one bucket each for days 1 through 7, one bucket for the interval between 7 and 14 days, and one bucket for more than 14 days.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has a higher frequency of patients with a length of stay between 8 and 14 days?",
    "answer": "AUMC",
    "rationale": "The orange bar for the (8,14) bucket is taller than the blue bar, indicating that the frequency of patients with a length of stay in that range is higher for AUMC than for MIMIC.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.06243v4",
    "pdf_url": null
  },
  {
    "instance_id": "bff749734e7f4b7380492039eac71434",
    "figure_id": "2007.00717v2-Figure6-1",
    "image_file": "2007.00717v2-Figure6-1.png",
    "caption": " Comparison of the observed rewards and the size of the partition for the four algorithms on the two ambulance problem with α = 1 and arrivals Fh = Beta(5, 2). We ommit confidence bars in this plot to help with readability.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four algorithms appears to be the most efficient in terms of the size of the partition?",
    "answer": "EpsilonQL",
    "rationale": "The plot on the right shows the size of the partition for each algorithm. EpsilonQL has the smallest partition size, which means it is the most efficient.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.00717v2",
    "pdf_url": null
  },
  {
    "instance_id": "910b6d76f17144a593cb5f1f3b7cdb8c",
    "figure_id": "2010.16274v2-Figure3-1",
    "image_file": "2010.16274v2-Figure3-1.png",
    "caption": " An example of the swapping mechanism. In this figure, we use M1 to M6 to denote addresses maintained by the mixing service. By swapping different user inputs and outputs, the relationship anonymity for all addresses is preserved. For instance, the relationship from A to A1 and A2 is anonymized.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In the example, how many users are there?",
    "answer": "There are 3 users: A, B, and C.",
    "rationale": "The figure shows three peeling chains, one for each user. Each peeling chain shows the inputs and outputs of a user, and the addresses used by the mixing service to anonymize the relationship between them.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.16274v2",
    "pdf_url": null
  },
  {
    "instance_id": "8d663a09885b47c4b2dcaeb2fe716020",
    "figure_id": "2011.12102v1-Figure9-1",
    "image_file": "2011.12102v1-Figure9-1.png",
    "caption": " The confusion matrix for the twelve activities.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activity is most likely to be confused with \"Drinking\"?",
    "answer": "Eating.",
    "rationale": "The confusion matrix shows that the model is most likely to confuse \"Drinking\" with \"Eating\", as the corresponding cell has the highest value (0.44) outside of the diagonal.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.12102v1",
    "pdf_url": null
  },
  {
    "instance_id": "86b5d000e54f4da59bb529d8a8cdf417",
    "figure_id": "1906.01140v2-Figure2-1",
    "image_file": "1906.01140v2-Figure2-1.png",
    "caption": " Rough instance boxes.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the green and pink boxes?",
    "answer": "The pink box is contained within the green box.",
    "rationale": "The figure shows two boxes, one green and one pink. The pink box is smaller than the green box and is located entirely within the green box.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.01140v2",
    "pdf_url": null
  },
  {
    "instance_id": "978879f7c06e4eb3948905c9e8ac4f7a",
    "figure_id": "2312.01473v1-Figure8-1",
    "image_file": "2312.01473v1-Figure8-1.png",
    "caption": " Success rates for zero-shot downstream task generalization for assembly tasks in CONSTRUCTION for model checkpoints over the course of free play. We compare RaIR + CEE-US (λ = 0.1) and RaIR with CEE-US, RND and Dis. We used five independent seeds.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four tasks has the highest success rate?",
    "answer": "Singletower 3.",
    "rationale": "The success rate for Singletower 3 is 100%, while the success rates for the other tasks are lower.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2312.01473v1",
    "pdf_url": null
  },
  {
    "instance_id": "40b3e825b89e4c6384274aaa39c555fd",
    "figure_id": "2109.09876v2-Figure8-1",
    "image_file": "2109.09876v2-Figure8-1.png",
    "caption": " MiniGrid-Empty-Random-6x6-v0",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four algorithms performs best in the MiniGrid Empty Domain environment?",
    "answer": "CRADOL",
    "rationale": "The figure shows the episodic return of four different algorithms over time. CRADOL (red line) has the highest episodic return throughout the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.09876v2",
    "pdf_url": null
  },
  {
    "instance_id": "94c5ebf7e6f741b2b06ec0dba150ce7d",
    "figure_id": "1809.04279v3-Figure1-1",
    "image_file": "1809.04279v3-Figure1-1.png",
    "caption": " Convergence rates of a GLM trained with REINFORCE verses the proposed DIRECT method. The DIRECT method greatly outperforms REINFORCE in iterations and wall-clock time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method converges faster, DIRECT or REINFORCE?",
    "answer": "DIRECT.",
    "rationale": "The DIRECT method converges to a stable ELBO value within 20 iterations, while REINFORCE takes much longer to converge and even after 100 iterations has not reached a stable value. This can be seen in the left panel of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.04279v3",
    "pdf_url": null
  },
  {
    "instance_id": "34f586a45a7740279c2ea96c9f8628c8",
    "figure_id": "2307.09696v2-Figure3-1",
    "image_file": "2307.09696v2-Figure3-1.png",
    "caption": " Comparisons between strict inverse consistency trained results (Top) and cross-sanity checked results (Bottom). Our relaxed sanity-checked result maintains a similar level of inverse consistency as φm→f ◦ φf→m is close to id transformation (second column). We can also observe that ours produces a more regular map, compared to the folded map from the model trained with strict inverse consistency. Best view zoomed.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two approaches (Strict or Ours) is more likely to produce folded maps?",
    "answer": "Strict",
    "rationale": "The figure shows that the \"Strict\" approach produces a folded map, while the \"Ours\" approach produces a more regular map. This can be seen in the third column of the figure, where the warped grid for the \"Strict\" approach is folded, while the warped grid for the \"Ours\" approach is not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.09696v2",
    "pdf_url": null
  },
  {
    "instance_id": "807d1c951da346f8a2dea8ae8f38d594",
    "figure_id": "2310.00708v1-Figure4-1",
    "image_file": "2310.00708v1-Figure4-1.png",
    "caption": " Histograms of Meta-Testing Performance in Sinusoid Regression Problems. With α = 0.7, we respectively visualize the comparison results, DR-MAML-vs-MAML and TR-MAML-vs-MAML in 5-shot (Two Sub-figures Left Side) and 10-shot (Two Sub-figures Right Side) cases, for a sample trial.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better in the 5-shot case, MAML or DR-MAML?",
    "answer": "DR-MAML",
    "rationale": "The histograms show the distribution of the mean squared error (MSE) for each method. In the 5-shot case, the histogram for DR-MAML is shifted to the left compared to the histogram for MAML, indicating that DR-MAML has a lower MSE and therefore performs better.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.00708v1",
    "pdf_url": null
  },
  {
    "instance_id": "411f82b6f4a5423b961a37a806aed724",
    "figure_id": "1805.02211v3-Figure3-1",
    "image_file": "1805.02211v3-Figure3-1.png",
    "caption": " Number of queries per app for top 17 apps.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which app is the most popular based on the number of queries?",
    "answer": "Google search",
    "rationale": "The figure shows the percentage of queries for each app. Google search has the highest percentage of queries, at around 40%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.02211v3",
    "pdf_url": null
  },
  {
    "instance_id": "512ecb3d934a4ce6badc967724cffdbd",
    "figure_id": "2201.02435v2-Figure2-1",
    "image_file": "2201.02435v2-Figure2-1.png",
    "caption": " Ablation studies of sub-modules in ST-SHN.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sub-module in ST-SHN has the highest MacroF1 score?",
    "answer": "The sub-module \"T\" has the highest MacroF1 score.",
    "rationale": "This can be seen in the bar graph labeled \"(a) NYC F1\". The height of the bar corresponding to \"T\" is the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.02435v2",
    "pdf_url": null
  },
  {
    "instance_id": "8c8ee65b3d874411a2e6c1c2fd332e04",
    "figure_id": "1905.11979v2-Figure7-1",
    "image_file": "1905.11979v2-Figure7-1.png",
    "caption": " Reward vs. number of intervention episodes (policy execution interventions) on MountainCar and Hopper. Our methods UNIF-INTERVENTION and DISC-INTERVENTION bridge most of the causal misidentification gap (between ORIGINAL (lower bound) and CONFOUNDED (upper bound), approaching ORIGINAL performance after tens of episodes. GAIL [19] (on Hopper) achieves this too, but after 1.5k episodes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the best performance on the MountainCar environment?",
    "answer": "DISC-INTERVENTION",
    "rationale": "The figure shows that DISC-INTERVENTION achieves the highest reward on the MountainCar environment, as its curve is closest to the ORIGINAL (upper bound) curve.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11979v2",
    "pdf_url": null
  },
  {
    "instance_id": "a8e42024d9dc41c09aa637a8c0e9c9ec",
    "figure_id": "2202.11797v2-Figure2-1",
    "image_file": "2202.11797v2-Figure2-1.png",
    "caption": " RDE-Explanations of a bird image taken from (Macdonald et al., 2021) with permission of the authors. The proposed optimisation methods (FW, AFW, LCG, LAFW) search for the smallest set of pixels that still maintain the classification of “bird”, if the other pixels are randomised. All produce a mask that creates a new bird head as a “superstimulus” mask. The sensitivity map does not show this behaviour. The distribution of the randomised pixels is independent from the selected set, which means the contours of the set will be visible and detectable by the pattern matching network. Modelling the noise after true distribution 𝑝(x𝑆𝑐 | x𝑆) (as in Equation (2.1)) would prevent this effect, since a monochrome selection of black pixels would likely be inpainted with black as well. Then no bird’s head would appear.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which explanation method produces the smallest mask that still maintains the classification of “bird”?",
    "answer": "It is not possible to tell from the image.",
    "rationale": "The image shows the masks produced by the different explanation methods, but it does not provide any information about the size of the masks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.11797v2",
    "pdf_url": null
  },
  {
    "instance_id": "3b7bb53908ac4ab4b71af96478735796",
    "figure_id": "1810.11953v4-Figure40-1",
    "image_file": "1810.11953v4-Figure40-1.png",
    "caption": " CIFAR-10 medium image shift, multivariate two-sample tests.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest p-value for all three levels of perturbation?",
    "answer": "NoRed.",
    "rationale": "The NoRed line is consistently above the other lines in all three subplots, indicating that it has the highest p-value for all three levels of perturbation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.11953v4",
    "pdf_url": null
  },
  {
    "instance_id": "72e3562cd5c04ce4afe0245ee265286b",
    "figure_id": "2104.02699v2-Figure5-1",
    "image_file": "2104.02699v2-Figure5-1.png",
    "caption": " Quantitative comparison. We compare ReStyle with current state-of-the-art optimization-based and encoder-based methods by analyzing reconstruction via three evaluation metrics — ID similarity for faces, L2 loss for cars, and LPIPS loss for churches — while measuring each method’s inference time. Each encoder-based method is represented using a ? symbol. The corresponding hybrid method is marked using a dashed line of the same color with the ReStyle applied over the base method shown using a solid line of the same color. Optimization results are shown using a dashed green line. Methods based on pSp are shown in red with methods based on e4e shown in blue. Finally, results obtained using IDInvert [52] are shown in orange. Note that both axes are shown in log-scale.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest inference time for a given ID similarity score on CelebA-HQ?",
    "answer": "ReStyle e4e",
    "rationale": "The figure shows that ReStyle e4e (solid blue line) achieves the highest ID similarity score for a given inference time compared to all other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.02699v2",
    "pdf_url": null
  },
  {
    "instance_id": "b88f74a2dd3441b8bbe34a07ef0ebd9f",
    "figure_id": "2101.11342v1-Figure20-1",
    "image_file": "2101.11342v1-Figure20-1.png",
    "caption": " EnTranNAS-DST (λ = 0.05) normal cell searched on ImageNet (the result in Table 4).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different types of convolutional layers are used in the normal cell of EnTranNAS-DST?",
    "answer": "Three.",
    "rationale": "The figure shows three different types of convolutional layers: 3x3 separable convolution, 5x5 dilated convolution, and 3x3 convolution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.11342v1",
    "pdf_url": null
  },
  {
    "instance_id": "babbff2cf4214c759c8a3db084839bc7",
    "figure_id": "2004.13117v3-Figure5-1",
    "image_file": "2004.13117v3-Figure5-1.png",
    "caption": " Top-1 answer passage for question in turn 2.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which film is the final installment in Nolan's Batman film trilogy?",
    "answer": "The Dark Knight Rises",
    "rationale": "The passage states that \"The Dark Knight Rises is a 2012 superhero film directed by Christopher Nolan...Featuring the DC Comics character Batman, the film is the final installment in Nolan's Batman film trilogy\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.13117v3",
    "pdf_url": null
  },
  {
    "instance_id": "ef1a0715adee4875bbbf713b6258bb66",
    "figure_id": "2103.00755v2-Figure7-1",
    "image_file": "2103.00755v2-Figure7-1.png",
    "caption": " An instance of SyntheticModel2with attribute Z = u shown in red and Z = v shown in blue.",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which attribute is more likely to be associated with the red points in the figure?",
    "answer": " Attribute u.",
    "rationale": " The caption states that the red points represent instances of Synthetic Model II with attribute Z = u. \n\n**Figure type:** Plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.00755v2",
    "pdf_url": null
  },
  {
    "instance_id": "56035648bf68434c8e2016cddcdb5605",
    "figure_id": "2006.08844v2-Figure5-1",
    "image_file": "2006.08844v2-Figure5-1.png",
    "caption": " DualRC-Net vs other state-of-the-art methods on HPatches. For other methods, top 2000 feature points are selected. By enforcing mutual nearest neighbour, they roughly generate 1000 matches for each image pairs. Hence we select top 1000 matches for fair comparison.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best overall?",
    "answer": "DualRC-Net.",
    "rationale": "The figure shows the MMA (Mean Matching Accuracy) for different methods on the HPatches dataset. DualRC-Net consistently has the highest MMA across all three evaluation categories (illumination, viewpoint, and overall).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.08844v2",
    "pdf_url": null
  },
  {
    "instance_id": "f8cc0c952ce44b74a776d2ef4fea584c",
    "figure_id": "2306.03355v1-Figure11-1",
    "image_file": "2306.03355v1-Figure11-1.png",
    "caption": " Histograms of cosine similarity and Percentage of false negative of all pairs in a batch for embeddings trained using different sampling methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sampling method results in the highest cosine similarity between embeddings?",
    "answer": "RWR",
    "rationale": "The RWR curve is shifted furthest to the right in the cosine similarity plot, indicating that it has the highest average cosine similarity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.03355v1",
    "pdf_url": null
  },
  {
    "instance_id": "c4ac38759e3a48fc9b42ce1b04b09e07",
    "figure_id": "1907.08268v2-Figure1-1",
    "image_file": "1907.08268v2-Figure1-1.png",
    "caption": " Reconstruction model processing given an input molecule. Location-specific representations computed via message passing are passed through fully-connected layers outputting probabilities for each legal operation.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the legal operations that can be performed on the molecule?",
    "answer": "The legal operations are INSERT, STOP, and DELETE.",
    "rationale": "The figure shows a list of legal operations that can be performed on the molecule. These operations are listed in the \"Legal Operations\" box on the right side of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.08268v2",
    "pdf_url": null
  },
  {
    "instance_id": "a798c9eed1e345878e028a24de519588",
    "figure_id": "1911.00962v1-Figure2-1",
    "image_file": "1911.00962v1-Figure2-1.png",
    "caption": " Left: The only possible transport plan in one-hot target case. Right: the ground matrix using arc length as ground metric.",
    "figure_type": "** schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the distance between server $s_0$ and server $s_2$? ",
    "answer": " 2",
    "rationale": " The ground matrix on the right shows the distance between each pair of servers. The entry in the row corresponding to server $s_0$ and the column corresponding to server $s_2$ is 2, indicating that the distance between these two servers is 2. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.00962v1",
    "pdf_url": null
  },
  {
    "instance_id": "e2143548661c40a9ae327ee875551237",
    "figure_id": "1905.05460v2-Figure1-1",
    "image_file": "1905.05460v2-Figure1-1.png",
    "caption": " An example of cognitive graph for multi-hop QA. Each hop node corresponds to an entity (e.g., “Los Angeles”) followed by its introductory paragraph. The circles mean ans nodes, answer candidates to the question. Cognitive graph mimics human reasoning process. Edges are built when calling an entity to “mind”. The solid black edges are the correct reasoning path.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which film is directed by Todd Phillips and features scenes filmed at the Quality Cafe in Los Angeles?",
    "answer": "Old School",
    "rationale": "The figure shows that Old School is a 2003 film directed by Todd Phillips. It also shows that the Quality Cafe was a location featured in a number of Hollywood films, including \"Old School.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.05460v2",
    "pdf_url": null
  },
  {
    "instance_id": "4161a0d8f15c47f088aba98786fc271c",
    "figure_id": "2203.06342v1-Figure17-1",
    "image_file": "2203.06342v1-Figure17-1.png",
    "caption": " Comprehension types and human–model performance gap. The triangle markers indicate mean values and the black bars indicate medians.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of comprehension question shows the biggest difference in performance between humans and the model?",
    "answer": "Factoid questions.",
    "rationale": "The box plot for factoid questions shows the largest difference between the median human performance and the median model performance. The median human performance is around 40%, while the median model performance is around 0%. This indicates that humans are much better at answering factoid questions than the model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.06342v1",
    "pdf_url": null
  },
  {
    "instance_id": "0cc27a0083d34c5bb5def6ff1ac676a4",
    "figure_id": "1907.12629v2-Figure4-1",
    "image_file": "1907.12629v2-Figure4-1.png",
    "caption": " Convergence of Vanilla Bin-MobileNet vs MoBiNet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model shows faster convergence?",
    "answer": "Vanilla Bin-MobileNet",
    "rationale": "Vanilla Bin-MobileNet has a lower training and testing loss at each epoch.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.12629v2",
    "pdf_url": null
  },
  {
    "instance_id": "0e850a4933e3495b8dad649562553c25",
    "figure_id": "2211.11875v1-Figure4-1",
    "image_file": "2211.11875v1-Figure4-1.png",
    "caption": " “Good” flip examples from the VQA experiments. The green texts mark the correctly selected answers, while the red texts indicate incorrectly selected answers.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is more accurate at answering questions about the images?",
    "answer": "ConCoRD",
    "rationale": "The figure shows that ConCoRD correctly answers all the questions about the images, while the base model incorrectly answers some of the questions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11875v1",
    "pdf_url": null
  },
  {
    "instance_id": "b816dfdbf23d4826b5f949c4e2cbe091",
    "figure_id": "2311.02002v1-Figure3-1",
    "image_file": "2311.02002v1-Figure3-1.png",
    "caption": " Training error and validation accuracy of NNAG, SGD, SVRG, and NNAG+SVRG when used for training a simple CNN on CIFAR10 dataset. Lower and upper confidence bounds with significance level of 0.68 are drawn with similar color to their corresponding line.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four optimization methods achieves the highest validation accuracy?",
    "answer": "NNAG+SVRG",
    "rationale": "The figure on the right shows the validation accuracy of the four optimization methods over the training epochs. The blue line, which represents NNAG+SVRG, is the highest of the four lines, indicating that this method achieves the highest validation accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2311.02002v1",
    "pdf_url": null
  },
  {
    "instance_id": "fe06004df66740bcaeaf994baa9b8c7d",
    "figure_id": "1908.05005v3-FigureA.4-1",
    "image_file": "1908.05005v3-FigureA.4-1.png",
    "caption": "Figure A.4: The normalized intensity distribution of a PSF kernel of our proposed PSF blur.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the shape of the PSF kernel?",
    "answer": "The PSF kernel is circular.",
    "rationale": "The image shows a heatmap of the intensity distribution of the PSF kernel. The highest intensity is in the center of the image, and the intensity decreases as you move away from the center. This indicates that the PSF kernel is circular.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.05005v3",
    "pdf_url": null
  },
  {
    "instance_id": "d636503bbeeb4e0ab6145393d5a6a730",
    "figure_id": "2108.13753v2-Figure9-1",
    "image_file": "2108.13753v2-Figure9-1.png",
    "caption": " Large versions of Fig.3. (Left) Results for DSPRITES. (Right) Results for 3DSHAPES.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which disentanglement metric performs the best on DSPRITES and 3DSHAPES?",
    "answer": "BetaVAE",
    "rationale": "The disentanglement scores for BetaVAE are consistently higher than the other models across both datasets and all metrics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.13753v2",
    "pdf_url": null
  },
  {
    "instance_id": "431507c132394e6093b4830c0a6ab10e",
    "figure_id": "2208.05514v2-Figure1-1",
    "image_file": "2208.05514v2-Figure1-1.png",
    "caption": " Illustration of the error generated by our method and the existing method in estimating gradient distribution on edge. The blue curve shows the continuous distribution. The thick lines in green and red are the estimation of our method and the existing method, while their estimation error to the continuous distribution is presented in thin green slash and light grey background.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has a higher error in estimating the gradient distribution on the edge?",
    "answer": "The existing method.",
    "rationale": "The figure shows that the error of the existing method (light gray background) is larger than the error of our method (thin green slash) for most of the edge values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.05514v2",
    "pdf_url": null
  },
  {
    "instance_id": "e07d195df3144ce1bcdcf8f174937849",
    "figure_id": "2307.09619v1-Figure12-1",
    "image_file": "2307.09619v1-Figure12-1.png",
    "caption": " Median post-personalization loss across FedC4 validation clients, with different numbers of batches used in each client’s local computation. Error bars indicate the 10th and 90th percentiles. All runs are equalized to process the same number of tokens in total.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has a lower median post-personalization loss across FedC4 validation clients?",
    "answer": "FedAvg",
    "rationale": "The solid line in the plot represents FedAvg, and the dashed line represents FedSGD. The solid line is consistently lower than the dashed line, indicating that FedAvg has a lower median post-personalization loss.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.09619v1",
    "pdf_url": null
  },
  {
    "instance_id": "4e4543920c7046c381d46e7a84b89498",
    "figure_id": "1805.08819v4-Figure2-1",
    "image_file": "1805.08819v4-Figure2-1.png",
    "caption": " (A) A representative selection of ILSVRC12 images and their ClickMe maps. The transparency channel of these images is set to the proportion of ClickMe bubbles for that location across participants, making feature visibility reflect importance. Animals are outlined in blue and non-animals in red. (B) Features identified in “top-down” ClickMe maps are more diagnostic for object recognition than those identified in “bottom-up” Salicon maps. A rapid visual categorization experiment compared human performance in detecting animals when features were revealed according to ClickMe maps (blue curve) or Salicon maps (red curve). ClickMe-masked and Salicon-masked image exemplars are shown for the condition in which 100% of important features are visible, demonstrating how “bottom-up” saliency is not necessarily relevant to the task. For clarity, we omitted data between 1-10% of features visible from this plot where accuracy was chance for participants of both groups. Error bars are S.E.M. ***: p <0.001 (statistical testing with randomization tests detailed in the Appendix).",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature source, ClickMe or Salicon, leads to better human categorization accuracy when a greater percentage of the image is revealed?",
    "answer": "ClickMe.",
    "rationale": "Panel B of the figure shows that the blue curve (ClickMe) is consistently above the red curve (Salicon), indicating that human categorization accuracy is higher when features are revealed according to ClickMe maps compared to Salicon maps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.08819v4",
    "pdf_url": null
  },
  {
    "instance_id": "c3256819182c4aec880433e2acbb210a",
    "figure_id": "1903.03906v1-Figure4-1",
    "image_file": "1903.03906v1-Figure4-1.png",
    "caption": " Visualisation of the RBP relational model partition structure for five relational data sets: (left to right) Digg, Flickr, Gplus, Facebook and Twitter.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the five relational data sets has the most complex partition structure?",
    "answer": "Facebook",
    "rationale": "The Facebook partition structure has the most boxes, indicating that it has the most complex structure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.03906v1",
    "pdf_url": null
  },
  {
    "instance_id": "4e37307148f849138c1df30d4add70bb",
    "figure_id": "1811.12814v1-Figure6-1",
    "image_file": "1811.12814v1-Figure6-1.png",
    "caption": " Performance comparison on Kinetics-400 dataset. The clip level top-1 accuracy is shown one the left, while the video level top-1 accuracy is shown on the right.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in terms of video accuracy?",
    "answer": "Ours",
    "rationale": "The bar corresponding to \"Ours\" in the \"Video Accuracy (10 clips)\" plot is the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.12814v1",
    "pdf_url": null
  },
  {
    "instance_id": "6874f5f197e7452686826ee0a9462326",
    "figure_id": "2105.06067v1-Figure2-1",
    "image_file": "2105.06067v1-Figure2-1.png",
    "caption": " Popularity drift between: (a) two successive stages 𝐷𝑃 (𝑡, 𝑡 + 1); (b) the first and present stage 𝐷𝑃 (1, 𝑡).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which platform shows the least popularity drift between two successive stages?",
    "answer": "Douban.",
    "rationale": "The plot in Figure (a) shows the popularity drift between two successive stages for three platforms: Kwai, Douban, and Tencent. The y-axis represents the popularity drift (DP), and the x-axis represents the stage number (t). The plot shows that Douban has the smallest DP values across all stages, indicating that it has the least popularity drift between two successive stages.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.06067v1",
    "pdf_url": null
  },
  {
    "instance_id": "bedd9123b94746a294135a228cd3d94f",
    "figure_id": "2303.09166v1-Figure2-1",
    "image_file": "2303.09166v1-Figure2-1.png",
    "caption": " Examples of image/text pairs.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object in the image is described incorrectly?",
    "answer": "The object in the bottom-right of the image.",
    "rationale": "The caption describes the object as a \"lawngreen colored cow,\" but the object does not resemble a cow.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.09166v1",
    "pdf_url": null
  },
  {
    "instance_id": "59fd0e8c9ee94b498abc1c6129e884bd",
    "figure_id": "2312.02019v1-Figure8-1",
    "image_file": "2312.02019v1-Figure8-1.png",
    "caption": " Samples of training curve in phase 2 of AIME. The first three showcase the typical successful training curves, while the remaining three demonstrate the failure cases. The true_action is referring to evaluating the trajectories with the true action sequence.",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following figures is an example of a failure case of the training curve in phase 2 of AIME?",
    "answer": "Action mse.",
    "rationale": "The caption mentions that the first three figures are examples of successful training curves, while the last three figures are examples of failure cases. The fourth figure from the left shows the action mse for AIME.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2312.02019v1",
    "pdf_url": null
  },
  {
    "instance_id": "6965b0fd2b4d4aba935e043c69b19388",
    "figure_id": "2103.01171v2-Figure1-1",
    "image_file": "2103.01171v2-Figure1-1.png",
    "caption": " Various EDP values in a grid world with two goals. Values are presented as EDP(s, π̂g1 |π̂g2)/EDP(s, π̂g2 |π̂g1).",
    "figure_type": "Table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the expected discounted payoff (EDP) of state (3, 2) under the optimal policy for reaching goal g1, given that the optimal policy for reaching goal g2 is also followed?",
    "answer": "3/2",
    "rationale": "The EDP value for state (3, 2) is shown in the cell at the intersection of row 3 and column 2. The value is presented as a fraction, with the numerator representing the EDP under the optimal policy for reaching goal g1 and the denominator representing the EDP under the optimal policy for reaching goal g2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.01171v2",
    "pdf_url": null
  },
  {
    "instance_id": "7961a9e6ffc3436bb878e0c3301087ee",
    "figure_id": "2306.05179v2-Figure5-1",
    "image_file": "2306.05179v2-Figure5-1.png",
    "caption": " Performance of ChatGPT across different subject categories.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language did ChatGPT perform best in for the math category?",
    "answer": "Thai (th)",
    "rationale": "The figure shows the performance of ChatGPT across different subject categories for different languages. The math category is represented by the brown line, and the Thai language is represented by the red line. The red line is the highest in the math category, indicating that ChatGPT performed best in Thai for that category.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.05179v2",
    "pdf_url": null
  },
  {
    "instance_id": "14bb545e3ac44b4a93ae1ac0401e8cd5",
    "figure_id": "2304.02205v1-Figure6-1",
    "image_file": "2304.02205v1-Figure6-1.png",
    "caption": " The improved performance curve over training epochs of DKVMN and NCDMmodels.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better in the early epochs of training when cognitive features are used?",
    "answer": "DKVMN",
    "rationale": "The figure shows the accuracy of the DKVMN and NCDM models over training epochs. When cognitive features are used, DKVMN has a higher accuracy than NCDM in the early epochs of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.02205v1",
    "pdf_url": null
  },
  {
    "instance_id": "d855d6cac2e94847aaf6ae5917af0a5e",
    "figure_id": "1909.04625v1-Figure2-1",
    "image_file": "1909.04625v1-Figure2-1.png",
    "caption": " Non-Coordination Agreement experiments for English (number) and French (number and gender).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best for English number agreement?",
    "answer": "LSTM (enWiki)",
    "rationale": "The figure shows the results of different models for English number agreement. The y-axis shows the agreement score, and the x-axis shows the different models. The LSTM (enWiki) model has the highest agreement score, which indicates that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.04625v1",
    "pdf_url": null
  },
  {
    "instance_id": "67bcef77b3644558b04d222eb72dea79",
    "figure_id": "1906.04659v3-Figure9-1",
    "image_file": "1906.04659v3-Figure9-1.png",
    "caption": " Test accuracies on CIFAR10 for clean data using a stopping criterion based on train accuracy. Higher is better.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture performs best on CIFAR10 for clean data using a stopping criterion based on train accuracy?",
    "answer": "StableNet-50",
    "rationale": "The figure shows the test accuracies on CIFAR10 for clean data using a stopping criterion based on train accuracy. The network with the highest accuracy is StableNet-50, with an accuracy of around 94.5%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04659v3",
    "pdf_url": null
  },
  {
    "instance_id": "9beea88c78a24adf9e59fb7605fa4f3f",
    "figure_id": "2205.09797v2-Figure20-1",
    "image_file": "2205.09797v2-Figure20-1.png",
    "caption": " Hyper-parameter tuning results for disentanglement weight (λdecor) on Multi-MNIST.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which decorrelation_weight value yields the highest validation accuracy?",
    "answer": "20.0",
    "rationale": "The box plot for decorrelation_weight value of 20.0 has the highest median and upper quartile values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.09797v2",
    "pdf_url": null
  },
  {
    "instance_id": "915de8cfe3d44d5d9ff110bf9e0504f5",
    "figure_id": "2004.07514v1-Figure6-1",
    "image_file": "2004.07514v1-Figure6-1.png",
    "caption": " Visualization of predictions of two models (LGI and LGI–SQAN) and their temporal attention weights o computed before regression.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better in terms of temporal attention?",
    "answer": "LGI-SQAN",
    "rationale": "LGI-SQAN's attention weights are more focused on the relevant frames (e.g., the woman sanding the table and dipping the brush into paint) than LGI's attention weights, which are more spread out across the entire sequence. This suggests that LGI-SQAN is better able to attend to the most important information in the video when making its predictions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.07514v1",
    "pdf_url": null
  },
  {
    "instance_id": "5d0b0b7724dd4d20bc073503e6764db9",
    "figure_id": "2211.09371v3-Figure10-1",
    "image_file": "2211.09371v3-Figure10-1.png",
    "caption": " Additional visualization of captioning cases on MSCOCO test set. The newly generated details are underlined.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following captions is most accurate for the first image?",
    "answer": "\"A bathroom with a sink and a mirror.\"",
    "rationale": "The first image shows a bathroom with a sink and a mirror. The other captions are incorrect because they either describe a different image or include details that are not present in the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.09371v3",
    "pdf_url": null
  },
  {
    "instance_id": "111d1fa6a9f04225bf0f95bddfe636c5",
    "figure_id": "2202.08816v3-Figure3-1",
    "image_file": "2202.08816v3-Figure3-1.png",
    "caption": " Influence of 𝛼 on (a) BA-Shapes and (b) Mutag0.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two datasets, BA-Shapes or Mutag0, shows a more significant change in F_NS and F_1 as alpha increases?",
    "answer": "Mutag0",
    "rationale": "The plots in Figure (b) show a more dramatic change in both F_NS and F_1 as alpha increases, compared to the plots in Figure (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.08816v3",
    "pdf_url": null
  },
  {
    "instance_id": "70714e22023d4c0d9ebe635ce5c2e238",
    "figure_id": "2306.10191v2-Figure3-1",
    "image_file": "2306.10191v2-Figure3-1.png",
    "caption": " Performance of Neural Priming and comparable methods in the few-shot setting. We find consistent improvement across shot numbers and datasets. In particular, Neural Priming especially excels for fine-grained datasets such as FGVCAircraft and Flowers102. We hypothesize that such fine-grained captioned images are not well represented in LAION-2B, therefore revisting this subset of data improves the model more.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the FGVC-Aircraft dataset?",
    "answer": "Neural Priming",
    "rationale": "The figure shows the accuracy of different methods on the FGVC-Aircraft dataset. The line for Neural Priming is consistently higher than the lines for the other methods, indicating that it performs best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.10191v2",
    "pdf_url": null
  },
  {
    "instance_id": "3c05a1f321c740bb9c197feeb2a65e14",
    "figure_id": "2112.03497v2-Figure7-1",
    "image_file": "2112.03497v2-Figure7-1.png",
    "caption": " Counts of linked entity types across all WMT language pairs. Notice the y-axis log-scale: many entities are linked differently on non-English input.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which entity type has the largest difference between the number of common and source-only entities?",
    "answer": "PERSON",
    "rationale": "The figure shows the counts of linked entity types across all WMT language pairs, with the y-axis on a log scale. The bars are divided into two parts: common entities (hatched) and source-only entities (solid). The difference between the two parts is largest for the PERSON entity type.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.03497v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd976a532fb54c9fb2fda18e171c6e57",
    "figure_id": "1902.00177v2-Figure2-1",
    "image_file": "1902.00177v2-Figure2-1.png",
    "caption": " Top: Training performance of the deterministic surrogate (left) and the LRT surrogate for stochastic binary weights and continuous neurons (right). The vertical axis represents network depth against the variance of the means σ2 m. Both surrogates were trained with σ2 b = 0. Thus, as σ2 m → 1 we approach criticality in both cases. Overlaid are curves proportional to the correlation depth scale ξc. Bottom: Training performance of the deterministic surrogate and its binary counterparts after training on the MNIST dataset for 5 epochs. Left: performance of the continuous surrogate. Centre: the performance of the stochastic binary network, averaged over 5 Monte Carlo samples. Right: 100 Monte Carlo samples. The deterministic binary evaluation is similar to a single Monte Carlo sample, resembling the central figure.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which surrogate model performs better when approaching criticality?",
    "answer": "The deterministic surrogate model.",
    "rationale": "As σ2 m → 1, the deterministic surrogate model shows a sharper increase in performance compared to the LRT surrogate model. This is evident from the steeper slope of the performance curve for the deterministic surrogate model in the top left panel of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.00177v2",
    "pdf_url": null
  },
  {
    "instance_id": "d478b4b3aea14a49a83bbce784b6044e",
    "figure_id": "2210.14102v1-Figure13-1",
    "image_file": "2210.14102v1-Figure13-1.png",
    "caption": " The performance of interpolations along a non-linear path connecting two minima trained with adapter tuning. For (a-c), two minima are trained with different training data order. For (d-f), two minima are trained with in-distribution data of the same task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which adapter performs the best for in-distribution data of the same task?",
    "answer": "The SST-2 adapter.",
    "rationale": "Figure (f) shows that the SST-2 adapter achieves the highest Test ACC for all interpolation steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.14102v1",
    "pdf_url": null
  },
  {
    "instance_id": "7207eb5fe7224b4191dd7e5175c09fb8",
    "figure_id": "2105.11645v2-Figure4-1",
    "image_file": "2105.11645v2-Figure4-1.png",
    "caption": " tSuc results w.r.t. bias c for PAAp transferring from Den121 (white-box model) to VGG19, Inc-v3, and Res50 (blackbox model). We observe the highest results when c=0, i.e., polynomial with pure second-order terms.",
    "figure_type": "** Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which model has the highest tSuc value when c = 0? ",
    "answer": " VGG19. ",
    "rationale": " The plot shows that the blue line, which represents the Den121 -> VGG19 model, has the highest tSuc value when c = 0.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.11645v2",
    "pdf_url": null
  },
  {
    "instance_id": "c194984b634548adaece3288780d7d9e",
    "figure_id": "2006.14884v2-Figure16-1",
    "image_file": "2006.14884v2-Figure16-1.png",
    "caption": " Deadline-Aware Scheduling on W4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which scheduling algorithm performs best in terms of application throughput?",
    "answer": "pFabric-EDF.",
    "rationale": "The plot in Figure (a) shows that pFabric-EDF has the highest throughput for all load levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.14884v2",
    "pdf_url": null
  },
  {
    "instance_id": "37fcfadfdc4a42a3938208406477d77f",
    "figure_id": "1902.07208v3-Figure8-1",
    "image_file": "1902.07208v3-Figure8-1.png",
    "caption": " Hybrid approaches to transfer learning: reusing a subset of the weights and slimming the remainder of the network, and using synthetic Gabors for conv1. For Resnet, we look at the e ect of reusing pretrained weights up to Block2, and slimming the remainder of the network (halving the number of channels), randomly initializing those layers, and training end to end. This matches performance and convergence of full transfer learning. We also look at initializing conv1 with synthetic Gabor lters (so no use of pretrained weights), and the rest of the network randomly, which performs equivalently to reusing conv1 pretrained weights. This result generalizes to di erent architectures, e.g. CBR-LargeW on the right.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following approaches resulted in the fastest convergence to a Test AUC of 0.91 for the ResNet50 model?",
    "answer": "The Synthetic Gabor approach.",
    "rationale": "The figure shows that the Synthetic Gabor approach reached a Test AUC of 0.91 in 25425 steps, while the other approaches took longer. The RandInit approach took 69069 steps, the Slim approach took 8208 steps, and the Transfer approach took 8008 steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.07208v3",
    "pdf_url": null
  },
  {
    "instance_id": "0ccc5d27dc6f4560b712db1d396ffa24",
    "figure_id": "2210.11744v3-Figure3-1",
    "image_file": "2210.11744v3-Figure3-1.png",
    "caption": " F1 distribution on AfroLID Dev set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of languages in the AfroLID Dev set have an F1 score of 100 or higher?",
    "answer": "13.54%",
    "rationale": "The plot shows the distribution of F1 scores for the AfroLID Dev set. The y-axis shows the number of languages, and the x-axis shows the F1 score. The yellow circle highlights the languages with an F1 score of 100 or higher. The legend shows that there are 69 languages in this category, which is 13.54% of the total number of languages in the AfroLID Dev set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.11744v3",
    "pdf_url": null
  },
  {
    "instance_id": "4db1d68a641541058261208ad4374550",
    "figure_id": "2306.06410v1-Figure2-1",
    "image_file": "2306.06410v1-Figure2-1.png",
    "caption": " Illustration of OpenSR training system. In the second stage, parameters other than those in the transformer decoder are frozen to maintain the cross modality alignment state achieved in the first stage. We propose three different tuning and inference strategies in the third stage for scenarios with different scales of labeled visual utterances in the target domain.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What are the three different tuning and inference strategies proposed in the third stage of OpenSR training system?",
    "answer": " Zero-shot, Few-shot, and Full-shot.",
    "rationale": " The figure shows three different tuning and inference strategies in the third stage, which are labeled as Zero-shot, Few-shot, and Full-shot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.06410v1",
    "pdf_url": null
  },
  {
    "instance_id": "e9ad12b46bae4443add56d63f97a09d7",
    "figure_id": "1711.03483v1-Figure2-1",
    "image_file": "1711.03483v1-Figure2-1.png",
    "caption": " Overview of early fusion, middle fusion, and late fusion techniques. Round-corner rectangles denote word embeddings. Green is related to images and blue to text, orange round-corner rectangles are multimodal embeddings built from textual and visual resources. “sim” stands for an example of an evaluation task, namely word similarity.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which fusion technique combines the image and text embeddings at the word level?",
    "answer": "Early fusion.",
    "rationale": "In early fusion, the image and text embeddings are combined at the word level, as shown in the figure. This is evident by the fact that the \"word i\" and \"word j\" embeddings are directly connected to both the \"Images\" and \"Texts\" boxes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.03483v1",
    "pdf_url": null
  },
  {
    "instance_id": "92b8603153744caf8f095de45260478f",
    "figure_id": "2012.12482v1-Figure6-1",
    "image_file": "2012.12482v1-Figure6-1.png",
    "caption": " Sample results from different density crowd images. The columns represent the original image, ground truth and topological maps by TopoCount, and the estimated density map by the integration of Bayesian (Ma et al. 2019) + TopoCount.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which image is the estimated density map most accurate?",
    "answer": "The image with 61 people.",
    "rationale": "The image with 61 people has the highest F-score (0.92) and the lowest count error (0). This indicates that the estimated density map is most accurate for this image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.12482v1",
    "pdf_url": null
  },
  {
    "instance_id": "ba5f38631f904059b259be1bfb789107",
    "figure_id": "1906.00794v2-Figure3-1",
    "image_file": "1906.00794v2-Figure3-1.png",
    "caption": " Scatter plot of the subjective evaluation results: Naturalness (horizontal axis) and similarity to the target (vertical axis) for the considered models and references.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models is closest to the target in terms of both naturalness and similarity?",
    "answer": "Target",
    "rationale": "The Target is the point that is highest on the vertical axis (similarity to the target) and furthest to the right on the horizontal axis (naturalness).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.00794v2",
    "pdf_url": null
  },
  {
    "instance_id": "ceb73e89e97b4281a97566b4f7b54068",
    "figure_id": "1809.03359v2-Figure2-1",
    "image_file": "1809.03359v2-Figure2-1.png",
    "caption": " Performance profiles of model trained with different widths for relaxed DDs.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which model performs best when the optimality gap ratio is small?",
    "answer": " RL-UB-4 (w=2)",
    "rationale": " The figure shows the performance profiles of different models trained with different widths for relaxed DDs. The x-axis shows the optimality gap ratio, and the y-axis shows the percentage of instances. The model with the highest curve at a given optimality gap ratio is the best performing model. In this case, RL-UB-4 (w=2) has the highest curve at small optimality gap ratios, indicating that it is the best performing model when the optimality gap ratio is small.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.03359v2",
    "pdf_url": null
  },
  {
    "instance_id": "75ca3337ef4043e7bd8f0c6ee00d677b",
    "figure_id": "1906.04160v1-Figure4-1",
    "image_file": "1906.04160v1-Figure4-1.png",
    "caption": " Our trained models are person-specific. For every speaker audio input (row) we apply all other individually trained speaker models (columns). Color saturation corresponds to L1 loss values on a held out test set (lower is better). For each row, the entry on the diagonal is lightest as models work best using the input speech of the person they were trained on.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which speaker's voice is most accurately modeled by the model trained on Kagan's speech?",
    "answer": "Kagan",
    "rationale": "The diagonal of the figure shows the L1 loss values for each model when applied to the speech of the person it was trained on. The lightest color on the diagonal corresponds to the lowest L1 loss value, which indicates the most accurate model. In this case, the lightest color on the diagonal is for the model trained on Kagan's speech, indicating that it is the most accurate model for Kagan's voice.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04160v1",
    "pdf_url": null
  },
  {
    "instance_id": "796cb54046474996a8586bd800da5800",
    "figure_id": "1805.08948v2-Figure1-1",
    "image_file": "1805.08948v2-Figure1-1.png",
    "caption": " Performance of PSRL (no adaptivity), concurrent UCRL (no diversity), Thompson resampling (no commitment) and seed sampling in the tabular problem of learning how to swing and keep upright a pole attached to a cart that moves left and right on an infinite rail.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the performance of PSRL compare to that of Thompson resampling?",
    "answer": "PSRL performs better than Thompson resampling.",
    "rationale": "The plot shows that the reward of PSRL is higher than that of Thompson resampling for all numbers of parallel agents in the learning episode.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.08948v2",
    "pdf_url": null
  },
  {
    "instance_id": "f5af53baaf854931a1c6ac8a2327e3de",
    "figure_id": "2106.10199v5-Figure4-1",
    "image_file": "2106.10199v5-Figure4-1.png",
    "caption": " Change in bias components (MRPC task).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the largest change in bias for the MRPC task?",
    "answer": "Layer 1",
    "rationale": "The figure shows the change in bias components for each layer of the model. The x-axis shows the layer number, and the y-axis shows the bias component. The color of each square represents the magnitude of the change in bias, with darker colors indicating a larger change. The square for Layer 1 is the darkest, indicating that it has the largest change in bias.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.10199v5",
    "pdf_url": null
  },
  {
    "instance_id": "b04f546ead4a429d810c69b6509da3c4",
    "figure_id": "2209.14345v1-FigureA-5-1",
    "image_file": "2209.14345v1-FigureA-5-1.png",
    "caption": "Figure A-5: We compare the effect of pre-training with a different length of the input audio used during pre-training, considering cropping the input spectrograms to 32, 160, 320, and 480 frames. Results are shown evaluated on the individual HEAR-L tasks (ESC-50 (green), Speech Commands 5h (grey), GTZAN Genre (light blue), FSD50K (dark blue), CREMA-D (purple), as well as the HEAR-L average (red). Since the scores on the individual HEAR-L tasks have different scales, we show the MinMax scaled scores, where the best performing input length for each task is set to 1 and the worst performing to 0.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which input length resulted in the best performance on the HEAR-L average task?",
    "answer": "480 frames.",
    "rationale": "The red line in the figure represents the HEAR-L average task, and it can be seen that the red line is highest at 480 frames.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.14345v1",
    "pdf_url": null
  },
  {
    "instance_id": "cdc7af0dda8d42f4aa9639e596995c1c",
    "figure_id": "2010.15466v1-Figure2-1",
    "image_file": "2010.15466v1-Figure2-1.png",
    "caption": " The extracted syntactic information in POS labels (a), syntactic constituents (b), and dependency relations (c) for “Salt” in the example sentence, where associated contextual features and the corresponding instances of syntactic information are highlighted in blue.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following best describes the syntactic role of the word \"Salt\" in the sentence \"The capital of Utah is Salt Lake City\"?",
    "answer": "Compound noun.",
    "rationale": "The figure shows that the word \"Salt\" is part of a compound noun \"Salt Lake City\". This is evident in part (c) of the figure, which shows the dependency relations between the words in the sentence. The word \"Salt\" is labeled as \"comp.\", which indicates that it is a complement of the noun \"City\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.15466v1",
    "pdf_url": null
  },
  {
    "instance_id": "5fd5ae896b32402786a31f9783d27fa7",
    "figure_id": "2307.02064v2-Figure19-1",
    "image_file": "2307.02064v2-Figure19-1.png",
    "caption": " Imagination in the Multi Doors Keys environment with seven keys.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which agent successfully collects all seven keys and reaches the goal state?",
    "answer": "S4WM",
    "rationale": "The S4WM agent is the only one that collects all seven keys and reaches the goal state, as indicated by the yellow square in the bottom right corner of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.02064v2",
    "pdf_url": null
  },
  {
    "instance_id": "15797474cd3f468bb090919f14f23718",
    "figure_id": "2010.01810v2-Figure6-1",
    "image_file": "2010.01810v2-Figure6-1.png",
    "caption": " Qualitative results for conventional and proposed models on SUN dataset [40]: CA[42], StructureFlow[26], NS-OUT [41], and our model.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model produces the most realistic results?",
    "answer": "Our model.",
    "rationale": "The figure shows that our model produces images that are most similar to the input images. The other models produce images that are either blurry or have artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.01810v2",
    "pdf_url": null
  },
  {
    "instance_id": "5e60a48bb1ee47e6900e2c7a1a5e1289",
    "figure_id": "2203.09615v2-Figure10-1",
    "image_file": "2203.09615v2-Figure10-1.png",
    "caption": " Performance of each program under 25% and 50% local memory when the three native programs, Snappy (S), Memcached (M), and XGBoost (X), co-run with a managed application. Canvas ran with all optimizations enabled.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which program has the best performance when co-run with XGBoost on FastSwap with 25% local memory?",
    "answer": "Snappy (S)",
    "rationale": "The figure shows the elapsed time for each program when co-run with XGBoost on FastSwap with 25% local memory. Snappy (S) has the lowest elapsed time, indicating the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.09615v2",
    "pdf_url": null
  },
  {
    "instance_id": "5e065628c0974eb5bd1909b4fa3d83a0",
    "figure_id": "2104.04128v1-Figure6-1",
    "image_file": "2104.04128v1-Figure6-1.png",
    "caption": " Proportion of common examples in top 50 samples between pairs of attribution methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two attribution methods have the most similar top 50 samples in SST?",
    "answer": "RIF (Linear) and GD (Linear).",
    "rationale": "The figure shows the proportion of common examples in the top 50 samples between pairs of attribution methods. The value in the cell at the intersection of the RIF (Linear) and GD (Linear) rows and columns is 1.0, indicating that the two methods have the most similar top 50 samples in SST.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.04128v1",
    "pdf_url": null
  },
  {
    "instance_id": "5fd09fc27560473caafd087a3fe54c13",
    "figure_id": "2010.01185v6-Figure6-1",
    "image_file": "2010.01185v6-Figure6-1.png",
    "caption": " Histograms of the relative entropies of the auxiliary variables in a 24 layer RVAE.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the highest relative entropy for the auxiliary variables?",
    "answer": "Layer 23",
    "rationale": "The height of the bars in the histograms indicates the relative entropy of the auxiliary variables. Layer 23 has the highest bars, indicating that it has the highest relative entropy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.01185v6",
    "pdf_url": null
  },
  {
    "instance_id": "939a8f0edc00429b805e6b26305a4ef8",
    "figure_id": "2203.08075v2-Figure5-1",
    "image_file": "2203.08075v2-Figure5-1.png",
    "caption": " Predictions from RoBERTa and VinVL in the subtask of objects’ sizes. c is the current object and A is the set of all other comparable objects. #(c > a)/|A| indicates the ratio of predicting the current object larger than others. As c > a and a > c should not appear simultaneously, the sum of the two solid bars is expected to be 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "For which object does VinVL predict the highest ratio of being larger than other objects?",
    "answer": "Truck",
    "rationale": "The orange bar for \"truck\" in the VinVL plot is the highest among all objects.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.08075v2",
    "pdf_url": null
  },
  {
    "instance_id": "43519302cb85410dbc217d9a14aec5f6",
    "figure_id": "2304.05170v2-Figure2-1",
    "image_file": "2304.05170v2-Figure2-1.png",
    "caption": " IoU on adjacent frames. (a) Compared to MOT17 and DanceTrack, SportsMOT has a lower score, indicating that objects have faster motion. (b) In SportsMOT, the category of football has the lowest IoU score, which means that football players often have fast motion.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the lowest average IoU score?",
    "answer": "Football",
    "rationale": "The figure shows that the average IoU score for Football is the lowest among all the datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.05170v2",
    "pdf_url": null
  },
  {
    "instance_id": "04406bc36316499681e727cd2b8e31e3",
    "figure_id": "2303.17569v2-Figure27-1",
    "image_file": "2303.17569v2-Figure27-1.png",
    "caption": " Complete comparisons with all methods on the Backlit300 test dataset. Our results do not contain artifacts and over-exposed regions. Moreover, Our result has the most natural color in the enlightened area and has the most pleasing contrast.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most natural-looking image?",
    "answer": "CLIP-LIT",
    "rationale": "The figure shows the results of different methods for low-light image enhancement. The CLIP-LIT result is the most natural-looking because it has the most pleasing contrast and does not contain artifacts or over-exposed regions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.17569v2",
    "pdf_url": null
  },
  {
    "instance_id": "effa21a19c2b47feb333e98749652fc7",
    "figure_id": "2107.06393v2-Figure4-1",
    "image_file": "2107.06393v2-Figure4-1.png",
    "caption": " Hybrid memoised wake-sleep (HMWS) learns faster than the baselines: reweighted wake-sleep (RWS) and VIMCO based on the marginal likelihood, in both the time series model (left), and the scene understanding models with learning shape and color (middle) and learning shape only (right). HMWS also learns better scene understanding models. The gradient estimator of VIMCO was too noisy and failed to learn the time series model. (Median with the shaded interquartile ranges over 20 runs is shown.)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of marginal likelihood?",
    "answer": "HMWS",
    "rationale": "The figure shows the marginal likelihood of three algorithms over time. HMWS consistently has the highest marginal likelihood, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.06393v2",
    "pdf_url": null
  },
  {
    "instance_id": "e39b8f54dd5a48ccba3933e99376f722",
    "figure_id": "2110.14038v4-Figure7-1",
    "image_file": "2110.14038v4-Figure7-1.png",
    "caption": " PR-BCD (DICE dashed) on the large datasets (transfer) where the adversarial accuracy denotes the accuracy after attacking with budget ∆ = ǫm.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the arXiv dataset when attacked with a budget of 0.1?",
    "answer": "Soft Medoid GDC",
    "rationale": "The plot in (b) shows the adversarial accuracy of different models on the arXiv dataset. The Soft Medoid GDC model has the highest adversarial accuracy at a budget of 0.1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14038v4",
    "pdf_url": null
  },
  {
    "instance_id": "fe71fdbeeed84195a49a5e4bde57a133",
    "figure_id": "2106.07992v2-Figure3-1",
    "image_file": "2106.07992v2-Figure3-1.png",
    "caption": " (a): the observed measurements and generated measurements by NSIBF-RECON, NSIBF-PRED and NSIBF in one anomalous period; (b): the trace of anomaly scores generated by NSIBF and residual errors generated by NSIBF-RECON and NSIBF-PRED; (c): the ROC curve for NSIBF-RECON, NSIBF-PRED and NSIBF for anomaly detection of the simulated CPS example in the qualitative experiment.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods, NSIBF-RECON, NSIBF-PRED, or NSIBF, has the best performance in terms of anomaly detection?",
    "answer": "NSIBF-RECON.",
    "rationale": "The ROC curve in (c) shows that NSIBF-RECON has the highest area under the curve (AUC), which indicates better performance in terms of anomaly detection.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07992v2",
    "pdf_url": null
  },
  {
    "instance_id": "3a8aaeec9dc04c4d98b0c9d666b63805",
    "figure_id": "2110.15253v1-Figure4-1",
    "image_file": "2110.15253v1-Figure4-1.png",
    "caption": " Summary of features for AO trained on English to French translation. (a) Sample attention matrix. (b) The encoder (orange) and decoder (purple) temporal components, with a square and star marking the first and last time step, respectively. Once again, quantities are projected onto the temporal component PCs. The inset shows the attention matrix from the temporal components, i.e. the softmax of µD s · µE t . (c) The dot product between the most common output word readouts and the most common input word input components, χE x.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the figure shows the attention matrix for the encoder and decoder?",
    "answer": "Panel (b), inset.",
    "rationale": "Panel (b) shows the encoder and decoder temporal components, with a square and star marking the first and last time step, respectively. The inset shows the attention matrix from the temporal components, which is the softmax of µD s · µE t.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.15253v1",
    "pdf_url": null
  },
  {
    "instance_id": "2b6cbc38e3c346ffbe9f589052cb7455",
    "figure_id": "2204.09560v2-Figure2-1",
    "image_file": "2204.09560v2-Figure2-1.png",
    "caption": " Networks see reduced ability to fit new targets over the course of training in two demonstrative Atari environments.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three algorithms is the most stable?",
    "answer": "Rainbow",
    "rationale": "The Rainbow algorithm has the least amount of variance in its performance across the two environments.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.09560v2",
    "pdf_url": null
  },
  {
    "instance_id": "e6698ca292984168b78f801d972f64e5",
    "figure_id": "2210.06718v3-Figure6-1",
    "image_file": "2210.06718v3-Figure6-1.png",
    "caption": " Screenshots of the training processes of our approach Hy-Q (top row) and RND (bottom row). With only 0.1m offline samples of which half is from a random policy and half is from a high quality policy (with reward around 6400), our approach learns significantly faster than RND.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is able to achieve a higher reward with fewer online samples?",
    "answer": "Hy-Q",
    "rationale": "The screenshots show that Hy-Q is able to reach a reward of 6600 after 10 million online samples, while RND only reaches a reward of 400 after 11 million online samples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06718v3",
    "pdf_url": null
  },
  {
    "instance_id": "b77e16c95efa48f18a58f410f7bde235",
    "figure_id": "2211.12713v1-Figure1-1",
    "image_file": "2211.12713v1-Figure1-1.png",
    "caption": " Comparison of AutoAE and the recently proposed attacks on the CIFAR-10 adversarial training model. AutoAE achieves the best robustness evaluation with only a small number of gradient evaluations.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attack method is the most efficient in terms of the number of gradient evaluations required to achieve a certain level of robust accuracy?",
    "answer": "AutoAE.",
    "rationale": "The figure shows that AutoAE achieves the best robust accuracy with only a small number of gradient evaluations compared to the other attack methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.12713v1",
    "pdf_url": null
  },
  {
    "instance_id": "ebaa4d9788c34298ae0a72dbc13037a9",
    "figure_id": "2010.09875v2-Figure12-1",
    "image_file": "2010.09875v2-Figure12-1.png",
    "caption": " WideResNet 28-10 on CIFAR-10 and CIFAR-10-C, averaged over 3 random seeds. SKCE: Squared kernel calibration error computed in Widmann et al. (2019). DCE: Debiased calibration error in Kumar et al. (2019). Red: Ensembles without Mixup; Blue: Ensembles with Mixup; Green: Ensembles with CAMixup (ours). Both SKCE and DCE give consistent rankings on calibration error to the ranking in Fig. 5 and Fig. 6. This plot shows that our proposed CAMixup is effective in reducing Mixup calibration error when combined with ensembles.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods shown in the plot has the lowest calibration error?",
    "answer": "CAMixup (green)",
    "rationale": "The plot shows the calibration error for different methods, and CAMixup has the lowest error in all cases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.09875v2",
    "pdf_url": null
  },
  {
    "instance_id": "381cce1d7e294703b171866bf34a636f",
    "figure_id": "2207.13080v3-Figure6-1",
    "image_file": "2207.13080v3-Figure6-1.png",
    "caption": " Illustrating the AP scores of Deformable-DETR on train and val under longer training epochs. We can see that, with longer training epochs, e.g., from 50 epochs to 75 epochs, the AP scores on train set consistently improves while the AP scores on val set saturates on COCO object detection benchmark.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the AP scores on the validation set as the number of epochs increases?",
    "answer": "The AP scores on the validation set saturate.",
    "rationale": "The figure shows that the AP scores on the validation set increase from 43.7 to 46.8 as the number of epochs increases from 12 to 50. However, the AP scores on the validation set remain relatively constant at around 46.8 when the number of epochs is increased from 50 to 75. This suggests that the AP scores on the validation set have saturated and are no longer improving with additional training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.13080v3",
    "pdf_url": null
  },
  {
    "instance_id": "9db32eae3bb54605bb412c159cd685a9",
    "figure_id": "1906.00925v2-Figure12-1",
    "image_file": "1906.00925v2-Figure12-1.png",
    "caption": " The texture maps of terrains, Skull, GeologicalSample, and DinoRing for different resolutions.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the textures shown has the highest resolution?",
    "answer": "Ground Truth",
    "rationale": "The figure shows the texture maps of terrains, Skull, GeologicalSample, and DinoRing for different resolutions. The Ground Truth column shows the original textures, which have the highest resolution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.00925v2",
    "pdf_url": null
  },
  {
    "instance_id": "226bf2a414654fefbfd51afe4a0e5a66",
    "figure_id": "2303.11052v3-Figure5-1",
    "image_file": "2303.11052v3-Figure5-1.png",
    "caption": " Qualitative comparison on DTU dataset [19] and LLFF dataset [28]. Our method can render images with fewer artifacts.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces images with the fewest artifacts?",
    "answer": "Ours",
    "rationale": "The images in column (c) are labeled \"Ours\" and appear to have the fewest artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.11052v3",
    "pdf_url": null
  },
  {
    "instance_id": "65b2fc8650b748fa9658865b2fae2a2b",
    "figure_id": "2002.06189v2-Figure17-1",
    "image_file": "2002.06189v2-Figure17-1.png",
    "caption": " Dependence of λ(x) on ε (η = 0.1)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between λ(x) and ε?",
    "answer": "λ(x) decreases with increasing ε.",
    "rationale": "The plot in (a) shows that λ(x) decreases as ε increases. This means that the Lyapunov exponent becomes smaller as the perturbation parameter ε increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.06189v2",
    "pdf_url": null
  },
  {
    "instance_id": "42a5f9e7de994464a41724ae38297a35",
    "figure_id": "2006.01921v1-Figure3-1",
    "image_file": "2006.01921v1-Figure3-1.png",
    "caption": " Count (y-axis) of dissatisfied and satisfied feedback among different rating groups (x-axis). The red line indicates the best cut (rating=3.5) between SAT/DSAT labels.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which rating group was the count of dissatisfied feedback closest to the count of satisfied feedback?",
    "answer": "Rating group 3.",
    "rationale": "The bars representing dissatisfied and satisfied feedback are closest in height in the rating group 3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.01921v1",
    "pdf_url": null
  },
  {
    "instance_id": "1f8aec612dec4c569f516fee1f04e527",
    "figure_id": "1904.09288v1-Figure5-1",
    "image_file": "1904.09288v1-Figure5-1.png",
    "caption": " Comparison of frame-mAP (%) of our models trained with and without temporal extension.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better, the one with temporal extension or the one without?",
    "answer": "The model with temporal extension performs better.",
    "rationale": "The figure shows that the frame-mAP (%) of the model with temporal extension is higher than the frame-mAP (%) of the model without temporal extension for all three steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.09288v1",
    "pdf_url": null
  },
  {
    "instance_id": "27cc1140deba4cb69f7c57154a04115e",
    "figure_id": "2210.00660v3-Figure2-1",
    "image_file": "2210.00660v3-Figure2-1.png",
    "caption": " Non-termination ratios, rnt(L)’s, as a function of L in log-log scale for (a) RNN and (b) LSTM trained on WikiText-2 when using greedy search. We report mean (curve) ± st.dev. (shaded area) across 10 random experiments. For all configurations, both ST+ (non-red dashed) proposed by Welleck et al. (2020) and our NMST+ (non-red solid) are consistent with respect to greedy search since rnt(L) goes to 0 as L increases. However, softmax parametrization (VA+, red dotted) is inconsistent with respect to greedy search since its rnt(L) does not converge to 0 as L→∞.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model exhibits the highest non-termination ratio for small values of L?",
    "answer": "VA+",
    "rationale": "The figure shows that the red dotted line, which represents VA+, is consistently higher than the other lines for small values of L.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.00660v3",
    "pdf_url": null
  },
  {
    "instance_id": "373be6e4e25042ef973191c2f3193fbb",
    "figure_id": "2102.11535v4-Figure2-1",
    "image_file": "2102.11535v4-Figure2-1.png",
    "caption": " Example of linear regions divided by a ReLU network1",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many input dimensions does the ReLU network have?",
    "answer": "2",
    "rationale": "The x- and y-axes are labeled \"Input dim 1\" and \"Input dim 2\", respectively. This indicates that the network takes two input dimensions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.11535v4",
    "pdf_url": null
  },
  {
    "instance_id": "71ea30f11b7b46938980a645c7dfcf86",
    "figure_id": "2009.01974v4-Figure12-1",
    "image_file": "2009.01974v4-Figure12-1.png",
    "caption": " Step-non-i.i.d CIFAR-10 experiments accuracy curves of SCAFFOLD on ResNet20.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves higher accuracy on the CIFAR-10 dataset with a ResNet20 architecture?",
    "answer": "FedBE + SCAFFOLD",
    "rationale": "The figure shows the test accuracy curves for two algorithms, SCAFFOLD and FedBE + SCAFFOLD, on the CIFAR-10 dataset with a ResNet20 architecture. The orange curve, which represents FedBE + SCAFFOLD, is consistently higher than the blue curve, which represents SCAFFOLD. This indicates that FedBE + SCAFFOLD achieves higher accuracy than SCAFFOLD on this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.01974v4",
    "pdf_url": null
  },
  {
    "instance_id": "fbed59f63e5d4295b657c11dfa53cdee",
    "figure_id": "2005.00912v1-Figure3-1",
    "image_file": "2005.00912v1-Figure3-1.png",
    "caption": " Citation box plots for papers: published 2010–2016 (top) and published in 2014 (bottom).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of publication received the highest number of citations in 2014?",
    "answer": "Journal",
    "rationale": "The box plot for journals in the bottom panel of the figure has the highest median value, which is indicated by the horizontal line inside the box.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.00912v1",
    "pdf_url": null
  },
  {
    "instance_id": "e409275134bb4551b68ee6809bdb2715",
    "figure_id": "2212.01197v4-Figure7-1",
    "image_file": "2212.01197v4-Figure7-1.png",
    "caption": " The data distribution of each client on MNIST, Cifar10, and Cifar100 in pathological heterogeneous setting. The size of the circle represents the number of samples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which client has the most data for MNIST?",
    "answer": "Client 18 has the most data for MNIST.",
    "rationale": "The size of the circle represents the number of samples, and the largest circle in the MNIST plot is for client 18.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.01197v4",
    "pdf_url": null
  },
  {
    "instance_id": "8e285b0b4dfc41f9b8f76ff73894ddca",
    "figure_id": "2111.11704v2-Figure5-1",
    "image_file": "2111.11704v2-Figure5-1.png",
    "caption": " Point cloud reconstruction results using two baselines. Note that point cloud has been colorized for a visualization purpose.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate point cloud reconstruction?",
    "answer": "Ours.",
    "rationale": "The figure shows the point cloud reconstruction results for three different methods: PU, PC, and Ours. The results for Ours are visually the closest to the ground truth, indicating that it is the most accurate method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.11704v2",
    "pdf_url": null
  },
  {
    "instance_id": "0fcaad84923f44b6a60f3e899b3aebd9",
    "figure_id": "2106.06406v2-Figure6-1",
    "image_file": "2106.06406v2-Figure6-1.png",
    "caption": " Scatter plots of waveform audio signals from the test set under different choices of the conditional information for PriorGrad vocoder. Left: V/UV label-based prior. Middle: Phoneme label-based prior. Right: Energy-based prior.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three priors leads to the most diverse set of magnitudes for the waveform audio signals?",
    "answer": "The energy-based prior.",
    "rationale": "The scatter plot for the energy-based prior (right) shows a wider range of magnitudes than the scatter plots for the other two priors. This suggests that the energy-based prior is able to generate a more diverse set of audio signals.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.06406v2",
    "pdf_url": null
  },
  {
    "instance_id": "66498e7d0d444e3da6bec3b93b5e5cd4",
    "figure_id": "2102.04051v1-Figure6-1",
    "image_file": "2102.04051v1-Figure6-1.png",
    "caption": " Posterior probability of data generated from initialized (“Init”) or trained (“Trained”) generators. The boxes indicate first, second (i.e., median), and third quantiles. The line plot indicates mean value.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of class has higher naturalness probability?",
    "answer": "Open class.",
    "rationale": "The box plots in the left panel of the figure show that the naturalness probability of open classes is higher than that of closed classes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.04051v1",
    "pdf_url": null
  },
  {
    "instance_id": "5e31d311af6f4bbb8bd8ba65b4d6259b",
    "figure_id": "2007.14634v2-Figure9-1",
    "image_file": "2007.14634v2-Figure9-1.png",
    "caption": " VI using a fully-factorized Gaussian. The first two columns show results for two different step-sizes, and the third one using the best step-size chosen retrospectively. (Higher ELBO is better.)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm and configuration consistently achieves the highest ELBO across all model types and step sizes?",
    "answer": "Alg. 1 with 50 samples (black solid line)",
    "rationale": "The black solid line, representing Alg. 1 with 50 samples, consistently reaches the highest ELBO values across all model types (Hier Reg, Log Reg, BNN) and step sizes (0.005, 0.01, and best n). This indicates that this configuration generally performs better in terms of maximizing the evidence lower bound.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.14634v2",
    "pdf_url": null
  },
  {
    "instance_id": "a271b17c373a43ee9a8aa45d2fe3850b",
    "figure_id": "2210.13001v1-Figure9-1",
    "image_file": "2210.13001v1-Figure9-1.png",
    "caption": " Distribution of the final matching score in SPICED, which includes some pairs of scientific findings that are automatically labeled based on their extreme textual similarity (high or low), in addition to the annotated pairs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common Information Matching Score (IMS) in SPICED?",
    "answer": "The most common IMS is between 1 and 2.",
    "rationale": "The figure shows the distribution of IMS in SPICED. The x-axis represents the IMS, and the y-axis represents the density of the data. The peak of the curve is between 1 and 2, indicating that the most common IMS is in this range.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.13001v1",
    "pdf_url": null
  },
  {
    "instance_id": "3054529082224f368c69cbc673610fcc",
    "figure_id": "2102.11503v3-Figure1-1",
    "image_file": "2102.11503v3-Figure1-1.png",
    "caption": " We show the BaseGen and NovelGen performance tradeoff (for best validation snapshots): over the choice of a set of four meta-learning and two supervised pre-training methods on mini-M (a) and tiered-M (b); over the number of ways to train PN on mini-M and different learning rates to train FOMAML on FC-M (c); over the use of FIX-ML (S,Q) generation strategy or not (ML) with SVM, RR and PN on cifar-M in (d).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pre-training method led to the highest BaseGen performance on the mini-M dataset?",
    "answer": "PN",
    "rationale": "The BaseGen performance of each pre-training method is shown in the left column of each plot. In (a), the BaseGen performance of PN is the highest, reaching approximately 94.5%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.11503v3",
    "pdf_url": null
  },
  {
    "instance_id": "27fb5ed925e14b1e854a820809509d69",
    "figure_id": "2207.10909v2-Figure3-1",
    "image_file": "2207.10909v2-Figure3-1.png",
    "caption": " Illustration of the effects on Dynamic Ball Query. All experiments are evaluated on KITTI val set. λ is the scale parameter of resource budget loss in Eq. 10. Latency here is evaluated by a single RTX2080Ti GPU with a batch size of 16. (a) reports the comparison on both accuracies of Car class and overall latency distribution. (b) indicates the latency reduction of query & grouping operation and MLP network in different SA layers. (c) reflects the activation distribution of point features in different SA layers. (d) shows the proportion of point features go through different groups of MSG. \"Small\" and \"Large\" means activating on group branches with small and large radii respectively. \"Kill\" represents blocking all groups, while \"Small & Large\" means going through all scales of groups.",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer of set abstraction results in the lowest latency for the MLP network?",
    "answer": "Layer 4.",
    "rationale": "Figure (b) shows the latency of the MLP network decreases as the layer of set abstraction increases, with the lowest latency achieved at layer 4.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.10909v2",
    "pdf_url": null
  },
  {
    "instance_id": "4039224f842e4015bb8ecc02e93b9836",
    "figure_id": "1809.00699v1-Figure2-1",
    "image_file": "1809.00699v1-Figure2-1.png",
    "caption": " Comparison results of a variety of methods in terms of precision/recall curves.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest precision for a recall of 0.15?",
    "answer": "CNN+ATT",
    "rationale": "The precision-recall curve for CNN+ATT is the highest at a recall of 0.15.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.00699v1",
    "pdf_url": null
  },
  {
    "instance_id": "6867f906be30405eb03883789c68a21c",
    "figure_id": "2006.04176v2-Figure5-1",
    "image_file": "2006.04176v2-Figure5-1.png",
    "caption": " Comparison of the performance of our agent (DAIMC) with DQN, A2C and PPO2. The bold line represents the mean, and shaded areas the standard deviation over multiple training runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the agents performed the best?",
    "answer": "DAIMC",
    "rationale": "The plot shows that DAIMC achieved the highest reward per round.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04176v2",
    "pdf_url": null
  },
  {
    "instance_id": "15a32145871745b9bd9f86cf06ef0076",
    "figure_id": "2305.16498v2-Figure6-1",
    "image_file": "2305.16498v2-Figure6-1.png",
    "caption": "Figure 6 | Average success rate over 50 evaluations for online imitation learning for robomimic tasks. Uncertainty intervals depict quartiles over 10 seeds. To assess convergence across seeds, performance is chosen using the highest 25th percentile of the averaged success during learning.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the PickPlaceCan task?",
    "answer": "csii performs the best on the PickPlaceCan task.",
    "rationale": "The figure shows the average success rate for different imitation learning methods on three tasks. The height of the bars represents the success rate, and the error bars represent the quartiles over 10 seeds. For the PickPlaceCan task, the csii method has the highest success rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16498v2",
    "pdf_url": null
  },
  {
    "instance_id": "c0d7b2bc9aba4ca6b7d3f02ef6a1d2d7",
    "figure_id": "1903.08336v2-Figure18-1",
    "image_file": "1903.08336v2-Figure18-1.png",
    "caption": " Robot Perspective while Learning Ĵ+ s for Hbase. Starting with Ĵ+ s t=0 and offset target location (the yellow chain segmentation), our Hadamard-Broyden update learns the correct visual servoing parameters to center the robot on the target in real-time. The target is centered vertically after five updates (t = 5) and horizontally after fourteen (t = 14). We show the complete visual servo trajectory of the target object through image space on the bottom right. This figure corresponds with the experiment shown in Figures 8-9.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many updates did it take for the robot to center itself on the target?",
    "answer": "14 updates",
    "rationale": "The caption states that the target was centered horizontally after 14 updates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.08336v2",
    "pdf_url": null
  },
  {
    "instance_id": "a47d2e8e36bf4a73b09efe9f316a52f0",
    "figure_id": "2202.11847v1-Figure3-1",
    "image_file": "2202.11847v1-Figure3-1.png",
    "caption": " The executable commands frequency. The search command has the highest frequency since each dialogue begins with a search request (BR: background removal).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the second most frequent command used in the dialogues?",
    "answer": "The second most frequent command is \"brightness\".",
    "rationale": "The pie chart shows the frequency of different commands used in the dialogues. The \"brightness\" command has a frequency of 12.8%, which is the second highest after the \"search\" command.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.11847v1",
    "pdf_url": null
  },
  {
    "instance_id": "6a4f019f9b2744e7be1dbbee0ea127b5",
    "figure_id": "2103.08490v2-Figure5-1",
    "image_file": "2103.08490v2-Figure5-1.png",
    "caption": " mBERT improvements over the NER baseline for examples with different entropy. Consistency loss helps examples with higher entropy more.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best for examples with high entropy?",
    "answer": "MVR",
    "rationale": "The figure shows that the MVR model has the highest F1 gain for buckets with high entropy (buckets 7-10).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.08490v2",
    "pdf_url": null
  },
  {
    "instance_id": "49201afd741944169b09c31240be7dc7",
    "figure_id": "1811.04154v1-Figure5-1",
    "image_file": "1811.04154v1-Figure5-1.png",
    "caption": " Entity linking accuracy on joint training with different amounts of LRL data (x-axis is not to scale).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which entity linking method performs the best with the least amount of LRL training data?",
    "answer": "jv-id",
    "rationale": "The plot shows that jv-id has the highest accuracy for all amounts of LRL training data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.04154v1",
    "pdf_url": null
  },
  {
    "instance_id": "043bd2b092ca4556835de651cb1e1001",
    "figure_id": "1911.00077v1-Figure3-1",
    "image_file": "1911.00077v1-Figure3-1.png",
    "caption": " 2-dimensional image representations obtained by applying t-SNE on Inception features for images generated by (a) HDGAN, and (b) StackGAN. Black points correspond to real images.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which of the two models, HDGAN or StackGAN, generates images that are more similar to real images?",
    "answer": "StackGAN",
    "rationale": "The figure shows that the generated images from StackGAN are more clustered with the real images in the t-SNE space, which indicates that they are more similar to real images in terms of their features.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.00077v1",
    "pdf_url": null
  },
  {
    "instance_id": "74ba7e7e362541878a4b4fdba6268759",
    "figure_id": "2011.15050v1-Figure2-1",
    "image_file": "2011.15050v1-Figure2-1.png",
    "caption": " Timely review submission. Bold labels indicate dates at which deadlines were set with the original deadline on day X and two extensions. 90% confidence intervals are computed using the method of Wilson (1927). Experimental reviewers have higher engagement and completion rates than other reviewers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which group of reviewers had the highest engagement and completion rates?",
    "answer": "The experimental reviewers.",
    "rationale": "The figure shows that the experimental reviewers (in blue) consistently have the highest bars, indicating the highest rates of both engagement and completion.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.15050v1",
    "pdf_url": null
  },
  {
    "instance_id": "b0db9479c43242099177aef4350f076a",
    "figure_id": "2202.02912v1-Figure3-1",
    "image_file": "2202.02912v1-Figure3-1.png",
    "caption": " Importance of Dialogue Content & Act Features.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model(s) show the least difference in gated attention weight between Cont. (MTL) and Act(MTL)?",
    "answer": "MWWOZ and ReDial.",
    "rationale": "The boxplots for Cont. (MTL) and Act(MTL) in MWWOZ and ReDial overlap more than the other models, indicating that the difference in gated attention weight is smaller for these models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.02912v1",
    "pdf_url": null
  },
  {
    "instance_id": "81bf85ac430847db928df1ba60cce12f",
    "figure_id": "2007.16189v3-Figure5-1",
    "image_file": "2007.16189v3-Figure5-1.png",
    "caption": " The effects of (a) frame rate, (b) segment length, and (c) data augmentation on classification accuracy in the downstream linear classification task with the labeled S dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which factor has the greatest impact on classification accuracy?",
    "answer": "Data augmentation.",
    "rationale": "The figure shows that data augmentation results in the highest top-1 accuracy, which is approximately 75%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.16189v3",
    "pdf_url": null
  },
  {
    "instance_id": "1a51300f6bc94c2a85a8876651cf8352",
    "figure_id": "2002.07017v2-Figure2-1",
    "image_file": "2002.07017v2-Figure2-1.png",
    "caption": " Information Plane determined by I(x; z) (x-axis) and I(y; z) (y-axis). Different objectives are compared based on their target.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which objective function achieves both sufficiency and high mutual information between the input and the latent representation?",
    "answer": "Supervised IB.",
    "rationale": "The figure shows that Supervised IB is the only objective function that falls within the feasible region and also has high mutual information between the input and the latent representation. This is because Supervised IB explicitly maximizes the mutual information between the input and the latent representation, while also ensuring that the latent representation is sufficient for the task at hand.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.07017v2",
    "pdf_url": null
  },
  {
    "instance_id": "814f3c0426654ab4944f9a97ea2ccbbc",
    "figure_id": "2003.07018v4-Figure5-1",
    "image_file": "2003.07018v4-Figure5-1.png",
    "caption": " Visual comparison of different methods for (a) 4× and (b) 8× image super-resolution.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most realistic results for 4x super-resolution?",
    "answer": "DRN-L (Ours)",
    "rationale": "The figure shows the results of different methods for 4x super-resolution. DRN-L (Ours) produces the most realistic results, as it is able to recover the fine details of the zebra's stripes and the texture of the grass.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.07018v4",
    "pdf_url": null
  },
  {
    "instance_id": "88969baab2714a0d8326b630cd0794a9",
    "figure_id": "2212.09097v2-Figure2-1",
    "image_file": "2212.09097v2-Figure2-1.png",
    "caption": " An example that illustrates how to find where a teacher model can help a student model. Given a sentence pair of the transfer set, both the teacher and student models try to predict a target word given the source sentence and the partial translation. How well a model predicts can be quantified as a real-valued number. The target words on which the teacher performs better than the student are highlighted in red. Other words are highlighted in blue.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the table, which word in the target sentence is the teacher model most likely to predict correctly while the student model is most likely to predict incorrectly?",
    "answer": "\"make\"",
    "rationale": "The teacher model has a prediction score of 0.9 for the word \"make,\" while the student model has a score of only 0.3. This is the largest difference in scores between the two models for any word in the sentence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.09097v2",
    "pdf_url": null
  },
  {
    "instance_id": "df11739ffc094ef89a16d4288280dffb",
    "figure_id": "2010.02523v1-Figure4-1",
    "image_file": "2010.02523v1-Figure4-1.png",
    "caption": " Performance of data sampling strategies on the En→X system.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which data sampling strategy performed better on the En→X system, fixed T=5 or dynamic T(k)?",
    "answer": "Dynamic T(k) performed better.",
    "rationale": "The figure shows that the performance of dynamic T(k) is higher than that of fixed T=5 for all the systems except for De and Fi.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02523v1",
    "pdf_url": null
  },
  {
    "instance_id": "da54eacfb3264fffbef52e5d57491b05",
    "figure_id": "2101.04442v1-Figure1-1",
    "image_file": "2101.04442v1-Figure1-1.png",
    "caption": " Imperfect ground truth examples (electronic zoomin recommended): (a) A ground truth image from CBSD dataset (Arbeláez et al. 2011) suffering from zipper effect, an artificial jagged pattern around edges; (b) Color moire in an image from ImageNet dataset (Russakovsky et al. 2015). Such artifact appears as false coloring due to interpolation error; (c) Noticeable residual noise in the collected “clean” image from Renoir dataset (Anaya and Barbu 2018).",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the following image artifacts is caused by interpolation error?",
    "answer": " Color moire.",
    "rationale": " The caption states that color moire is caused by interpolation error. This artifact is visible in the second image (b) as false coloring. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.04442v1",
    "pdf_url": null
  },
  {
    "instance_id": "c35dc6d55fd64c0b8c8db687f3d4ea40",
    "figure_id": "1905.11545v4-Figure4-1",
    "image_file": "1905.11545v4-Figure4-1.png",
    "caption": " Regression with data from various Bregman divergences using PBDL and linear metric learning.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which divergence measure appears to have the smallest distance between the Bregman regression and Mahalanobis regression?",
    "answer": "The Mahalanobis distance.",
    "rationale": "The Mahalanobis distance plot shows the smallest difference between the Bregman regression and Mahalanobis regression lines. The other plots show a larger difference between the two lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11545v4",
    "pdf_url": null
  },
  {
    "instance_id": "18ea5e8a2b5d48ff9787771656313074",
    "figure_id": "1811.08048v1-Figure2-1",
    "image_file": "1811.08048v1-Figure2-1.png",
    "caption": " A simple qualitative theory about friction, shown graphically (left) and formally (right). For example, q-(smoothness,friction) indicates that if smoothness increases, friction decreases.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does smoothness affect friction?",
    "answer": "Smoothness decreases friction.",
    "rationale": "The arrow from \"smoothness\" to \"friction\" points downwards, which indicates that an increase in smoothness leads to a decrease in friction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.08048v1",
    "pdf_url": null
  },
  {
    "instance_id": "89cdaee014534970a18668642b1f7916",
    "figure_id": "1909.05995v2-Figure6-1",
    "image_file": "1909.05995v2-Figure6-1.png",
    "caption": " Top-7 predictions for the unseen classes based on classification score on AwA2. Misclassifications are in red border.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which animal was most often misclassified by the model?",
    "answer": "Dolphin.",
    "rationale": "Three of the top-7 predictions for the unseen class \"Dolphin\" are incorrect, as indicated by the red boxes. This is more than any other animal in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.05995v2",
    "pdf_url": null
  },
  {
    "instance_id": "6198876db4204d60bf46b42b9eea7cf0",
    "figure_id": "2103.02150v1-Figure8-1",
    "image_file": "2103.02150v1-Figure8-1.png",
    "caption": " Boxplot of the percentage of runs that converged to an optimal policy for each 3×3 payoff matrix.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converged to an optimal policy the most consistently across different payoff matrices?",
    "answer": "The IQ-L algorithm.",
    "rationale": "The boxplot for IQ-L shows the highest median value and the smallest interquartile range, indicating that it converged to an optimal policy more often and with less variability than the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.02150v1",
    "pdf_url": null
  },
  {
    "instance_id": "266a0b1b5f5d4dd994177c6fcd8b1f41",
    "figure_id": "1908.07078v4-Figure2-1",
    "image_file": "1908.07078v4-Figure2-1.png",
    "caption": " Swiss roll graph (left) and its latent representation using SIG-VAE (middle) and VGAE (right). The latent representations (middle and right) are heat maps in R3. We expect that the embedding of the Swiss roll graph with inner-product decoder to be a curved plane in R3, which is clearly captured better by SIG-VAE.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, SIG-VAE or VGAE, better captures the curvature of the Swiss roll graph in its latent representation?",
    "answer": "SIG-VAE.",
    "rationale": "The latent representation of the Swiss roll graph using SIG-VAE (middle) is a curved plane in R3, which is closer to the true structure of the Swiss roll graph. The latent representation using VGAE (right) is more spread out and does not capture the curvature as well.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.07078v4",
    "pdf_url": null
  },
  {
    "instance_id": "4d628c58315144abb70a2e889c146d3c",
    "figure_id": "2302.14268v1-Figure6-1",
    "image_file": "2302.14268v1-Figure6-1.png",
    "caption": " Visualization for experimental results on partial point clouds. Shapes drawn for every three shapes from the left side to the right side are the input point cloud, reconstructions, and the predicted canonical object shape. Please zoom in for details.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the objects in the figure is the most difficult to reconstruct?",
    "answer": "The eyeglasses.",
    "rationale": "The figure shows that the reconstructions of the eyeglasses are less accurate than the reconstructions of the other objects. This is likely because the eyeglasses are a more complex shape than the other objects.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.14268v1",
    "pdf_url": null
  },
  {
    "instance_id": "5b2579f3719a46d8b0ee322167e68a19",
    "figure_id": "2305.11262v1-Figure2-1",
    "image_file": "2305.11262v1-Figure2-1.png",
    "caption": " Learning curves of the LMD method for debiasing the four bias categories on CDial-GPT.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which bias category appears to be the easiest to debias?",
    "answer": "Age appears to be the easiest to debias.",
    "rationale": "The training loss for Age is lower than the training loss for the other bias categories, indicating that the model is learning to debias Age more quickly than the other bias categories.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.11262v1",
    "pdf_url": null
  },
  {
    "instance_id": "5b4dfbf488ec48e080d92050a3833d81",
    "figure_id": "2205.03071v1-Figure3-1",
    "image_file": "2205.03071v1-Figure3-1.png",
    "caption": " Results of sample efficiency analysis. We compare KECP with strong baselines with different numbers of training samples K over MRQA 2019 shared tasks. “Full” denotes to the models trained over full training data.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the SQuAD1.1 task with a full training dataset?",
    "answer": "Splinter.",
    "rationale": "The figure shows the F1 scores of different models on different question answering tasks. The F1 score is a measure of how well the model performs. The Splinter model has the highest F1 score on the SQuAD1.1 task with a full training dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.03071v1",
    "pdf_url": null
  },
  {
    "instance_id": "1ebdb81f39ca49fd864adf7b9892a2ef",
    "figure_id": "2103.07013v1-Figure3-1",
    "image_file": "2103.07013v1-Figure3-1.png",
    "caption": " SPL vs. wall-clock time (RGB agents) on a RTX 3090 over 48 hours (time required to reach 2.5 billion samples with BPS). BPS exceeds 80% SPL in 10 hours and achieves a significantly higher SPL than the baselines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the highest SPL after 48 hours of training?",
    "answer": "BPS",
    "rationale": "The figure shows that the green line, which represents BPS, is higher than the other two lines after 48 hours of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.07013v1",
    "pdf_url": null
  },
  {
    "instance_id": "610f49fc21d64f14a55575c4e6312545",
    "figure_id": "2206.01311v2-Figure26-1",
    "image_file": "2206.01311v2-Figure26-1.png",
    "caption": " Average constraint function value (averaged across 5 training seeds) for HalfCheetahConstrained environment.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which constraint function best approximates the true constraint function?",
    "answer": "The ICRL constraint function.",
    "rationale": "The figure shows that the ICRL constraint function is the closest to the true constraint function. The other constraint functions are either too smooth or too noisy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.01311v2",
    "pdf_url": null
  },
  {
    "instance_id": "26fe27bc31df498d820e39c811beebb4",
    "figure_id": "2212.09387v2-Figure4-1",
    "image_file": "2212.09387v2-Figure4-1.png",
    "caption": " The visualization on text generation. The average value of gates σ(G(j) i ) (red bars) and the average of L1 norm of P(j) i (blue bars) on each layer, according to Eq. (7). The values are extracted for the sentiment aspect, including negative (top left) and positive (top right), and topic aspect, including Asian (bottom left) and American (bottom right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sentiment aspect has the highest average value of L1 norm of P(j) i on the last layer?",
    "answer": "The negative sentiment aspect.",
    "rationale": "The blue bars in the top left plot show the average value of L1 norm of P(j) i for the negative sentiment aspect on each layer. The height of the blue bars on the last layer is higher than the height of the blue bars in the other plots, indicating that the negative sentiment aspect has the highest average value of L1 norm of P(j) i on the last layer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.09387v2",
    "pdf_url": null
  },
  {
    "instance_id": "6d2d69e290df4ce49d8686b89f268db6",
    "figure_id": "2004.12770v3-Figure6-1",
    "image_file": "2004.12770v3-Figure6-1.png",
    "caption": " Attention maps, intermediate answers, and halting probabilities captured from DACT for the image and question shown. Three steps were needed to arrive at the answer. The first two steps output wrong answers with high uncertainty (pn ≈ 1). The last step, however, has identified the relevant object and can thus answer correctly and with confidence.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object in the image is the question asking about?",
    "answer": "The big object.",
    "rationale": "The question asks \"The big object that is made of the same material as the small gray cylinder is what color?\". This indicates that the question is asking about the big object.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.12770v3",
    "pdf_url": null
  },
  {
    "instance_id": "de6976310582468996fddcae3c08d05c",
    "figure_id": "2004.05484v1-Figure5-1",
    "image_file": "2004.05484v1-Figure5-1.png",
    "caption": " Model embeddings of all English and Chinese questions and candidates from XQuAD-R, visualized under 2D PCA.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language pair has the most similar question and candidate embeddings?",
    "answer": "English-English.",
    "rationale": "The En-En plot shows the most overlap between the question and candidate embeddings, indicating that they are the most similar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.05484v1",
    "pdf_url": null
  },
  {
    "instance_id": "78b2595dced54f62a855a33392fafa16",
    "figure_id": "1809.11086v2-Figure3-1",
    "image_file": "1809.11086v2-Figure3-1.png",
    "caption": " Effect of different batch sizes on the prediction accuracy of the character-level language modeling task on the Penn Treebank corpus.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which quantization level provides the highest accuracy for all batch sizes?",
    "answer": "Full-precision.",
    "rationale": "The bars representing full-precision are consistently higher than the bars representing binary and ternary quantization for all batch sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.11086v2",
    "pdf_url": null
  },
  {
    "instance_id": "3908ca58a0ba474f85f7767c3657e7df",
    "figure_id": "2007.12986v2-Figure6-1",
    "image_file": "2007.12986v2-Figure6-1.png",
    "caption": " Performance of the estimators for di erent slate sizes (i.e., sequence lengths). Note again the log scale on the y axis.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which estimator performs the best for all sequence lengths?",
    "answer": "IPS",
    "rationale": "The figure shows the performance of three estimators (RIPS, IIPS, and IPS) for different sequence lengths. The y-axis shows the root mean squared error (RMSE) on a logarithmic scale, so lower values indicate better performance. The IPS line is consistently below the other two lines, indicating that it has the lowest RMSE for all sequence lengths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.12986v2",
    "pdf_url": null
  },
  {
    "instance_id": "eb7b9ca5d9524412af5c8cc278ac52d0",
    "figure_id": "2105.13913v7-Figure7-1",
    "image_file": "2105.13913v7-Figure7-1.png",
    "caption": " Logistic Regression: Convergence of ℎ(x𝑡 ) and 𝑔(x𝑡 ) vs. 𝑡 and wall-clock time for the a8a LIBSVM dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms converges the fastest?",
    "answer": "B-AFW",
    "rationale": "The figure shows the convergence of ℎ(x𝑡 ) and 𝑔(x𝑡 ) vs. 𝑡 and wall-clock time for the a8a LIBSVM dataset. The B-AFW algorithm converges the fastest, as its lines reach the lowest values first.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.13913v7",
    "pdf_url": null
  },
  {
    "instance_id": "25df92a47fa54791b7e593be62a87875",
    "figure_id": "2310.17290v1-Figure8-1",
    "image_file": "2310.17290v1-Figure8-1.png",
    "caption": " The case study of the uncommon set predicted by the TOIST model, the blue box is the ground truth, and the red box is the prediction result. When no object in the model prediction result reaches the probability threshold, the output is the empty set.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object in the image is predicted by the TOIST model with the highest probability?",
    "answer": "The piano.",
    "rationale": "The piano is the only object in the image where the red box (prediction result) completely overlaps with the blue box (ground truth). This indicates that the TOIST model predicted the piano with the highest probability.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.17290v1",
    "pdf_url": null
  },
  {
    "instance_id": "bdd531f61f5a487b93fa43e5d8fb13f7",
    "figure_id": "2302.01312v3-Figure4-1",
    "image_file": "2302.01312v3-Figure4-1.png",
    "caption": " Mean KL divergence on 50 randomly sampled test set inputs as data was added to the training sets ( 1 50 ∑50 i=1 DKL(Pi ∥ Qi), where Pi is the ground truth conditional distribution and Qi is the model’s conditional distribution conditioned on xi).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the Hopper-v2 dataset?",
    "answer": "Nflows Base",
    "rationale": "The Nflows Base line in the Hopper-v2 plot is consistently lower than the other lines, indicating that it has the lowest KL divergence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.01312v3",
    "pdf_url": null
  },
  {
    "instance_id": "614420d7c8f341e0aab35cacfce2b821",
    "figure_id": "2211.01910v2-Figure18-1",
    "image_file": "2211.01910v2-Figure18-1.png",
    "caption": " Zero-shot test accuracy of best performing instructions on 6 Instruction Induction tasks. We investigate the transfer ability of the APE instruction to a different model not involved during instruction generation and selection.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which instruction proposal/scoring model performed the best on the Antonyms task?",
    "answer": "InstructGPT",
    "rationale": "The figure shows the test accuracy of the best-performing instructions on six Instruction Induction tasks. The color of each cell in the heatmap indicates the test accuracy, with darker colors indicating higher accuracy. For the Antonyms task, the cell corresponding to InstructGPT has the darkest color, indicating that it had the highest test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.01910v2",
    "pdf_url": null
  },
  {
    "instance_id": "fd8bbf6e0c3f4dfb81f2ae0eb7c39da3",
    "figure_id": "2006.12972v2-Figure3-1",
    "image_file": "2006.12972v2-Figure3-1.png",
    "caption": " Predicting the Hénon-Heiles system with initial state q0 = (−0.48,−0.02), p0 = (−0.08, 0.18) from t = 0 to t = 8 using models trained on noisy data. All models were given a clean initial state to see how well they extracted true dynamics from noisy training. The SSINN predicts the trajectory nearly exactly and only deviates in the last few time steps. All models learned to conserve the Hamiltonian, but the energy perturbations for the SSINN model are very reduced.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best in predicting the Hénon-Heiles system?",
    "answer": "The SSINN model performed the best.",
    "rationale": "The figure shows that the SSINN model's predictions for the trajectory and energy of the system were closest to the true values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.12972v2",
    "pdf_url": null
  },
  {
    "instance_id": "d3bf65cc7f5e4e3dac2fe8e76ee276ae",
    "figure_id": "1709.07776v2-Figure3-1",
    "image_file": "1709.07776v2-Figure3-1.png",
    "caption": " energy distribution comparison of layer “conv1 1”, “conv1 2”, “conv2 1” and “conv2 2”. The horizontal axis represents normalized magnitude from 0.8 to 1, and the area shows the comparison of each layer’s normalized energy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the highest energy distribution at a normalized magnitude of 0.95?",
    "answer": "Conv2_2",
    "rationale": "The figure shows that the green line, which represents Conv2_2, is the highest at a normalized magnitude of 0.95.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1709.07776v2",
    "pdf_url": null
  },
  {
    "instance_id": "4e2de9eb4be64648a13b7e3c8c6044b3",
    "figure_id": "1911.11942v2-Figure3-1",
    "image_file": "1911.11942v2-Figure3-1.png",
    "caption": " Results with different GNN layers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GNN layer achieves the best R@20 performance on the Diginetica dataset?",
    "answer": "FGNN-Gated",
    "rationale": "The figure shows the R@20 performance of different GNN layers on three datasets. The FGNN-Gated layer achieves the highest R@20 score on the Diginetica dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.11942v2",
    "pdf_url": null
  },
  {
    "instance_id": "dc04afd8783e4f7d88933cb30981addf",
    "figure_id": "2202.11912v2-Figure5-1",
    "image_file": "2202.11912v2-Figure5-1.png",
    "caption": " Three paths between an a baseline (r1, r2) and an input (s1, s2). Each path corresponds to a differ nt attribution method. The path P2 corresponds to the path used by integrated gradients.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which path corresponds to the integrated gradients attribution method?",
    "answer": "P2",
    "rationale": "The caption explicitly states that the path P2 corresponds to the path used by integrated gradients.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.11912v2",
    "pdf_url": null
  },
  {
    "instance_id": "962bb3d87e3a4f2eafb8789febc616ff",
    "figure_id": "2007.07210v2-Figure2-1",
    "image_file": "2007.07210v2-Figure2-1.png",
    "caption": " Performance comparison for ℓ2 untargeted attacks on ImageNet classifiers for all methods and 𝜖 thresholds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attack method is the most successful for ResNet50 when the query budget is 1000 and epsilon is 5.0?",
    "answer": "Sign-OPT attack",
    "rationale": "The figure shows the success rate of different attack methods for different query budgets and epsilon thresholds. When the query budget is 1000 and epsilon is 5.0, the Sign-OPT attack has the highest success rate for ResNet50.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.07210v2",
    "pdf_url": null
  },
  {
    "instance_id": "bef28f03460644a7b3c194adf114f5a3",
    "figure_id": "2005.09234v1-Figure6-1",
    "image_file": "2005.09234v1-Figure6-1.png",
    "caption": " Averaged AUC of the VAE, VIDNN, and VPDNN",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three models performs the best for the Slider device?",
    "answer": "VIDNN",
    "rationale": "The bar for VIDNN in the Slider category is the tallest, indicating the highest AUC value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.09234v1",
    "pdf_url": null
  },
  {
    "instance_id": "8d756d9718b74c26adb2a2b0b6e0a773",
    "figure_id": "2304.05390v2-Figure9-1",
    "image_file": "2304.05390v2-Figure9-1.png",
    "caption": " Gender fairness and style fairness results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in terms of gender fairness?",
    "answer": "Paella",
    "rationale": "The figure shows that Paella has the highest score for gender fairness, as indicated by the blue bar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.05390v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd487a1bb2a54bb5b4c8eaff8135f363",
    "figure_id": "2111.11632v2-Figure5-1",
    "image_file": "2111.11632v2-Figure5-1.png",
    "caption": " Good variable orders lead to more efficient computation of Fπ(x). Consider the PC p shown in (a). (b): If variable order X1, X2, X3 is used, we need to evaluate 20 PC units in total. (c): The optimal variable order X3, X2, X1 allows us to compute Fπ(x) by only evaluating 13 PC units.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which variable order results in a more efficient computation of Fπ(x)?",
    "answer": "The optimal variable order is X3, X2, X1.",
    "rationale": "The figure shows that the variable order X3, X2, X1 allows us to compute Fπ(x) by only evaluating 13 PC units, while the variable order X1, X2, X3 requires us to evaluate 20 PC units.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.11632v2",
    "pdf_url": null
  },
  {
    "instance_id": "08f20ea665234f858f596650bc339028",
    "figure_id": "2305.19162v1-Figure5-1",
    "image_file": "2305.19162v1-Figure5-1.png",
    "caption": " Recall and Training Time with respect to the choice of k. Experiments are run for one epoch on a single Tesla T4 GPU.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the choice of k and the recall?",
    "answer": "The recall increases as the choice of k increases.",
    "rationale": "The blue line in the figure shows the recall as a function of the choice of k. The recall increases from about 88% for k = 10 to about 95% for k = 200.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19162v1",
    "pdf_url": null
  },
  {
    "instance_id": "fb7511281a76416ca480fa7d5ebcac0e",
    "figure_id": "1905.04982v5-Figure18-1",
    "image_file": "1905.04982v5-Figure18-1.png",
    "caption": " Movement interpolation. The different colours correspond to Fig. 5. Discontinuities are marked by blue boxes.",
    "figure_type": "** Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method(s) produce discontinuities in the movement interpolation? ",
    "answer": " VampPrior and IWAE. ",
    "rationale": " The figure shows the movement interpolation for three different methods: VHP + REWO, VampPrior, and IWAE. The blue boxes in the figure mark the discontinuities in the movement interpolation. VHP + REWO does not have any discontinuities, while VampPrior and IWAE have several discontinuities. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.04982v5",
    "pdf_url": null
  },
  {
    "instance_id": "5893065f89cf465797d046c96aac91b6",
    "figure_id": "2206.09113v2-Figure6-1",
    "image_file": "2206.09113v2-Figure6-1.png",
    "caption": " Training speed of different methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest?",
    "answer": "STEP w/o preprocessing is the fastest.",
    "rationale": "The figure shows the training speed of different methods, measured in seconds per epoch. The STEP w/o preprocessing bar is the tallest, which indicates that it is the fastest method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.09113v2",
    "pdf_url": null
  },
  {
    "instance_id": "d8a63a892bab4448bea596ae55d9cee3",
    "figure_id": "2305.16498v2-Figure28-1",
    "image_file": "2305.16498v2-Figure28-1.png",
    "caption": "Figure 28 | Normalized performance of csil again baselines for online imitation learning for Adroit tasks . Uncertainty intervals depict quartiles over 10 seeds. csil exhibits stable convergence with both sample and demonstration efficiency. Many baselines cannot achieve stable convergence due to the high-dimensional action space. sacfd is an oracle baseline that combines the demonstrations with the true (shaped) reward. 𝑛 refers to demonstration trajectories.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the highest return on the door-v0 task with n=10 demonstrations?",
    "answer": "sacfd",
    "rationale": "The figure shows the normalized performance of different algorithms on the door-v0 and hammer-v0 tasks. The sacfd line is the highest on the door-v0 task with n=10 demonstrations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16498v2",
    "pdf_url": null
  },
  {
    "instance_id": "9effb07d7dd7472e920a4fb7fa9fc552",
    "figure_id": "2304.05390v2-Figure16-1",
    "image_file": "2304.05390v2-Figure16-1.png",
    "caption": " Qualitative results. Sample # 6.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following prompts do you think was given to the AI models?\nA. \"A red traffic light at a crosswalk\"\nB. \"A toilet with a sandwich on it\"\nC. \"A person petting a cat on the floor\"\nD. All of the above",
    "answer": "D. All of the above",
    "rationale": "The figure shows a variety of images generated by different AI models. The prompts for these images are not explicitly stated, but we can infer them from the images themselves. For example, the first image in the top row shows a red traffic light at a crosswalk, so we can infer that the prompt for this image was something like \"A red traffic light at a crosswalk.\" Similarly, the second image in the top row shows a toilet with a sandwich on it, so we can infer that the prompt for this image was something like \"A toilet with a sandwich on it.\" Finally, the last image in the bottom row shows a person petting a cat on the floor, so we can infer that the prompt for this image was something like \"A person petting a cat on the floor.\" Therefore, we can conclude that the answer to the question is D. All of the above.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.05390v2",
    "pdf_url": null
  },
  {
    "instance_id": "f70989c085ad4da1bf5910b48c00c527",
    "figure_id": "2206.05260v3-Figure8-1",
    "image_file": "2206.05260v3-Figure8-1.png",
    "caption": " Reliability plots for (a) CE, (b) mixup, (c) BS, (d) uncalibrated BalPoE (trained with ERM), and (e) BalPoE (trained with mixup). Computed over CIFAR-10-LT-100 test set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method results in the lowest ECE and MCE values?",
    "answer": "BalPoE trained with mixup.",
    "rationale": "The ECE and MCE values are shown in the legend of each plot. BalPoE trained with mixup has the lowest ECE and MCE values of 6.8% and 12.4%, respectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.05260v3",
    "pdf_url": null
  },
  {
    "instance_id": "80be8f5ea3c24bcfa98038bb379698e2",
    "figure_id": "1912.04427v4-Figure11-1",
    "image_file": "1912.04427v4-Figure11-1.png",
    "caption": " Accuracy and sparsity of tickets produced by IMP and Sequential CS after re-training, starting from weights of epoch 2. Tickets are extracted from a ResNet-20 trained on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves higher accuracy with fewer weights remaining?",
    "answer": "Iterative Mag.Pr",
    "rationale": "The plot shows that Iterative Mag.Pr (blue line) achieves higher accuracy than CS (Sequential) (purple line) for all percentages of weights remaining. This means that Iterative Mag.Pr is able to achieve higher accuracy with fewer weights, which is desirable for resource-constrained applications.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.04427v4",
    "pdf_url": null
  },
  {
    "instance_id": "02e4eec220bf474c8d2f92a03a65452a",
    "figure_id": "1811.11064v1-Figure6-1",
    "image_file": "1811.11064v1-Figure6-1.png",
    "caption": " Generated staircases displaying desired inferences",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which block is the largest?",
    "answer": "The yellow block is the largest.",
    "rationale": "The yellow block is at the top of the tallest stack of blocks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.11064v1",
    "pdf_url": null
  },
  {
    "instance_id": "df10d7e566b7476da21d006d32d25f44",
    "figure_id": "2212.11185v1-Figure3-1",
    "image_file": "2212.11185v1-Figure3-1.png",
    "caption": " Improvements in CDR model log-likelihood from including each predictor on the exploratory partition of Natural Stories self-paced reading data (left) and Dundee eye-tracking data (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which predictor has the greatest effect on the CDR model log-likelihood for the Natural Stories SPR data?",
    "answer": "Attn-W",
    "rationale": "The bar graph shows that the Attn-W predictor has the largest positive impact on the CDR model log-likelihood for the Natural Stories SPR data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.11185v1",
    "pdf_url": null
  },
  {
    "instance_id": "4a938a7408d04e789eac5000397dc338",
    "figure_id": "2104.08078v2-Figure2-1",
    "image_file": "2104.08078v2-Figure2-1.png",
    "caption": " Average transfer gains using different classifiers for predicting the set of most promising sources.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the largest average transfer gain for predicting the set of most promising sources?",
    "answer": "All-C",
    "rationale": "The figure shows the average transfer gains for different classifiers. The All-C method has the highest average transfer gain of 11.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.08078v2",
    "pdf_url": null
  },
  {
    "instance_id": "f857db1b9f344c9aa9c267cec611b2dd",
    "figure_id": "2202.07301v4-Figure13-1",
    "image_file": "2202.07301v4-Figure13-1.png",
    "caption": " Training Curves (Reacher). In the three graphs from left to right, the UOR-RL algorithms are trained under k = 0, 1 and 21, and the y-axis represent the average return of all trajectories, E1 and the average return of worst 10% trajectories respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which UOR-RL algorithm performed the best under k = 21?",
    "answer": "DB-UOR-RL",
    "rationale": "The figure shows that the training curve for DB-UOR-RL under k = 21 is the highest among all the UOR-RL algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.07301v4",
    "pdf_url": null
  },
  {
    "instance_id": "b061c3d9ae2240bca1b0ec22eace623e",
    "figure_id": "2302.12400v1-Figure10-1",
    "image_file": "2302.12400v1-Figure10-1.png",
    "caption": " Visualization of loss (entropy) surface. Models are learned on ImageNet-C of Gaussian noise with severity level 5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is more robust to Gaussian noise?",
    "answer": "ResNet50-GN, SAR (Ours)",
    "rationale": "The loss surface of ResNet50-GN, SAR (Ours) is smoother and has a lower minimum value than the loss surface of ResNet50-GN, Tent. This indicates that ResNet50-GN, SAR (Ours) is more robust to Gaussian noise.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.12400v1",
    "pdf_url": null
  },
  {
    "instance_id": "6e2db247b4ed4082bad2f5fe9b8efd10",
    "figure_id": "2009.08576v2-Figure5-1",
    "image_file": "2009.08576v2-Figure5-1.png",
    "caption": " Percent of neurons (conv. channels) with sparsity ≥ s% at the highest matching sparsity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest sparsity for ResNet-50 on ImageNet?",
    "answer": "SynFlow (Shuffled Layerwise)",
    "rationale": "The figure shows that the SynFlow (Shuffled Layerwise) curve is the highest for ResNet-50 on ImageNet, indicating that it achieves the highest sparsity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.08576v2",
    "pdf_url": null
  },
  {
    "instance_id": "3cadfb481b364126bd6747986756676c",
    "figure_id": "2204.02668v1-Figure3-1",
    "image_file": "2204.02668v1-Figure3-1.png",
    "caption": " Output instance of the reduction for a Unary Bin Packing instance consisting of four items of sizes 2, 3, 1, and 3 with β = 3 and B = 3. The activity timeline indicated in the figure corresponds to an assignment that adds items 1 and 3 to the first bin, item 2 to the second bin, and item 4 to the third bin.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which item is placed in the second bin?",
    "answer": "Item 2.",
    "rationale": "The activity timeline in the figure shows that item 2 is placed in the second bin. This is evident from the fact that the timeline for item 2 is contained within the grey shaded area of the second bin.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.02668v1",
    "pdf_url": null
  },
  {
    "instance_id": "5322348376264ebcaaadb7183b5085d6",
    "figure_id": "2212.10543v2-Figure3-1",
    "image_file": "2212.10543v2-Figure3-1.png",
    "caption": " A screenshot of the human evaluation interface on Amazon Mechanical Turk.",
    "figure_type": "Screenshot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which rewrite is less toxic, according to the definition of toxicity provided in the instructions?",
    "answer": "Rewrite B is less toxic than Rewrite A.",
    "rationale": "Rewrite A is still sexist because it suggests that men are the only ones who can deal with construction. Rewrite B removes the gender bias by simply stating that someone should help with the problem.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10543v2",
    "pdf_url": null
  },
  {
    "instance_id": "3f6120333ffe46febf3a4aeb9e924456",
    "figure_id": "2205.05793v1-Figure1-1",
    "image_file": "2205.05793v1-Figure1-1.png",
    "caption": " The DAG of an augmented BN modelling a simple fictional medical treatment scenario.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the DAG, can symptoms directly influence test results?",
    "answer": "No.",
    "rationale": "The arrows in the DAG represent the direction of influence between variables. There is no arrow pointing directly from Symptoms to Test Results, indicating that Symptoms cannot directly influence Test Results. Instead, Symptoms can influence Test Results indirectly through Strain and Treatment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.05793v1",
    "pdf_url": null
  },
  {
    "instance_id": "04cd06faf9d54003bbd849c304a7719c",
    "figure_id": "2006.02958v3-Figure9-1",
    "image_file": "2006.02958v3-Figure9-1.png",
    "caption": " The effect of tile granularity on query time compared to untiled videos. All videos used a one second tile layout duration. Objects occupy <20% of each frame on average in “sparse”, and ≥20% in “dense” videos.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tile granularity and video density combination resulted in the highest improvement in query time?",
    "answer": "Fine tiles and sparse videos.",
    "rationale": "The figure shows that the improvement in query time is highest for fine tiles and sparse videos, as shown in subplot (d).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.02958v3",
    "pdf_url": null
  },
  {
    "instance_id": "422c100621294c9ebfc01ac11df90379",
    "figure_id": "1909.03939v2-Figure6-1",
    "image_file": "1909.03939v2-Figure6-1.png",
    "caption": " The weight of bootstrapping.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which DVPG parameter resulted in the highest return after 100,000 steps?",
    "answer": "DVPG_0.9",
    "rationale": "The line for DVPG_0.9 is the highest at the end of the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.03939v2",
    "pdf_url": null
  },
  {
    "instance_id": "455c4a63cd204d99a8e8eed67563aa5c",
    "figure_id": "2007.08259v2-Figure7-1",
    "image_file": "2007.08259v2-Figure7-1.png",
    "caption": " ECE(%) before and after various calibration methods for more DA methods on Visda.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which calibration method has the least effect on ECE(%) for AFN?",
    "answer": "TransCal",
    "rationale": "The figure shows that the ECE(%) for AFN with TransCal is very close to the ECE(%) for AFN without any calibration (Vanilla).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.08259v2",
    "pdf_url": null
  },
  {
    "instance_id": "22d6d961cc184ce49278fd4239defe83",
    "figure_id": "2204.11823v1-Figure9-1",
    "image_file": "2204.11823v1-Figure9-1.png",
    "caption": " InsetGAN Results. We show the combined results of six different human bodies generated from the given baseline model and six faces generated from the FFHQ [40] model. For every single face, shown on the left corner of each grid, we jointly optimize it with three different bodies.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different bodies were generated for each face in the InsetGAN results?",
    "answer": "Three.",
    "rationale": "The caption states that \"For every single face, shown on the left corner of each grid, we jointly optimize it with three different bodies.\" The figure shows six different faces, and each face is shown with three different bodies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.11823v1",
    "pdf_url": null
  },
  {
    "instance_id": "e0160c5a635144289cbdef55ed3a7285",
    "figure_id": "2305.19454v2-Figure4-1",
    "image_file": "2305.19454v2-Figure4-1.png",
    "caption": " Ablation Study of Chase. GACP denotes gradual amenable channel pruning (50% channel sparsity), SM indicates soft memory bound, GE represents global parameter exploration.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sparsity level achieved the highest accuracy for RigL+GACP+SM+GE (Chase) on CIFAR-100?",
    "answer": "90%",
    "rationale": "The figure shows that RigL+GACP+SM+GE (Chase) achieved the highest accuracy of 73.04% at a sparsity level of 90% on CIFAR-100.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19454v2",
    "pdf_url": null
  },
  {
    "instance_id": "56a43a845f9942488a5e5387a3471b28",
    "figure_id": "1910.09943v2-Figure4-1",
    "image_file": "1910.09943v2-Figure4-1.png",
    "caption": " LCB and CB are primarily designed for settings where K is much larger than L. Despite this, our LP method always obtains better label assignment scores, and often obtains better ARI cluster identification scores, when we fix L = 20 and let K vary from 50 to 500.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of label accuracy and ARI scores when K is much larger than L?",
    "answer": "LP method",
    "rationale": "The figure shows that the LP method consistently achieves higher label accuracy and often obtains better ARI cluster identification scores compared to other methods when L is fixed at 20 and K varies from 50 to 500.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.09943v2",
    "pdf_url": null
  },
  {
    "instance_id": "80a75dd4df914aac87bfdab88856de3c",
    "figure_id": "2301.11494v3-Figure6-1",
    "image_file": "2301.11494v3-Figure6-1.png",
    "caption": " Error analysis on a synthetic video. The top row plots the inference errors of velocity, vorticity, and compressibility. The bottom row plots the future prediction errors, which consider both the dynamics error of the velocity and the perceptual error of the generated image sequence.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest velocity prediction error?",
    "answer": "HPM (Ours)",
    "rationale": "The bottom row of the figure shows the future prediction errors for velocity. The HPM (Ours) line is the lowest of all the lines, indicating that it has the lowest velocity prediction error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.11494v3",
    "pdf_url": null
  },
  {
    "instance_id": "6c535d6bcb1d470a9267b186861a0e88",
    "figure_id": "2004.04725v3-Figure10-1",
    "image_file": "2004.04725v3-Figure10-1.png",
    "caption": " Top-5 classes with biggest performance boost when using Concrete DropBlock. Animal classes are emphasized using green color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class of objects benefited the most from using Concrete DropBlock?",
    "answer": "Cat",
    "rationale": "The figure shows the relative mAP change for different classes of objects when using Concrete DropBlock. The cat class has the highest relative mAP change, which means that it benefited the most from using Concrete DropBlock.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.04725v3",
    "pdf_url": null
  },
  {
    "instance_id": "edd7ccf58ba14ef28ab9e1521effca95",
    "figure_id": "2006.06377v2-Figure4-1",
    "image_file": "2006.06377v2-Figure4-1.png",
    "caption": " Training loss w.r.t epochs for ResNet18 and VGG16 on CIFAR10 dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the lowest training loss on the CIFAR10 dataset for both ResNet18 and VGG16 architectures, in both IID and Non-IID settings?",
    "answer": "SyncSGD",
    "rationale": "The figure shows the training loss for different algorithms on the CIFAR10 dataset. The SyncSGD algorithm consistently achieves the lowest training loss across all settings.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06377v2",
    "pdf_url": null
  },
  {
    "instance_id": "cfdd9c3e9c4f49d7b2d30539c4d6c05f",
    "figure_id": "2203.10581v1-Figure6-1",
    "image_file": "2203.10581v1-Figure6-1.png",
    "caption": " Comparison of clustering configurations for the intermediate task (hk-means stands for Hartigan’s Kmeans). The results with no inter-training (BERT) are also presented for comparison. Each point is the average of five repetitions (± SEM). X axis denotes the number of labeling instances used for fine-tuning (in log scale).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which configuration of BERT achieved the highest accuracy on the AG's News dataset?",
    "answer": "BERT_hk-means(GloVe)",
    "rationale": "The figure shows the accuracy of different BERT configurations on different datasets. The line for BERT_hk-means(GloVe) on the AG's News dataset is the highest of all the lines, indicating that it achieved the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.10581v1",
    "pdf_url": null
  },
  {
    "instance_id": "f389b12ad1fd4b2a8aeef1e7ff4caffc",
    "figure_id": "2202.03347v1-Figure4-1",
    "image_file": "2202.03347v1-Figure4-1.png",
    "caption": " Comparison of performance in unknown categories.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest average accuracy (ACC) across all categories?",
    "answer": "Ours",
    "rationale": "The figure shows the accuracy (ACC) and average precision (AP) for different methods on various categories. The last column shows the mean ACC and AP across all categories. We can see that \"Ours\" has the highest mean ACC.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.03347v1",
    "pdf_url": null
  },
  {
    "instance_id": "7e27607c3d1348e596460aea97ef7504",
    "figure_id": "1912.09893v3-Figure1-1",
    "image_file": "1912.09893v3-Figure1-1.png",
    "caption": " Chemical and social (with degree) benchmark results are shown together with published results (when available). For each of them, we report validation and test accuracies of the evaluated models, together with published results if available.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieved the highest accuracy on the COLLAB dataset?",
    "answer": "GraphSAGE",
    "rationale": "The figure shows the accuracy of different models on different datasets. The COLLAB dataset is shown in the bottom right corner of the figure. The GraphSAGE model has the highest accuracy on this dataset, as indicated by the purple bar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.09893v3",
    "pdf_url": null
  },
  {
    "instance_id": "9bad12d9c24d4b15b45cfe9ad6621686",
    "figure_id": "1910.03151v4-Figure1-1",
    "image_file": "1910.03151v4-Figure1-1.png",
    "caption": " Comparison of various attention modules (i.e., SENet [14], CBAM [33], A2-Nets [4] and ECA-Net) using ResNets [11] as backbone models in terms of classification accuracy, network parameters and FLOPs, indicated by radiuses of circles. Note that our ECA-Net obtains higher accuracy while having less model complexity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network has the highest accuracy and the lowest number of parameters?",
    "answer": "ECA-Net152",
    "rationale": "The figure shows the accuracy and number of parameters for various networks. ECA-Net152 has the highest accuracy and the lowest number of parameters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.03151v4",
    "pdf_url": null
  },
  {
    "instance_id": "5a7bc29172644172a79eece5badbae2e",
    "figure_id": "2106.04784v1-Figure2-1",
    "image_file": "2106.04784v1-Figure2-1.png",
    "caption": " Histograms of data entropy in log scale. Blue histograms indicate the entropy distributions of 50K training examples of CIFAR-10 and the others indicate those of various proxy data constructed using selection methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which selection method results in the most similar entropy distribution to the original data?",
    "answer": "The proposed method.",
    "rationale": "The figure shows the histograms of data entropy for different selection methods. The blue histogram represents the original data, and the other histograms represent the different selection methods. The proposed method's histogram is the most similar to the original data's histogram.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04784v1",
    "pdf_url": null
  },
  {
    "instance_id": "76923a68792a41c0a92c2a64d1668eda",
    "figure_id": "2202.03052v2-Figure4-1",
    "image_file": "2202.03052v2-Figure4-1.png",
    "caption": " Qualitative results on an unseen task grounded QA. We design a new task called grounded question answering, where the model should answer a question about a certain region in the image. More samples are provided in Figure 10 in Appendix C.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many cars are there in the image?",
    "answer": "There are two cars in the image.",
    "rationale": "The image shows two cars, one on the left and one on the right. The car on the left is tan, and the car on the right is gray.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.03052v2",
    "pdf_url": null
  },
  {
    "instance_id": "f78eedfc42854475b0034792bee6abfe",
    "figure_id": "2010.07070v1-Figure8-1",
    "image_file": "2010.07070v1-Figure8-1.png",
    "caption": " Profile d15.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many profiles are shown in the figure?",
    "answer": "Two profiles are shown in the figure.",
    "rationale": "The figure shows two separate sets of lines with arrows, each with three labeled points. Each set of lines with arrows represents a profile.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.07070v1",
    "pdf_url": null
  },
  {
    "instance_id": "8cbc380a4cd34f56ae7d8c8165e1ac1b",
    "figure_id": "1810.01032v4-Figure8-1",
    "image_file": "1810.01032v4-Figure8-1.png",
    "caption": " Learning curves from five reward robust RL algorithms (see Algorithm 3) on CartPole game with true rewards (r) ,",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm shows the fastest convergence to the highest number of steps per episode?",
    "answer": "DDQN.",
    "rationale": "The learning curves in (e) show that DDQN converges to the highest number of steps per episode in the fewest episodes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.01032v4",
    "pdf_url": null
  },
  {
    "instance_id": "94287828752d48a3aed839816fad93d7",
    "figure_id": "2310.00093v2-Figure6-1",
    "image_file": "2310.00093v2-Figure6-1.png",
    "caption": " Example distilled images from 32x32 CIFAR10/100 (IPC10), 64x64 Tiny ImageNet (IPC1), and 64x64 ImageNet-1K (IPC1).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the datasets is the most complex?",
    "answer": "ImageNet-1K",
    "rationale": "The ImageNet-1K dataset contains 1,000 classes of images, while the other datasets contain fewer classes. This means that the ImageNet-1K dataset is more complex and requires a more powerful model to achieve good performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.00093v2",
    "pdf_url": null
  },
  {
    "instance_id": "b0ddb6598c004c0d8c5b8cb2e99746f4",
    "figure_id": "2106.14405v2-Figure17-1",
    "image_file": "2106.14405v2-Figure17-1.png",
    "caption": " Grad-CAM saliency maps for three different scenes from cameras mounted on the Head and the Arm. Notice that the arm-joints are considered particularly salient in both cases across scenes.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which camera is more likely to capture the arm joints in the scene?",
    "answer": "The arm camera.",
    "rationale": "The Grad-CAM saliency maps show that the arm joints are considered particularly salient in the images captured by the arm camera, but not in the images captured by the head camera.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.14405v2",
    "pdf_url": null
  },
  {
    "instance_id": "ac1aee2a528548ebb5621c33fa1bd382",
    "figure_id": "2006.03143v2-Figure4-1",
    "image_file": "2006.03143v2-Figure4-1.png",
    "caption": " Learning comparison on CIFAR-10. Solid loss curves measure the SBN expected loss. Doted loss curves indicate the relaxed objectives used by respective methods (where applicable). Solid accuracy curves are using 10-sample expected predictive probabilities of SBN and dotted curves only 1-sample predictive probabilities. All curves are smoothed over iterations and shaded areas denote 3ˆstd w.r.t. smoothing. The automatically found learning rates are displayed in the legend.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the best validation accuracy with No Augmentation?",
    "answer": "LocalRepararam",
    "rationale": "The figure shows that the LocalRepararam method has the highest validation accuracy among all the methods with No Augmentation. This can be seen by comparing the different colored lines in the top right plot of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.03143v2",
    "pdf_url": null
  },
  {
    "instance_id": "e4d3322e589c4a018b25c9492ba855db",
    "figure_id": "2310.13402v1-Figure6-1",
    "image_file": "2310.13402v1-Figure6-1.png",
    "caption": " Evaluation on six benchmark problems (for each column) in the form of coverage curves estimated on a set of 10k test instances. If the curve is on (above) the diagonal line, then p̂(θ|x) is calibrated (conservative). Expected coverage of HPDRs at 19 evenly spaced levels on the [0.05; 0.95] interval is estimated following eq. (2) by solving an optimization problem to find ΘHPDR",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "For which of the benchmark problems does the calibrated estimator provide the most conservative coverage?",
    "answer": "The Spatial SIR benchmark problem.",
    "rationale": "The calibrated estimator's coverage curve for the Spatial SIR problem is consistently above the diagonal line, indicating that it provides conservative coverage for all credibility levels. This is not the case for the other benchmark problems, where the calibrated estimator's coverage curve falls below the diagonal line for some credibility levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.13402v1",
    "pdf_url": null
  },
  {
    "instance_id": "c5982915bda44c9bbb5e93bca0bb67e5",
    "figure_id": "2203.09435v2-Figure7-1",
    "image_file": "2203.09435v2-Figure7-1.png",
    "caption": " F1 on MasakhaNER with different amount of labeled data. Pseudo MLM becomes beneficial when the labeled training data is small.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best when the amount of labeled data is small?",
    "answer": "Pseudo MLM.",
    "rationale": "The plot shows that the F1 score for Pseudo MLM is higher than the other two methods when the amount of labeled data is small (5000 and 6000).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.09435v2",
    "pdf_url": null
  },
  {
    "instance_id": "760612fe3e994297a79952e969f161d7",
    "figure_id": "2105.10861v1-Figure5-1",
    "image_file": "2105.10861v1-Figure5-1.png",
    "caption": " Confusion matrix for the 10 most frequent relations on the RST–DT test set. The vertical axis represents true and horizontal axis represents predicted relations. The relations are: Elaboration (EL), Attribution (AT), Joint (JO), Same-Unit (SA), Contrast (CONT), Background (BA), Explanation (EX), Cause (CA), Temporal (TEM), Condition (COND).",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which relation has the highest precision?",
    "answer": "Same-Unit (SA)",
    "rationale": "The precision of a relation is the proportion of correctly predicted instances to all predicted instances of that relation. This can be calculated by dividing the diagonal value (true positives) by the sum of the values in the corresponding column (all predicted instances). In this case, the diagonal value for Same-Unit (SA) is 0.86, and the sum of the values in the corresponding column is 0.91. Therefore, the precision of Same-Unit (SA) is 0.86/0.91 = 0.945, which is the highest among all relations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.10861v1",
    "pdf_url": null
  },
  {
    "instance_id": "37d5bb38ef7b4bec8d8cd622da91ee01",
    "figure_id": "2010.11918v2-Figure1-1",
    "image_file": "2010.11918v2-Figure1-1.png",
    "caption": " Standard adapter fine-tuning vs. AdapterDrop fine-tuning. The left model includes adapters at every layer whereas the right model has adapters dropped at the first layer. The arrows to the right of each model indicate the information flow for the Forward and Backward pass through the model.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has adapters at every layer?",
    "answer": "The left model has adapters at every layer.",
    "rationale": "The figure shows two models, one on the left and one on the right. The left model has adapters at every layer, while the right model has adapters dropped at the first layer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.11918v2",
    "pdf_url": null
  },
  {
    "instance_id": "6b1c121d93ba4b69908bd74ff6b2bcad",
    "figure_id": "1910.00754v1-Figure7-1",
    "image_file": "1910.00754v1-Figure7-1.png",
    "caption": " Qualitative results of the semantic alignment on the JLAD dataset: (a) source image, (b) target image, (c) CNNgeo [24], (d) CNNinlier [25], (e) A2Net [27], (f) RTNs [15], (g) NCNet [26], and (h) Ours. The source images were warped to the target images using correspondences.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most visually appealing result?",
    "answer": "Ours.",
    "rationale": "The warped images produced by our method are the most visually appealing because they are the most similar to the target images. The other methods produce warped images that are either too blurry or too distorted.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.00754v1",
    "pdf_url": null
  },
  {
    "instance_id": "afc3f2cad2044546addba579a65414d6",
    "figure_id": "2309.13248v1-Figure4-1",
    "image_file": "2309.13248v1-Figure4-1.png",
    "caption": " Visualization of datasets. The first and second rows show images from the Movi-B and Movi-D, respectively. The remaining four images belong to the KITTI.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset contains images of cars driving on city streets?",
    "answer": "The KITTI dataset.",
    "rationale": "The last four images in the figure show cars driving on city streets. These images are from the KITTI dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.13248v1",
    "pdf_url": null
  },
  {
    "instance_id": "d7566c40e0b5470abea3ec7014589e49",
    "figure_id": "2205.11672v2-Figure3-1",
    "image_file": "2205.11672v2-Figure3-1.png",
    "caption": " Waterbirds: Distribution of the second highest PCA feature.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which waterbird species has the most variable second highest PCA feature?",
    "answer": "The pink waterbird species.",
    "rationale": "The pink waterbird species has the widest distribution of the second highest PCA feature, as shown by the wider spread of the bars in the histogram.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.11672v2",
    "pdf_url": null
  },
  {
    "instance_id": "3cfb6b0a0e734099b8e9b5bdcbf40904",
    "figure_id": "2302.02209v4-Figure2-1",
    "image_file": "2302.02209v4-Figure2-1.png",
    "caption": " Expressiveness hierarchy: A → B iff A B. By Proposition A.20, rwl2 and rawl + 2 are incomparable. The case of rwl + 2 rawl + 2 is analogous to Proposition A.18.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between propositions A.17 and A.18?",
    "answer": "Propositions A.17 and A.18 are incomparable.",
    "rationale": "The figure shows that there is no arrow pointing from A.17 to A.18, and there is no arrow pointing from A.18 to A.17. This means that neither proposition implies the other.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.02209v4",
    "pdf_url": null
  },
  {
    "instance_id": "735860e0d53946b58a4e4696c7a3bcbf",
    "figure_id": "1903.12020v3-Figure7-1",
    "image_file": "1903.12020v3-Figure7-1.png",
    "caption": " The correlation plots between the diversity scores computed by different metrics and human evaluation. The red lines are the best fit lines to the data.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which diversity metric has the highest correlation with human evaluation?",
    "answer": "Self-CIDER.",
    "rationale": "The plot shows that the data points for Self-CIDER are closest to the red line, which indicates the best fit line to the data. This means that the Self-CIDER metric has the highest correlation with human evaluation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.12020v3",
    "pdf_url": null
  },
  {
    "instance_id": "72ccd8f47d6f463295015598ecbd99e8",
    "figure_id": "2303.01959v1-Figure9-1",
    "image_file": "2303.01959v1-Figure9-1.png",
    "caption": " Impact of the teacher model’s architecture on the certified accuracy of PointCert in the black-box setting of Scenario III. Student model architecture is PointNet and dataset is ModelNet40.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which teacher model architecture results in the highest certified accuracy for PointCert in the black-box setting of Scenario III?",
    "answer": "DGCNN",
    "rationale": "The plot shows the certified accuracy of PointCert as a function of the number of added points, t, for three different teacher model architectures: PointNet, PointNet++, and DGCNN. The blue line, which represents DGCNN, is consistently higher than the other two lines, indicating that DGCNN results in the highest certified accuracy for PointCert.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.01959v1",
    "pdf_url": null
  },
  {
    "instance_id": "d0af7ddae4a14e8ebb7510ea0c79fe51",
    "figure_id": "2111.13119v1-Figure20-1",
    "image_file": "2111.13119v1-Figure20-1.png",
    "caption": " Plots show how many times pre-trained policies executed an action during 100 episodes when transferred to new environments. Counts are averages over both SingleEnvs and MultiEnv transfers. The table reports the resulting action probability. Last column shows the normalized entropy of the distribution (a random distribution has entropy 1). ‘Panoramic change-only’ overfits to moving forward, while ‘egocentric change-only’ to turns. On the contrary, C-BET moves less, interacts more, and overfit less to any action as shown by its higher entropy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which policy overfits the least to any action?",
    "answer": "C-BET",
    "rationale": "The figure shows the action probability for each policy. C-BET has the highest entropy, which indicates that it overfits the least to any action.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.13119v1",
    "pdf_url": null
  },
  {
    "instance_id": "839a5e2085364610a53ff924e0ca9b03",
    "figure_id": "2105.13351v2-Figure3-1",
    "image_file": "2105.13351v2-Figure3-1.png",
    "caption": " Model accuracy on the PathTracker challenge. Video analysis models were trained to solve 32 (a) and 64 frame (b) versions of challenge, which featured the target object and 14 identical distractors. Models were tested on PathTracker datasets with the same number of frames but 1, 14, or 25 distractors (left/middle/right). Grey hatched boxes denote 95% bootstrapped confidence intervals for humans. Only our InT Circuit rivaled humans on each dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the PathTracker challenge with 64 frames and 25 distractors?",
    "answer": "InT Circuit",
    "rationale": "The bar graph shows the accuracy of different models on the PathTracker challenge with different numbers of frames and distractors. The InT Circuit model has the highest accuracy for all combinations of frames and distractors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.13351v2",
    "pdf_url": null
  },
  {
    "instance_id": "fb281bb4b6d04e7080ab5211e633c6cb",
    "figure_id": "2112.15110v2-Figure3-1",
    "image_file": "2112.15110v2-Figure3-1.png",
    "caption": " Subjective evaluation results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best overall in terms of faithfulness, creativity, naturalness, and musicality?",
    "answer": "The proposed method.",
    "rationale": "The figure shows that the proposed method achieved the highest average score across all four categories.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.15110v2",
    "pdf_url": null
  },
  {
    "instance_id": "42c7b15d370b449cae2f96d1dcd3c0a8",
    "figure_id": "2107.11170v3-Figure1-1",
    "image_file": "2107.11170v3-Figure1-1.png",
    "caption": " Accuracy v.s. FLOPs on ImageNet. Our SkipblockNet model trained with the proposed bias loss outperforms previous well-performing compact neural networks trained with the crossentropy loss.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest accuracy on ImageNet?",
    "answer": "SkipNet.",
    "rationale": "The figure shows the accuracy of different models on ImageNet, and SkipNet has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.11170v3",
    "pdf_url": null
  },
  {
    "instance_id": "5047101579a8418781f390ffc6e3f7f8",
    "figure_id": "2106.15845v2-Figure15-1",
    "image_file": "2106.15845v2-Figure15-1.png",
    "caption": " 10 generated molecules with the highest QED scores. The numbers are QED, SA, GSK3β, and JNK3 scores, respectively. We highlight the QED score in red among four different scores.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which molecule has the highest QED score?",
    "answer": "The molecule in the top left corner.",
    "rationale": "The QED score is highlighted in red. The molecule in the top left corner has the highest QED score of 0.95.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15845v2",
    "pdf_url": null
  },
  {
    "instance_id": "9b47adf007fe4067b17f2e4aec34e5ad",
    "figure_id": "2004.00184v2-Figure5-1",
    "image_file": "2004.00184v2-Figure5-1.png",
    "caption": " Superimposed SDR histograms of trained VAE decoder and encoder for different hidden layers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which hidden layer has the most similar SDR histograms for the encoder and decoder?",
    "answer": "The higher level hidden layer.",
    "rationale": "The figure shows that the SDR histograms for the encoder and decoder are most similar at the higher level hidden layer. This is because the higher level hidden layer represents more abstract features of the data, which are less sensitive to noise and variations in the input data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.00184v2",
    "pdf_url": null
  },
  {
    "instance_id": "b11d0d260df343b68e515acbdf2850c1",
    "figure_id": "1908.05005v3-Figure4-1",
    "image_file": "1908.05005v3-Figure4-1.png",
    "caption": " (a−c) CD and rCD for several network backbones of the DeepLabv3+ architecture evaluated on PASCAL VOC 2012, the Cityscapes dataset, and ADE20K. MobileNet-V2 is the reference model in each case. rCD and CD values below 100% represent higher robustness than the reference model. In almost every case, model robustness increases with model performance (i.e. mIoU on clean data). Xception-71 is the most robust network backbone on each dataset. (d) CD and rCD for non-DeepLabv3+ based models evaluated on Cityscapes. While CD decreases with increasing performance on clean data, rCD is larger than 100%.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network backbone is the most robust on the Cityscapes dataset according to the rCD metric?",
    "answer": "Xception-71",
    "rationale": "The rCD values for each network backbone are shown in Figure (b). Xception-71 has the lowest rCD value, which indicates that it is the most robust network backbone on the Cityscapes dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.05005v3",
    "pdf_url": null
  },
  {
    "instance_id": "ed3137786615438c96e44a719d3dd0d4",
    "figure_id": "2107.07075v2-Figure4-1",
    "image_file": "2107.07075v2-Figure4-1.png",
    "caption": " The final training error barrier between children on subsets of a 1000 highest (green) and lowest (orange) EL2N score examples, and randomly selected training subset (blue) as a function of the spawning time. Left to right: different dataset and network combinations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset and network combination has the largest difference in training error barrier between the highest and lowest EL2N score examples?",
    "answer": "CIFAR10 + ResNet18",
    "rationale": "The difference between the green and orange lines is largest for the CIFAR10 + ResNet18 plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.07075v2",
    "pdf_url": null
  },
  {
    "instance_id": "45ca06a9270a46f9986febb5f04f8c00",
    "figure_id": "2108.04628v1-Figure4-1",
    "image_file": "2108.04628v1-Figure4-1.png",
    "caption": " Visualization of the learned 3D object shape deformations: We visualize a template shape V and averaged shapes of 6 different fine-grained categories. Each shape characterizes a fine-grained category of, for instance, birds on (a,b) head, (c,d) body, and (e,f) tail types. We utilize such shape deformation as an additional cue to discriminate subtle intra-class variation for the object recognition task.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the bird's body is most responsible for the difference between the shapes in (a) and (b)?",
    "answer": "The head.",
    "rationale": "The figure shows the averaged shapes of six different fine-grained categories of birds. The shapes in (a) and (b) are both of the head of the bird. The only difference between the two shapes is the size and shape of the head.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.04628v1",
    "pdf_url": null
  },
  {
    "instance_id": "e0f930f257764f4abe7cdfe395f1e965",
    "figure_id": "2207.10909v2-Figure1-1",
    "image_file": "2207.10909v2-Figure1-1.png",
    "caption": " Statistics of latency, background ratio, and size distribution on both KITTI val (Geiger et al., 2012) and Waymo val (Sun et al., 2020) sets. (a) reveals that the MLP network occupies the largest latency. \"Q & G\" means query and grouping operation. (b) reflects that redundant background points significantly dominate the input points of each stage. (c) means the distribution on varying object sizes (measuring in 3 √ volume, where volume is the volume of ground truth).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stage of the model has the highest background ratio?",
    "answer": "Stage 1.",
    "rationale": "Figure (b) shows the background ratio for each stage of the model. The box plot for Stage 1 has the highest median value, indicating that it has the highest background ratio.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.10909v2",
    "pdf_url": null
  },
  {
    "instance_id": "5127595226454613884734d3e6785c93",
    "figure_id": "2203.06768v4-Figure16-1",
    "image_file": "2203.06768v4-Figure16-1.png",
    "caption": " Trading off recourse costs against robustness by choosing the invalidation target r in our PROBE framework. We generated recourses by setting r ∈ {0.20,0.25,0.30,0.35.0.40} and σ2 = 0.01 for the logistic regression classifier.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which dataset has the highest cost when the invalidation target is 0.20? ",
    "answer": " Adult",
    "rationale": " The figure shows that the cost for the Adult dataset is higher than the cost for the other two datasets when the invalidation target is 0.20. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.06768v4",
    "pdf_url": null
  },
  {
    "instance_id": "3eac06b0295b4429818eb597a6772caf",
    "figure_id": "2103.04568v2-Figure1-1",
    "image_file": "2103.04568v2-Figure1-1.png",
    "caption": " Empirical convergence of TOS for two different formulations ((Split 1) and (Split 2)) compared against FW for solving the relaxed QAP formulation (23). The [top] row corresponds to the results for the chr12a dataset and the [bottom] row for the esc128 dataset (from QAPLIB). In both cases, TOS exhibits locally linear convergence whereas FW converges sublinearly.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges faster, TOS or FW?",
    "answer": "TOS converges faster than FW.",
    "rationale": "The plots show that the nonstationarity error and infeasibility error for TOS decrease more rapidly than for FW.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.04568v2",
    "pdf_url": null
  },
  {
    "instance_id": "a208330ae72444dab1394e649b5a4e38",
    "figure_id": "1805.02211v3-Figure6-1",
    "image_file": "1805.02211v3-Figure6-1.png",
    "caption": " Query length distribution with respect to number of terms and characters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of queries have more than 9 terms?",
    "answer": "Less than 5%.",
    "rationale": "The bar chart in (a) shows that the percentage of queries with more than 9 terms is less than 5%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.02211v3",
    "pdf_url": null
  },
  {
    "instance_id": "974269726bd844dd80c82563d310fbb5",
    "figure_id": "2302.14372v2-Figure15-1",
    "image_file": "2302.14372v2-Figure15-1.png",
    "caption": " Offline learning curves with 1.2 million iterations. The x-axis is the number of iterations and the y-axis is the normalized score. Performance was averaged over 5 random seeds, after using a smoothing window of size 10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the Hopper environment?",
    "answer": "InAc",
    "rationale": "The InAc curve is the highest for the Hopper Expert and Hopper M-Expert environments.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.14372v2",
    "pdf_url": null
  },
  {
    "instance_id": "d1b26b35da704821a704f540f38bb99a",
    "figure_id": "2207.03264v2-Figure1-1",
    "image_file": "2207.03264v2-Figure1-1.png",
    "caption": " Hybrid GDSolver Architecture",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two main steps involved in training a hybrid model?",
    "answer": "Gradient descent and solver-based training.",
    "rationale": "The figure shows that the untrained model is first fed into a gradient descent step, which is then fed into a solver-based training step. The output of the solver-based training step is the final hybrid trained model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.03264v2",
    "pdf_url": null
  },
  {
    "instance_id": "9b106d3e12e54e0e80a90bc680271a8c",
    "figure_id": "1910.00051v2-Figure4-1",
    "image_file": "1910.00051v2-Figure4-1.png",
    "caption": " A partial derivation of the string in Figure 3. The stack operations follow closely each step in the derivation, where GEN-FRAG and GEN-LABEL are invoked when rewriting a non-terminal T and a terminal L respectively. In the result of each step, the leftmost function is underlined, and is rewritten in the fragment in blue in the next step. On the other hand, a REDUCE operation is invoked when a generated fragment does not contain non-terminals T to expand further (in this partial derivation, this is the case of the result of production r2).",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which action in the table corresponds to rewriting a non-terminal T?",
    "answer": "GEN-FRAG",
    "rationale": "The caption states that \"GEN-FRAG and GEN-LABEL are invoked when rewriting a non-terminal T and a terminal L respectively.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.00051v2",
    "pdf_url": null
  },
  {
    "instance_id": "72f8de52c39a42d1a6fb47b650c5c600",
    "figure_id": "2207.00429v1-Figure1-1",
    "image_file": "2207.00429v1-Figure1-1.png",
    "caption": " Compositional RL problem graph.",
    "figure_type": "** \n\nSchematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \n\nWhich of the following masses is closest to the source of force F_5?",
    "answer": " \n\nm_5",
    "rationale": " \n\nThe image shows a graph where the masses are represented by boxes and the forces are represented by arrows. The force F_5 is applied to the mass m_5, which is therefore closest to the source of the force.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.00429v1",
    "pdf_url": null
  },
  {
    "instance_id": "54a6933a265b444faac05f97e8f8416b",
    "figure_id": "2308.08871v2-Figure9-1",
    "image_file": "2308.08871v2-Figure9-1.png",
    "caption": " We train models on SMAL r and test on TOSCA r.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models in the figure is most similar to the source image?",
    "answer": "Ours",
    "rationale": "The \"Ours\" model is the most similar to the source image in terms of the overall shape and pose of the cat.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.08871v2",
    "pdf_url": null
  },
  {
    "instance_id": "8b2ee353c42b4432a9158e9230cc35f1",
    "figure_id": "2112.02813v1-Figure7-1",
    "image_file": "2112.02813v1-Figure7-1.png",
    "caption": " Experimental results comparing MDPGT, MDPG and DPG in the lineworld environment with 5 agents (left) and 10 agents (right) using fully-connected topology.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in the lineworld environment with 5 agents?",
    "answer": "MDPGT",
    "rationale": "The figure shows the rewards obtained by each algorithm over the course of 10,000 trajectories. In the left plot, which shows the results for the lineworld environment with 5 agents, MDPGT consistently achieves higher rewards than the other two algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.02813v1",
    "pdf_url": null
  },
  {
    "instance_id": "1b9375d867e64069a865704ea3f99c44",
    "figure_id": "2204.00706v1-Figure1-1",
    "image_file": "2204.00706v1-Figure1-1.png",
    "caption": " Empirical means of Ut versus t averaged over 100 trials over t ∈ [1 : 50000]. Left is case 1 (with multiple optimal policies), right case 2 (with a single optimal policy supported on one arm).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest expected reward at time step 50000 in case 1?",
    "answer": "BwCR",
    "rationale": "The figure shows that the blue line, which represents the BwCR algorithm, is above the other two lines at time step 50000. This indicates that BwCR has the highest expected reward at that time step.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.00706v1",
    "pdf_url": null
  },
  {
    "instance_id": "c6d37aaf48b94cf69a22247b81c529da",
    "figure_id": "2306.00006v3-Figure3-1",
    "image_file": "2306.00006v3-Figure3-1.png",
    "caption": " (a) and (b) are respectively the Euclidean distance statistics of the homophily (NN) edges that connect normal nodes and the non-homophily (N-A) edges that connect normal and abnormal nodes on BlogCatalog and Amazon. (c) Homophily of normal nodes vs. (d) the number of non-homophily edges with increasing truncation iterations/depths.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which dataset exhibits a higher degree of homophily, BlogCatalog or Amazon?",
    "answer": "BlogCatalog.",
    "rationale": "The homophily of normal nodes in BlogCatalog is higher than that in Amazon as shown in figure (c). The non-homophily edges in BlogCatalog decrease faster than those in Amazon as the truncation depth increases as shown in figure (d). This means that there are fewer connections between normal and abnormal nodes in BlogCatalog than in Amazon.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.00006v3",
    "pdf_url": null
  },
  {
    "instance_id": "2f4b82b835394940806f650684557316",
    "figure_id": "2208.11663v1-Figure7-1",
    "image_file": "2208.11663v1-Figure7-1.png",
    "caption": " Example of a linearized input and output for PEER-Edit. Newlines are added for better readability and not part of the model input; inputs, outputs and provided reference documents are slightly shortened.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the name of the long jumper who competed at the 2008 Summer Olympics and is a 4-time World Champion?",
    "answer": "Brittney Reese",
    "rationale": "The passage states that \"Brittney Reese\" is a long jumper who competed at the 2008 Summer Olympics and is a 4-time World Champion.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.11663v1",
    "pdf_url": null
  },
  {
    "instance_id": "b13a21da2ee84d83b3f21609a83ba948",
    "figure_id": "2102.06477v3-Figure2-1",
    "image_file": "2102.06477v3-Figure2-1.png",
    "caption": " Results on the motivating example described in Section 2 (σ = 0.05). We see that the marginal posteriors get sharper around the ground truth parameter when more observations are available.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods is the most efficient in terms of the number of observations needed to reach a certain level of accuracy?",
    "answer": "h-ABC.",
    "rationale": "The figure shows that h-ABC consistently requires fewer observations than the other two methods to achieve the same level of accuracy. This is evident from the fact that the h-ABC curve is always below the other two curves.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.06477v3",
    "pdf_url": null
  },
  {
    "instance_id": "e4d1b0bda2ff4ec2b25bde2f3953ed7d",
    "figure_id": "2106.02034v2-Figure5-1",
    "image_file": "2106.02034v2-Figure5-1.png",
    "caption": " Visualization of the progressively sparsified tokens. We show the original input image and the sparsification results after the three stages, where the masks represent the corresponding tokens are discarded. We see our method can gradually focus on the most representative regions in the image. This phenomenon suggests that the DynamicViT has better interpretability.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stage of sparsification best captures the most important information in the image?",
    "answer": "Stage 3.",
    "rationale": "The figure shows that as the stages progress, the images become more and more sparse, but the important features of the images are still preserved. This suggests that the later stages of sparsification are able to capture the most important information in the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02034v2",
    "pdf_url": null
  },
  {
    "instance_id": "aecdea81cc2b4427a0b5652ca3e995b1",
    "figure_id": "2105.04556v2-Figure6-1",
    "image_file": "2105.04556v2-Figure6-1.png",
    "caption": " The model predicts the instance of tray (on the left) which is closer to the fruits (goal objects) than the one on the right.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tray is closer to the fruits?",
    "answer": "The tray on the left is closer to the fruits.",
    "rationale": "The image shows a robot in a room with two trays. The tray on the left is closer to the fruits than the tray on the right.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.04556v2",
    "pdf_url": null
  },
  {
    "instance_id": "512c490961a44567a3e27d3515f28101",
    "figure_id": "2204.05673v1-Figure4-1",
    "image_file": "2204.05673v1-Figure4-1.png",
    "caption": " Association heatmap of BERT-Large on the verb dataset. The objects (sources) on the y-axis are grouped by the room in which they are most likely to be located according to the HowToKB Dataset.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object is most associated with the verb \"wash\" according to the cosine score?",
    "answer": "Clothes",
    "rationale": "The heatmap shows the association scores between objects and verbs. The cosine score is a measure of similarity between two vectors. In this case, the vectors represent the embeddings of the objects and verbs. The higher the cosine score, the more similar the two vectors are. The object \"clothes\" has the highest cosine score with the verb \"wash\", which indicates that they are most associated with each other.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.05673v1",
    "pdf_url": null
  },
  {
    "instance_id": "00c279d1d9754fa0895cb558cebabc2e",
    "figure_id": "1903.11328v2-Figure15-1",
    "image_file": "1903.11328v2-Figure15-1.png",
    "caption": " Comparison of video summaries generated with (A) random and (B) DR-DSN importance scores.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method of scoring importance leads to more videos being judged \"A Much More Than B\" or \"A More Than B\"?",
    "answer": "DR-DSN",
    "rationale": "The figure shows that for most of the videos, the bars for \"A Much More Than B\" and \"A More Than B\" are higher for DR-DSN than for random scoring.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.11328v2",
    "pdf_url": null
  },
  {
    "instance_id": "f988b5124f7641d8b4a3dd95fdad39f7",
    "figure_id": "2307.01827v2-Figure9-1",
    "image_file": "2307.01827v2-Figure9-1.png",
    "caption": " Experiments of reconstruction from models trained on a a fixed training set size (500 samples) for different number of classes. Number of “good\" reconstruction is shown for each model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest number of good reconstructions?",
    "answer": "The model trained on 10 classes.",
    "rationale": "The number of good reconstructions is shown in the top right corner of each plot. The plot for the 10-class model shows the highest number of good reconstructions, which is 68.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.01827v2",
    "pdf_url": null
  },
  {
    "instance_id": "32c3f9d00a424eabb6e85920a17d524a",
    "figure_id": "2202.03958v2-Figure7-1",
    "image_file": "2202.03958v2-Figure7-1.png",
    "caption": " The t-SNE visualization on unseen PACS domain.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method appears to have better separation between clusters?",
    "answer": "Ours",
    "rationale": "The \"Ours\" plot shows more distinct clusters with less overlap between different colors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.03958v2",
    "pdf_url": null
  },
  {
    "instance_id": "69dda390115243c6bde187d5126ef740",
    "figure_id": "2201.06503v3-Figure8-1",
    "image_file": "2201.06503v3-Figure8-1.png",
    "caption": " The upper bounds (UB) in Eq. (11) and Eq. (12) under full-timesteps (K=N ) and 100- timesteps (K=100) trajectories on CIFAR10 (LS) and CIFAR10 (CS).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the tighter upper bound on CIFAR10 (LS) with 100 timesteps?",
    "answer": "Eq. (12)",
    "rationale": "In Figure (b), the green dotted line representing the upper bound in Eq. (12) is below the red dotted line representing the upper bound in Eq. (11). This indicates that Eq. (12) has a tighter upper bound than Eq. (11) for CIFAR10 (LS) with 100 timesteps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.06503v3",
    "pdf_url": null
  },
  {
    "instance_id": "92fc0049467245fabe24bf2b69424436",
    "figure_id": "2210.05528v1-Figure4-1",
    "image_file": "2210.05528v1-Figure4-1.png",
    "caption": " Accuracy-computation cost curves for cascading with MaxProb (in blue) and Random baseline (in black) methods in K=3 setting. Accuracy-cost values of individual models M1, M2, and M3 are shown in red. Note that M1 here is different from M1 in Figure 2. MaxProb outperforms Random baseline as it achieves higher AUC.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest accuracy and FLOPs cost?",
    "answer": "Model M3.",
    "rationale": "Model M3 is represented by the red dot on the Accuracy-FLOPs curves for each dataset. This red dot is the furthest to the right on the x-axis, which means it has the highest FLOPs cost, and the highest on the y-axis, which means it has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.05528v1",
    "pdf_url": null
  },
  {
    "instance_id": "2ec5ceb605904b999b1e42baa946f2fe",
    "figure_id": "2110.08263v3-Figure2-1",
    "image_file": "2110.08263v3-Figure2-1.png",
    "caption": " Average running time of one iteration on a single GeForce RTX 3090 GPU.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the shortest running time with CPL?",
    "answer": "Pseudo-Label",
    "rationale": "The bar corresponding to the Pseudo-Label method with CPL is the shortest among all the bars in the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.08263v3",
    "pdf_url": null
  },
  {
    "instance_id": "2548f08cc3de4ac2a6365a234ba7572f",
    "figure_id": "2204.01264v1-Figure5-1",
    "image_file": "2204.01264v1-Figure5-1.png",
    "caption": " Qualitative comparison on probabilistic shape completion of a single object. cGCA is the only method that can produce a continuous surface.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most complete and detailed 3D shapes?",
    "answer": "cGCA",
    "rationale": "The figure shows that cGCA is the only method that can produce a continuous surface, while the other methods produce incomplete or fragmented shapes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.01264v1",
    "pdf_url": null
  },
  {
    "instance_id": "73af924b00bd4aa79f7ab9abc1d6e454",
    "figure_id": "2307.09858v1-Figure2-1",
    "image_file": "2307.09858v1-Figure2-1.png",
    "caption": " Reliability diagrams for GCN on CiteSeer w/o calibration. The confidence of a well-calibrated model should closely match its accuracy, namely, aligned with the diagonal. Below the diagonal represents over-confident, and above the diagonal represents under-confident.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does it mean when the output accuracy is below the diagonal line?",
    "answer": "The model is over-confident.",
    "rationale": "The diagonal line in the reliability diagrams represents the ideal case where the model's confidence matches its accuracy. When the output accuracy is below the diagonal line, it means that the model is more confident in its predictions than it should be.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.09858v1",
    "pdf_url": null
  },
  {
    "instance_id": "2364abfb5fbb49fbbd4ae687bede69eb",
    "figure_id": "2106.05933v2-Figure93-1",
    "image_file": "2106.05933v2-Figure93-1.png",
    "caption": " Sparsity over layers for wav2vec-base finetuned for Russian ru at 80% sparsity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the sparsity of the 6th layer of the wav2vec-base model when it is fine-tuned for Russian at 80% sparsity?",
    "answer": "The sparsity of the 6th layer is 75.772%.",
    "rationale": "The figure shows a bar chart with the sparsity of each layer of the wav2vec-base model. The x-axis of the chart shows the layer number, and the y-axis shows the sparsity percentage. The 6th layer is labeled \"nu_bert_0.8_mask\", and its corresponding bar shows a sparsity of 75.772%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05933v2",
    "pdf_url": null
  },
  {
    "instance_id": "57696c71735e43b59d179d5a44c5f4b4",
    "figure_id": "2212.06988v1-Figure6-1",
    "image_file": "2212.06988v1-Figure6-1.png",
    "caption": " We compare RAEB to two RAEB variants on Delivery Ant and Electric Ant. The results show that these variants struggle to achieve outstanding performance in both environments compared to RAEB.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the Delivery Ant environment?",
    "answer": "RAEB.",
    "rationale": "The figure shows the average return of three algorithms on the Delivery Ant environment. RAEB has the highest average return, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.06988v1",
    "pdf_url": null
  },
  {
    "instance_id": "8b45957f53934da592be518ca6fa71bb",
    "figure_id": "1904.12257v2-Figure4-1",
    "image_file": "1904.12257v2-Figure4-1.png",
    "caption": " Qualitative evaluations on Video Deblurring Dataset [36]. The proposed method generates much sharper images with higher PSNR and SSIM.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the sharpest image?",
    "answer": "The ground truth image is the sharpest image.",
    "rationale": "The figure shows the results of different deblurring methods on a blurry image. The ground truth image is the original image, which is not blurry.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.12257v2",
    "pdf_url": null
  },
  {
    "instance_id": "8204f96fe9584198bf5bbdf99b62eed0",
    "figure_id": "2204.01016v1-Figure10-1",
    "image_file": "2204.01016v1-Figure10-1.png",
    "caption": " Performance at NER for English (en)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which acquisition function performs best for English NER in the second round?",
    "answer": "MNLP",
    "rationale": "The plot shows the performance of different acquisition functions for English NER across three rounds. The performance is measured on the y-axis, and the round is measured on the x-axis. MNLP has the highest performance in the second round.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.01016v1",
    "pdf_url": null
  },
  {
    "instance_id": "7fe91e40670e4c13b453c6d73cba1ba9",
    "figure_id": "1908.06917v1-Figure4-1",
    "image_file": "1908.06917v1-Figure4-1.png",
    "caption": " Undirected relation example (dbo:sisterStation) that reflects bi-directional association between the adjacent entities (Missouri radio stations). LC-QuAD question #4486: “In which city is the sister station of KTXY located?” (correct answer: dbr:California,Missouri, dbr:Missouri; missing answer: dbr:Eldon,Missouri). DBpedia does not model bidirectional relations and the relation direction is selected at random in these cases. LC-QuAD does not reflect bidirectionality either by picking only one of the directions as the correct one and rejecting correct solutions (dbr:KZWY → dbr:Eldon,Missouri). QAmp was able to retrieve this false negative sample due to the default undirectionality assumption built into the question interpretation model.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which radio station is located in California, Missouri?",
    "answer": "KATi 94.3 COUNTRY",
    "rationale": "The figure shows a network of radio stations in Missouri. Each node represents a radio station, and the edges represent the \"sister station\" relationship between them. The node for KATi 94.3 COUNTRY is connected to the node for California, Missouri, indicating that this radio station is located in that city.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.06917v1",
    "pdf_url": null
  },
  {
    "instance_id": "0ffe3f34538345f3a88ff342c1be0dbe",
    "figure_id": "2204.12288v1-Figure4-1",
    "image_file": "2204.12288v1-Figure4-1.png",
    "caption": " Parameter studies.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which parameter has the biggest impact on Recall@10 and MRR@10?",
    "answer": "Dimension",
    "rationale": "The figure shows that Recall@10 and MRR@10 increase with increasing dimension, while the other parameters have a smaller impact.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.12288v1",
    "pdf_url": null
  },
  {
    "instance_id": "ec6c5edc1d184ad6ad28eb52bb50347a",
    "figure_id": "2001.03483v1-Figure8-1",
    "image_file": "2001.03483v1-Figure8-1.png",
    "caption": " Comparison of different classification results. We trained several models from scratch (■) or fine-tuned pre-trained models, where all the weights (▼), the last block (●) or the last layer (⭑) were trainable. Further, we trained a SVM using HOG features. We used the standard X5 training data (F, in blue) or replaced half of it with the randomized data (F&R, in green). After training, we retained models with the best total accuracy on the X5 test data and evaluate them on the i3 and Tucson test data. The models have difficulties to generalize to the test data and perform even worse in unknown vehicles, but including the randomized data helps to generalize to unseen objects.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the X5 test data when trained with the F&R data?",
    "answer": "SqueezeNet",
    "rationale": "The figure shows that the SqueezeNet model trained with the F&R data achieved the highest total classification accuracy on the X5 test data. This is evident from the green square marker being the highest among the green markers in the first plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.03483v1",
    "pdf_url": null
  },
  {
    "instance_id": "91d281882b4d48158785f9013c4295f5",
    "figure_id": "1904.04562v1-Figure4-1",
    "image_file": "1904.04562v1-Figure4-1.png",
    "caption": " Performance curve of the proposed DVNs for the joint learning on CIFAR-10 (task 1) and CIFAR-100 (task 2).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the performance of the DVNs for task 1 compare to the performance for task 2?",
    "answer": "The DVNs perform better on task 1 than on task 2.",
    "rationale": "The accuracy for task 1 is higher than the accuracy for task 2 for both the single unit and all units configurations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.04562v1",
    "pdf_url": null
  },
  {
    "instance_id": "0b105225e0c54f9080d3d41a2a723553",
    "figure_id": "1906.09531v2-Figure3-1",
    "image_file": "1906.09531v2-Figure3-1.png",
    "caption": " Estimation error δ(v) = v(πe)− v̂H(πe) for different values of H (minimum 0, maximum 100). Shaded area denotes standard error over different random seeds.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three tasks has the lowest estimation error for H=100?",
    "answer": "HalfCheetah.",
    "rationale": "The plot for HalfCheetah has the lowest y-value at H=100.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.09531v2",
    "pdf_url": null
  },
  {
    "instance_id": "3c6619ba242b45e4a36319b475d4f63d",
    "figure_id": "2006.06983v4-Figure13-1",
    "image_file": "2006.06983v4-Figure13-1.png",
    "caption": " Decoupling is verified on M-Type.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does decoupling improve the accuracy of the M-Type model?",
    "answer": "Yes.",
    "rationale": "The figure shows that the accuracy of the \"Heter-aware, coupled\" model is higher than the accuracy of the \"Heter-aware, decoupled\" model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06983v4",
    "pdf_url": null
  },
  {
    "instance_id": "291456b6abc4426d9236154eee0c94c2",
    "figure_id": "1909.09754v2-Figure7-1",
    "image_file": "1909.09754v2-Figure7-1.png",
    "caption": " Online solutions at time instances t = {3.5, 7.0, 10.5, 14} computed by the FOM, POD–LSPG, conservative LSPG, Deep LSPG and Deep Conservation. All conservative methods employ NΩ̄ = 1 subdomains with the autoencoder of the latent dimension p = 4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method(s) are most accurate at predicting the behavior of the conserved variable u at time t = 7.0?",
    "answer": "FOM and Deep LSPG.",
    "rationale": "The figure shows that the FOM and Deep LSPG lines are closest to the actual values of the conserved variable u at time t = 7.0. This can be seen by comparing the lines to the black squares in the figure, which represent the actual values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.09754v2",
    "pdf_url": null
  },
  {
    "instance_id": "4838c34790f04ec49d91255215f39a92",
    "figure_id": "2011.08900v1-Figure10-1",
    "image_file": "2011.08900v1-Figure10-1.png",
    "caption": " Subject recognition accuracy as a function of gesture class, sorted by accuracy. Some gestures allow subjects to be more easily recognized than others. Table in Supplementary Material shows the original data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which gesture allows subjects to be recognized most easily?",
    "answer": "Gesture 80.",
    "rationale": "The figure shows that the accuracy for gesture 80 is the highest, at nearly 30%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.08900v1",
    "pdf_url": null
  },
  {
    "instance_id": "bd171438e068495793f7cec0964b18bf",
    "figure_id": "2012.00073v2-Figure2-1",
    "image_file": "2012.00073v2-Figure2-1.png",
    "caption": " Cell-level grouping strategy, illustrated on a hypothetical input matrix with 60 events and 10 features. The pruning algorithm grouped the first 53 events together. Events −4 and 0, and features 3, 6, and 9 were considered relevant (Shapley value greater than \\ ).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many features are considered relevant in the image?",
    "answer": "Three features are considered relevant.",
    "rationale": "The figure shows that features 3, 6, and 9 have Shapley values greater than  , which means they are considered relevant.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.00073v2",
    "pdf_url": null
  },
  {
    "instance_id": "8214211d938a4d8da74ef0691ceab21a",
    "figure_id": "2205.00477v1-Figure2-1",
    "image_file": "2205.00477v1-Figure2-1.png",
    "caption": " (a) Test errors of the RF predictors (solid lines) and kernel predictors (dashed lines) w.r.t. different regularization. Note that, the ridgeless RF predictors exhibit a double descent curve. (b) Test accuracies of the compared methods on the MNIST dataset. (c) Training loss (solid lines) and the trace of kernel Tr(K) (dashed lines) of the RF predictors on the MNIST dataset.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest test error for the MNIST dataset?",
    "answer": "RFTK.",
    "rationale": "The test errors of the compared methods are shown in subfigure (a). The RFTK method has the lowest test error of all the methods, as shown by the green line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.00477v1",
    "pdf_url": null
  },
  {
    "instance_id": "eb6c013b967e4b99a3d639fddbb8ed67",
    "figure_id": "2111.08550v3-Figure7-1",
    "image_file": "2111.08550v3-Figure7-1.png",
    "caption": " The transfer results of the hyper-controller among PyBullet environments with A2 3 = 6 transferring cases in total. Each figure’s title represents the target task, and each specific line indicates the source task. For example, the green line in the middle figure represents transferring the hyper-controller learned on HopperBullet to Walker2dBullet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task had the highest average return after 100k steps?",
    "answer": "Walker2dBullet",
    "rationale": "The figure shows the average return for each task over 100k steps. The Walker2dBullet task has the highest average return at the end of the training period.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.08550v3",
    "pdf_url": null
  },
  {
    "instance_id": "9ba485af1a2c4a6fba5682331f91df02",
    "figure_id": "2306.16195v1-Figure4-1",
    "image_file": "2306.16195v1-Figure4-1.png",
    "caption": " The embeddings of entities (in red) and generic words (in blue) contained in the post projected by t-SNE. We extract these embeddings from 30 conversational pairs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, GNN + BART or Ours + BART, seems to produce more distinct clusters of entities and generic words?",
    "answer": "Ours + BART.",
    "rationale": "In the figure, the embeddings of entities and generic words are projected by t-SNE. This method allows us to visualize high-dimensional data in a lower-dimensional space. In the figure, the entities are shown in red and the generic words are shown in blue. We can see that in the figure for Ours + BART, the red and blue points are more clearly separated into distinct clusters than in the figure for GNN + BART. This suggests that Ours + BART is better at separating entities from generic words.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.16195v1",
    "pdf_url": null
  },
  {
    "instance_id": "0cc3a4ff2871495689f10cf67249fc6b",
    "figure_id": "2201.10075v2-Figure12-1",
    "image_file": "2201.10075v2-Figure12-1.png",
    "caption": " Evaluating the per-frame synthesis quality when performing 8× interpolation on the XTEST-2K [56] benchmark.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods evaluated in the figure has the highest PSNR values across all frames?",
    "answer": "Ours",
    "rationale": "The green line in the figure, which represents the \"Ours\" method, is consistently above the other two lines, indicating that it has the highest PSNR values for all frames.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.10075v2",
    "pdf_url": null
  },
  {
    "instance_id": "86d31665141d40b6b441b90403413eac",
    "figure_id": "2111.07117v1-Figure7-1",
    "image_file": "2111.07117v1-Figure7-1.png",
    "caption": " Examples of the four dataset used in this work.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows a robot arm manipulating objects?",
    "answer": "GQN-Jaco",
    "rationale": "The GQN-Jaco dataset shows a robot arm manipulating a purple ball in a green environment. This can be seen in the second row of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.07117v1",
    "pdf_url": null
  },
  {
    "instance_id": "8b31b9b615fd4587b5aa707193af9979",
    "figure_id": "2309.07499v1-Figure9-1",
    "image_file": "2309.07499v1-Figure9-1.png",
    "caption": " Visual Evaluation Results : mFR. y-axis : mFR, x-axis : CLIP Student Network architectures, vertical bar : Teacher. This figure analyses our method on the ImageNet-P dataset when various CLIP model architectures are used as students (x-axis) and tuned using various single modal networks as teacher (vertical bar). Vertical bar labelled None correspond to APT baseline. It can be observed that out method provides significant gains over the APT baseline, especially when ViT-S model is used as the teacher.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which CLIP student network architecture achieves the highest mFR when ViT-S is used as the teacher?",
    "answer": "ViT-B",
    "rationale": "The figure shows that the mFR for ViT-B is the highest when ViT-S is used as the teacher.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.07499v1",
    "pdf_url": null
  },
  {
    "instance_id": "cfec86fc4f9d40049ce163876d246b43",
    "figure_id": "2006.07972v2-Figure8-1",
    "image_file": "2006.07972v2-Figure8-1.png",
    "caption": " Temporal relative R2 over the US mainland of ML models discussed in section 4 for temperature prediction over 2017-2018. Large positive values (green) closer to 1 indicates better predictive skills.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models performed best in the south-central region of the US?",
    "answer": "CNN-LSTM",
    "rationale": "The figure shows that the CNN-LSTM model has the largest positive values (green) in the south-central region of the US, indicating better predictive skills compared to other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.07972v2",
    "pdf_url": null
  },
  {
    "instance_id": "7f11cd1c865f4b42aaef658f093a28b8",
    "figure_id": "1912.02413v4-Figure4-1",
    "image_file": "1912.02413v4-Figure4-1.png",
    "caption": " `2-norm of classifier weights for different learning manners. Specifically, “BBN-ALL” indicates the `2-norm of the combination of Wc and Wr in our model. σ in the legend is the standard deviation of `2-norm for ten classes.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which learning method has the lowest average L2 norm of classifier weights across all classes?",
    "answer": " CE",
    "rationale": " The L2 norm of classifier weights is represented by the height of the bars in the plot. The legend shows the average L2 norm (σ) for each learning method across all classes. The lowest average L2 norm is for CE (σ = 0.422).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.02413v4",
    "pdf_url": null
  },
  {
    "instance_id": "e65d93293d7f4f93813758318b2b18c6",
    "figure_id": "2106.10807v1-Figure6-1",
    "image_file": "2106.10807v1-Figure6-1.png",
    "caption": " An example heatmap of an untargeted poisoning measuring the attack distribution on the crafting network. Note how this attack “well distributes” the target labels (i.e. not every class is perturbed into the same class). We hypothesize this is important for a successful attack.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class is most likely to be misclassified as a \"ship\"?",
    "answer": "\"Truck\"",
    "rationale": "The heatmap shows the proportion of each true class that is classified as each target class. The square in the row for \"truck\" and the column for \"ship\" is the brightest, indicating that the highest proportion of \"truck\" images are misclassified as \"ship\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.10807v1",
    "pdf_url": null
  },
  {
    "instance_id": "3c722d18c4ff4ca8ae4046cec97573f8",
    "figure_id": "2307.05721v1-Figure2-1",
    "image_file": "2307.05721v1-Figure2-1.png",
    "caption": " The Generic Assembly Box consists of 11 standard parts and 25 non-standard parts and requires 4 different standard tools during assembly.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following tools is required to assemble the Generic Assembly Box?\n(A) A screwdriver\n(B) A wrench\n(C) A hammer\n(D) A saw",
    "answer": "(A) and (B)",
    "rationale": "The photograph shows several tools, including screwdrivers and wrenches. The caption states that four different standard tools are required for assembly. While it is possible that other tools are also required, the photograph provides evidence that screwdrivers and wrenches are among the tools needed.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.05721v1",
    "pdf_url": null
  },
  {
    "instance_id": "51e95514eac94d538d496c4989504f61",
    "figure_id": "1911.08618v1-Figure3-1",
    "image_file": "1911.08618v1-Figure3-1.png",
    "caption": " Examples with different approaches in each column for improving attention using explanation in a self supervised manner. The first column indicates the given target image and its question and answer. Starting from second column, it indicates the Attention map (left) / Grad-CAM map (right) for Stack Attention Network, MSE based approach, Coral based approach, MMD based approach, Adversarial based approach respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach appears to be focusing on the most relevant regions of the image for answering the question \"Would it be a good idea to put goldfish inside the open part of this fixture?\"",
    "answer": "The MMD approach.",
    "rationale": "The MMD approach has the highest concentration of red in the open part of the toilet, which is the most relevant region for answering the question. The other approaches, such as SAN and MSE, have more diffused attention maps, which suggests that they are not focusing on the most relevant regions of the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.08618v1",
    "pdf_url": null
  },
  {
    "instance_id": "cee6cfc677b44066baa99963513b3395",
    "figure_id": "2209.07924v3-Figure8-1",
    "image_file": "2209.07924v3-Figure8-1.png",
    "caption": " The qualitative results for 6 datasets. For each class in all datasets, multiple explanation graphs with the class probability of 1 predicted by the GNNs are displayed. If the dataset has the node feature or edge feature, the different colors in the nodes and edges correspondingly represent different values in the node feature and edge feature.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset(s) contain molecules that are cyclic?",
    "answer": "MUTAG, Red Cyclic, Green Cyclic, Cyclic",
    "rationale": "The figure shows that the MUTAG dataset contains molecules that are both cyclic and acyclic. The Red Cyclic and Green Cyclic datasets only contain molecules that are cyclic. The Cyclic dataset only contains molecules that are cyclic.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.07924v3",
    "pdf_url": null
  },
  {
    "instance_id": "a547e7fbd1b34fbb955899c6acf9cf7c",
    "figure_id": "2006.09447v4-Figure5-1",
    "image_file": "2006.09447v4-Figure5-1.png",
    "caption": " Average returns comparison of LIAM against six ablated versions of LIAM in the double speaker-listener environment.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the ablated versions of LIAM performs the best in the double speaker-listener environment?",
    "answer": "LIAM-No-Act-Recon",
    "rationale": "The plot shows that LIAM-No-Act-Recon has the highest average return of all the ablated versions of LIAM.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.09447v4",
    "pdf_url": null
  },
  {
    "instance_id": "55b886a9c4cf4761a7436b72cade08d5",
    "figure_id": "2010.04893v1-Figure3-1",
    "image_file": "2010.04893v1-Figure3-1.png",
    "caption": " Training curves on MuJoCo-v2 benchmarks. Solid curves are average returns over 7 runs, and shaded areas indicate one standard deviation. The dashed lines are the returns of SAC and PPO at the maximum training steps (200k steps for the first three tasks, and 300k steps for Humanoid-v2), and the dotted lines are the returns at 1 million steps (retrived from the original papers [9, 20] and opensourced benchmarks [6, 1]).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the performance of SAC compare to PPO on the HalfCheetah task?",
    "answer": "SAC outperforms PPO on the HalfCheetah task.",
    "rationale": "The figure shows that SAC (green curve) achieves a higher return than PPO (purple curve) at the end of training (200k steps).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.04893v1",
    "pdf_url": null
  },
  {
    "instance_id": "6e0d0b658024428ea5253c3759851aab",
    "figure_id": "2210.01542v1-Figure13-1",
    "image_file": "2210.01542v1-Figure13-1.png",
    "caption": " Performance ablating either spectral normalization or rescaling from our Hyperbolic PPO agent stabilized with S-RYM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four tasks shown in the figure is the easiest for the Hyperbolic PPO + S-RYM agent to learn?",
    "answer": "Fruitbot.",
    "rationale": "The Hyperbolic PPO + S-RYM agent achieves the highest episodic return on the Fruitbot task, and it does so in the fewest number of training steps. This suggests that the Fruitbot task is the easiest for the agent to learn.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.01542v1",
    "pdf_url": null
  },
  {
    "instance_id": "a3f9ef7f001a40f99be9340ae9f48825",
    "figure_id": "2303.02760v2-Figure8-1",
    "image_file": "2303.02760v2-Figure8-1.png",
    "caption": " Example generations with five scenes from a diffusion generative model trained on Human-Art. Notably, Shadow Play is a novel scene for existing generative models.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the scenes is most likely to be novel for existing generative models?",
    "answer": "Shadow Play.",
    "rationale": "The caption states that \"Shadow Play is a novel scene for existing generative models.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.02760v2",
    "pdf_url": null
  },
  {
    "instance_id": "792b42e07fa7460bac23d1d7597619ef",
    "figure_id": "1810.01032v4-Figure4-1",
    "image_file": "1810.01032v4-Figure4-1.png",
    "caption": " Learning curves from five reward robust RL algorithms on CartPole game with true rewards (r) , noisy rewards",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the CartPole game with true rewards?",
    "answer": "DDQN",
    "rationale": "The figure shows that DDQN has the lowest number of steps per episode, which means that it is able to learn the task more quickly and efficiently than the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.01032v4",
    "pdf_url": null
  },
  {
    "instance_id": "23abe35b6b564ee99c3b7e051812cfe4",
    "figure_id": "2310.17139v1-Figure7-1",
    "image_file": "2310.17139v1-Figure7-1.png",
    "caption": " Ablation studies on 6 D4RL tasks over 3 seeds with one standard error shaded.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method consistently achieves the highest cumulative returns across all tasks and expert levels?",
    "answer": "MICO + RS + EBS",
    "rationale": "The figure shows the cumulative returns for different methods on 6 D4RL tasks. The lines for MICO + RS + EBS are consistently higher than the lines for the other methods, indicating that it achieves the highest cumulative returns.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.17139v1",
    "pdf_url": null
  },
  {
    "instance_id": "7d14ea5925914683a9c3ece4d48be870",
    "figure_id": "2204.05426v2-Figure5-1",
    "image_file": "2204.05426v2-Figure5-1.png",
    "caption": " Accuracy of human annotations when provided with PROTOTEX explanations or PROTOTEX explanations + prediction. Model Performance: the accuracy of the model generating the explanations. Baseline: Annotation accuracy without explanations. Random: Randomly selected examples for explanation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which condition has the highest accuracy?",
    "answer": "Model + Explanations",
    "rationale": "The figure shows that the accuracy of the \"Model + Explanations\" condition is the highest, reaching almost 80%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.05426v2",
    "pdf_url": null
  },
  {
    "instance_id": "ba06fa01b4814272946c9476c7d35349",
    "figure_id": "2009.08330v3-Figure1-1",
    "image_file": "2009.08330v3-Figure1-1.png",
    "caption": " Relative score improvements against models with M-BERT embeddings for three tasks.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the models tested achieves the highest relative score for the POS task when trained on 100 sentences?",
    "answer": " M+W+C",
    "rationale": " The plot for the POS task shows that the M+W+C model (blue line) has the highest relative score when trained on 100 sentences.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.08330v3",
    "pdf_url": null
  },
  {
    "instance_id": "7cba7faebaec4ef59bdc7b78b946f873",
    "figure_id": "2303.08010v3-Figure10-1",
    "image_file": "2303.08010v3-Figure10-1.png",
    "caption": " Real-world latency and throughput uncertainty efficiency gains for window-based cascades for EfficientNet-B0→B4. We use a NVIDIA V100 32GB GPU to measure throughput and an Intel Xeon E5-2698v4 CPU to measure latency.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest throughput?",
    "answer": "The single model has the highest throughput.",
    "rationale": "The single model has the highest throughput because its curve is the furthest to the right on the throughput plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.08010v3",
    "pdf_url": null
  },
  {
    "instance_id": "d4561ce7cc4d46c49e6a143cae8bffcf",
    "figure_id": "2012.03665v1-Figure3-1",
    "image_file": "2012.03665v1-Figure3-1.png",
    "caption": " Venn diagram showing distribution of incidents containing command lines, queries, or stack traces (in percentage).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of incidents contain all three of the following: command lines, queries, and stack traces?",
    "answer": "0.2%",
    "rationale": "The number 0.2 is located in the center of the Venn diagram, where all three circles overlap. This indicates that 0.2% of incidents contain all three elements.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.03665v1",
    "pdf_url": null
  },
  {
    "instance_id": "9c9b33ebc61e4fd1b75e02a1882bf062",
    "figure_id": "2309.10810v1-Figure5-1",
    "image_file": "2309.10810v1-Figure5-1.png",
    "caption": " Comparison on Old Photo Restoration on Challenging Cases. For a severely damaged old photo, with one eye masked with scratch, while only DDNM [42] is able to complete the missing eye, its restoration quality is significantly low. In contrast, our PGDiff produces high-quality restored outputs with natural color and complete faces.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods produced the highest quality restoration of the old photo?",
    "answer": "Our PGDiff",
    "rationale": "The figure shows the original photo and the restored photos produced by several different methods. The restored photo produced by our PGDiff is the most visually appealing and realistic, with natural color and a complete face.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.10810v1",
    "pdf_url": null
  },
  {
    "instance_id": "c89fb1f021c345dca55397b7b2b229d9",
    "figure_id": "2111.15141v2-Figure2-1",
    "image_file": "2111.15141v2-Figure2-1.png",
    "caption": " Sampling performance on rings-shape density function with 100 steps. The gradient information can help PIS-Grad and MCMC algorithm improve sampling performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the worst sampling performance?",
    "answer": "VI-NF",
    "rationale": "The VI-NF algorithm produces a sample distribution that is the least similar to the target distribution, which is a ring shape. This is evident from the figure, which shows that the VI-NF samples are concentrated in a small region of the space, rather than being spread out evenly around the ring.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.15141v2",
    "pdf_url": null
  },
  {
    "instance_id": "fb659939e832488fb8aa3a50ab04da42",
    "figure_id": "2108.02831v1-Figure1-1",
    "image_file": "2108.02831v1-Figure1-1.png",
    "caption": " The figure illustrates the performance of DPNE algorithm compared to various ways one can apply the DPSU algorithm for n-gram extraction. Here ‘DPSU-all’ refers to running DPSU on all the different length n-grams together. ‘DPSU-even’ refers to splitting the privacy budget evenly and running DPSU to learn k-grams separately for each k. ‘DPSU-single’ refers to spending all the privacy budget to learn k-grams for a single k. Note that for large k, DPNE learns many more k-grams than DPSU even when DPSU uses all its privacy budget to learn just k-grams for that particular value of k. Here ε = 4, δ = 10−7.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of the number of extracted n-grams for n-gram size greater than 5?",
    "answer": "DPNE.",
    "rationale": "The figure shows that the DPNE line is above all other lines for n-gram sizes greater than 5. This means that DPNE extracts more n-grams than the other algorithms for these n-gram sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.02831v1",
    "pdf_url": null
  },
  {
    "instance_id": "79d0bd06be4c4cb283f33ef0fb311899",
    "figure_id": "2210.04287v1-Figure2-1",
    "image_file": "2210.04287v1-Figure2-1.png",
    "caption": " Accuracy improvements over zero-shot CLIP. On all the 11 classification benchmarks, our method outperforms the CLIP and CoOp baselines by non-trivial margins.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest accuracy improvement over zero-shot CLIP on the FGVC-Aircraft dataset?",
    "answer": "DeFO (ours)",
    "rationale": "The figure shows the accuracy improvements of two methods, CoOp and DeFO (ours), over zero-shot CLIP on 11 classification benchmarks. The bars for DeFO (ours) are consistently higher than the bars for CoOp, indicating that DeFO (ours) achieves higher accuracy improvements on all benchmarks, including FGVC-Aircraft.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.04287v1",
    "pdf_url": null
  },
  {
    "instance_id": "a2b7b6adbacd45d6a6cc1ad323c2c3d3",
    "figure_id": "1808.09060v1-Figure4-1",
    "image_file": "1808.09060v1-Figure4-1.png",
    "caption": " Comparison by language of BASELINE system to +EXT, +CHAR, and +POS.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the highest LAS score for the +POS system?",
    "answer": "Swedish",
    "rationale": "The +POS bar for Swedish is the highest among all the languages.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.09060v1",
    "pdf_url": null
  },
  {
    "instance_id": "96c314173de342f2805d944280096a4d",
    "figure_id": "2212.00261v1-Figure2-1",
    "image_file": "2212.00261v1-Figure2-1.png",
    "caption": " Agreement score for human- and random-labelled tasks. Left: AS measured on three architectures: ResNet-18, MLP and ViT. Center: AS measured on ResNet-18 for different numbers of training images N . The standard deviation is over four random tasks and three data splits. Right: Ablating the sources of stochasticity present in A. Each row shows when one of the 1) initialization, 2) data-order or 3) CUDA is stochastic and the other two sources are fixed, and the bottom is when all the sources are fixed (see Sec. 3.2). The differentiation between human- and random-labelled tasks stably persists across different architectures, data sizes and sources of randomness.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which architecture achieves the highest agreement score for human-labelled tasks?",
    "answer": "ResNet-18.",
    "rationale": "The leftmost plot shows the distribution of agreement scores for human-labelled tasks across three architectures: ResNet-18, MLP, and ViT. The ResNet-18 distribution is shifted to the right, indicating that it achieves higher agreement scores than the other two architectures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.00261v1",
    "pdf_url": null
  },
  {
    "instance_id": "5f9b3fee88474d50b68bfee5c31e7975",
    "figure_id": "2202.10986v1-Figure2-1",
    "image_file": "2202.10986v1-Figure2-1.png",
    "caption": " An example network used in the lower bound of the approximation ratio of Greedy for µv = 2 and µw = 1 + 1 2 · 2 = 2. e claim in the proof is that for any arbitrary network such that the rst cash injection made by Greedy is t1, and the highest threat index is an integer, e.g. 2 in this case, the network in this gure achieves at most the same approximation ratio, while satisfying properties (P1) and (P2).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the threat index of node w?",
    "answer": "2",
    "rationale": "The threat index of a node is the sum of the weights of all edges incident to that node. In this case, there are two edges incident to node w, one with weight 1 and one with weight 1/2. Therefore, the threat index of node w is 1 + 1/2 = 2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.10986v1",
    "pdf_url": null
  },
  {
    "instance_id": "ffce86c00dfe47b8a830e800e0e4cfde",
    "figure_id": "2104.07941v1-Figure5-1",
    "image_file": "2104.07941v1-Figure5-1.png",
    "caption": " Broccoli vs. table-based learning: Box plots of (a) within-user and (b) within-word differences in retention rates (Sec. 6.1). Positive values represent that Broccoli performs better.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which condition resulted in a higher retention rate in the long term, Broccoli or table-based learning?",
    "answer": "Broccoli.",
    "rationale": "The box plots in the figure show the distribution of retention-rate differences between Broccoli and table-based learning. Positive values represent that Broccoli performs better. In both the short and long term, the median difference is positive, indicating that Broccoli generally resulted in a higher retention rate than table-based learning.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.07941v1",
    "pdf_url": null
  },
  {
    "instance_id": "f4b829f72c41466293aef1a9faa1b5d2",
    "figure_id": "1909.13247v2-Figure9-1",
    "image_file": "1909.13247v2-Figure9-1.png",
    "caption": " Feature similarities between embedding features in the receptive fields (or sampling locations). We use the DAVIS-2017 validation set for the experiments.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest cosine similarity between embedding features in the receptive fields (or sampling locations)?",
    "answer": "Deformable",
    "rationale": "The bar chart shows the cosine similarity for different methods. The Deformable method has the highest bar, which indicates it has the highest cosine similarity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.13247v2",
    "pdf_url": null
  },
  {
    "instance_id": "7ffb3984ffb74b89b913117bd2e76cd0",
    "figure_id": "1909.04079v2-Figure3-1",
    "image_file": "1909.04079v2-Figure3-1.png",
    "caption": " Evaluating the quality of estimated intervals - For each dataset, we plot the expected calibration vs observed calibration on test data to demonstrate the generality of the proposed PI estimator, when compared to baselines. Further, we plot expected calibration vs average PI width to show how the estimated intervals compare to existing uncertainty estimators.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method consistently produces the narrowest confidence intervals across all datasets?",
    "answer": "The Proposed (Sigma Fit) method.",
    "rationale": "The plot on the right side of each subfigure shows the average confidence interval width for each method as a function of the expected calibration level. The Proposed (Sigma Fit) method has the lowest line in all four subfigures, indicating that it produces the narrowest confidence intervals.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.04079v2",
    "pdf_url": null
  },
  {
    "instance_id": "198a34e8432a4c6382aaab75d700cb5b",
    "figure_id": "2308.03594v1-FigureV-1",
    "image_file": "2308.03594v1-FigureV-1.png",
    "caption": "Figure V: Visual comparison of FeatEnHancer with several LLIE approaches and a previous state-of-art dark object detection method on the ExDark dataset. Zoom in for the best view.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most visually appealing results?",
    "answer": "FeatEnhancer",
    "rationale": "The figure shows that FeatEnhancer produces the most visually appealing results, with sharper edges and more detail than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.03594v1",
    "pdf_url": null
  },
  {
    "instance_id": "e50cda71dfa7444c8625dac9781fc15b",
    "figure_id": "1908.01977v1-Figure1-1",
    "image_file": "1908.01977v1-Figure1-1.png",
    "caption": " Skin detection results by our approach vs. solutions of a UNet and a tradition Gaussian Mixture Model (GMM). The intersection-over-union (IoU) rates demonstrate our approach has a better performance.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, GMM, UNet, or Ours, is the most accurate for skin detection?",
    "answer": "Ours.",
    "rationale": "The figure shows the results of skin detection by the three methods. The IoU rates, which measure the accuracy of the methods, are shown below each image. The IoU rates for our method are consistently higher than those for the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.01977v1",
    "pdf_url": null
  },
  {
    "instance_id": "39562a397a4c4d21a693549af1a68fc6",
    "figure_id": "2006.07665v1-Figure5-1",
    "image_file": "2006.07665v1-Figure5-1.png",
    "caption": " A comparison of different methods in scatter plot. The plotting points are the predictions of network with y-coordinates being the predicted scores and x-coordinates being the ground-truth scores. The ground-truth samples are plotted in dotted line.",
    "figure_type": "** plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method has the most accurate predictions? ",
    "answer": " MUSDL. ",
    "rationale": " The scatter plot for MUSDL shows the data points clustered more closely to the ground truth line than the other two methods. This indicates that the predictions made by MUSDL are more accurate than the predictions made by Regression and USDL. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.07665v1",
    "pdf_url": null
  },
  {
    "instance_id": "29cf06258a54493497e9494f183b09b2",
    "figure_id": "2306.07930v1-Figure16-1",
    "image_file": "2306.07930v1-Figure16-1.png",
    "caption": " Distributions of relevance scores R[𝑖, 𝑗] for each source node 𝑖 ∈ 𝑉 and the top 100 nodes 𝑗 considered as",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which data set has the highest median relevance score?",
    "answer": "YT-10k",
    "rationale": "The median relevance score is the horizontal line inside the box in the box plot. The YT-10k data set has the highest median relevance score, as its horizontal line is located higher than the other data sets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.07930v1",
    "pdf_url": null
  },
  {
    "instance_id": "5c430e77dccb4776b3a63c3f9b892e4b",
    "figure_id": "2301.12559v1-Figure2-1",
    "image_file": "2301.12559v1-Figure2-1.png",
    "caption": " A comparison of various MLR algorithm. Depicted is the percentage of runs, out of 50 random initializations, for which Flatent > 2σ (see (5)), as a function of the sample size n. The dimension and noise level are fixed at d = 300 and σ = 10−2. Mixture: K = 3 with p = (0.7, 0.2, 0.1) (left panel); K = 5 with p = (0.63, 0.2, 0.1, 0.05, 0.02) (right panel).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in terms of failure probability?",
    "answer": "The oracle algorithm.",
    "rationale": "The figure shows the failure probability of different algorithms as a function of the sample size. The oracle algorithm has the lowest failure probability for all sample sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.12559v1",
    "pdf_url": null
  },
  {
    "instance_id": "cb9ce85f06464091ae557d196b66f34b",
    "figure_id": "1907.04490v1-Figure4-1",
    "image_file": "1907.04490v1-Figure4-1.png",
    "caption": " (a) The torque τ required to generate the characters ’a’, ’d’ and ’e’ in black. Using these samples DeLaN was trained offline and learns the red trajectory. DeLaN can not only learn the desired torques but also disambiguate the individual torque components even though DeLaN was trained on the super-imposed torques. Using Equation 6 DeLaN can represent the inertial force HÜq (b), the Coriolis and Centrifugal forces c(q, Ûq) (c) and the gravitational force g(q) (d). All components match closely the ground truth data. (e) shows the offline MSE of the feed-forward neural network and DeLaN for each joint.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which force component contributes the most to the torque required to generate the character 'a'?",
    "answer": "The gravitational force g(q).",
    "rationale": "The figure shows that the gravitational force g(q) has the largest magnitude and closely matches the shape of the torque required to generate the character 'a'.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.04490v1",
    "pdf_url": null
  },
  {
    "instance_id": "85c92b05f85f4bb48aa96b540f213224",
    "figure_id": "2305.16001v1-Figure7-1",
    "image_file": "2305.16001v1-Figure7-1.png",
    "caption": " Kendall 𝜏𝑏 correlations between the rankings of different centrality measures and the ranking according to the SIR model for infection probabilities 𝛽 ∈ {0.1, 0.2, . . . , 0.8}. Thi denotes the 𝑛-th order temporal H-index.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which centrality measure has the highest correlation with the SIR model for infection probabilities across all datasets?",
    "answer": "Temporal H-index (n=64)",
    "rationale": "The figure shows the Kendall 𝜏𝑏 correlations between the rankings of different centrality measures and the ranking according to the SIR model for infection probabilities 𝛽 ∈ {0.1, 0.2, . . . , 0.8}. The temporal H-index (n=64) has the highest correlation for all datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16001v1",
    "pdf_url": null
  },
  {
    "instance_id": "3391aff3378b41d39abcf0211952f9a6",
    "figure_id": "2212.04129v2-Figure5-1",
    "image_file": "2212.04129v2-Figure5-1.png",
    "caption": " Training curves of ViT-H. Our method (with different training budgets) is compared with E2E-DeiT [38].",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method achieves the highest ImageNet accuracy? ",
    "answer": " DeiT + Ours. ",
    "rationale": " The figure shows the ImageNet accuracy of different methods as a function of training cost. The DeiT + Ours method achieves the highest ImageNet accuracy of 84.3%, which is higher than the other methods shown in the figure. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.04129v2",
    "pdf_url": null
  },
  {
    "instance_id": "29635b09f4f0439a84eaef46abd2f5bb",
    "figure_id": "2104.03424v1-Figure5-1",
    "image_file": "2104.03424v1-Figure5-1.png",
    "caption": " Ablation of ensemble agreement causes divergence. The model eventually detects objects everywhere.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the model's detections when ensemble agreement is ablated?",
    "answer": "The model eventually detects objects everywhere.",
    "rationale": "The figure shows the results of ablating ensemble agreement in a deep learning model. The top row shows the original model's detections, while the bottom row shows the detections after ensemble agreement is ablated. It can be seen that the model in the bottom row detects objects everywhere, even in areas where there are no objects. This suggests that ensemble agreement is important for preventing the model from making false detections.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.03424v1",
    "pdf_url": null
  },
  {
    "instance_id": "a16afa71be9944709f1a71d9095fbe19",
    "figure_id": "1906.09217v1-Figure7-1",
    "image_file": "1906.09217v1-Figure7-1.png",
    "caption": " The performance comparison on Comics.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the Comics dataset according to the Recall@k metric?",
    "answer": "NextItNet",
    "rationale": "The figure shows the Recall@k and NDCG@k metrics for different models on the Comics dataset. The NextItNet model has the highest Recall@k values for all values of k, indicating that it performed the best according to this metric.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.09217v1",
    "pdf_url": null
  },
  {
    "instance_id": "ef54484e47c4466dbbbb41d2be433ca5",
    "figure_id": "2010.13309v2-Figure2-1",
    "image_file": "2010.13309v2-Figure2-1.png",
    "caption": " The proposed variational quantum circuit for 2 × 2 QCNN.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three main stages of the QCNN computing process?",
    "answer": "Encoding, quantum circuit, and decoding.",
    "rationale": "The figure shows the three stages of the QCNN computing process. The encoding stage takes the input data and converts it into a quantum state. The quantum circuit stage applies a series of quantum gates to the quantum state. The decoding stage measures the quantum state and converts it back into classical data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.13309v2",
    "pdf_url": null
  },
  {
    "instance_id": "bdd0ff8bf37e4034bf0f3126402ec491",
    "figure_id": "2210.15067v2-Figure8-1",
    "image_file": "2210.15067v2-Figure8-1.png",
    "caption": " The composition of edit actions as the update ratio changes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common type of edit action when the update ratio is less than 20%?",
    "answer": "Insert",
    "rationale": "The figure shows that the green bars, which represent the proportion of insert actions, are the tallest for all update ratios less than 20%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.15067v2",
    "pdf_url": null
  },
  {
    "instance_id": "e7142c33d79e4ea880374d542456693d",
    "figure_id": "2202.13170v1-Figure5-1",
    "image_file": "2202.13170v1-Figure5-1.png",
    "caption": " Quantitative comparison with state-of-the-art SOD methods in terms of Precision-Recall curves.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the best overall performance on the PASCAL-S dataset?",
    "answer": "OURS",
    "rationale": "The OURS method has the highest precision for all recall values on the PASCAL-S dataset, as shown by the red solid line in the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.13170v1",
    "pdf_url": null
  },
  {
    "instance_id": "d8a0d81594f846c7bff53924423c13ef",
    "figure_id": "2209.14110v2-Figure3-1",
    "image_file": "2209.14110v2-Figure3-1.png",
    "caption": " Vanilla OGD versus our meta-learning versions of OGD for different values of the learning rate η in Endgame B.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which learning rate results in the fastest convergence to the Nash equilibrium for the meta-learning (avg. best-in-hindsight) method in the sorted stacks setting?",
    "answer": "η = 0.001",
    "rationale": "The figure shows that for the sorted stacks setting, the meta-learning (avg. best-in-hindsight) method converges to the Nash equilibrium fastest when η = 0.001. This is because the orange line, which represents the meta-learning (avg. best-in-hindsight) method with η = 0.001, reaches the Nash equilibrium (indicated by the horizontal dashed line) in the fewest number of tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.14110v2",
    "pdf_url": null
  },
  {
    "instance_id": "87110b04931d47b38d5e4e9141f0d77b",
    "figure_id": "1905.07121v2-Figure4-1",
    "image_file": "1905.07121v2-Figure4-1.png",
    "caption": " Comparison of success rate versus number of model queries across different network architectures for untargeted SimBA (solid line) and SimBA-DCT (dashed line) attacks. Both methods can successfully construct adversarial perturbations within 20, 000 queries with high probability. DenseNet is the most vulnerable against both attacks, admitting a success rate of almost 100% after only 6,000 queries for SimBA and 4000 queries for SimBA-DCT. Inception v3 is much more difficult to attack for both methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following networks is the most difficult to attack for both SimBA and SimBA-DCT methods?",
    "answer": "Inception v3",
    "rationale": "The figure shows that Inception v3 has the lowest success rate for both SimBA and SimBA-DCT methods, even after 20,000 queries.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.07121v2",
    "pdf_url": null
  },
  {
    "instance_id": "b21b2998d93e4fa588252e0716f317f3",
    "figure_id": "2007.01195v3-Figure2-1",
    "image_file": "2007.01195v3-Figure2-1.png",
    "caption": " Although IMGEPs succeed to reach a high-diversity in their respective BC space, they are poorly-diverse in all the others. (left) Diversity for all IMGEP variants measured in each analytic BC space. For better visualisation the resulting diversities are divided by the maximum along each axis. Mean and std-deviation shaded area curves are depicted. (right). Examples of patterns discovered by the IMGEPs that are consider diverse in their respective BC space. See Appendix B.1.1 for details.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which IMGEP variant achieves the highest diversity in the Patch-BetaVAE space?",
    "answer": "IMGEP-PatchBetaVAE.",
    "rationale": "The figure on the left shows the diversity of different IMGEP variants in different BC spaces. The line for IMGEP-PatchBetaVAE is the highest in the Patch-BetaVAE space, indicating that it achieves the highest diversity in that space.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.01195v3",
    "pdf_url": null
  },
  {
    "instance_id": "8424477cfbe64c338371b06e5da5ebfd",
    "figure_id": "2005.00792v4-Figure10-1",
    "image_file": "2005.00792v4-Figure10-1.png",
    "caption": " Date distribution of gold articles for questions. Each question is made from gold articles. The dates denote release dates of news articles and they range from 01-01-2019 to 11-31-2019.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "During which month of 2019 were the most gold articles released?",
    "answer": "October",
    "rationale": "The figure shows the number of gold articles released each month in 2019. The tallest bars in the figure correspond to October, indicating that the most gold articles were released during that month.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.00792v4",
    "pdf_url": null
  },
  {
    "instance_id": "e84cff6e81fe401d9f59d619e7a1b7fe",
    "figure_id": "2106.00085v3-Figure6-1",
    "image_file": "2106.00085v3-Figure6-1.png",
    "caption": " Rank–frequency distributions for different samples. All follow a remarkably similar trend.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model best fits the test set data?",
    "answer": "The Transformer (AS) model.",
    "rationale": "The figure shows the rank-frequency distributions for different samples. The Transformer (AS) model is the closest to the test set data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.00085v3",
    "pdf_url": null
  },
  {
    "instance_id": "0d5897fe4dea44fcbe50ca3bcb2172e0",
    "figure_id": "2103.08784v2-Figure10-1",
    "image_file": "2103.08784v2-Figure10-1.png",
    "caption": " Retrieved top 10 images from the query \"The sun hits the floor in a rustic bedroom.\" (Top picture is the ground truth.)",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the retrieved images most closely matches the ground truth image?",
    "answer": "The image in the bottom right corner of the grid.",
    "rationale": "The ground truth image shows a bed in a nook with sunlight hitting the floor. The image in the bottom right corner of the grid is the only retrieved image that also shows a bed in a nook with sunlight hitting the floor.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.08784v2",
    "pdf_url": null
  },
  {
    "instance_id": "9f68c24d93894e36b9668ae96a6162e2",
    "figure_id": "2005.10785v2-Figure12-1",
    "image_file": "2005.10785v2-Figure12-1.png",
    "caption": " Trajectories of SGD, clipped-SGD, d-clipped-SGD and clipped-SSTM applied to solve logistic regression problem on australian dataset. For SGD and its clipped variants stepsize γ = 20 L was used. For clipped-SGD we used λ = 18.62 and for d-clipped-SGD the parameters are as follows: λ0 = 74.47, l = 1500, α = 0.9. Parameters for clipped-SSTM are the same as in the corresponding cell in Table 4.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimization algorithm converges fastest to the optimal solution?",
    "answer": "clipped-SSTM",
    "rationale": "The figure shows the trajectories of different optimization algorithms applied to solve a logistic regression problem on the australian dataset. The y-axis shows the difference between the current objective function value and the optimal objective function value, and the x-axis shows the number of passes through the data. The clipped-SSTM algorithm converges to the optimal solution faster than the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.10785v2",
    "pdf_url": null
  },
  {
    "instance_id": "c7c2d902215e4aff9dd54a5dedac344f",
    "figure_id": "2110.02095v1-Figure4-1",
    "image_file": "2110.02095v1-Figure4-1.png",
    "caption": " The effect of sample size on power law curves. The curves are fitted to the convex hull of experiments as well as all data points from Figure 2. We use the points from higher US accuracies as held out data. Prediction error captures the difference between power law prediction and the observed value of the DS accuracy. Fitting error captures the difference of power law values from the points that are used in calculating power law parameters. We plot fitting error and prediction error as the number of samples changes. More details on how these values are computed are provided in Appendix C.1.1.These errors are very small for both choices of data points and robust to the number of samples when the number of samples is as low as 500.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the error change as the sample size increases?",
    "answer": "The error decreases as the sample size increases.",
    "rationale": "The figure shows that the error decreases as the sample size increases for both the convex hull and all data points. This is true for both the fitting error and the prediction error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02095v1",
    "pdf_url": null
  },
  {
    "instance_id": "bea2c7e7ff4d4e57ac31f75020d354fb",
    "figure_id": "2108.09265v2-Figure2-1",
    "image_file": "2108.09265v2-Figure2-1.png",
    "caption": " Examples of causal models—with treatment X and outcome Y—where the ATE can be identified by different data sources returning different subsets of variables.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the causal models in the figure can be used to estimate the average treatment effect (ATE) using only observational data?",
    "answer": "The confounder-mediator graph (c).",
    "rationale": "The ATE can be estimated using observational data if there is no unobserved confounding between the treatment and the outcome. In the instrumental variable graph (a) and the two IVs graph (b), there is unobserved confounding between the treatment and the outcome, indicated by the dashed arrows. In the confounder-mediator graph (c), there is no unobserved confounding between the treatment and the outcome, so the ATE can be estimated using observational data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.09265v2",
    "pdf_url": null
  },
  {
    "instance_id": "1608ab43d3d54b829d53fb2c1efbc39c",
    "figure_id": "2004.03809v2-Figure6-1",
    "image_file": "2004.03809v2-Figure6-1.png",
    "caption": " Learning curves of the interaction between each user agent and the benchmark system policy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best?",
    "answer": "MADPL",
    "rationale": "The MADPL line is the highest at the end of the graph, indicating that it had the highest success rate compared to the benchmark.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.03809v2",
    "pdf_url": null
  },
  {
    "instance_id": "06dc7df3000040e2baafb26d951bc6d6",
    "figure_id": "2110.11222v2-Figure9-1",
    "image_file": "2110.11222v2-Figure9-1.png",
    "caption": " Correlation between the evaluation scores of two runs that share network initialization and seed experience, but otherwise have different randomness. Correlation is calculated across ten runs and plotted against time. The correlation is noisy for many environments but oscillates around zero, suggesting that there is little actual correlation and that network initialization and seed experience have a small effect on performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which environment shows the most consistent correlation between the evaluation scores of two runs?",
    "answer": "Walker",
    "rationale": "The figure shows the correlation between the evaluation scores of two runs for each environment. The walker environment shows the most consistent correlation, with the correlation oscillating around 0.5 for the entire duration of the experiment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.11222v2",
    "pdf_url": null
  },
  {
    "instance_id": "79a71e2d11864df891b3553c40b9670e",
    "figure_id": "2102.09750v2-Figure2-1",
    "image_file": "2102.09750v2-Figure2-1.png",
    "caption": " Different number of steps.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method requires the least memory?",
    "answer": "The ACA method.",
    "rationale": "The plot shows that the ACA method has the lowest memory usage for all numbers of steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.09750v2",
    "pdf_url": null
  },
  {
    "instance_id": "329bc304fb8b4f409d481a36928ef3a5",
    "figure_id": "2106.07153v2-Figure6-1",
    "image_file": "2106.07153v2-Figure6-1.png",
    "caption": " Max, mean, and mean squared error for 3-way marginals on ACS PA-18 (workloads = 4096) with privacy budgets ε ∈ {0.1, 0.15, 0.2, 0.25, 0.5, 1} and δ = 1 n2 . We evaluate public-data-assisted algorithms with the following public datasets: Left: 2018 California (CA-18); Center: 2010 Pennsylvania (PA-10); Right: 2018 Ohio (PA-10). The x-axis uses a logarithmic scale. Results are averaged over 5 runs, and error bars represent one standard error.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of mean squared error (MSE) for all three public datasets?",
    "answer": "GEM",
    "rationale": "The figure shows the MSE for four algorithms (GEM, GEM^Pub, PMW^Pub, and PEP^Pub) on three different public datasets (CA-18, PA-10, and OH-18). The MSE for GEM is consistently lower than the MSE for the other three algorithms across all three datasets and for all privacy budget values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07153v2",
    "pdf_url": null
  },
  {
    "instance_id": "a45a7d4780544caeb4bee90979d668d5",
    "figure_id": "2306.08299v1-Figure7-1",
    "image_file": "2306.08299v1-Figure7-1.png",
    "caption": " material introduction",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two main categories of supplementary materials provided?",
    "answer": "Code and Technical",
    "rationale": "The figure shows a tree structure with \"supplementary_material\" at the top. This branches into two main categories: \"Code\" and \"Technical\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.08299v1",
    "pdf_url": null
  },
  {
    "instance_id": "77c0511bbb5d48d78b86edc4a21a916f",
    "figure_id": "2210.07547v1-Figure4-1",
    "image_file": "2210.07547v1-Figure4-1.png",
    "caption": " The independence study on covariance between reconstructed features. The hidden dimension of BERT-whitening and Kernel-Whitening are set to 64. All models are trained 50000 steps with batch 32.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest covariance between reconstructed features at the end of training?",
    "answer": "BERT-base",
    "rationale": "The plot shows the covariance between reconstructed features for different models as a function of training steps. The BERT-base model has the highest covariance at the end of training, as shown by the black line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.07547v1",
    "pdf_url": null
  },
  {
    "instance_id": "b82925e453d643be930c7ea9ac2a2477",
    "figure_id": "1908.01977v1-Figure6-1",
    "image_file": "1908.01977v1-Figure6-1.png",
    "caption": " Curves of IoU, IoU Top-1 rate with respect to the probability threshold, and Precision-Recall on our dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest IoU Top-1 rate?",
    "answer": "Ours",
    "rationale": "The IoU Top-1 rate is shown in the middle plot. The line labeled \"Ours\" is the highest for most of the range of the x-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.01977v1",
    "pdf_url": null
  },
  {
    "instance_id": "e4a7ea56b2554e369dbeec70c7a9d8b4",
    "figure_id": "1712.04415v1-Figure3-1",
    "image_file": "1712.04415v1-Figure3-1.png",
    "caption": " Five most predictive micro-expressions, from left to right: Frowning, Eyebrows raising, Lip corners up, Lips protruded and Head Side Turn.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the five micro-expressions is most likely to indicate surprise?",
    "answer": "Eyebrows raising.",
    "rationale": "The caption states that the five micro-expressions are listed from left to right. The second micro-expression from the left is eyebrows raising, which is often associated with surprise.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1712.04415v1",
    "pdf_url": null
  },
  {
    "instance_id": "7084c8dd0b334cefbbf28799c71a5c02",
    "figure_id": "2111.01395v1-Figure5-1",
    "image_file": "2111.01395v1-Figure5-1.png",
    "caption": " Number for power method to converge at each convolutional layer in the 6C2F model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which convolutional layer has the lowest average number of iterations to converge?",
    "answer": "Conv 2",
    "rationale": "The mean of each distribution is indicated by the red vertical line in each subplot. Conv 2 has the lowest mean value, which indicates that it takes the fewest iterations to converge on average.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.01395v1",
    "pdf_url": null
  },
  {
    "instance_id": "e1f33337824545a6b6186f6c0c1248e0",
    "figure_id": "1908.04915v1-Figure4-1",
    "image_file": "1908.04915v1-Figure4-1.png",
    "caption": " The visualization of the domain transfer process and corresponding generated captions. The red keywords indicate incorrect descriptions while the green words mean the correct keywords.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the original Duke Style?",
    "answer": "The top left image shows the original Duke Style.",
    "rationale": "The caption states that the top row of images shows the original Duke Style, while the bottom row shows the transferred CUHK-PEDES Style.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.04915v1",
    "pdf_url": null
  },
  {
    "instance_id": "697065283057407ea3f65ffd434b21fd",
    "figure_id": "2106.03746v2-Figure3-1",
    "image_file": "2106.03746v2-Figure3-1.png",
    "caption": " CIFAR-100, training from scratch, top-1 accuracy measured every 10 epochs.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better when trained from scratch on CIFAR-100?",
    "answer": "CvT-13.",
    "rationale": "The plot shows that CvT-13 has the highest top-1 accuracy across all epochs when compared to the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03746v2",
    "pdf_url": null
  },
  {
    "instance_id": "a190e44dacf1458db1aaea9331b3c715",
    "figure_id": "1806.00064v1-Figure2-1",
    "image_file": "1806.00064v1-Figure2-1.png",
    "caption": " Tensor fusion via tensor outer product",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the operation performed on Z to get h?",
    "answer": "Matrix multiplication.",
    "rationale": "The figure shows Z being multiplied by W to get h.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.00064v1",
    "pdf_url": null
  },
  {
    "instance_id": "cafb70d4b4bc44be98242fa0ce0fd6dc",
    "figure_id": "2302.14332v2-Figure1-1",
    "image_file": "2302.14332v2-Figure1-1.png",
    "caption": " Comparison of speed and accuracy (based on AUC metric) for existing image-based robot pose estimation methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest and most accurate?",
    "answer": "Ours.",
    "rationale": "The figure shows the speed and accuracy of different image-based robot pose estimation methods. The x-axis shows the speed in frames per second (FPS), and the y-axis shows the accuracy in terms of the area under the curve (AUC). The method labeled \"Ours\" is located in the upper right corner of the figure, indicating that it is both the fastest and most accurate method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.14332v2",
    "pdf_url": null
  },
  {
    "instance_id": "5ba5180809f548d283f6525ad272603f",
    "figure_id": "2210.04888v1-Figure10-1",
    "image_file": "2210.04888v1-Figure10-1.png",
    "caption": " Visual Comparison on UBCFashion & AIST. Zoom in for the best view.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most realistic images?",
    "answer": "Ours.",
    "rationale": "The images in the \"Ours\" column are the most realistic, as they show the most detail and have the most natural-looking poses. The other methods produce images that are either too blurry or too stiff.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.04888v1",
    "pdf_url": null
  },
  {
    "instance_id": "73b75e60477f41b18e5f21ee52041866",
    "figure_id": "2104.05847v1-Figure1-1",
    "image_file": "2104.05847v1-Figure1-1.png",
    "caption": " Comparison of confusion matrices on MNLI development set (in-domain). X-axis and Y-axis represent the predicted and gold labels, respectively. TAT produces an accuracy gain of 1.7 absolute points.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which fine-tuning method produces a higher accuracy on the MNLI development set?",
    "answer": "TAT fine-tuning.",
    "rationale": "The figure shows that TAT fine-tuning produces an accuracy of 30.60%, while standard fine-tuning produces an accuracy of 29.93%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.05847v1",
    "pdf_url": null
  },
  {
    "instance_id": "e9b0f2a5e80141ddb44c09ec19e98fe1",
    "figure_id": "2201.02533v2-Figure18-1",
    "image_file": "2201.02533v2-Figure18-1.png",
    "caption": " A showcase of additional material decomposition results using our approach.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the different types of material properties that can be decomposed from an image?",
    "answer": "Base color, normal, specular, and glossiness.",
    "rationale": "The figure shows the different types of material properties that can be decomposed from an image. The \"Base Color\" column shows the base color of the object, the \"Normal\" column shows the surface normals of the object, the \"Specular\" column shows the specular reflection of the object, and the \"Glossiness\" column shows the glossiness of the object.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.02533v2",
    "pdf_url": null
  },
  {
    "instance_id": "16097d70f1e44068b822cf59391c2591",
    "figure_id": "2210.10462v2-Figure3-1",
    "image_file": "2210.10462v2-Figure3-1.png",
    "caption": " Analysis of the number of warm-up epochs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the most significant difference between the Micro F1 and Macro F1 scores?",
    "answer": "IMDB",
    "rationale": "The figure shows the Micro F1 and Macro F1 scores for four datasets. The IMDB dataset has the largest gap between the two lines, indicating the most significant difference between the two scores.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.10462v2",
    "pdf_url": null
  },
  {
    "instance_id": "eaeb5a9423b34c0d8ccb99befd72797f",
    "figure_id": "1908.05168v1-Figure9-1",
    "image_file": "1908.05168v1-Figure9-1.png",
    "caption": " Pixel–discussions are back–projections of output scores to input domain that show pixel–wise contributions to the scores. By comparing contributions among all scores, we make pixels vote independently and find that they focus on objects.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pixel in the image of the white wolf contributed the most to the score for \"white wolf\"?",
    "answer": "The pixel in the center of the wolf's face.",
    "rationale": "The pixel-discussion for the white wolf shows that the pixel in the center of the wolf's face has the highest contribution to the score for \"white wolf\". This is because this pixel is the most representative of the wolf's face, and therefore the most likely to be used by the model to identify the wolf.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.05168v1",
    "pdf_url": null
  },
  {
    "instance_id": "efd469abf5bf48b9ad35400b26bdc2bd",
    "figure_id": "2211.15612v2-Figure12-1",
    "image_file": "2211.15612v2-Figure12-1.png",
    "caption": " The performance of different algorithms on the medium-quality datasets (StarCraft II)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the medium-quality StarCraft II datasets?",
    "answer": "Our algorithm performs the best.",
    "rationale": "The figure shows the episode return for different algorithms over hundreds of steps. The episode return is a measure of how well the algorithm is performing. Our algorithm consistently achieves a higher episode return than the other algorithms, which means that it is performing better.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.15612v2",
    "pdf_url": null
  },
  {
    "instance_id": "9033ad813b034e4c96b2f17f6fe68a3d",
    "figure_id": "2305.19588v1-Figure8-1",
    "image_file": "2305.19588v1-Figure8-1.png",
    "caption": " The ratio of 2 in Theorem 10 is tight: G1 and G2 belong in the same MEC with ν(G1) = 2 and ν(G2) = 1. The dashed arcs represent the covered edges and the boxed vertices represent a minimum vertex cover of the covered edges.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two graphs has a larger minimum edge cover?",
    "answer": "G1",
    "rationale": "The dashed arcs in the figure represent the covered edges, and the boxed vertices represent a minimum vertex cover of the covered edges. We can see that G1 has more dashed arcs than G2, which means that it has a larger minimum edge cover.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19588v1",
    "pdf_url": null
  },
  {
    "instance_id": "e340baa2307a4793a4300460f8c6cc91",
    "figure_id": "2002.06715v2-Figure4-1",
    "image_file": "2002.06715v2-Figure4-1.png",
    "caption": " Comparison between BatchEnsemble and single model on WMT English-German and English-French. Training stops after the model reaches targeted validation perplexity. BatchEnsemble gives a faster convergence by taking the advantage of multiple models. (a): Validation loss of WMT16 English-German task. (b): Validation loss of WMT14 English-French task. Big: Tranformer big model. Base: Transformer base model. BE: BatchEnsemble. Single: Single model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model converges faster on the English-French task, BigSingle or BaseBE?",
    "answer": "BaseBE",
    "rationale": "The plot in (b) shows that the BaseBE model reaches a lower validation loss faster than the BigSingle model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.06715v2",
    "pdf_url": null
  },
  {
    "instance_id": "a0874a676e97432ba5908aa48d6c20cc",
    "figure_id": "2106.01345v2-Figure4-1",
    "image_file": "2106.01345v2-Figure4-1.png",
    "caption": " Sampled (evaluation) returns accumulated by Decision Transformer when conditioned on the specified target (desired) returns. Top: Atari. Bottom: D4RL medium-replay datasets.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which game does the Decision Transformer perform the best in terms of achieving the desired return?",
    "answer": "Pong.",
    "rationale": "The figure shows the performance of the Decision Transformer across different games. In Pong, the Decision Transformer closely follows the best trajectory in the dataset, achieving the desired return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01345v2",
    "pdf_url": null
  },
  {
    "instance_id": "09d826f772a340e4ba672b84bf63be30",
    "figure_id": "2006.05065v2-Figure11-1",
    "image_file": "2006.05065v2-Figure11-1.png",
    "caption": " Additional results on cross-distillation. \"SD\" and \"CD\" refers to \"self-distillation\" and \"cross-distillation\" respectively. The top rows of each experiment show bar charts of accuracy on the test set for each experiment conducted, while the bottom rows are bar charts of expected calibration error.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method consistently performs better in terms of accuracy across all datasets and architectures?",
    "answer": "Self-distillation (SD)",
    "rationale": "The bar charts in the top row of each experiment show the accuracy of each method. In all cases, the red bar (representing SD) is higher than the brown bar (representing CD), indicating that SD achieves higher accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.05065v2",
    "pdf_url": null
  },
  {
    "instance_id": "9e1988af748348f699d00dd51c32c82b",
    "figure_id": "2109.05716v1-Figure2-1",
    "image_file": "2109.05716v1-Figure2-1.png",
    "caption": " Recall@64 differences between BLINK and MuVER on entities with 1 to 100 sentences in their descriptions. We partition the entities by the number of sentences in entity descriptions and calculate metrics within each bin. The size for each bin is 5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on entities with a small number of sentences in their descriptions?",
    "answer": "MuVER",
    "rationale": "The plot shows that MuVER has a higher Recall@64 than BLINK for all bins except the last one. This means that MuVER is better at retrieving relevant entities when there are fewer sentences in their descriptions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.05716v1",
    "pdf_url": null
  },
  {
    "instance_id": "9965598a175d44fabf9737f6dfd06774",
    "figure_id": "1909.05073v4-Figure1-1",
    "image_file": "1909.05073v4-Figure1-1.png",
    "caption": " Overview of different weight pruning dimensions.",
    "figure_type": "** Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which type of sparsity removes individual weights from the convolution kernel?",
    "answer": " Irregular sparsity.",
    "rationale": " The figure shows three types of sparsity: regular sparsity, channel sparsity, and irregular sparsity. Regular sparsity removes entire filters from the convolution kernel. Channel sparsity removes entire channels from the convolution kernel. Irregular sparsity removes individual weights from the convolution kernel.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.05073v4",
    "pdf_url": null
  },
  {
    "instance_id": "692c21892e264a4d9ed3713b9f19748a",
    "figure_id": "2105.11686v6-Figure13-1",
    "image_file": "2105.11686v6-Figure13-1.png",
    "caption": " Five-layer NN. The first to fourth columns of each row are for the input weights of neurons from the first to the fourth hidden layers, respectively. The color indicates D(u,v) of two hidden neurons’ input weights, whose indexes are indicated by the abscissa and the ordinate, respectively. The training data is 80 points sampled from a 5-dimensional function ∑3 k=1 3 sin(10xk + 1), where each xk is uniformly sampled from [−4, 2]. n = 80, d = 5, m = 18, dout = 1, var = 0.0082. lr = 1×10−4, 1×10−4, 1×10−4, 5×10−5, 5×10−5 and epoch is 400, 400, 400, 3000, 360, 400 for tanh(x), x tanh(x), x2 tanh(x), x2tanh(x), sigmoid(x), softplus(x), respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activation function seems to produce the most consistent D(u, v) values across all hidden layers?",
    "answer": "The sigmoid activation function.",
    "rationale": "We can see from the figure that the D(u, v) values for the sigmoid activation function are more consistent across all hidden layers than for the other activation functions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.11686v6",
    "pdf_url": null
  },
  {
    "instance_id": "743459c0498045c494075c27dd35d406",
    "figure_id": "2206.07707v1-Figure8-1",
    "image_file": "2206.07707v1-Figure8-1.png",
    "caption": " Rate Distortion Curve. This graph shows the rate-distortion tradeoffs of different methods on the ‘Night Fury’ RTMV scene, where the y-axis is PSNR and the x-axis is bitrate (in log-scale). Single-bitrate architectures are represented with a dot. For Mip-NeRF (purple), the filtering mechanism can move the dot vertically, but not horizontally. Our compressed architecture (red and blue) has variable-bitrate and is able to dynamically scale the bitrate to different levels of details. Our architecture is more compact than feature-grid methods like NGLOD (yellow) and achieves better quality than postprocessing methods like k-means VQ (gray and green).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest PSNR at a bitrate of 1000 kb?",
    "answer": "mip-NeRF",
    "rationale": "The figure shows that the mip-NeRF curve is the highest at a bitrate of 1000 kb.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.07707v1",
    "pdf_url": null
  },
  {
    "instance_id": "fed9221cb8f14c42ac447d8435d6b63b",
    "figure_id": "1808.07371v2-Figure6-1",
    "image_file": "1808.07371v2-Figure6-1.png",
    "caption": " Face image comparison on held-out data. We compare frame-by-frame synthesis (FBF), adding temporal smoothing (FBF+TS) and our full model (FBF+TS+FG).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods produces the most realistic-looking face image?",
    "answer": "FBF + TS + FG",
    "rationale": "The figure shows four different methods for synthesizing face images. The Ground Truth image is the most realistic-looking, and the FBF + TS + FG image is the closest to the Ground Truth image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.07371v2",
    "pdf_url": null
  },
  {
    "instance_id": "d9962a60114a4e7aac1b156ad23dfd18",
    "figure_id": "2303.09826v2-Figure21-1",
    "image_file": "2303.09826v2-Figure21-1.png",
    "caption": " Qualitative comparisons with SOTA methods. ‘∗’ denotes fine-tune on animation dataset AVC-Train [37]. Our VQD-SR is capable to recover visually natural and sharper lines with fewer artifacts.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods tested is able to recover the sharpest lines with the fewest artifacts?",
    "answer": "VQD-SR",
    "rationale": "The figure shows that VQD-SR is able to recover sharper lines with fewer artifacts than the other methods tested. This is evident in the zoomed-in crops of the images, where the lines in the VQD-SR images are more distinct and less blurry than the lines in the other images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.09826v2",
    "pdf_url": null
  },
  {
    "instance_id": "474bdfd45af14503897b169060c47488",
    "figure_id": "2305.15167v1-Figure3-1",
    "image_file": "2305.15167v1-Figure3-1.png",
    "caption": " Predictive performance of using Shapley prior to predict explanations generated from different explanation algorithms on the diabetes dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which explanation prediction algorithm has the best performance for DeepSHAP explanations?",
    "answer": "Neural Network",
    "rationale": "The bar corresponding to the Neural Network for DeepSHAP explanations has the lowest RMSE value compared to the other explanation prediction algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.15167v1",
    "pdf_url": null
  },
  {
    "instance_id": "4470552aef114eac9fb9ac1b947e1eb4",
    "figure_id": "2211.11208v2-Figure9-1",
    "image_file": "2211.11208v2-Figure9-1.png",
    "caption": " Training convergency with the discriminator designs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two discriminator designs converges faster?",
    "answer": "The Synthetic Rendering design converges faster.",
    "rationale": "The Synthetic Rendering line decreases more quickly than the FLAME Parameters line, indicating that it is converging faster.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11208v2",
    "pdf_url": null
  },
  {
    "instance_id": "c8cbb590033f46bd8bafa388127d9e16",
    "figure_id": "2211.11448v3-Figure11-1",
    "image_file": "2211.11448v3-Figure11-1.png",
    "caption": " More visual comparisons on ClebA-HQ [45] dataset for F space methods. Our method performance better in both reconstruction and editing. ↑ means an increment of the manipulation attribute.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods shown in the figure produces the most realistic results for smile editing?",
    "answer": "CLCAE",
    "rationale": "The CLCAE method produces the most realistic results for smile editing because the edited images look the most natural and do not have any noticeable artifacts. This can be seen in the figure by comparing the \"Smile\" rows of each method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11448v3",
    "pdf_url": null
  },
  {
    "instance_id": "05a34c7df7ab4d99870f18f3734d3c6d",
    "figure_id": "2005.01810v1-Figure4-1",
    "image_file": "2005.01810v1-Figure4-1.png",
    "caption": " Distance manipulation probing task results with subject as target word. Vertical ranges show 95% confidence intervals computed with non-parametric bootstrap. Each cluster of adjacent bars of the same shade represents the three different tested information types—from left to right: number, gender, animacy",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which encoder performed the best on the distance manipulation probing task for the word \"judge\"?",
    "answer": "ELMo",
    "rationale": "The figure shows the accuracy of different encoders on the distance manipulation probing task for different words. For the word \"judge\", ELMo has the highest accuracy, which is indicated by the height of the bar for ELMo.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.01810v1",
    "pdf_url": null
  },
  {
    "instance_id": "b25c6b7098754c7294a1250f95b63408",
    "figure_id": "2003.14229v1-Figure2-1",
    "image_file": "2003.14229v1-Figure2-1.png",
    "caption": " Qualitative results for the compared methodologies in the video for the recipe “Hash Browns” from the YouCook2 dataset. The vertical bars inside the rectangles indicate the selected frames for each method. GT stands to ground truth, and the contiguous black blocks indicate the annotated video segment. The competitors, SSFF and FFNet, present a poor frame selection in terms of GT coverage in comparison to ours.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods, GT, Ours, SSFF, and FFNet, appears to be the most accurate at selecting frames that represent the cooking process?",
    "answer": "Ours",
    "rationale": "The figure shows that the Ours method selects frames that are more evenly distributed throughout the cooking process, while the other methods select frames that are more clustered together. This suggests that the Ours method is more accurate at capturing the different steps involved in cooking.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.14229v1",
    "pdf_url": null
  },
  {
    "instance_id": "33696896d0d0499499157e02601dd4e5",
    "figure_id": "2008.12905v1-Figure3-1",
    "image_file": "2008.12905v1-Figure3-1.png",
    "caption": " (a) Order graph of Fig. 1. The edge weight and the optimal route plan are shown beside each edge. (b) An example order graph. (c) First iteration of clustering for order graph in Fig. 3(b). (d) Second iteration of clustering for order graph in Fig. 3(b) for MAXO = 3. (e) Second iteration of clustering for order graph in Fig. 3(b) for MAXO = 2.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the average cost of the cluster formed in Figure 3(c)?",
    "answer": "0.5",
    "rationale": "Figure 3(c) shows the first iteration of clustering for the order graph in Figure 3(b). The cluster formed in this iteration consists of nodes O1 and O2. The average cost of this cluster is 0.5, as indicated in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.12905v1",
    "pdf_url": null
  },
  {
    "instance_id": "f8ae9e0c61214e88b31289cc596cdcb4",
    "figure_id": "2303.00749v1-Figure2-1",
    "image_file": "2303.00749v1-Figure2-1.png",
    "caption": " Performance illustration in novel view rendering on a challenging nuScenes scene Caesar et al. (2019), (a) the state-of-the-art method Barron et al. (2022) produces poor results with blurred texture details and plenty of depth errors, (b) our S-NeRF can achieve accurate depth maps and fine texture details with fewer artifacts. (d) Our method can also be used for the reconstruction of moving vehicles which is impossible for previous NeRFs. It can synthesize better novel views compared with the mesh method Chen et al. (2021b).",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method produces the most accurate depth maps and fine texture details with fewer artifacts?",
    "answer": " S-NeRF.",
    "rationale": " The figure shows that S-NeRF produces sharper images with fewer artifacts than the other methods. This is because S-NeRF is able to better capture the geometry of the scene and the appearance of the objects in it.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.00749v1",
    "pdf_url": null
  },
  {
    "instance_id": "68458af25a25490b92905aee332713c3",
    "figure_id": "2205.14762v2-Figure9-1",
    "image_file": "2205.14762v2-Figure9-1.png",
    "caption": " Empirical distribution functions of the stopping times based on 100 observations for tests configured at the 𝛼 = 0.05 level testing the null hypothesis 𝐹𝑎 = 𝐹𝑏 . (Left) Under the null hypothesis: distributions for arms A and B are Gamma(10, 10). (Right) Under the alternative hypothesis: distributions for arms A and B are Gamma(10, 10) and Gamma(10, 11) respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which test is most likely to reject the null hypothesis when the alternative hypothesis is true?",
    "answer": "The Howard-Ramdas test.",
    "rationale": "The Howard-Ramdas test has the highest empirical distribution function (EDF) for small values of n, which means that it is more likely to reject the null hypothesis when the alternative hypothesis is true.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.14762v2",
    "pdf_url": null
  },
  {
    "instance_id": "c42f914ce66a419c9b2b50b9443bb167",
    "figure_id": "2012.15781v2-Figure7-1",
    "image_file": "2012.15781v2-Figure7-1.png",
    "caption": " Experiments on ANLI and Amazon-WILDS.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four models performed the best on the ANLI dataset?",
    "answer": "The \"helpful\" model.",
    "rationale": "The figure shows the loss and accuracy of four different models on the ANLI dataset. The \"helpful\" model has the lowest loss and the highest accuracy, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.15781v2",
    "pdf_url": null
  },
  {
    "instance_id": "9e5b7f421e914f1f857ae4b4327e2c04",
    "figure_id": "1905.12862v2-Figure4-1",
    "image_file": "1905.12862v2-Figure4-1.png",
    "caption": " Overall performance of difference choices of embedding dimension d in All Items scenario.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best when the embedding dimension is 100?",
    "answer": "SAERS",
    "rationale": "The figure shows that SAERS has the highest AUC when d=100.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12862v2",
    "pdf_url": null
  },
  {
    "instance_id": "d7918f0bf83c418fab0873505de699b3",
    "figure_id": "2002.12687v6-Figure6-1",
    "image_file": "2002.12687v6-Figure6-1.png",
    "caption": " mIoU results under various distance thresholds (0-0.1) for compared algorithms.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest mIoU score?",
    "answer": "PointNet++",
    "rationale": "The plot shows the mIoU score for different algorithms at different distance thresholds. PointNet++ has the highest score at all distance thresholds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.12687v6",
    "pdf_url": null
  },
  {
    "instance_id": "1e7408c331c4467a874d4255a4e7df63",
    "figure_id": "1911.06396v1-Figure4-1",
    "image_file": "1911.06396v1-Figure4-1.png",
    "caption": " ROC curves with 90% confidence interval using 860 subjects randomly selected 100 times. ArcFace is displayed at a different scale for better visualization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three face recognition models, FaceNet, VGGFace2, or ArcFace, has the best performance for identifying young people?",
    "answer": "ArcFace.",
    "rationale": "The ROC curve for ArcFace is shifted further to the left than the ROC curves for FaceNet and VGGFace2, indicating that ArcFace has a higher true positive rate for a given false positive rate. This is especially true for young people, as the orange line for ArcFace is consistently higher than the orange lines for the other two models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.06396v1",
    "pdf_url": null
  },
  {
    "instance_id": "c3dbe616b34d4d07afff2f415d307475",
    "figure_id": "1911.12511v1-Figure9-1",
    "image_file": "1911.12511v1-Figure9-1.png",
    "caption": " Learning curves for different agents in Zork.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which agent performed the best in Zork?",
    "answer": "SC + Masking",
    "rationale": "The figure shows the learning curves for three agents in Zork. The SC + Masking agent has the highest average reward, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.12511v1",
    "pdf_url": null
  },
  {
    "instance_id": "953e1606f2f24dd695d8424f6f689084",
    "figure_id": "2104.01374v2-Figure3-1",
    "image_file": "2104.01374v2-Figure3-1.png",
    "caption": " Visualizing what HIERARCHICAL DIVNOISING encodes at different latent layers. Here we inspect the contribution of each latent layer in our n“6 latent layer hierarchy when trained with data containing structured noise. To inspect contribution of latent variable zi, we fix the latent variables of layers j ą i, draw k “ 6 conditional samples for layer i, and take the mean of the conditional distributions at layers m ă i generating then a total of k denoised samples (columns). This is repeated for every latent variable layer (rows). Please see Appendix A.4 for details. An interesting observation is that structures that resemble the structured line artefacts are only visible in layers 1 and 2, thus motivating the proposed HDN3´6 network (see Tables 3 and 4).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer of the HIERARCHICAL DIVNOISING network is responsible for removing the structured line artifacts?",
    "answer": "Layers 1 and 2.",
    "rationale": "The figure shows that structures resembling the structured line artifacts are only visible in layers 1 and 2. This suggests that these layers are responsible for removing the artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.01374v2",
    "pdf_url": null
  },
  {
    "instance_id": "8b87f449f39541e7af006a4e4f82358d",
    "figure_id": "2010.07125v5-Figure6-1",
    "image_file": "2010.07125v5-Figure6-1.png",
    "caption": " An example of DRE: an illustration of the dynamics in u5’s personal item network. (a) Initially. (b) After u5 adopts iPad. (c) Expectation.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the probability that u5 will adopt AirPods after adopting iPad?",
    "answer": "0.16",
    "rationale": "The figure shows the dynamics of u5's personal item network. The numbers on the dashed lines represent the probability of adopting a certain item given that another item has already been adopted. In this case, the probability of adopting AirPods after adopting iPad is 0.16.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.07125v5",
    "pdf_url": null
  },
  {
    "instance_id": "7d48453301be4e08a410fde6294d8594",
    "figure_id": "2204.14173v1-Figure6-1",
    "image_file": "2204.14173v1-Figure6-1.png",
    "caption": " Sample locally-dense games from the benchmark set.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the games in the figure is the most densely connected?",
    "answer": "Game 10-05-3.",
    "rationale": "Game 10-05-3 has the most edges per node, as evidenced by the high degree of connectivity between the nodes in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.14173v1",
    "pdf_url": null
  },
  {
    "instance_id": "d1ca2f76f2a44e68b6837328b0962aa9",
    "figure_id": "2205.01850v1-Figure3-1",
    "image_file": "2205.01850v1-Figure3-1.png",
    "caption": " The size projection scores, where the x-axis indicates the object groups. Outliers are omitted. All three models perform reasonably well, as larger objects have higher cosine similarities in general.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of cosine similarity for large objects?",
    "answer": "CLIP",
    "rationale": "The boxplot for CLIP is the highest for large objects, indicating that it has the highest median cosine similarity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.01850v1",
    "pdf_url": null
  },
  {
    "instance_id": "5d1a3489d6f74fe587328e8e477df8a9",
    "figure_id": "2210.05367v2-Figure3-1",
    "image_file": "2210.05367v2-Figure3-1.png",
    "caption": " Learning curves of all algorithms in eight maps of StarCraft II.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on map 3s_vs_4z?",
    "answer": "COMA",
    "rationale": "The figure shows the learning curves of all algorithms on eight maps of StarCraft II. The learning curve for COMA on map 3s_vs_4z is the highest, indicating that it performs the best on this map.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.05367v2",
    "pdf_url": null
  },
  {
    "instance_id": "acab2d03dc79456198333ef62d300517",
    "figure_id": "2006.01964v1-Figure14-1",
    "image_file": "2006.01964v1-Figure14-1.png",
    "caption": " Depth maps projected in a virtual GS image plane, each created using one direction of the flow. Darker means closer. Notice the wrongly estimated flow in the bottom half of the image creates errors in the depth estimation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the image shows the wrongly estimated flow?",
    "answer": "The bottom half of the image.",
    "rationale": "The depth maps in the bottom half of the image are more noisy and less accurate than the depth maps in the top half of the image. This is because the flow in the bottom half of the image is wrongly estimated.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.01964v1",
    "pdf_url": null
  },
  {
    "instance_id": "f40c92cb8c784089871a39428ba8911d",
    "figure_id": "2302.04002v1-Figure4-1",
    "image_file": "2302.04002v1-Figure4-1.png",
    "caption": " (a) and (b) plot the InC/InW and InC/OoD discrimination in the image and video domain. We set the SoftMax method training from scratch as the original point and divide the coordinate system into 4 quadrants (Q1 to Q4). (TS: Train from Scratch. TP: Train from Pre-training. OE: Outlier Exposure.)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which methods are the best at discriminating InC from both InW and OoD in the image domain?",
    "answer": "TS w/ OE and TP w/ OE",
    "rationale": "The figure shows that TS w/ OE and TP w/ OE are the only methods that are located in the top right quadrant (Q1) of the image domain plot, which means that they have the highest AUROC-InC/InW and AUROC-InC/OoD scores.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.04002v1",
    "pdf_url": null
  },
  {
    "instance_id": "74b4b6f185174a4bb04e21c386943d36",
    "figure_id": "2209.14860v2-Figure3-1",
    "image_file": "2209.14860v2-Figure3-1.png",
    "caption": " Object Discovery on synthetic datasets (mean ± standard dev., 5 seeds) with 11 (MOVi-C) and 24 slots (MOVi-E). We report foreground adjusted rand index (FG-ARI) and mean best overlap (mBO). DINOSAUR uses a ViT-B/8 encoder with the MLP decoder.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best on the MOVi-C dataset according to the FG-ARI metric?",
    "answer": "DINOSAUR",
    "rationale": "The figure shows that DINOSAUR achieved the highest FG-ARI score on the MOVi-C dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.14860v2",
    "pdf_url": null
  },
  {
    "instance_id": "d5402a274e334dcc984328aefde2e149",
    "figure_id": "2210.03730v1-Figure1-1",
    "image_file": "2210.03730v1-Figure1-1.png",
    "caption": " A high-level illustration of SpeechUT. After pre-trained with speech-to-unit and unit-to-text tasks (blue arrows), the model with a shared unit encoder enables speech-to-text tasks for fine-tuning (red arrow).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two tasks that SpeechUT is pre-trained with?",
    "answer": "Speech-to-unit and unit-to-text.",
    "rationale": "The blue arrows in the figure show the pre-training tasks. The speech-to-unit task takes speech as input and outputs units, while the unit-to-text task takes units as input and outputs text.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.03730v1",
    "pdf_url": null
  },
  {
    "instance_id": "607725573e914802bdd726284ad6bb6e",
    "figure_id": "2112.04163v1-Figure6-1",
    "image_file": "2112.04163v1-Figure6-1.png",
    "caption": " Compared with the most competitive baseline on each dataset (NIQE for CelebA-HQ, LPIPS for AFHQ, SIFID for Yosemite, GMM-GIQA for Church, and PSNR for Bedroom), RISA achieves better performance on selecting the generated image with higher quality.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model generated the most realistic images?",
    "answer": "RISA",
    "rationale": "The checkmarks in the figure indicate that RISA's generated images were judged to be more realistic than the baseline model's generated images for each of the five datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.04163v1",
    "pdf_url": null
  },
  {
    "instance_id": "8f841a7f5d994aa19af06da2596a505b",
    "figure_id": "1810.11953v4-Figure41-1",
    "image_file": "1810.11953v4-Figure41-1.png",
    "caption": " CIFAR-10 small image shift, univariate two-sample tests + Bonferroni aggregation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest p-value for 10% perturbed samples?",
    "answer": "NoRed",
    "rationale": "The plot in (a) shows the p-values for different methods for 10% perturbed samples. The NoRed method has the highest p-value, as indicated by the orange line being the highest on the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.11953v4",
    "pdf_url": null
  },
  {
    "instance_id": "71ab733da6174711bb87211a676a1c83",
    "figure_id": "2106.04399v2-Figure8-1",
    "image_file": "2106.04399v2-Figure8-1.png",
    "caption": " The list of building blocks used in molecule design. The stem, the atom which connects the block to the rest of the molecule, is highlighted.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the building blocks of molecules?",
    "answer": "The building blocks of molecules are atoms.",
    "rationale": "The figure shows a list of different building blocks used in molecule design. Each building block is composed of one or more atoms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04399v2",
    "pdf_url": null
  },
  {
    "instance_id": "8c94ab8c2396465c90ab0c23c6ea564e",
    "figure_id": "1912.07095v1-Figure2-1",
    "image_file": "1912.07095v1-Figure2-1.png",
    "caption": " Diagram of our truecasing model. The input is characters, and the output hidden states from the BiLSTM are used to predict binary labels, ‘U’ for upper case, and ‘L’ for lower case. A sentence fragment is shown, but the B-LSTM takes entire sentences as input.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the input to the B-LSTM model?",
    "answer": "The input to the B-LSTM model is characters.",
    "rationale": "The figure shows that the input to the B-LSTM model is a sequence of characters, represented by the letters \"n\", \"a\", \"m\", \"e\", \"w\", \"a\", \"s\", \"a\", \"l\", \"a\", and \"n\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.07095v1",
    "pdf_url": null
  },
  {
    "instance_id": "82654ad09b68467d930d77303f49fe52",
    "figure_id": "2203.15867v1-Figure5-1",
    "image_file": "2203.15867v1-Figure5-1.png",
    "caption": " Distribution of the number of tokens across contextual descriptions in IMAGECODE.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method generated descriptions with the highest number of tokens?",
    "answer": "CID",
    "rationale": "The CID curve peaks at a higher number of tokens than the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.15867v1",
    "pdf_url": null
  },
  {
    "instance_id": "1d380808ec2e42c09177844de9557b69",
    "figure_id": "2112.02646v3-Figure4-1",
    "image_file": "2112.02646v3-Figure4-1.png",
    "caption": " Comparison of the explanations generated for an uncertain input (far left) by the baselines, GLAM-CLUE, and CLUE. H is uncertainty, d is input distance, c = H + λxd is cost. Low uncertainties in some baseline schemes are invalidated by unrealistic distances. GLAM 1/2/3 are described in the Experiments/GLAM-CLUE section. CLUE 1/2 are generated from λx = 0 and λx = 0.03 respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the explanation with the lowest uncertainty and the most realistic distance?",
    "answer": "CLUE 2.",
    "rationale": "The figure shows that CLUE 2 has the lowest uncertainty (H = 1.0) and a relatively low distance (d = 26.5). This suggests that CLUE 2 is able to generate explanations that are both certain and realistic.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.02646v3",
    "pdf_url": null
  },
  {
    "instance_id": "582c01d379b74237b3a529faec18b28f",
    "figure_id": "1809.01812v1-Figure1-1",
    "image_file": "1809.01812v1-Figure1-1.png",
    "caption": " Two NCE-based estimation algorithms, using ranking objective and binary objective respectively. ∑K k=0 pX,Y (x, ȳk) ∏ j 6=k pN (ȳj), and",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two types of NCE-based estimation algorithms?",
    "answer": "Ranking and binary.",
    "rationale": "The figure shows two algorithms, one for ranking and one for binary classification.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.01812v1",
    "pdf_url": null
  },
  {
    "instance_id": "6809f9ab154846f8bc20a0b00706a439",
    "figure_id": "2210.12429v1-Figure6-1",
    "image_file": "2210.12429v1-Figure6-1.png",
    "caption": " Average ratings difference (ScoreHuman - ScoreMachine) of “possible” and “comfort” questions by demographic.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which demographic group perceived the biggest difference in comfort between human and machine translations?",
    "answer": "People aged 50 or older.",
    "rationale": "The plot shows the average difference in comfort ratings between human and machine translations for different demographic groups. The \"diff-comfort\" bars for the 50 or older age group are the highest, indicating that this group perceived the biggest difference in comfort.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12429v1",
    "pdf_url": null
  },
  {
    "instance_id": "cb52bcb61731465984ece4837d8725ae",
    "figure_id": "2207.08220v2-Figure5-1",
    "image_file": "2207.08220v2-Figure5-1.png",
    "caption": " Downstream task results in different epochs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which self-supervised learning method performs the best on the COCO Detection task?",
    "answer": "Fast-MoCo",
    "rationale": "The figure shows the performance of different self-supervised learning methods on the COCO Detection task. The red line represents Fast-MoCo, which achieves the highest AP_bb_all score at all epochs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.08220v2",
    "pdf_url": null
  },
  {
    "instance_id": "e0c42d8008ee4c1babb7129326c85116",
    "figure_id": "1904.01160v1-Figure6-1",
    "image_file": "1904.01160v1-Figure6-1.png",
    "caption": " Median ℓ2 distance comparison of targeted adversarial noises generated using resnet18 (up) and inceptionv3 (down) as substitute model on Tiny-Imagenet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attack method consistently produces the smallest median ℓ2 distance across all models and substitute models?",
    "answer": "The pointwise attack.",
    "rationale": "The figure shows the median ℓ2 distance for different attack methods on different models and substitute models. The pointwise attack consistently has the lowest median ℓ2 distance across all models and substitute models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.01160v1",
    "pdf_url": null
  },
  {
    "instance_id": "0fd53968f47242b2ac793cca3b0e46be",
    "figure_id": "2305.15871v3-Figure2-1",
    "image_file": "2305.15871v3-Figure2-1.png",
    "caption": " Performance of the SBI methods in terms of RMSE and MMD for both the Ricker model and OUP. Each box represents the median and interquartile range (IQR), while the whiskers extend to the furthest points within 1.5 times the IQR from the edges of the box. For the well-specified case (ϵ = 0%), the proposed NPE-RS and ABC-RS methods perform similar to their counterpart NPE and ABC, respectively. Under misspecification (ϵ > 0%), NPE-RS and ABC-RS achieve lower RMSE and MMD values, demonstrating robustness to model misspecification.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of RMSE and MMD for the Ricker model under misspecification?",
    "answer": "ABC-RS.",
    "rationale": "The figure shows that for the Ricker model, ABC-RS has the lowest median RMSE and MMD values when ϵ > 0%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.15871v3",
    "pdf_url": null
  },
  {
    "instance_id": "f404910987174f5788dfa88a993adad2",
    "figure_id": "1810.11953v4-Figure10-1",
    "image_file": "1810.11953v4-Figure10-1.png",
    "caption": " MNIST large Gaussian noise shift, multivariate two-sample tests.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest p-value for all levels of perturbation?",
    "answer": "NoRed",
    "rationale": "The figure shows the p-values of different methods for different levels of perturbation. The NoRed method has the highest p-value for all levels of perturbation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.11953v4",
    "pdf_url": null
  },
  {
    "instance_id": "81c3490d23d84f54938f0c17394840fe",
    "figure_id": "2204.12294v1-Figure2-1",
    "image_file": "2204.12294v1-Figure2-1.png",
    "caption": " Number of collectedmedical articles in our dataset according to their publication year.",
    "figure_type": "** \nplot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \nIn which year was the number of articles published the highest?",
    "answer": " \n2020",
    "rationale": " \nThe figure shows a bar chart with the number of articles published on the y-axis and the year of publication on the x-axis. The bar for 2020 is the tallest, indicating that the number of articles published in 2020 was the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.12294v1",
    "pdf_url": null
  },
  {
    "instance_id": "fc68de1dcdee4f2ab91c042e4e94a4b1",
    "figure_id": "2004.02767v1-Figure5-1",
    "image_file": "2004.02767v1-Figure5-1.png",
    "caption": " FURs of convolutional layers in ResNet-18 0.8×. We sampled 8 out of 16 layers for better visualization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer of the ResNet-18 0.8× model has the lowest FUR at the end of the search?",
    "answer": "Layer 16",
    "rationale": "The plot shows the FUR of different layers of the ResNet-18 0.8× model over the course of the search. At the end of the search (iteration 8), layer 16 has the lowest FUR.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.02767v1",
    "pdf_url": null
  },
  {
    "instance_id": "f00d0488a40f45e5bdb88f667addd3b9",
    "figure_id": "2110.10031v2-Figure4-1",
    "image_file": "2110.10031v2-Figure4-1.png",
    "caption": " Accuracy-to-{number of samples} for various CL methods on CIFAR10, CIFAR100, TinyImageNet and ImageNet Our CLIB is consistent at maintaining high accuracy throughout inference while other CL methods are not as consistent.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which continual learning (CL) method consistently maintains high accuracy throughout inference on all four datasets?",
    "answer": "CLIB (Ours)",
    "rationale": "The figure shows that the CLIB (Ours) method consistently maintains high accuracy throughout inference on all four datasets (CIFAR10, CIFAR100, TinyImageNet, and ImageNet), while other CL methods experience significant drops in accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.10031v2",
    "pdf_url": null
  },
  {
    "instance_id": "d4d7c0f789ce4cfe91e3b39c323a72d4",
    "figure_id": "1806.02958v2-Figure3-1",
    "image_file": "1806.02958v2-Figure3-1.png",
    "caption": " Results of CNN and RNN experiments. GGT dominates in training loss across both tasks, and generalizes better on the RNN task. Top: CIFAR-10 classification with a 3-branch ResNet. Bottom: PTB character-level language modeling with a 3-layer LSTM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performed the best on the CIFAR-10 classification task?",
    "answer": "GGT",
    "rationale": "The top left plot shows the training loss for the CIFAR-10 classification task. GGT has the lowest training loss of all the optimizers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.02958v2",
    "pdf_url": null
  },
  {
    "instance_id": "f6c38f6b84b44ca5ad13f1be2f0e7b86",
    "figure_id": "1810.06758v3-Figure15-1",
    "image_file": "1810.06758v3-Figure15-1.png",
    "caption": " Nearest neighbors of the top left generated image in ImageNet training set in terms of VGG16 fc7 features",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the images in the figure is most similar to the top left image?",
    "answer": "The image in the top right corner.",
    "rationale": "The images in the figure are arranged in order of similarity to the top left image, with the most similar images being closest to the top left image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.06758v3",
    "pdf_url": null
  },
  {
    "instance_id": "6b93b3c7e8a54437819190243155834f",
    "figure_id": "2104.02322v1-Figure5-1",
    "image_file": "2104.02322v1-Figure5-1.png",
    "caption": " Tradeoff between video quality and bits-per-pixel for different approaches on 28 videos from Vimeo. To achieve 30dB PSNR, SRVC requires 10% and 25% of the bits-per-pixel required by H.264 and H.265.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which video compression standard achieves the highest PSNR at a bit rate of 0.1 bits per pixel?",
    "answer": "H.265 480p + Bicubic",
    "rationale": "The figure shows that the H.265 480p + Bicubic curve is the highest at a bit rate of 0.1 bits per pixel.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.02322v1",
    "pdf_url": null
  },
  {
    "instance_id": "634275c9a0764a64af53d8b8838a42fb",
    "figure_id": "2204.02877v2-Figure9-1",
    "image_file": "2204.02877v2-Figure9-1.png",
    "caption": " Episodic MC return distribution of adapted policy during GA process.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which policy has the highest return in Environment 3?",
    "answer": "Policy 6",
    "rationale": "The figure shows the episodic MC return distribution of six different policies in five different environments. In Environment 3, the return distribution of Policy 6 is shifted to the right compared to the other policies, indicating that it has the highest return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.02877v2",
    "pdf_url": null
  },
  {
    "instance_id": "c5d194af70ab4b5c926f18dd8e78f61a",
    "figure_id": "2002.06440v1-Figure2-1",
    "image_file": "2002.06440v1-Figure2-1.png",
    "caption": " Convergence rates of various methods in two federated learning scenarios: training VGG-9 on CIFAR-10 with J = 16 clients and training LSTM on Shakespeare dataset with J = 66 clients.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the fastest convergence rate on the CIFAR-10 dataset?",
    "answer": "FedAvg",
    "rationale": "The plot in Figure (c) shows that FedAvg reaches a test accuracy of 0.8 after approximately 2 GB of communication, while the other methods require more communication to reach the same level of accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.06440v1",
    "pdf_url": null
  },
  {
    "instance_id": "876f7ef8091341aa8f3510ee5f29dcc8",
    "figure_id": "2302.11381v3-Figure3-1",
    "image_file": "2302.11381v3-Figure3-1.png",
    "caption": " α = 10−50. Green curve: y = γx.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has a larger step size?",
    "answer": "The INC algorithm has a larger step size.",
    "rationale": "The figure on the right shows the step size of each algorithm as a function of the number of iterations. The INC algorithm has a step size that increases linearly with the number of iterations, while the ADA algorithm has a step size that decreases with the number of iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.11381v3",
    "pdf_url": null
  },
  {
    "instance_id": "1855ee8c220446f298f79c8ef84bcd6b",
    "figure_id": "2110.02722v2-Figure10-1",
    "image_file": "2110.02722v2-Figure10-1.png",
    "caption": " Illustration of two-sample testing with the proposed distance vs log moments and MMD on real datasets - Bioinformatics and Social Networks. The plots show the p-value of the test T . Test based on the proposed distance is better than the other two tests.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three tests performs the best in terms of identifying differences between the Bioinformatics and Social Networks datasets?",
    "answer": "The proposed distance test.",
    "rationale": "The proposed distance test produces the most significant p-values for all comparisons between the Bioinformatics and Social Networks datasets, as indicated by the darker blue colors in the heatmaps. This suggests that the proposed distance test is more effective at identifying differences between these two types of data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02722v2",
    "pdf_url": null
  },
  {
    "instance_id": "4b39bce22eba4df3bf71de9dc18e4aae",
    "figure_id": "2210.05062v3-Figure6-1",
    "image_file": "2210.05062v3-Figure6-1.png",
    "caption": " Histograms describing the variance in model results across all runs on the 30 CLRS tasks",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best in the 80-100% score range?",
    "answer": "RT (Ours)",
    "rationale": "The bar for RT (Ours) in the 80-100% score range is the tallest, indicating that this model had the highest number of random runs in this score range.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.05062v3",
    "pdf_url": null
  },
  {
    "instance_id": "101a49e81fde490d8b601d5830a594d6",
    "figure_id": "1902.09667v1-Figure2-1",
    "image_file": "1902.09667v1-Figure2-1.png",
    "caption": " Comparison of different ranking functions using P@k metric in all domains. Higher values are better.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which ranking function performs the best in the HT Domain?",
    "answer": "The ENSEMBLE ranking function.",
    "rationale": "The figure shows the P@k values for different ranking functions in the HT Domain. The ENSEMBLE ranking function has the highest P@k value of 80%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.09667v1",
    "pdf_url": null
  },
  {
    "instance_id": "eec638332e3646ecada40d2b64471446",
    "figure_id": "2207.04049v1-Figure4-1",
    "image_file": "2207.04049v1-Figure4-1.png",
    "caption": " Comparison of the performance of ITE estimation under different values of 𝛽 in linear setting on GoodReads.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of  εATE  when 𝛽 = 5.0?",
    "answer": "GCN-HSIC.",
    "rationale": "The bar corresponding to GCN-HSIC is the highest when 𝛽 = 5.0 in subfigure (b).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.04049v1",
    "pdf_url": null
  },
  {
    "instance_id": "8c8d5439de434090a0e7018eda3052eb",
    "figure_id": "1811.01146v3-Figure4-1",
    "image_file": "1811.01146v3-Figure4-1.png",
    "caption": " Stochastic Up-Sampling of CloGAN. A) SVHN. B) E-MNIST. The contribution of upsampling is indicated by a positive gap between CloGAN and Frozen-CloGAN as more tasks are learned. We also compare to the MT condition in which training is re-started at each task, eliminating forward-transfer of possible shared task features.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on Task 8 with a buffer size of 1% of the dataset?",
    "answer": "MT",
    "rationale": "The figure shows the accuracy of different methods on different tasks with different buffer sizes. The MT line is the highest at Task 8 with a buffer size of 1%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.01146v3",
    "pdf_url": null
  },
  {
    "instance_id": "3d9922c309dc413395d6b4d4315abaef",
    "figure_id": "2011.07439v1-Figure6-1",
    "image_file": "2011.07439v1-Figure6-1.png",
    "caption": " (a) λ “ t10´200, 10´150, 10´100, 10´50, 10´20, 10´5, λoptu. (b) λ “ tλopt, 0.1, 0.3, 0.5, 0.7, 0.9, 0.99u. (c) The structure of the target dense teacher network.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between λ and sparsity?",
    "answer": "Sparsity increases as λ increases.",
    "rationale": "The figure shows that the log of sparsity increases as the log of λ increases. This means that the sparsity of the network increases as the regularization parameter λ increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.07439v1",
    "pdf_url": null
  },
  {
    "instance_id": "447b06461bcf43b68268b8fb5c696a71",
    "figure_id": "2006.12871v2-Figure6-1",
    "image_file": "2006.12871v2-Figure6-1.png",
    "caption": " PPCA self-masking known: Imputation RMSE at varying missing rates on UCI datasets. The variation in missing rate is obtained by changing the cutoff point using an offset, so that an offset = 0 corresponds to using the mean as the cutoff point while an offset = 1 corresponds to using the mean plus one standard deviation as the cutoff point. Results are averages over 2 runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the largest difference in RMSE between PPCA and not-MIWAE PPCA?",
    "answer": "Red",
    "rationale": "The Red dataset shows the largest difference in RMSE between PPCA and not-MIWAE PPCA, with PPCA having a much lower RMSE than not-MIWAE PPCA at all offset values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.12871v2",
    "pdf_url": null
  },
  {
    "instance_id": "0d9f972a57aa4c2981de07e5a3b4b9d5",
    "figure_id": "2106.04228v2-Figure1-1",
    "image_file": "2106.04228v2-Figure1-1.png",
    "caption": " Existing results depending on the slack η. Our result is highlighted in red.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of strategies are stable when the slack is between 1 and 2?",
    "answer": "Stable decentralized strategies.",
    "rationale": "The figure shows that stable decentralized strategies are located in the region between 1 and 2 on the x-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04228v2",
    "pdf_url": null
  },
  {
    "instance_id": "294d3b669dbe4f71a0a26c528ead7026",
    "figure_id": "2306.05579v2-Figure1-1",
    "image_file": "2306.05579v2-Figure1-1.png",
    "caption": " The regret of different methods in settings with both time-invariant and time-varying graphs",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest regret in the time-invariant graph setting?",
    "answer": "DrFed-UCB",
    "rationale": "The shaded area in Figure (a) shows the range of regret values for each method. The DrFed-UCB line is at the bottom of the shaded area, indicating that it has the lowest regret.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.05579v2",
    "pdf_url": null
  },
  {
    "instance_id": "b5b5ca0bddc34d1a9aa92fc4d77daf01",
    "figure_id": "2106.05387v2-Figure3-1",
    "image_file": "2106.05387v2-Figure3-1.png",
    "caption": " Training performance (showing mean and standard deviation averaged over 5 runs) for the three difficulty levels: Easy (left), Medium (middle), Hard (right). Higher normalized score is better, while lower number of steps is better. Our Method refers to our SceneIT technique.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the \"Hard\" difficulty level?",
    "answer": "Our Method",
    "rationale": "The figure shows that Our Method achieves the highest normalized score and the lowest number of steps on the Hard difficulty level.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05387v2",
    "pdf_url": null
  },
  {
    "instance_id": "c3262b0d06d74c4aab4c61e4e82908dc",
    "figure_id": "1905.11828v2-Figure8-1",
    "image_file": "1905.11828v2-Figure8-1.png",
    "caption": " Privacy loss under different tightness",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the highest privacy for a tightness of 0.5?",
    "answer": "AsyncDPOP with (k_p = 2, k_c = ∞)",
    "rationale": "The figure shows the privacy loss of different algorithms as a function of tightness. At a tightness of 0.5, the line for AsyncDPOP with (k_p = 2, k_c = ∞) is the highest, indicating that this algorithm achieves the highest privacy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11828v2",
    "pdf_url": null
  },
  {
    "instance_id": "f216114cc0644bb7afd53f72ff2a5fae",
    "figure_id": "2306.15713v1-Figure11-1",
    "image_file": "2306.15713v1-Figure11-1.png",
    "caption": " Our agent is initiated with very low velocity. It has learned that it must speed up in order to merge into traffic.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the green line in the figure represent?",
    "answer": "The green line represents the path of the agent.",
    "rationale": "The green line is shown to be moving from the left lane to the right lane, which is the action of merging into traffic.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.15713v1",
    "pdf_url": null
  },
  {
    "instance_id": "bc13d32813f646d28e55e7296ac5bc91",
    "figure_id": "1905.09591v1-Figure4-1",
    "image_file": "1905.09591v1-Figure4-1.png",
    "caption": " Generative networks used in this paper",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the generative networks shown in the figure has the largest input size?",
    "answer": "G0",
    "rationale": "G0 takes a 28x28 image with c channels as input, while the other networks take either a 32x32 image with c channels (G1) or a random Gaussian vector of size 256k (G2 and G3).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.09591v1",
    "pdf_url": null
  },
  {
    "instance_id": "4927adcc24b34e4893aca85702833baf",
    "figure_id": "1805.11057v2-Figure11-1",
    "image_file": "1805.11057v2-Figure11-1.png",
    "caption": " Testing reconstructions produced by our DPLC model with WAE G?, along with the original image (green border), for LSUN bedrooms. The reconstructions are blurry at all rates.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How do the reconstructions produced by the DPLC model with WAE G? compare to the original image?",
    "answer": "The reconstructions are blurry at all rates.",
    "rationale": "The figure shows the original image (green border) and the reconstructions produced by the DPLC model with WAE G? at different bit rates. The reconstructions are all blurry, and the quality of the reconstructions does not improve significantly as the bit rate increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.11057v2",
    "pdf_url": null
  },
  {
    "instance_id": "023e629efa1b4a91b301936efdef4dac",
    "figure_id": "2005.05672v1-Figure2-1",
    "image_file": "2005.05672v1-Figure2-1.png",
    "caption": " Silver evaluation results in Pearson’s r. Languages (x-axis) are sorted according to mean correlation.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the highest correlation with valence?",
    "answer": "English.",
    "rationale": "The plot shows the correlation between different emotions and different languages. The language with the highest correlation with valence is the language with the highest point on the blue line, which is English.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.05672v1",
    "pdf_url": null
  },
  {
    "instance_id": "1d6fbdfd2bfe4c9195f8d8ff44fb8053",
    "figure_id": "2211.16199v3-Figure13-1",
    "image_file": "2211.16199v3-Figure13-1.png",
    "caption": " Comparison of final inferred latent graph (after 1,000 epochs of training) for the Wisconsin dataset by (a) the GCN-dDGM˚-EHS and (b) the GCN-dDGM-EHS model, both with k “ 10.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two models, GCN-dDGM˚-EHS or GCN-dDGM-EHS, is better at preserving the original graph structure?",
    "answer": "GCN-dDGM˚-EHS",
    "rationale": "The figure shows the final inferred latent graphs for the Wisconsin dataset by the two models. The GCN-dDGM˚-EHS model (a) produces a graph that is more similar to the original graph than the GCN-dDGM-EHS model (b). This can be seen by the fact that the nodes in the GCN-dDGM˚-EHS graph are more clustered together, and the edges are more likely to connect nodes that are close together in the original graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.16199v3",
    "pdf_url": null
  },
  {
    "instance_id": "198a87514bcc477383c41d861c0c464f",
    "figure_id": "2205.15142v2-Figure13-1",
    "image_file": "2205.15142v2-Figure13-1.png",
    "caption": " The value of loss along the line between the first point in the trajectory of GD with small learning rate and different points in the trajectory of GD with a large learning rate. For more detailed explanation of the settings, refer to Section 5.2. Each line corresponds to the value of loss measured on 30 points along the line between the initialization and the parameters after an step. The step number for each line is written in the box located on the top-right of the plot.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the loss and the distance norm?",
    "answer": "The loss increases as the distance norm increases.",
    "rationale": "The figure shows that the loss curves are all upward-trending, which means that the loss increases as the distance norm increases. This is true for all of the different step numbers shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.15142v2",
    "pdf_url": null
  },
  {
    "instance_id": "eacd60dcc4694f76a6034de05656533a",
    "figure_id": "2008.09777v4-Figure24-1",
    "image_file": "2008.09777v4-Figure24-1.png",
    "caption": " Scatter plots of the predicted performance against the true performance of the GNN GIN/XGB surrogate models trained with different ratios of training data. \"RS\" indicates that the training set only includes architectures from random search, \"mixed\" indicates the training set includes architectures from all optimizers. Training set sizes are identical for the two cases. The test set contains architectures from all optimizers. For better display, we show 1000 randomly sampled architectures (blue) and 1000 architectures sampled from the top 1000 architectures (orange). For each case we also show the R2 and Kendall-τ coefficients on the whole test set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model and training set combination performs the best according to the R2 metric?",
    "answer": "GNN GIN mixed with a training ratio of 100%.",
    "rationale": "The R2 metric measures the goodness of fit of a model. The higher the R2 value, the better the model fits the data. In the figure, the GNN GIN mixed model with a training ratio of 100% has the highest R2 value of 0.90.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.09777v4",
    "pdf_url": null
  },
  {
    "instance_id": "e3b0ada729db4beb97f5e1781a76a61f",
    "figure_id": "2103.01197v2-Figure8-1",
    "image_file": "2103.01197v2-Figure8-1.png",
    "caption": " A sample from the sort-of-clevr dataset.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many objects are in the image?",
    "answer": "6",
    "rationale": "The image shows six objects of different shapes and colors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.01197v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd0526685658437ea309f0463c14154a",
    "figure_id": "2208.11125v1-Figure5-1",
    "image_file": "2208.11125v1-Figure5-1.png",
    "caption": " Hits@1and recall of preserved align. on EN-FR15K.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best in terms of Hits@1 and recall of preserved alignment?",
    "answer": "LargeGNN (CSG)",
    "rationale": "The figure shows that the blue line, which represents LargeGNN (CSG), is consistently higher than the other two lines, indicating that it has the highest Hits@1 and recall of preserved alignment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.11125v1",
    "pdf_url": null
  },
  {
    "instance_id": "d57044ec57014fa4b86cb79dff4ca270",
    "figure_id": "2109.02707v2-Figure10-1",
    "image_file": "2109.02707v2-Figure10-1.png",
    "caption": " An example from Rotowire which requires reasoning to perform information extraction. The article in Rotowire reports a game between the two teams “Nets” and “Wizards”. From the sentence: “The Nets seized control of this game from the very start, opening up a 31 - 14 lead after the first quarter”, humans can infer that the point of “Wizards” is 14, which is still difficult for machines.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many points did the Nets score in the first quarter?",
    "answer": "31 points.",
    "rationale": "The table in the figure shows that the Nets scored 31 points in the first quarter.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.02707v2",
    "pdf_url": null
  },
  {
    "instance_id": "d07798b544ff4316bd1a6849a4048f03",
    "figure_id": "2010.09345v4-Figure37-1",
    "image_file": "2010.09345v4-Figure37-1.png",
    "caption": " Discovered concepts using ACE for 3 classes on CIFAR-10 (Top) and QuickDraw (Bottom). We show the top 3 concepts according to their TCAV scores. Each concept consists of 4 segments extracted from images of the class. They are shown in 2 rows, the first contains the segments and the second shows where the segment was extracted from.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three classes of objects in the CIFAR-10 dataset seems to have the most consistent and well-defined concepts?",
    "answer": "Ships.",
    "rationale": "The concepts for ships are all relatively similar, consisting of a long, thin hull and a superstructure. In contrast, the concepts for trucks and cars are more varied, with some concepts focusing on the front of the vehicle, while others focus on the side or back.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.09345v4",
    "pdf_url": null
  },
  {
    "instance_id": "c6321c0abc1540f2ba009f1fd22ed22c",
    "figure_id": "1909.12255v3-Figure23-1",
    "image_file": "1909.12255v3-Figure23-1.png",
    "caption": " Additional results of SV-RL on DQN (Part A).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How do the performances of DQN and DQN+SV compare in the Alien game?",
    "answer": "DQN+SV performs better than DQN in the Alien game.",
    "rationale": "The plot for the Alien game shows that the orange line (DQN+SV) is generally higher than the blue line (DQN), indicating that DQN+SV achieves higher scores than DQN.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.12255v3",
    "pdf_url": null
  },
  {
    "instance_id": "aeccad99640d41ffb59325febb225534",
    "figure_id": "2105.14953v1-Figure3-1",
    "image_file": "2105.14953v1-Figure3-1.png",
    "caption": " ODE state 𝒉(𝑡) and pairwise attention 𝜎 (𝒂(𝑡)) examples. (a) A 3-dimensional ODE state describing the traffic condition of the three regions of New York (b) A 3 × 3 pairwise attention matrix describing the attention among the three regions. Each region has the highest attention to itself, followed by attention to neighboring regions. This attention says that a traffic condition in a region at time 𝑡 + 𝑠 has high correlations to the traffic conditions in the same and neighboring regions at time 𝑡 .",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which region has the highest attention to itself?",
    "answer": "Uptown",
    "rationale": "The diagonal elements of the attention matrix represent the attention of each region to itself. The element in the first row and first column of the attention matrix is 0.5, which is the highest value on the diagonal. Therefore, Uptown has the highest attention to itself.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14953v1",
    "pdf_url": null
  },
  {
    "instance_id": "d71e1ce246f74cae96b9aa3190783333",
    "figure_id": "2210.12487v1-Figure1-1",
    "image_file": "2210.12487v1-Figure1-1.png",
    "caption": " A logical passage and the corresponding logic metagraph in the proposed MetaLogic. Given a logical passage, the goal is to generate the full metagraph including the chain of reasoning with conditions of rebuttal, the node formulae, and the degrees of certainty.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the condition of rebuttal for the chain of reasoning in the logic metagraph?",
    "answer": "The condition of rebuttal is ¬□(¬□v3 → v1).",
    "rationale": "The condition of rebuttal is shown as an arrow pointing to the node for sentence 4. This indicates that the chain of reasoning is rebutted if ¬□(¬□v3 → v1) is true.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12487v1",
    "pdf_url": null
  },
  {
    "instance_id": "25f58ed6fa2b4130b0a2cf2b7e7fbff8",
    "figure_id": "2001.11128v1-Figure3-1",
    "image_file": "2001.11128v1-Figure3-1.png",
    "caption": " Relative improvements (in percentage) on speech recognition on many languages with CPC-8k features over Spectrogram features. Each column correspond to language code explained in Table 3. Note that en is Nigerian English and fr is African French.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language shows the least improvement with CPC-8k features over Spectrogram features?",
    "answer": "cs",
    "rationale": "The bar corresponding to cs is the shortest, indicating the least improvement.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.11128v1",
    "pdf_url": null
  },
  {
    "instance_id": "134c1dbed07441948b306fe4c9e014b4",
    "figure_id": "2002.11318v5-Figure19-1",
    "image_file": "2002.11318v5-Figure19-1.png",
    "caption": " Visualization of performance of CuSP based on PGD against other baseline strategies on CIFAR10 for StdCNN/ResNet18 model (each index corresponds to a row in Table 5). On the left, we compare CuSP against various baselines. On the right, we zoom in to compare different variants of CuSP.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does CuSP perform against the baseline strategies on CIFAR10 for StdCNN/ResNet18 model?",
    "answer": "CuSP performs better than the baseline strategies.",
    "rationale": "The figure shows the performance of CuSP and the baseline strategies on CIFAR10 for StdCNN/ResNet18 model. The x-axis shows the spatial accuracy and the y-axis shows the PGD accuracy. CuSP is shown in blue and the baseline strategies are shown in purple. The points for CuSP are located above and to the right of the points for the baseline strategies, indicating that CuSP has higher spatial accuracy and PGD accuracy than the baseline strategies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11318v5",
    "pdf_url": null
  },
  {
    "instance_id": "24cf9ea567a640b0982b9b1333d5df09",
    "figure_id": "2110.11420v2-Figure2-1",
    "image_file": "2110.11420v2-Figure2-1.png",
    "caption": " Selected keyframes for video “v25.mpg” in VSUMM dataset in comparison to the ground-truth (GT) and MSR [12]",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, MSR or Ours, is more accurate in generating keyframes for the video \"v25.mpg\"?",
    "answer": "Ours is more accurate.",
    "rationale": "The keyframes generated by Ours are more similar to the ground-truth keyframes than the keyframes generated by MSR. This can be seen in the images, where the keyframes generated by Ours are visually closer to the ground-truth keyframes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.11420v2",
    "pdf_url": null
  },
  {
    "instance_id": "8c445aff8fdc457291b8de69083c2a03",
    "figure_id": "2004.06502v1-Figure9-1",
    "image_file": "2004.06502v1-Figure9-1.png",
    "caption": " Video screen cut of the label-to-image qualitative comparison.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most realistic images?",
    "answer": "Ground Truth",
    "rationale": "The Ground Truth images are the most realistic because they are the actual images that the other methods are trying to reproduce.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.06502v1",
    "pdf_url": null
  },
  {
    "instance_id": "6d83b951e8f74ce3b73edfb65fc69eb7",
    "figure_id": "1905.05659v2-Figure4-1",
    "image_file": "1905.05659v2-Figure4-1.png",
    "caption": " Reward vs. number of iterations of the three selection strategies on MovieLens and Cora.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which selection strategy performed the best on MovieLens?",
    "answer": "NC",
    "rationale": "The NC line is the highest of the three lines in the MovieLens plot, indicating that it had the highest reward.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.05659v2",
    "pdf_url": null
  },
  {
    "instance_id": "16a989b671ba472b88126fcdd1f3bb58",
    "figure_id": "2010.11647v2-Figure2-1",
    "image_file": "2010.11647v2-Figure2-1.png",
    "caption": " Generated fake image samples from the plain VAE and the proposed QVAE.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two models, VAE or QVAE, produces sharper and more realistic images?",
    "answer": "QVAE",
    "rationale": "The images generated by QVAE are sharper and have more details than the images generated by VAE. For example, the QVAE images have clearer eyes, noses, and mouths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.11647v2",
    "pdf_url": null
  },
  {
    "instance_id": "f0db6c19fa594431960bd15faea80fef",
    "figure_id": "2302.09155v1-Figure2-1",
    "image_file": "2302.09155v1-Figure2-1.png",
    "caption": " SARI scores of ctrlSIM ′s outputs arranged by angles",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which output of ctrlSIM had the highest SARI score?",
    "answer": "ERI->RS",
    "rationale": "The bar chart shows the SARI scores for each output of ctrlSIM. The bar for ERI->RS is the highest, indicating that it has the highest SARI score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.09155v1",
    "pdf_url": null
  },
  {
    "instance_id": "db719f35e31a4857bd79689696fa8e42",
    "figure_id": "2311.04550v1-Figure3-1",
    "image_file": "2311.04550v1-Figure3-1.png",
    "caption": " Figures (a), (b) and (c) report the accepted loss (AL) for all methods with different rejection rates on abalone, auto-mpg and concrete datasets, respectively.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function performs the best on the abalone dataset?",
    "answer": "MLP-Plug.",
    "rationale": "The figure shows that the MLP-Plug loss function has the lowest AL loss for all rejection rates on the abalone dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2311.04550v1",
    "pdf_url": null
  },
  {
    "instance_id": "a03c5aac36294b4aa723dd834adee043",
    "figure_id": "1711.08566v1-Figure8-1",
    "image_file": "1711.08566v1-Figure8-1.png",
    "caption": " Initial a) and final b) maps for the ‘lost poses’ experiment. Observations are shown in orange, poses are black arrows, and ground truth (walls) is represented by the black lines. Poses involved in human constraints are colored blue.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the difference between the initial and final maps in the 'lost poses' experiment?",
    "answer": "The initial map has more poses than the final map.",
    "rationale": "The initial map (a) has several poses that are not present in the final map (b). This suggests that the algorithm has removed some poses during the optimization process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.08566v1",
    "pdf_url": null
  },
  {
    "instance_id": "a07aa793adc74c569da717d1e1fd3df6",
    "figure_id": "2211.12040v3-Figure2-1",
    "image_file": "2211.12040v3-Figure2-1.png",
    "caption": " Visualizations of low-level and high-level tasks. Top line: comparison of different INRs methods fitting an image. The ground truth is on the left. Bottom line: the results of object detection (odd column) and instance segmentation (even column) on the MS COCO dataset [20].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the INRs methods shown in the figure is most effective at fitting the image?",
    "answer": "Ours",
    "rationale": "The figure shows that our method produces an image that is most similar to the ground truth image. This is evident in the sharper details and more accurate representation of the scene.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.12040v3",
    "pdf_url": null
  },
  {
    "instance_id": "3dd427cdbb0e4009b80aff40e794a94f",
    "figure_id": "1904.10754v2-Figure2-1",
    "image_file": "1904.10754v2-Figure2-1.png",
    "caption": " Illustration of shape analogy.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between shapes SA, SB, SC, and SX?",
    "answer": "SA is to SB as SC is to SX.",
    "rationale": "The figure shows four different shapes, labeled SA, SB, SC, and SX. The first two shapes are similar in that they are both smooth and rounded, while the last two shapes are similar in that they are both angular and have sharp edges. This suggests that the relationship between the shapes is one of analogy, where the first two shapes are analogous to the last two shapes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.10754v2",
    "pdf_url": null
  },
  {
    "instance_id": "da79839a1d8e4bcabe5898e048151f51",
    "figure_id": "2004.10904v2-Figure14-1",
    "image_file": "2004.10904v2-Figure14-1.png",
    "caption": " Results on 3D reconstruction for four real transparent objects. All shapes are reconstructed from 10 views, except the monkey in the last row that uses 12 views. We first present reconstruction results from two input views (columns 1-6). From left to right, the odd rows show the input image and the reconstructed shapes under different lighting and materials. The corresponding outputs using the ground-truth shapes rendered from the same view are shown in the even rows. We also render the reconstructed shapes and ground-truth shapes from a novel view direction that has not been used to build the visual hull (columns 7-8). In each instance, we observe that the reconstructions are close to the ground truth despite the challenging shapes, complex light paths and small number of views used for 3D reconstruction.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many views were used to reconstruct the monkey in the last row?",
    "answer": "12 views.",
    "rationale": "The caption states that \"All shapes are reconstructed from 10 views, except the monkey in the last row that uses 12 views.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.10904v2",
    "pdf_url": null
  },
  {
    "instance_id": "de4d34e408ce40c88370572434e1fe56",
    "figure_id": "2307.02770v2-Figure22-1",
    "image_file": "2307.02770v2-Figure22-1.png",
    "caption": " Third set (193–288) of images among the 500 non-curated censored generation samples with a reward model ensemble and without backward guidance and recurrence. Malign images are labeled with red borders and positioned at the beginning for visual clarity. Qualitatively and subjectively speaking, we observe that censoring makes the malign images less severely “broken” compared to the malign images of the uncensored generation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the malign images?",
    "answer": "The malign images are the images with red borders.",
    "rationale": "The caption states that \"Malign images are labeled with red borders.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.02770v2",
    "pdf_url": null
  },
  {
    "instance_id": "cb2d3aa2d57d4e09b52e44335d6255a0",
    "figure_id": "2105.08683v1-Figure1-1",
    "image_file": "2105.08683v1-Figure1-1.png",
    "caption": " A Knowledge graph with numeric attributes associated to triples.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which skill is more likely to be required by an ML Engineer, JavaScript or Python?",
    "answer": "Python.",
    "rationale": "The \"requiresSkill\" relationship between \"ML Engineer\" and \"Python\" has a higher weight (0.98) than the relationship between \"ML Engineer\" and \"JavaScript\" (0.13).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.08683v1",
    "pdf_url": null
  },
  {
    "instance_id": "c1f55dd78540486f912d848ec9b213e5",
    "figure_id": "2205.02904v1-Figure11-1",
    "image_file": "2205.02904v1-Figure11-1.png",
    "caption": " Learning volumetric ARAP [Sorkine and Alexa 2007] deformations. We train our network to predict the boundary-surface deformation induced by volumetric ARAP deformations of a single model, conditioned on the positions of predesignated constraint handles. In each triplet we show, from left to right, the ground truth, us, and the result computed by a fixed-triangulation architecture, similar to [Tan et al. 2018].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most accurate results?",
    "answer": "The ground truth (GT) method.",
    "rationale": "The figure shows three different methods for predicting boundary-surface deformation: GT, Our Method, and Fixed-Triangulation MLP. The GT method is shown in the left column of each triplet, and it is clear that the results are the most accurate. The other two methods produce results that are less accurate, as evidenced by the artifacts and distortions in the images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.02904v1",
    "pdf_url": null
  },
  {
    "instance_id": "58b61a569456471487e6fcfdda53cd22",
    "figure_id": "2010.09046v2-Figure3-1",
    "image_file": "2010.09046v2-Figure3-1.png",
    "caption": " Results of the models that are first pretrained on Medical, Law, EUbookshop, Koran, IT, and GlobalVoices datasets and then finetuned on a Subtitles dataset. (A) is a performance comparison with respect to the number of words for adaptation. (B) is the number of iterations until the convergence during the finetuning stage with respect to the number of words. (C) is the number of iterations until convergence, where the BLEU is validating scores calculated by the average of En-De and De-En.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best in terms of BLEU score?",
    "answer": "MetaGUMT",
    "rationale": "Figure (A) shows that MetaGUMT achieved the highest BLEU score of all the models, regardless of the number of words used for adaptation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.09046v2",
    "pdf_url": null
  },
  {
    "instance_id": "d755807c82bd47c2a83e3ea3dbf0dcf8",
    "figure_id": "2112.05125v2-Figure3-1",
    "image_file": "2112.05125v2-Figure3-1.png",
    "caption": " Distribution of PAN named entities",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which named entity class is the most frequent in the PAN dataset?",
    "answer": "person",
    "rationale": "The bar chart shows that the \"person\" class has the highest percentage (95.46%) of all named entities in the PAN dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.05125v2",
    "pdf_url": null
  },
  {
    "instance_id": "767e4856eb8c4fdcb5e080fd4b109d6c",
    "figure_id": "2212.00373v3-Figure2-1",
    "image_file": "2212.00373v3-Figure2-1.png",
    "caption": " Average accuracy (%) on test sets at different noise levels δ. SOIREDL-ipt and RE2RNN-ipt represent the learnt SOIREs or automata, whereas SOIREDL-net and RE2RNNnet represent the neural networks. (a) Positive and negative strings. (b) Positive strings only.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method performs best for positive and negative strings at a noise level of 0.15?",
    "answer": " SOIREDL-ipt",
    "rationale": " The figure shows the accuracy of different methods for different noise levels. For positive and negative strings at a noise level of 0.15, SOIREDL-ipt has the highest accuracy. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.00373v3",
    "pdf_url": null
  },
  {
    "instance_id": "82e9d54d5e6f4e95bfc010af7ae69f79",
    "figure_id": "2209.12714v1-Figure3-1",
    "image_file": "2209.12714v1-Figure3-1.png",
    "caption": " Distribution of greedy oracles over top beams across three validation sets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which summarization method performs best in terms of the Greedy Oracle metric?",
    "answer": "CNN/DM",
    "rationale": "The plot shows the distribution of Greedy Oracle scores for three summarization methods across different validation sets. The CNN/DM method has the highest distribution of scores, indicating that it performs best according to this metric.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.12714v1",
    "pdf_url": null
  },
  {
    "instance_id": "c3324ac96c714fb7ad3edf192213907c",
    "figure_id": "2003.05593v4-Figure1-1",
    "image_file": "2003.05593v4-Figure1-1.png",
    "caption": " State-of-the-art part segmentation performance comparison on ShapeNet, where IoU denotes intersection-over-union.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the best instance mean IoU and categorical mean IoU on ShapeNet?",
    "answer": "Ours",
    "rationale": "The figure shows the instance mean IoU and categorical mean IoU for different part segmentation methods on ShapeNet. The method labeled \"Ours\" has the highest values for both metrics, indicating that it achieves the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.05593v4",
    "pdf_url": null
  },
  {
    "instance_id": "f6e22d8a3de04e8e964f3f23a2bb24dc",
    "figure_id": "2010.01804v2-Figure6-1",
    "image_file": "2010.01804v2-Figure6-1.png",
    "caption": " Comparison of semi-supervised vertex classification accuracies with a few selected and labeled data by using different vertex selection methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which vertex selection method performs better on the Cora dataset?",
    "answer": "VIPool Select.",
    "rationale": "The plot shows that the accuracy of VIPool Select is consistently higher than that of Random Sampling for all numbers of selected vertices on the Cora dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.01804v2",
    "pdf_url": null
  },
  {
    "instance_id": "3f4073a48361431fa50cd1012df40612",
    "figure_id": "2301.11695v3-Figure3-1",
    "image_file": "2301.11695v3-Figure3-1.png",
    "caption": " Test performance v.s. training set size for the MNIST, Kuzushiji-MNIST and Fashion-MNIST datasets. We compare the ten-class classification accuracy of LegendreTron (LT), multinomial logistic regression (MLR) and ISGP-Linkgistic (ISGP) where the ISGP combines 10 one-vs-rest binary models while the former two algorithms model the probabilities of all 10 classes jointly.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the MNIST dataset?",
    "answer": "MLR",
    "rationale": "The figure shows the test performance of three models on three datasets. The MLR model has the highest classification accuracy on the MNIST dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.11695v3",
    "pdf_url": null
  },
  {
    "instance_id": "c272ddb2eef041e38d9ed8314a573411",
    "figure_id": "2009.08576v2-Figure16-1",
    "image_file": "2009.08576v2-Figure16-1.png",
    "caption": " For each actual sparsity (x-axis), the ratio of the effective parameter-count of the randomly shuffled ablation to the effective parameter-count of the network pruned with the unmodified pruning method. Note that we have zoomed into the range of ratios between 0.99 and 1.05.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pruning method is the most effective at reducing the effective parameter count of ResNet-50 on ImageNet?",
    "answer": "SynFlow",
    "rationale": "The figure shows that SynFlow has the lowest ratio of effective parameter count to the original network at all sparsity levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.08576v2",
    "pdf_url": null
  },
  {
    "instance_id": "8745ae1522e84244953f5fcfdd95774a",
    "figure_id": "2006.04139v2-FigureD.5-1",
    "image_file": "2006.04139v2-FigureD.5-1.png",
    "caption": "Figure D.5. Visual comparison of different SR methods on Sun80 [26] dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of recovering fine details in the image?",
    "answer": "TTSR(Ours)",
    "rationale": "The TTSR(Ours) method is able to recover the fine details in the image, such as the window panes and the boat's railing, more accurately than the other methods. This can be seen in the close-up images of the window and the boat.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04139v2",
    "pdf_url": null
  },
  {
    "instance_id": "812c2108da034fef8ccfc0081e3aa509",
    "figure_id": "2012.06979v1-Figure13-1",
    "image_file": "2012.06979v1-Figure13-1.png",
    "caption": " AFS vs baseline: RELATHE, k = 10. Top: full experiment. Bottom: Zoom in.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which selection strategy performed the best in the experiment?",
    "answer": "AFS",
    "rationale": "The figure shows that AFS (green line) has the lowest mutual information gap for all budgets, which means it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.06979v1",
    "pdf_url": null
  },
  {
    "instance_id": "9116805f85564a9099f918403645118e",
    "figure_id": "1809.10460v3-Figure3-1",
    "image_file": "1809.10460v3-Figure3-1.png",
    "caption": " t-SNE visualization of the d-vector embeddings of real and SEA-ALL-generated utterances, for both the LibriSpeech (T ≤ 5 mins) and VCTK (T ≤ 10 mins) evaluation datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has a larger overlap between real and generated utterances, according to the t-SNE visualization?",
    "answer": "LibriSpeech",
    "rationale": "The t-SNE visualization shows that the generated and real utterances are more closely clustered together in the LibriSpeech dataset than in the VCTK dataset. This suggests that the generated utterances are more similar to the real utterances in the LibriSpeech dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.10460v3",
    "pdf_url": null
  },
  {
    "instance_id": "ed577c37d2cc48bcbe79c189b32ddac7",
    "figure_id": "2003.01747v2-Figure1-1",
    "image_file": "2003.01747v2-Figure1-1.png",
    "caption": " Austen plot showing how strong an unobserved confounder would need to be to induce a bias of 2 in an observational study of the effect of combination blood pressure medications on diastolic blood pressure [Dor+16]. We chose this bias to equal the nominal average treatment effect estimated from the data. We model the outcome with Bayesian Additive Regression Trees and the treatment assignment with logistic regression—Austen plots accommodate any choice of models. The curve shows all values treatment and outcome influence that would induce a bias of 2. The colored dots show the influence strength of (groups of) observed covariates, given all other covariates. For example, an unobserved confounder with as much influence as the patient’s age might induce a bias of about 2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the minimum influence on the treatment that an unobserved confounder would need to have in order to induce a bias of 2 in the study?",
    "answer": "0.2",
    "rationale": "The Austen plot shows the relationship between the influence of a confounder on the treatment and the influence of the confounder on the outcome. The curve in the plot shows all values of treatment and outcome influence that would induce a bias of 2. The minimum influence on the treatment that an unobserved confounder would need to have in order to induce a bias of 2 is the point where the curve intersects the x-axis, which is approximately 0.2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.01747v2",
    "pdf_url": null
  },
  {
    "instance_id": "6536e809dbe448a8b8114a71032b071e",
    "figure_id": "2012.10066v1-Figure1-1",
    "image_file": "2012.10066v1-Figure1-1.png",
    "caption": " Illustration of Point Cloud Frame Interpolation. The blue and green point clouds are two input frames and the red point clouds are four interpolated frames. We zoom in an area to display the details for better visualization.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many frames are shown in the figure?",
    "answer": "Six.",
    "rationale": "The figure shows two input frames (blue and green) and four interpolated frames (red).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.10066v1",
    "pdf_url": null
  },
  {
    "instance_id": "16ac4d73e25e4d9691b70cba832d93cd",
    "figure_id": "2212.06801v2-Figure7-1",
    "image_file": "2212.06801v2-Figure7-1.png",
    "caption": " Model performance across scrambling conditions (none = original, unmodified items). Error bars denote 95% CI. Dashed line indicates random baseline.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the task of identifying indirect speech?",
    "answer": "text-davinci-002",
    "rationale": "The figure shows the proportion of correct responses for each model on each task. For the task of identifying indirect speech, the text-davinci-002 model had the highest proportion of correct responses across all scrambling conditions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.06801v2",
    "pdf_url": null
  },
  {
    "instance_id": "7948e269d98f431d9000f2f813fa5175",
    "figure_id": "2202.10986v1-Figure3-1",
    "image_file": "2202.10986v1-Figure3-1.png",
    "caption": " Similarly to Figure 2, for the case where the highest threat index is not an integer, e.g. µv = 3.6 = 1 + (1 + (1 + 0.6t1 t1 · 1)) and µw = 1 + t1",
    "figure_type": "** schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the value of µw?",
    "answer": " 1 + t1",
    "rationale": " The value of µw is shown in the top left corner of the figure. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.10986v1",
    "pdf_url": null
  },
  {
    "instance_id": "d8e17d8053f345d49c150947ff17ea04",
    "figure_id": "2104.11216v1-Figure7-1",
    "image_file": "2104.11216v1-Figure7-1.png",
    "caption": " Comparison of HierchVid [29] (blue), HistryRep [32] (orange) and ours (green) on video prediction. Top row: KD↓ (keypoint difference), bottom row: LPIPS↓ on loops extracted from cardio00 and cardio01.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the cardio01 dataset?",
    "answer": "Ours (green)",
    "rationale": "The figure shows that the green line (ours) has the lowest LPIPS score for both cardio00 and cardio01 datasets, which indicates that our method performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.11216v1",
    "pdf_url": null
  },
  {
    "instance_id": "4952ecff516940c88bea8821cb545b2b",
    "figure_id": "1905.11912v2-Figure2-1",
    "image_file": "1905.11912v2-Figure2-1.png",
    "caption": " Discrimination accuracy on CelestialBody and Wiki-WSJ with different portions of all valid samples. The x axis is in log-scale.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has higher discrimination accuracy, Wiki-WSJ or CelestialBody?",
    "answer": "CelestialBody",
    "rationale": "The red line in the figure represents the discrimination accuracy of CelestialBody, while the blue dashed line represents the discrimination accuracy of Wiki-WSJ. As the portion of negative samples increases, the discrimination accuracy of CelestialBody remains higher than that of Wiki-WSJ.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11912v2",
    "pdf_url": null
  },
  {
    "instance_id": "c967855c1f674eb4b40d072f59e0ea41",
    "figure_id": "2302.11211v1-Figure8-1",
    "image_file": "2302.11211v1-Figure8-1.png",
    "caption": " Comparison of M2 validity as a function of the l1 distance between input instance and the recourse for our DiRRAc method and ROAR on synthetic data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more valid according to the plot, DiRRAc or ROAR?",
    "answer": "DiRRAc",
    "rationale": "The plot shows that the M2 validity of DiRRAc is higher than that of ROAR for all values of l1 cost.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.11211v1",
    "pdf_url": null
  },
  {
    "instance_id": "510927894c094b12af97f1cb272f3d56",
    "figure_id": "2303.06060v5-Figure8-1",
    "image_file": "2303.06060v5-Figure8-1.png",
    "caption": " Overall model rankings of the similarity scores on Macaque-Face dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best according to the three similarity scores?",
    "answer": "Wide-SEW-ResNet14",
    "rationale": "The figure shows the similarity scores of different models on the Macaque-Face dataset. The Wide-SEW-ResNet14 model has the highest similarity score for all three metrics (SVCCA, TSVD-Regression, and RSA).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.06060v5",
    "pdf_url": null
  },
  {
    "instance_id": "85c789ba69fe4774a812c409e35dbedc",
    "figure_id": "2305.02093v1-Figure5-1",
    "image_file": "2305.02093v1-Figure5-1.png",
    "caption": " Time step vs. test utility on Stagger dataset. Each shaded area corresponds to one concept; the vertical dashed line shows when the drift happens.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best after the drift occurs?",
    "answer": "UFODT-US (Adaptive)",
    "rationale": "The figure shows that UFODT-US (Adaptive) has the highest test utility after the drift occurs, which is indicated by the vertical dashed line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.02093v1",
    "pdf_url": null
  },
  {
    "instance_id": "cf1859739b01480084c700abeea60351",
    "figure_id": "2302.13522v2-Figure3-1",
    "image_file": "2302.13522v2-Figure3-1.png",
    "caption": " NLP embeddings help the GNNmodel to learn both structural and node properties while random embeddings can only learn from the structure.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of embedding results in better accuracy for the IGB-HOM-tiny model?",
    "answer": "NLP node embeddings.",
    "rationale": "The figure shows that the IGB-HOM-tiny model achieves an accuracy of 69.62% with NLP node embeddings, while it only achieves an accuracy of 25.85% with random embeddings.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.13522v2",
    "pdf_url": null
  },
  {
    "instance_id": "2e42f7b8f4d9420786f42beae33cd818",
    "figure_id": "2103.15573v1-Figure4-1",
    "image_file": "2103.15573v1-Figure4-1.png",
    "caption": " Dense correspondences (visualized as optical flow) built via nearest neighbor search and the predicted visibility masks. Our results are more accurate, smooth, and free from obvious mistakes when compared to other methods. On the right, we show the visibility probability map obtained via the distance to the nearest neighbor. Note that our feature successfully captures occluded pixels (i.e., dark pixels) in many challenging cases. The method is effective for both intra-subjects (rows 1-3) and inter-subjects (row 4).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate and smooth dense correspondences?",
    "answer": "Ours.",
    "rationale": "The caption states that \"Our results are more accurate, smooth, and free from obvious mistakes when compared to other methods.\" The figure shows that the dense correspondences produced by our method are more visually appealing and less noisy than those produced by the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.15573v1",
    "pdf_url": null
  },
  {
    "instance_id": "88d2b73fe8634a72a21f98f35df9900c",
    "figure_id": "2210.05528v1-Figure3-1",
    "image_file": "2210.05528v1-Figure3-1.png",
    "caption": " Comparing accuracy of individual models M1 and M2 on the instances answered by each model when used as cascade for MNLI dataset in K=2 setting.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the accuracy of the cascade model on instances answered by M2?",
    "answer": "65.4%",
    "rationale": "The accuracy of the cascade model on instances answered by M2 is the average of the accuracy of M1 and M2 on those instances. M1 has an accuracy of 57.5% and M2 has an accuracy of 73.3% on those instances.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.05528v1",
    "pdf_url": null
  },
  {
    "instance_id": "4a128ca97b554ea49554d63cd5358698",
    "figure_id": "2210.07612v2-Figure2-1",
    "image_file": "2210.07612v2-Figure2-1.png",
    "caption": " Error curves for mean Bayes free energy with γ = 0.1 under a range of datasets; linear kernel with λ = λ∗ (left), and λ = 0.01 (right); curves for real data match Figure 1 (top).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest mean free energy when λ = λ∗?",
    "answer": "CIFAR-10",
    "rationale": "The blue line in the left panel of the figure, which represents CIFAR-10, is consistently above the other lines, indicating that it has the highest mean free energy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.07612v2",
    "pdf_url": null
  },
  {
    "instance_id": "c9c050323d4c4ee79f3f281d9a909222",
    "figure_id": "1811.11979v1-Figure5-1",
    "image_file": "1811.11979v1-Figure5-1.png",
    "caption": " Failure cases, where some domain-specific codes do not result in well-defined styles.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the bags in the image is least likely to be generated by a well-defined style code?",
    "answer": "The bag in the top right corner.",
    "rationale": "The bag in the top right corner is black and has a simple design. This suggests that it is less likely to be generated by a complex style code than the other bags, which have more intricate designs and colors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.11979v1",
    "pdf_url": null
  },
  {
    "instance_id": "da2db5960c314a7f94d3b912f705f28d",
    "figure_id": "1906.00513v3-Figure1-1",
    "image_file": "1906.00513v3-Figure1-1.png",
    "caption": " Examples of our generated question-relevant captions. During the training phase, our model selects the most relevant human captions for each question (marked by the same color).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the boy in the foreground doing?",
    "answer": "The boy in the foreground is surfing.",
    "rationale": "The boy is standing on a surfboard and riding a wave.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.00513v3",
    "pdf_url": null
  },
  {
    "instance_id": "7d73b39b7fcb44f7a5dc1aaf274237f5",
    "figure_id": "1812.07807v2-Figure3-1",
    "image_file": "1812.07807v2-Figure3-1.png",
    "caption": " The BLEU scores (%) of generated translations on the merged four test sets with respect to the lengths of source sentences. The numbers on X-axis of the figure stand for sentences longer than the corresponding length, e.g., 40 for source sentences with > 40 words.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best for short sentences (less than 20 words)?",
    "answer": "ShallowRNNMT.",
    "rationale": "The figure shows that ShallowRNNMT has the highest BLEU score for sentences with less than 20 words.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.07807v2",
    "pdf_url": null
  },
  {
    "instance_id": "96f85ebc02714ae89d291f25affdaf0e",
    "figure_id": "2003.01515v1-Figure5-1",
    "image_file": "2003.01515v1-Figure5-1.png",
    "caption": " Objective-Incentive Curve Analysis",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is better for incentive-sensitive merchants?",
    "answer": "DNN Model",
    "rationale": "The figure shows that the DNN Model has a higher uplift gain for incentive-sensitive merchants than the GE Model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.01515v1",
    "pdf_url": null
  },
  {
    "instance_id": "228571fabb8d49079a6c35c719b04fa8",
    "figure_id": "2201.06009v7-Figure6-1",
    "image_file": "2201.06009v7-Figure6-1.png",
    "caption": " ERT-CAT: Label accuracy increases with time for all values of clarification probabilities Pr(fbi).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of clarification probability results in the highest label accuracy?",
    "answer": "1.00",
    "rationale": "The blue line in the figure represents the label accuracy for a clarification probability of 1.00. This line is consistently higher than the other lines, indicating that a clarification probability of 1.00 results in the highest label accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.06009v7",
    "pdf_url": null
  },
  {
    "instance_id": "56e9b66c8ead4e409cb493c07dd77361",
    "figure_id": "1909.00352v1-Figure3-1",
    "image_file": "1909.00352v1-Figure3-1.png",
    "caption": " Distribution of the AMR graph diameter (left) and node degree (right) in the training set for LDC2015E86 (red) and LDC2017T10 (blue) datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has a larger average node degree?",
    "answer": "LDC2017T10.",
    "rationale": "The right-hand plot shows the distribution of node degrees for both datasets. The blue curve, which represents LDC2017T10, is shifted to the right compared to the red curve, which represents LDC2015E86. This indicates that the average node degree is larger for LDC2017T10.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.00352v1",
    "pdf_url": null
  },
  {
    "instance_id": "560e1c6d84d2497c82ae1c34083e613f",
    "figure_id": "1908.11425v2-Figure3-1",
    "image_file": "1908.11425v2-Figure3-1.png",
    "caption": " Distribution of topics predicted for the 5K audio utterances in eval100h. silver labels are predicted using human translations. The ST model has been trained on 20 hours of Spanish-English data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which topic was most frequently predicted by the ST model?",
    "answer": "Family-misc",
    "rationale": "The figure shows the distribution of topics predicted by the ST model, with the height of each bar representing the frequency of each topic. The bar for \"family-misc\" is the tallest, indicating that this topic was predicted most frequently.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.11425v2",
    "pdf_url": null
  },
  {
    "instance_id": "1efa51001aa5436dbf2fc86309a432d1",
    "figure_id": "2201.02510v1-Figure3-1",
    "image_file": "2201.02510v1-Figure3-1.png",
    "caption": " Precision-recall curve of MedText.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest precision at a recall of 0.5?",
    "answer": "BioBERT.",
    "rationale": "The precision-recall curve for BioBERT is above the other two curves at a recall of 0.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.02510v1",
    "pdf_url": null
  },
  {
    "instance_id": "ce031e1e884747e9b2b4c3601f91c80b",
    "figure_id": "2306.01386v1-Figure3-1",
    "image_file": "2306.01386v1-Figure3-1.png",
    "caption": " Example of DST with ChatGPT. P is abridged for brevity. The sentence “\"slots\" were updated [...]” was generated at each turn. In turn 2, Boolean slots are predicted correctly, and a value variant (“guesthouse”) is mapped to the correct label. In turn 4 a coreference is resolved correctly. In turn 5, dontcare is handled correctly. In turn 7, the model assumes implicit requests. In turn 9, ChatGPT hallucinates a slot which is not listed in P , but sensible.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In turn 9, what slot does ChatGPT hallucinate?",
    "answer": "taxi-type",
    "rationale": "In turn 9, the user asks \"I need the car type please,\" and ChatGPT responds with \"Yes it is a grey skoda.\" However, the slot \"taxi-type\" is not listed in the predefined list of slots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01386v1",
    "pdf_url": null
  },
  {
    "instance_id": "f31b56b896a54cc8aa44f5dd8b408239",
    "figure_id": "2212.04357v1-Figure4-1",
    "image_file": "2212.04357v1-Figure4-1.png",
    "caption": " Components distribution of CA4P-483.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which privacy policy component is mentioned most frequently?",
    "answer": "Data",
    "rationale": "The box plot for \"Data\" has the highest median value, which indicates that it is the most frequently mentioned privacy policy component.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.04357v1",
    "pdf_url": null
  },
  {
    "instance_id": "0b706f833cd5496c9ab8c0fb07693f51",
    "figure_id": "1905.12384v3-Figure13-1",
    "image_file": "1905.12384v3-Figure13-1.png",
    "caption": " Qualitative comparisons on Place2 with centering masks. A1 and A2 are attention maps of two adjacent pixels, the 1st, 2nd, and 3rd rows are the attention maps of up and down adjacent pixels, the 4th and 5th rows are the attention maps of left and right adjacent pixels.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the image do the attention maps A1 and A2 focus on?",
    "answer": "The attention maps A1 and A2 focus on the sky.",
    "rationale": "The attention maps A1 and A2 are shown in the last two columns of the figure. They are both blue, which indicates that the model is paying attention to the sky.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12384v3",
    "pdf_url": null
  },
  {
    "instance_id": "4d8f74414a1249e4a2582bee7a78ec07",
    "figure_id": "1902.10186v3-Figure7-1",
    "image_file": "1902.10186v3-Figure7-1.png",
    "caption": " Histogram of maximum adversarial JS Divergence (ε-max JSD) between original and adversarial attentions over all instances. In all cases shown, |ŷadv − ŷ| < ε. Encoders are specified in parantheses.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model and dataset combination shows the highest maximum JS Divergence between original and adversarial attentions?",
    "answer": "Diabetes (BiLSTM)",
    "rationale": "The histogram in (c) shows the highest peak among all the histograms, indicating that the Diabetes (BiLSTM) model and dataset combination has the highest maximum JS Divergence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.10186v3",
    "pdf_url": null
  },
  {
    "instance_id": "25a0736e26a147e480e79b9364457ee4",
    "figure_id": "2004.13153v1-Figure1-1",
    "image_file": "2004.13153v1-Figure1-1.png",
    "caption": " Distribution of approximation to optimal social cost for 1,000 runs of each mechanism on Cambridge 2015 knapsack voting data using budget distance (top) and Jaccard distance (bottom).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which mechanism had the lowest median approximation to minimal social cost according to both distance metrics?",
    "answer": "PRC_5",
    "rationale": "The boxplots show the distribution of approximation to minimal social cost for each mechanism. The median is represented by the horizontal line inside the box. For both budget distance and Jaccard distance, the boxplot for PRC_5 has the lowest median.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.13153v1",
    "pdf_url": null
  },
  {
    "instance_id": "fd3f315c728949029bffbf7859e3f07e",
    "figure_id": "1908.07831v1-Figure14-1",
    "image_file": "1908.07831v1-Figure14-1.png",
    "caption": " Metric scores v.s. ratio of text that is replaced from the end of the input sentence (Quora)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric score decreases the most as the tail text replacement ratio increases?",
    "answer": "BLEU",
    "rationale": "The figure shows that the BLEU metric score decreases from 45.22 to 19.32 as the tail text replacement ratio increases from 0 to 0.5. This is a larger decrease than the other two metrics shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.07831v1",
    "pdf_url": null
  },
  {
    "instance_id": "f3e9b4b0d600446698d87843deeac88d",
    "figure_id": "2001.09067v2-Figure3-1",
    "image_file": "2001.09067v2-Figure3-1.png",
    "caption": " Schematic overview of the reconstruction pipeline. Input is provided in the form of a transient data volume. A convolutional neural network maps this data to a 2D depth map.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the output of the convolutional neural network?",
    "answer": "A 2D depth map.",
    "rationale": "The figure shows that the convolutional neural network takes the transient data volume as input and outputs a 2D depth map.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.09067v2",
    "pdf_url": null
  },
  {
    "instance_id": "83051b0c14d04c48a84477accc38fe8d",
    "figure_id": "2106.06406v2-Figure4-1",
    "image_file": "2106.06406v2-Figure4-1.png",
    "caption": " Model convergence result of vocoder models measured by log-mel spectrogram mean absolute error (LSMAE).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model converges faster, DiffWave or PriorGrad?",
    "answer": "DiffWave converges faster than PriorGrad.",
    "rationale": "The figure shows that the LS-MAE of DiffWave decreases more rapidly than that of PriorGrad. This means that DiffWave is able to learn the underlying data distribution more quickly than PriorGrad.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.06406v2",
    "pdf_url": null
  },
  {
    "instance_id": "de2df323b80a4dbd90c1a625d41e5350",
    "figure_id": "2210.06456v2-Figure10-1",
    "image_file": "2210.06456v2-Figure10-1.png",
    "caption": " Results on all extractive QA OOD settings when training on NaturalQuestions with pre-trained models of increasing size.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the BioASQ OOD dataset when trained on NaturalQuestions?",
    "answer": "BERT Large",
    "rationale": "The figure shows that BERT Large achieves the highest F1 score on the BioASQ OOD dataset when trained on NaturalQuestions. This can be seen by looking at the topmost line in the plot in Figure (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06456v2",
    "pdf_url": null
  },
  {
    "instance_id": "63e952e8188a4e69adadde9ca84fd131",
    "figure_id": "1910.13857v1-Figure1-1",
    "image_file": "1910.13857v1-Figure1-1.png",
    "caption": " Convergence rates in the deterministic oracle setting when x∗ ∈ Boundary(K)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges to the optimal solution the fastest?",
    "answer": "AXGD",
    "rationale": "The figure shows the convergence rates of different algorithms. AXGD has the steepest slope, which means it converges to the optimal solution the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.13857v1",
    "pdf_url": null
  },
  {
    "instance_id": "278ee71f056e4b7c82ff02882ca19b49",
    "figure_id": "2210.07839v4-Figure7-1",
    "image_file": "2210.07839v4-Figure7-1.png",
    "caption": " Zero-shot audio to image retrieval results on VGGSound. Since the spectrograms are hard to read, we show their paired images in the dashed boxes for visualization purposes, only audios are used as queries.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of query is used in this study?",
    "answer": "Audio queries are used in this study.",
    "rationale": "The figure shows that the query audio is used to retrieve the top 4 most relevant videos.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.07839v4",
    "pdf_url": null
  },
  {
    "instance_id": "a4f8efebaf3d4dfcb306ecf5bb481246",
    "figure_id": "2102.01187v3-Figure9-1",
    "image_file": "2102.01187v3-Figure9-1.png",
    "caption": " Example MLP architecture on PGGAN.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the output dimension of the first linear layer in the MLP architecture shown in the figure?",
    "answer": "512",
    "rationale": "The first linear layer is shown in the top box of the figure. The text inside the box indicates that the layer has 512 output dimensions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.01187v3",
    "pdf_url": null
  },
  {
    "instance_id": "6dea4536404a4ec7a438e68228de1822",
    "figure_id": "2007.10701v2-Figure2-1",
    "image_file": "2007.10701v2-Figure2-1.png",
    "caption": " Our overall concept and the problems of previous works Reinhard et. al. [19], Monge-Kantorovitch Linear (MKL) [15], Fast Photo Style (FPS) [12], WCT2 [22], PhotoNAS [4] shown with PSNR↑ / LPIPS↓ [23]. (↑: higher is better, ↓: lower is better).",
    "figure_type": "** Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the previous works shown in the figure produces the most similar color style to the expected result?",
    "answer": " Ours.",
    "rationale": " The figure shows the results of different methods for synthesizing similar color styles. The expected result is shown in the bottom right corner of the figure. The method that produces the most similar color style to the expected result is Ours, as its output is visually the closest to the expected result.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.10701v2",
    "pdf_url": null
  },
  {
    "instance_id": "f7d28a273ef84ccbae1504e09a899545",
    "figure_id": "2110.02999v2-Figure16-1",
    "image_file": "2110.02999v2-Figure16-1.png",
    "caption": " Mapping between a Gaussian and a Mixture of 8 Gaussians in 2D by various methods. The colors green, blue, and peru represent input, pushforward, and output samples respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the color green represent in the figure?",
    "answer": "The color green represents the input samples.",
    "rationale": "The caption states that the colors green, blue, and peru represent input, pushforward, and output samples respectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02999v2",
    "pdf_url": null
  },
  {
    "instance_id": "79655d79b66d4b5e9ab85f5355c9dcd1",
    "figure_id": "2107.14795v3-Figure8-1",
    "image_file": "2107.14795v3-Figure8-1.png",
    "caption": " Qualitative examples of optical flow. For each image pair, we show the two frames (top), and then the estimated flow (bottom left) and the ground-truth flow (bottom right). In the left example, we see one person under heavy occlusion where the correct flow is propagated into a region with few details. Another person in the foreground has clothes with little texture and substantial blur, and yet the algorithm can propagate the flow across the entire region. In the center example, we see very large motions from both the dragon and the person, yet many fine structures are preserved like the pole. On the right, we see a forest scene with a few extremely small objects with very subtle motions (circled) which our algorithm is able to detect and segment correctly.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which example shows the algorithm's ability to detect and segment extremely small objects with subtle motions?",
    "answer": "The right example.",
    "rationale": "The right example shows a forest scene with a few extremely small objects with very subtle motions (circled) which the algorithm is able to detect and segment correctly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.14795v3",
    "pdf_url": null
  },
  {
    "instance_id": "85f6a433b0014642be058143dfd31dce",
    "figure_id": "2203.07228v1-Figure1-1",
    "image_file": "2203.07228v1-Figure1-1.png",
    "caption": " Group disparity for defendant state (C.E. Europe vs. The Rest) in ECtHR and legal area (Civil law vs. Penal law) in FSCS.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which legal area has the highest group disparity for defendant state in FSCS?",
    "answer": "Penal law.",
    "rationale": "The figure shows that the group disparity for defendant state in FSCS is 83.4% for Penal law, which is higher than the 66.8% for Civil law.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.07228v1",
    "pdf_url": null
  },
  {
    "instance_id": "93109333f2a14e3f9a4d8b96302b4d84",
    "figure_id": "2310.13870v1-Figure7-1",
    "image_file": "2310.13870v1-Figure7-1.png",
    "caption": " More examples on the performance of the spectral clustering algorithms for image segmentation.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best for image segmentation?",
    "answer": "Our Algorithm.",
    "rationale": "The figure shows the results of different spectral clustering algorithms for image segmentation on two different images. The results of Our Algorithm are the most visually appealing and closest to the original images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.13870v1",
    "pdf_url": null
  },
  {
    "instance_id": "628781e4d8d14706a07678b90fe12986",
    "figure_id": "2104.05882v1-Figure5-1",
    "image_file": "2104.05882v1-Figure5-1.png",
    "caption": " Discourse connective word distribution.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which discourse connective word is used most frequently in the development stage across all three languages?",
    "answer": "\"und\"",
    "rationale": "The table shows the distribution of discourse connective words in three languages (English, Chinese, and German) across three stages of development (Train, Development, and Test). The word \"und\" is used most frequently in the Development stage across all three languages.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.05882v1",
    "pdf_url": null
  },
  {
    "instance_id": "c57fe9504c9248a884cfc65e90d87a32",
    "figure_id": "1909.01264v2-Figure13-1",
    "image_file": "1909.01264v2-Figure13-1.png",
    "caption": " Downstream performance vs. measures of compression quality (fastText embeddings). We plot the performance of compressed fastText embeddings on question answering (SQuAD, left column) and sentiment analysis (SST-1, right column), in terms of the different measures of compression quality for these embeddings. We can see that the eigenspace overlap score E generally aligns better with downstream performance than the other measures of compression quality. To quantify this, in the title of each plot we include the Spearman correlation ρ between downstream performance and the measure of compression quality for that plot. We can see that the eigenspace overlap score attains the strongest correlations with downstream performance, as it has the largest values for |ρ|.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which measure of compression quality aligns best with downstream performance on question answering (SQuAD)?",
    "answer": "Eigenspace overlap score",
    "rationale": "The figure shows that the eigenspace overlap score generally aligns better with downstream performance than the other measures of compression quality. This is because it has the largest values for |ρ|, which is the Spearman correlation between downstream performance and the measure of compression quality.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.01264v2",
    "pdf_url": null
  },
  {
    "instance_id": "4bc223d4c9b9425981f5d70a817acff7",
    "figure_id": "2105.08909v1-Figure6-1",
    "image_file": "2105.08909v1-Figure6-1.png",
    "caption": " Performance in the warm-up phase. Main prediction model: DNN.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in the warm-up phase on the ML-1M dataset?",
    "answer": "GME-P",
    "rationale": "The figure shows the AUC (area under the curve) for different models on the ML-1M and Taobao datasets in the cold-start and warm-up phases. In the ML-1M dataset, GME-P has the highest AUC in the first and second warm-up phases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.08909v1",
    "pdf_url": null
  },
  {
    "instance_id": "e9a7dd9d8d1c4eb2b2a2c2cc8f2772dd",
    "figure_id": "2212.09968v2-Figure3-1",
    "image_file": "2212.09968v2-Figure3-1.png",
    "caption": " Distribution of six different types of instructions. removing information and replacing information are the most frequent types. Extrinsic errors are more likely to be corrected by removing while Intrinsic errors are more likely to be corrected by replacing.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of instruction is most likely to be used to correct an intrinsic error?",
    "answer": "Replacing information.",
    "rationale": "The figure shows that the ratio of intrinsic errors corrected by replacing information is higher than the ratio of intrinsic errors corrected by any other type of instruction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.09968v2",
    "pdf_url": null
  },
  {
    "instance_id": "394c80e611a64df99a91a3cca1ffb460",
    "figure_id": "2110.08205v3-Figure8-1",
    "image_file": "2110.08205v3-Figure8-1.png",
    "caption": " Runtime in milliseconds of FOCuS0, FOCuS, R-FOCuS and Yu-CUSUM in function of the length of the sequence (log-scale on both axes). Grey lines refer to an expected O(n) increase (dashed) and O(n2) increase (dotted).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the fastest runtime for a sequence of length 1000?",
    "answer": "Yu-CUSUM",
    "rationale": "The plot shows the runtime of different algorithms as a function of the sequence length. The Yu-CUSUM line is the lowest for a sequence length of 1000.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.08205v3",
    "pdf_url": null
  },
  {
    "instance_id": "2b3152d51ade4f04b75ad736c20abc26",
    "figure_id": "2110.02635v3-Figure2-1",
    "image_file": "2110.02635v3-Figure2-1.png",
    "caption": " Scatter plot of system-level zero-shot prediction results for each system.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best for the BC19 dataset?",
    "answer": "MN FT-aug",
    "rationale": "The MN FT-aug model has the lowest MSE for the BC19 dataset, as indicated by the red dots in the scatter plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02635v3",
    "pdf_url": null
  },
  {
    "instance_id": "15a65c9d938946729c2012a24e75bebc",
    "figure_id": "1811.11187v1-Figure9-1",
    "image_file": "1811.11187v1-Figure9-1.png",
    "caption": " Examples of symmetry annotations.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What type of symmetry is exhibited by the trash can?",
    "answer": "The trash can exhibits rotational symmetry.",
    "rationale": "The trash can can be rotated by any angle around its vertical axis and still look the same. This is a characteristic of rotational symmetry.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.11187v1",
    "pdf_url": null
  },
  {
    "instance_id": "a7eb4cef032f461ab7df7f05dfe94b0c",
    "figure_id": "2309.13944v2-Figure7-1",
    "image_file": "2309.13944v2-Figure7-1.png",
    "caption": " Hyperparamenter analysis of κ on Cora",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most sensitive to the choice of κ?",
    "answer": "GRACE-POT",
    "rationale": "The plot for GRACE-POT shows the largest change in Micro-F1 as κ is varied.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.13944v2",
    "pdf_url": null
  },
  {
    "instance_id": "da31231715b3405e9146cfa2bd0c7039",
    "figure_id": "2109.13492v3-Figure19-1",
    "image_file": "2109.13492v3-Figure19-1.png",
    "caption": " Latencies of sorting 10 GB data using various numbers of functions on PyWren and Pheromone-MR. The latency is broken down into: the interaction latency (for PyWren, the invocation and intermediate data I/O), and the latency for compute and I/O. The numbers indicate the former.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which system has the lowest latency for sorting 10 GB of data using 200 functions?",
    "answer": "Pheromone-MR",
    "rationale": "The figure shows that the latency for Pheromone-MR is lower than the latency for PyWren when using 200 functions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.13492v3",
    "pdf_url": null
  },
  {
    "instance_id": "eeea2655af494c6da9423a26fea45a71",
    "figure_id": "1812.06190v1-Figure11-1",
    "image_file": "1812.06190v1-Figure11-1.png",
    "caption": " The distribution overWi output by the model on the test set for each expression i in the order 0 = Anger, 1 = Disgust, 2 = Fear, 3 = Happy, 4 = Sad, 5 = Surprise.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which emotion is most similar to surprise, according to the model's output distribution?",
    "answer": "Happy.",
    "rationale": "The distributions for surprise and happy are both centered around similar values on the x-axis, and they have similar shapes. This suggests that the model often predicts similar outputs for these two emotions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.06190v1",
    "pdf_url": null
  },
  {
    "instance_id": "79f29ac1e1de409cad712c7693ce67fc",
    "figure_id": "1911.00886v1-Figure7-1",
    "image_file": "1911.00886v1-Figure7-1.png",
    "caption": " Performance of various reward structures",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which reward structure performs best in both the in-station and out-station datasets?",
    "answer": "rGAN sampling",
    "rationale": "The figure shows the performance of various reward structures on the in-station and out-station datasets. The rGAN sampling curve is consistently higher than the other curves, indicating that it performs best in both datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.00886v1",
    "pdf_url": null
  },
  {
    "instance_id": "fd9d21f2918e4c1d82d5db90d2fe4587",
    "figure_id": "1905.11436v3-FigureA.3-1",
    "image_file": "1905.11436v3-FigureA.3-1.png",
    "caption": "Figure A.3: Simple process model selection example: outputs from 5 candidate process models, over the last 50 time points.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model best captures the trend of the signal over time?",
    "answer": "The spline model.",
    "rationale": "The spline model is the only model that closely follows the general trend of the signal over time. The other models either overshoot or undershoot the signal at different points in time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11436v3",
    "pdf_url": null
  },
  {
    "instance_id": "505356d3e26a45afbe72aecd8b6762bd",
    "figure_id": "2308.11824v3-Figure7-1",
    "image_file": "2308.11824v3-Figure7-1.png",
    "caption": " Results on synthetic data for LLT = I . (A) Similar setting to Fig. 2, with parameters {N,C,K, λΣ, P} = {100, 40, 10, 1.0, 4}. Means µ(xc) and covariances Σ(xc) are shown in dashed lines and colored ellipses respectively for the ground truth (left) vs. Wishart (right) plotted in the PC space. Train and test trials yck are shown in small black and red dots. (B) Covariances inferred by different methods for two example conditions (x1 = 0◦,x15 = 168◦). Wishart captures the low-rank structure observed in the true matrices using a small number of trials per condition. (C1-C4) Log probability on held-out trials as a function of the number of dimensions (10-90), conditions (10-90), trials (8,16,32,64), and covariance smoothness (0.2, 0.4, 0.8, 1.6, 3.2, 6.4). For each plot, we vary one parameter and fix other parameters to {N,C,K, λΣ, P} = {100, 40, 10, 1.0, 4}. Error bars represent median ± SEM across 5 independent runs for each parameter configuration. (D1-D4) Mean operator norm between the true and inferred covariances showing agreement with the log probability results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method best captures the low-rank structure of the true covariance matrices?",
    "answer": "The Wishart method.",
    "rationale": "Panel B of the figure shows the covariances inferred by different methods for two example conditions. The Wishart method captures the low-rank structure observed in the true matrices using a small number of trials per condition, while the other methods do not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.11824v3",
    "pdf_url": null
  },
  {
    "instance_id": "2db895ec26f049a5a591983dbd5d3cd0",
    "figure_id": "2207.04387v1-FigureD.9-1",
    "image_file": "2207.04387v1-FigureD.9-1.png",
    "caption": "Figure D.9: Plots of estimation errors of the posterior means ‖θk − θ?‖2 (left) and ∣∣∣‖θk‖22 − ‖θ?‖22∣∣∣/d (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two algorithms, MYULA or LBMUMLA, has a lower estimation error of the posterior mean?",
    "answer": "MYULA",
    "rationale": "The left plot in Figure D.9 shows the estimation error of the posterior mean for both algorithms. MYULA has a lower estimation error than LBMUMLA.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.04387v1",
    "pdf_url": null
  },
  {
    "instance_id": "587664e1ea9a4da0941cd3da341ba6eb",
    "figure_id": "2306.12251v2-Figure1-1",
    "image_file": "2306.12251v2-Figure1-1.png",
    "caption": " Comparison of the anomaly detection performance, wall-clock time (on all datasets), and peak CPU/GPU memory utilization (on DGraph-Fin) among all models with default hyperparameters. Top three lines are in semi-supervised settings and the others are in fully-supervised settings. The color of the box plot represents the average score for each metric, while the central line within the box indicates the median score.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieved the highest Rec@K score in the fully-supervised setting?",
    "answer": "RF-XGB-Graph",
    "rationale": "The Rec@K score is shown in the third row of the figure. The RF-XGB-Graph model has the highest box plot in this row, indicating that it achieved the highest Rec@K score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.12251v2",
    "pdf_url": null
  },
  {
    "instance_id": "89492b81827a4f95b9262ba116e7c8fc",
    "figure_id": "2101.06837v2-Figure2-1",
    "image_file": "2101.06837v2-Figure2-1.png",
    "caption": " Selection in HAD array compared with selection in the full digital array.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which selection method produces the beam pattern that is closest to the desired beam pattern?",
    "answer": "Hybrid selection.",
    "rationale": "The figure shows the beam patterns for three different selection methods: antenna selection, RF chain selection, and hybrid selection. The desired beam pattern is also shown. The hybrid selection beam pattern is the closest to the desired beam pattern.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.06837v2",
    "pdf_url": null
  },
  {
    "instance_id": "fd416f777d9d4354aade04658cc3ad60",
    "figure_id": "2212.08698v1-Figure11-1",
    "image_file": "2212.08698v1-Figure11-1.png",
    "caption": " Generated images for subjective evaluation on LSUN-church dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method seems to perform the best in generating images that are both realistic and faithful to the original prompt?",
    "answer": "Ours.",
    "rationale": "The images in the \"Ours\" column are generally more realistic and closer to the original prompt than the images in the \"Diffusion\" and \"CLIP\" columns. For example, in the first row, the \"Ours\" image of a golden church is more realistic than the \"Diffusion\" image, which is blurry and has artifacts. Similarly, in the second row, the \"Ours\" image of a snowy church is more faithful to the prompt than the \"CLIP\" image, which shows a church with a blue sky.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.08698v1",
    "pdf_url": null
  },
  {
    "instance_id": "c6a74a3506eb466d8341db6d3f1e4ebe",
    "figure_id": "1810.12576v1-Figure2-1",
    "image_file": "1810.12576v1-Figure2-1.png",
    "caption": " Images on the diagonal are corrupted with the adversarial noise generated by CW [10] l2-norm attack, so the prediction confidence on the adversarial images is at least 95%. The prediction confidence on the original images is 99%.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which images in the figure are most likely to be misclassified by a machine learning model?",
    "answer": "The images on the diagonal.",
    "rationale": "The caption states that the images on the diagonal are corrupted with adversarial noise, which is designed to fool machine learning models. The prediction confidence on the adversarial images is at least 95%, while the prediction confidence on the original images is 99%. This suggests that the adversarial images are more likely to be misclassified by a machine learning model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.12576v1",
    "pdf_url": null
  },
  {
    "instance_id": "4bcc6a821ba045b8be7764e5c7c6b9f9",
    "figure_id": "1905.10138v2-Figure12-1",
    "image_file": "1905.10138v2-Figure12-1.png",
    "caption": " Relative execution time with CSR and the proposed scheme with various nFIFO size. 1.0 in y-axis means no pruning rate variation in each row in the case of CSR format, or no stall due to limited dpatch load bandwidth in the case of the proposed scheme.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the least variation in execution time for LeNet5?",
    "answer": "CSR.",
    "rationale": "The box plot for CSR is the narrowest, indicating that the data points are more tightly clustered around the median.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10138v2",
    "pdf_url": null
  },
  {
    "instance_id": "5115028239a54933a8379044a1e274f1",
    "figure_id": "2304.05866v1-Figure15-1",
    "image_file": "2304.05866v1-Figure15-1.png",
    "caption": " Qualitative Analysis on iNaturalist2019 (1010 classes). Examples of generations from various classes for evaluated baselines (Table 1). The baseline ADA suffers from mode collapse, whereas gSR suffers from class confusion particularly for tail classes, particularly for tail classes as seen above on the left. NoisyTwins generates diverse and class-consistent images across all categories.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the baselines shown in the figure is most likely to generate images that are diverse and class-consistent across all categories?",
    "answer": "NoisyTwins.",
    "rationale": "The caption states that \"NoisyTwins generates diverse and class-consistent images across all categories.\" This is also evident in the figure, where the images generated by NoisyTwins appear to be more diverse and representative of the different classes than the images generated by the other baselines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.05866v1",
    "pdf_url": null
  },
  {
    "instance_id": "b2a46302e1454ac9b69b3ede011457ce",
    "figure_id": "2209.15292v1-Figure13-1",
    "image_file": "2209.15292v1-Figure13-1.png",
    "caption": " Training efficiency comparison among CML-based competitors.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is the most efficient for training on the MovieLens-1m dataset?",
    "answer": "DPCML2",
    "rationale": "The figure shows the training time for different algorithms on different datasets. The x-axis shows the algorithms, and the y-axis shows the training time in seconds (log scale). The bars are colored according to the training time, with blue being the fastest and red being the slowest. In the MovieLens-1m plot, DPCML2 has the lowest bar, indicating it has the fastest training time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.15292v1",
    "pdf_url": null
  },
  {
    "instance_id": "34d09839c9ad4703ab6dc169fe0e0798",
    "figure_id": "2211.13345v3-Figure10-1",
    "image_file": "2211.13345v3-Figure10-1.png",
    "caption": " Cumulative benefit obtained as a function of cumulative effort cost (up to budget 100) on v6.3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of cumulative benefit obtained?",
    "answer": "DISCLOSE-v6.3",
    "rationale": "The plot shows the cumulative benefit obtained as a function of cumulative effort cost for three different methods: DISCLOSE-v6.3, Static-v6.3, and MCTS-v6.3. The red line, which represents DISCLOSE-v6.3, is consistently above the other two lines, indicating that this method obtains the highest cumulative benefit for any given effort cost.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.13345v3",
    "pdf_url": null
  },
  {
    "instance_id": "d141a0b33a16427da0a263d848f715f2",
    "figure_id": "2010.13870v1-Figure1-1",
    "image_file": "2010.13870v1-Figure1-1.png",
    "caption": " Pairwise comparisons between tasks with Transformer-XL. Rows and columns represent tasks, and one point represents a single noun’s performance on a pair of tasks. The four tasks on the lower right, with strongest correlations, all involve reflexive anaphora.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task is most correlated with the \"Simple (SVA)\" task?",
    "answer": "The \"Simple (IRA)\" task.",
    "rationale": "The plot shows the pairwise correlations between different tasks. The correlation coefficient between the \"Simple (SVA)\" task and the \"Simple (IRA)\" task is 0.95, which is the highest correlation coefficient in the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.13870v1",
    "pdf_url": null
  },
  {
    "instance_id": "7b7b9a9316914235ba90b51730f63b32",
    "figure_id": "2011.08474v3-Figure11-1",
    "image_file": "2011.08474v3-Figure11-1.png",
    "caption": " Results on Dataset (IV): More Distributed Data. See Appendix A.3.3 for discussions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of training loss, validation loss, recovered rank, and recovery error?",
    "answer": "FedDualAvg",
    "rationale": "The figure shows that FedDualAvg consistently has the lowest training loss, validation loss, recovered rank, and recovery error compared to the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.08474v3",
    "pdf_url": null
  },
  {
    "instance_id": "b11979c9848c42b6ae22221dceb66f2b",
    "figure_id": "2309.13546v2-Figure8-1",
    "image_file": "2309.13546v2-Figure8-1.png",
    "caption": " Learning curves of distinct data-free methods across ω ∈ {0.01, 0.1, 1.0} on FMNIST, SVHN, CIFAR-10 and CIFAR-100.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which data-free method achieves the highest local accuracy on CIFAR-10 when ω = 1.0?",
    "answer": "DENSE",
    "rationale": "The plot in Figure (g) shows the learning curves for CIFAR-10 with ω = 1.0. The DENSE method has the highest local accuracy among the three methods shown.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.13546v2",
    "pdf_url": null
  },
  {
    "instance_id": "25332078bf4746bbbeac442bb77eb9c4",
    "figure_id": "2010.12247v2-Figure6-1",
    "image_file": "2010.12247v2-Figure6-1.png",
    "caption": " Different tracking strategies (left) and comparison with the exploitation test used in OAM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tracking strategy performs the best in terms of cumulative regret?",
    "answer": "Ours",
    "rationale": "The figure on the right shows that the \"Ours\" strategy has the lowest cumulative regret compared to the OAM strategy. This is evident from the fact that the red line representing \"Ours\" is consistently below the blue dashed line representing OAM.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12247v2",
    "pdf_url": null
  },
  {
    "instance_id": "bdbd54fb3af14a60bcda2f61b2cf0507",
    "figure_id": "1903.03094v1-Figure8-1",
    "image_file": "1903.03094v1-Figure8-1.png",
    "caption": " Form for Crowdsourcing Task 5",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the object that is being described in the instructions?",
    "answer": "wood",
    "rationale": "The object is listed as \"wood\" in the bottom left corner of the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.03094v1",
    "pdf_url": null
  },
  {
    "instance_id": "5b2e6ad132144b8f8ae5130f52a5140a",
    "figure_id": "2002.03657v4-Figure2-1",
    "image_file": "2002.03657v4-Figure2-1.png",
    "caption": " Global Lipschitz constant upper bounds (left) and solver running time (right) for 1-hidden layer networks with respect to L∞-norm obtained by SHOR, HR-2, LipOpt-3, LipOpt-4 and LBS. We generate random networks of size 10, 20, 40, 80. For size 10, we consider sparsity 4, 8, 12, 16, 20; for size 20, we consider sparsity 8, 16, 24, 32, 40; for size 40 and 80, we consider sparsity 10, 20, 30, 40, 50, 60, 70, 80. In the meantime, we display median and quartiles over 10 random networks draws.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest running time for all network sizes and sparsity levels?",
    "answer": "LBS",
    "rationale": "The figure on the right shows the running time of each algorithm for different network sizes and sparsity levels. The LBS line is always below the other lines, indicating that it has the lowest running time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.03657v4",
    "pdf_url": null
  },
  {
    "instance_id": "db2507be22eb40b38e245fc0d848c520",
    "figure_id": "2112.09099v5-Figure5-1",
    "image_file": "2112.09099v5-Figure5-1.png",
    "caption": " Waterworld results",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three algorithms performed the best during execution?",
    "answer": "DMFG-AC",
    "rationale": "The bar graph in (b) shows the total food captured by each algorithm during execution. DMFG-AC has the highest bar, indicating that it captured the most food.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.09099v5",
    "pdf_url": null
  },
  {
    "instance_id": "a657731cfab4492b88b6549c895943c8",
    "figure_id": "2303.09152v1-Figure6-1",
    "image_file": "2303.09152v1-Figure6-1.png",
    "caption": " Reconstruction results on representative datasets of Replica (left), ScanNet (middle), and Tanks and Temples (right). The ground truth is presented on the bottom-most. Red boxes in sub-figures highlight those areas where distinctive differences can be observed.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the reconstruction methods best preserves the details of the scene?",
    "answer": "Ours (Grids)",
    "rationale": "The figure shows the reconstruction results of different methods on three datasets. The bottom row shows the ground truth, and the red boxes highlight areas where there are significant differences between the reconstructions and the ground truth. It can be seen that Ours (Grids) is the only method that accurately reconstructs the details of the scene, such as the chairs, table, and stairs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.09152v1",
    "pdf_url": null
  },
  {
    "instance_id": "656bb0ec870c4e8daf7df858f047aa43",
    "figure_id": "2002.01697v3-Figure2-1",
    "image_file": "2002.01697v3-Figure2-1.png",
    "caption": " Average reconstruction error (per pixel) of the images from the MNIST dataset shown in Figure 1. The error bars indicate half of a standard deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest reconstruction error for all numbers of measurements?",
    "answer": "VAE",
    "rationale": "The plot shows that the green line representing VAE consistently has the lowest reconstruction error for all values of the number of measurements.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.01697v3",
    "pdf_url": null
  },
  {
    "instance_id": "de90a08e54f046d9b2cd34e3a3d66951",
    "figure_id": "2209.15594v2-Figure2-1",
    "image_file": "2209.15594v2-Figure2-1.png",
    "caption": " The four stages of edge of stability (see Section 4.1), demonstrated on a simple loss function (see Appendix B).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stage of edge of stability is characterized by a blowup?",
    "answer": "Stage 2.",
    "rationale": "The figure shows that the change in sharpness is greatest in Stage 2, which is labeled \"Blowup.\" This is because the loss function is very steep at this point, and a small change in the input can lead to a large change in the output.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.15594v2",
    "pdf_url": null
  },
  {
    "instance_id": "957f2f83a0a54952acaf80fbdf82ce06",
    "figure_id": "1805.02971v2-Figure1-1",
    "image_file": "1805.02971v2-Figure1-1.png",
    "caption": " Cumulative regret along time horizon.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of cumulative regret?",
    "answer": "LUMB",
    "rationale": "The figure shows the cumulative regret for four different algorithms: LUMB, Thompson-Beta, Thompson-Corr, and UCB-MNL. LUMB has the lowest cumulative regret, which means it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.02971v2",
    "pdf_url": null
  },
  {
    "instance_id": "8b6f80c49e444ebebd04f0a1d308f0ba",
    "figure_id": "2010.15363v2-Figure5-1",
    "image_file": "2010.15363v2-Figure5-1.png",
    "caption": " The framework of MACR. The orange rectangles denote themain branch, i.e., the conventional recommender system. The blue and green rectangles denote the user and item modules, respectively.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the MACR framework represents the conventional recommender system?",
    "answer": "The orange rectangles.",
    "rationale": "The caption explicitly states that \"the orange rectangles denote the main branch, i.e., the conventional recommender system.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.15363v2",
    "pdf_url": null
  },
  {
    "instance_id": "6750d579aa48416f8bc734977d5de456",
    "figure_id": "2003.08938v7-Figure3-1",
    "image_file": "2003.08938v7-Figure3-1.png",
    "caption": " A toy environment.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the reward for taking action 1 in state S2?",
    "answer": "1",
    "rationale": "The figure shows a state transition diagram for a toy environment. The arrows represent the possible state transitions, and the labels on the arrows indicate the action taken and the reward received. The arrow from S2 to S2 indicates that taking action 1 in state S2 results in a reward of 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.08938v7",
    "pdf_url": null
  },
  {
    "instance_id": "1cc2a900e9214c04bfb7da759648c4ad",
    "figure_id": "2205.12514v2-Figure4-1",
    "image_file": "2205.12514v2-Figure4-1.png",
    "caption": " As in mono-pair experiments, including 10 NAV perturbations per pair shows larger robustness improvement than simply fine-tuning on 1-most.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which translation pair benefited the most from including 10 NAV perturbations per pair?",
    "answer": "ko-en",
    "rationale": "The figure shows that the BLEU score for ko-en increased the most when 10 NAV perturbations were included, compared to the baseline and the +1 condition.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.12514v2",
    "pdf_url": null
  },
  {
    "instance_id": "117a3d75b7a340f5b0adf3c78a584e2a",
    "figure_id": "2111.04178v4-Figure5-1",
    "image_file": "2111.04178v4-Figure5-1.png",
    "caption": " From left to right: (i) Each generator of MGAN learns one mode of a mixture of 8 gaussians, (ii) Mode Collapse of single-agent GAN’s, (iii) Single-agent GAN can’t discriminate between the modes.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the typical outcome of a Single-GAN when trained on a mixture of 8 gaussians?",
    "answer": "Mode collapse.",
    "rationale": "The table in the figure shows that the typical outcome of a Single-GAN is mode collapse, which occurs when the generator learns to produce only a few of the modes in the data distribution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.04178v4",
    "pdf_url": null
  },
  {
    "instance_id": "2e6c89ad02554f8e8fc7b13edc64b9f4",
    "figure_id": "2010.05981v2-Figure4-1",
    "image_file": "2010.05981v2-Figure4-1.png",
    "caption": " The shape-biased model and the texture-biased model are good/bad at classifying different object categories. We sort these object categories according to the model’s corresponding top-1 accuracy, where the righter one indicates a lower accuracy achieved by the model.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is more accurate for classifying images of buses?",
    "answer": "The shape-biased model.",
    "rationale": "The figure shows that the bus image is closer to the \"More Accurate\" end of the spectrum for the shape-biased model than for the texture-biased model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.05981v2",
    "pdf_url": null
  },
  {
    "instance_id": "4ef15a29986e4679a551d3f5813b1ba1",
    "figure_id": "2105.04656v2-Figure3-1",
    "image_file": "2105.04656v2-Figure3-1.png",
    "caption": " UMD performs competitively on the CREDIT dataset. The guarantee of Theorem 4 closely matches empirical behavior.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the CREDIT dataset?",
    "answer": "UMD.",
    "rationale": "The UMD method achieves the highest validity values in both the marginal and conditional validity plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.04656v2",
    "pdf_url": null
  },
  {
    "instance_id": "de44705a031a433b9d0a7b042434236a",
    "figure_id": "2203.11089v3-Figure9-1",
    "image_file": "2203.11089v3-Figure9-1.png",
    "caption": " Distribution of the lane category. Here we abbreviate single in 1, double in 2, white in W, yellow in Y, left in L, and right in R. Thus 1-W dash means the category of single white dash lanes",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of lane is the most common?",
    "answer": "Single white solid lanes",
    "rationale": "The bar for \"1-W solid\" is the tallest, indicating that there are more single white solid lanes than any other type of lane.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.11089v3",
    "pdf_url": null
  },
  {
    "instance_id": "5468e66f1f034d259147eccae6395c59",
    "figure_id": "2108.08810v2-FigureD.4-1",
    "image_file": "2108.08810v2-FigureD.4-1.png",
    "caption": "Figure D.4: Linear probes spatial localization. We train a linear probe on each individual token and plot the average accuracy over the test set, in percent. Here we plot the results for each token a subset of layers in 3 models: ViT-B/32 trained with a classification token (CLS) or global average pooling (GAP), as well as a ResNet50. Note the different scales of values in different sub-plots.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model shows the most consistent spatial localization across layers?",
    "answer": "ResNet50.",
    "rationale": "The figure shows the spatial localization of linear probes for different models and layers. The ResNet50 model shows a consistent pattern of spatial localization across all layers, while the ViT models show more variation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.08810v2",
    "pdf_url": null
  },
  {
    "instance_id": "ef0102fb81a34a8ea6940893e3fe6890",
    "figure_id": "1906.07987v1-Figure5-1",
    "image_file": "1906.07987v1-Figure5-1.png",
    "caption": " Average normalized MSVE for Lab2D and Atari environments in each data regime. For each number of train rollouts and scenario, we normalize the MSVE of each algorithm A by (MSVE(A)−minA′ MSVE(A′))/(maxA′ MSVE(A′)−minA′ MSVE(A′)). Equivalently, the worst algorithm is assigned relative MSVE 1, and the best one is assigned relative MSVE 0. Then, for each number of rollouts, we take the average across scenarios (i.e., all the 10 environments are worth the same). This allows for a reasonably fair comparison of performance in different domains.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in the Atari environments?",
    "answer": "Adaptive TD(3)",
    "rationale": "The figure shows the average relative MSVE for different algorithms in the Lab2D and Atari environments. The lower the MSVE, the better the algorithm performs. We can see that the Adaptive TD(3) line is consistently lower than the other lines, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.07987v1",
    "pdf_url": null
  },
  {
    "instance_id": "e6b315168b0741cf84e6578df4bb2020",
    "figure_id": "2204.11346v1-Figure8-1",
    "image_file": "2204.11346v1-Figure8-1.png",
    "caption": " The adaptive loss helps accelerate GCN training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the adaptive loss compare to the BPR loss in terms of training speed?",
    "answer": "The adaptive loss is faster than the BPR loss.",
    "rationale": "The figure shows that the adaptive loss converges faster than the BPR loss, which means that it takes fewer epochs to reach a good performance level.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.11346v1",
    "pdf_url": null
  },
  {
    "instance_id": "2f8e94d0f96d4e57a9ac8b0ba83941a3",
    "figure_id": "2304.10263v1-Figure7-1",
    "image_file": "2304.10263v1-Figure7-1.png",
    "caption": " IDa−b denotes the mean ArcFace similarity score between the input image and the 20 inverted images uniformly sampled from yaw angles between [−b◦, a◦]∪[a◦, b◦] and pitch angles between [−20◦, 20◦]. Our method has a higher ID score than other methods in different yaw ranges.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest ID score for all yaw ranges?",
    "answer": "PREIM3D",
    "rationale": "The green line in the plot represents the ID score for the PREIM3D method, and it is consistently higher than the other lines for all yaw ranges.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.10263v1",
    "pdf_url": null
  },
  {
    "instance_id": "e4b28ba7a0b74f7e898f83034bbc26d8",
    "figure_id": "2010.14298v1-Figure5-1",
    "image_file": "2010.14298v1-Figure5-1.png",
    "caption": " Machine translation results on the IWSLT14’ En-DE dataset. PSQ and BHQ achieve significantly lower gradient variance than PTQ, and converge even with a 5-bit gradient.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest gradient variance?",
    "answer": "BHQ",
    "rationale": "The plot in Figure (a) shows that BHQ has the lowest gradient variance for all bitwidths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.14298v1",
    "pdf_url": null
  },
  {
    "instance_id": "7422f75aba06452c99bbf06cabae2446",
    "figure_id": "2303.13582v1-FigureA13-1",
    "image_file": "2303.13582v1-FigureA13-1.png",
    "caption": "Figure A13. Samples of training images from the three scenes from the three datasets - Scannet [5], In-the-Wild and Tanks and Temples [18].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three datasets used to train the model?",
    "answer": "Scannet, In-the-Wild, and Tanks and Temples.",
    "rationale": "The caption of the figure explicitly states that the images are from these three datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.13582v1",
    "pdf_url": null
  },
  {
    "instance_id": "9e40330f429d488fa4fe5481eb5a7535",
    "figure_id": "2302.06729v1-Figure5-1",
    "image_file": "2302.06729v1-Figure5-1.png",
    "caption": " A histogram containing the distribution of incoming edges (a.k.a node’s degree) per reasoning step for each annotated dataset (training, development, and testing split combined), truncated to a maximum value of 8 incoming edges.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest proportion of reasoning steps with 1 incoming edge?",
    "answer": "GSM8K",
    "rationale": "The figure shows that the orange bar, which represents the GSM8K dataset, is the tallest for the category of 1 incoming edge. This indicates that GSM8K has the highest proportion of reasoning steps with 1 incoming edge.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.06729v1",
    "pdf_url": null
  },
  {
    "instance_id": "3b395e1e4dea459ea8a2d17e6b2e0c23",
    "figure_id": "2210.12798v1-Figure5-1",
    "image_file": "2210.12798v1-Figure5-1.png",
    "caption": " The average absolute entry values of the produced alignment matrices (window size=8).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which region of the alignment matrix has the highest average absolute entry values?",
    "answer": "The upper left corner of the matrix.",
    "rationale": "The color gradient in the figure indicates that the higher values are represented by lighter shades of red, and the lower values are represented by darker shades of purple. The upper left corner of the matrix has the lightest shades of red, indicating that this region has the highest average absolute entry values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12798v1",
    "pdf_url": null
  },
  {
    "instance_id": "a825970b618d4d78aaac57e7af79609c",
    "figure_id": "2006.11339v1-Figure2-1",
    "image_file": "2006.11339v1-Figure2-1.png",
    "caption": " Tube matching and video panoptic quality (VPQ) metric. An IoU is obtained by matching predicted and ground truth tubes. A frame-level false positive segment penalizes the whole predicted tube to get a low IoU. Each VPQk is computed by sliding the window through a video, and averaged by the number of frames. k indicate the temporal window size. VPQk is then averaged over different k values, to get a final VPQ score.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the 1-frame-span and the k-frame-span?",
    "answer": "The 1-frame-span is a subset of the k-frame-span.",
    "rationale": "The 1-frame-span is the set of ground truth and predicted tubes that overlap in a single frame. The k-frame-span is the set of ground truth and predicted tubes that overlap in a window of k frames. Since a single frame is a subset of a window of k frames, the 1-frame-span is a subset of the k-frame-span.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.11339v1",
    "pdf_url": null
  },
  {
    "instance_id": "7d34cb5acb9e49408989f60c675d60d5",
    "figure_id": "2011.14204v1-Figure4-1",
    "image_file": "2011.14204v1-Figure4-1.png",
    "caption": " Generalization results for SSD models trained on the seen VOC dataset. The top row shows macro-level AR@k for seen and unseen classes in VOC as well as their harmonic mean (AR-HM). SSD-agnostic-adv performs the best on ARUnseen and AR-HM, with a drop in AR-Seen, but the models that outperform SSD-agnostic-adv on AR-Seen do significantly worse on AR-Unseen and AR-HM. The second row shows micro-level results for the easy, medium, and hard unseen classes. SSD-agnostic-adv performs the best in all categories. The last row provides results of evaluation on the COCO data of 60 unseen classes. SSD-agnostic-adv achieves the best AR@k with a slight reduction for small-sized objects.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on unseen classes according to the AR-Unseen metric?",
    "answer": "SSD-agnostic-adv",
    "rationale": "The figure shows that the SSD-agnostic-adv model achieves the highest AR-Unseen score among all the models tested.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.14204v1",
    "pdf_url": null
  },
  {
    "instance_id": "6578753415a5455cadef4feccef7d759",
    "figure_id": "2210.02015v2-Figure5-1",
    "image_file": "2210.02015v2-Figure5-1.png",
    "caption": " Empirical running times of methods used for experimental comparisons. We utilized the linear quantile model on the MEPS dataset. For methods of \"Chzhen\" and \"Agarwal\", normalizing factors are applied to present the graph.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the fastest running time?",
    "answer": "The O(nlogn) method.",
    "rationale": "The figure shows the running time of different methods as a function of the number of data samples. The O(nlogn) method has the lowest running time for all data sample sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.02015v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd962fb438a64c15a1f3bedf2e7c4f74",
    "figure_id": "1811.11903v1-Figure4-1",
    "image_file": "1811.11903v1-Figure4-1.png",
    "caption": " 10 region description examples for an image in Visual Genome dataset [16], where each region description corresponds to a bounding box with the same color in the image. The descriptions range from the states of a single object (color, trait, action, etc.) to object relationships.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many objects are described in the image?",
    "answer": "10",
    "rationale": "The caption states that there are 10 region description examples, and each example corresponds to a bounding box in the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.11903v1",
    "pdf_url": null
  },
  {
    "instance_id": "8a8fcfcf16bc4d36ad6294299efb5453",
    "figure_id": "2109.03160v2-Figure4-1",
    "image_file": "2109.03160v2-Figure4-1.png",
    "caption": " SyntaxGym evaluation across circuits.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best on the Center Embedding circuit?",
    "answer": "roberta-base-1B-1",
    "rationale": "The figure shows that roberta-base-1B-1 achieved the highest EN SG score on the Center Embedding circuit.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.03160v2",
    "pdf_url": null
  },
  {
    "instance_id": "4ebaa8a896194b5498f2230a57772e1f",
    "figure_id": "2010.01986v1-Figure6-1",
    "image_file": "2010.01986v1-Figure6-1.png",
    "caption": " Running time v.s. the number of states.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm, SHMM or Gmove, is more efficient in terms of training time for both LA and NY datasets?",
    "answer": "SHMM",
    "rationale": "The figure shows that the training time for SHMM is consistently lower than that of Gmove for both LA and NY datasets. This indicates that SHMM is more efficient in terms of training time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.01986v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee1f0148c0aa4b5bb76d533b2e575745",
    "figure_id": "2109.01349v2-Figure11-1",
    "image_file": "2109.01349v2-Figure11-1.png",
    "caption": " More qualitative comparisons on the CUFED5 dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following algorithms produces the sharpest image?",
    "answer": "Our algorithm",
    "rationale": "The images produced by our algorithm are the sharpest and most closely resemble the reference images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.01349v2",
    "pdf_url": null
  },
  {
    "instance_id": "14a81e79ca5d46469d8e38036bfb6e81",
    "figure_id": "2103.16634v2-Figure13-1",
    "image_file": "2103.16634v2-Figure13-1.png",
    "caption": " Top-1 validation accuracy curves of ResNet-50 when trained on ImageNet with ND++, Switchable Whitening(SW), IterNorm using batch size of 2048.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which normalization technique achieves the highest accuracy on ImageNet?",
    "answer": "ND++",
    "rationale": "The figure shows the top-1 validation accuracy curves of ResNet-50 when trained on ImageNet with ND++, Switchable Whitening (SW), and IterNorm. ND++ achieves the highest accuracy at the end of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.16634v2",
    "pdf_url": null
  },
  {
    "instance_id": "ce0a330cbf1641d8932a7daed571bee1",
    "figure_id": "1911.02215v2-Figure4-1",
    "image_file": "1911.02215v2-Figure4-1.png",
    "caption": " Translation quality on the WMT14 and WMT 16 test sets over various input sentence lengths.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the WMT14 test set according to the Duplication metric?",
    "answer": "ReorderNAT (NAT) + LPD (s=7)",
    "rationale": "The table shows that the Duplication metric for ReorderNAT (NAT) + LPD (s=7) is the lowest for the WMT14 test set, indicating the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.02215v2",
    "pdf_url": null
  },
  {
    "instance_id": "45658d3a8be64f9db14f06c01f775adb",
    "figure_id": "2109.15207v1-Figure7-1",
    "image_file": "2109.15207v1-Figure7-1.png",
    "caption": " Visualization of a few R2R unseen episodes with nDTW(shortest,LAW step)< 0.8 shows us how dissimilar the goal-oriented shortest path (red) and the language-aligned path (blue) are.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which path is more likely to be the language-aligned path, the red path or the blue path?",
    "answer": "The blue path.",
    "rationale": "The caption states that the blue path is the language-aligned path, and the red path is the shortest path.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.15207v1",
    "pdf_url": null
  },
  {
    "instance_id": "456759380b2c4be78b53d2937939f927",
    "figure_id": "2306.01685v1-Figure2-1",
    "image_file": "2306.01685v1-Figure2-1.png",
    "caption": " The training loss of BERT-Large-Uncased using different optimizers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer resulted in the lowest training loss for BERT-Large-Uncased?",
    "answer": "MKOR-H",
    "rationale": "The figure shows the training loss of BERT-Large-Uncased using different optimizers. The MKOR-H optimizer resulted in the lowest training loss, as its line is the lowest on the graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01685v1",
    "pdf_url": null
  },
  {
    "instance_id": "6a86df03f029427784d91c251db5f995",
    "figure_id": "1905.07177v1-Figure3-1",
    "image_file": "1905.07177v1-Figure3-1.png",
    "caption": " Comparing BOX and S-BOX on the testing images with different edges. The first and forth columns (a), (g), (m), (d), (j) and (p) are input images with edge or corner. The second and fifth columns are middle line profiles for input, BOX filter and S-BOX filter. The third and sixth columns are the zoomed in region at the edge or corner location.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which filter, BOX or S-BOX, better preserves edges in the input image?",
    "answer": "S-BOX",
    "rationale": "The zoomed-in regions in the third and sixth columns of the figure show that the S-BOX filter produces a sharper edge than the BOX filter. This is because the S-BOX filter uses a smaller kernel size and weights the pixels near the edge more heavily.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.07177v1",
    "pdf_url": null
  },
  {
    "instance_id": "767789606e884abf95b053e322ebcd15",
    "figure_id": "2010.07777v1-Figure6-1",
    "image_file": "2010.07777v1-Figure6-1.png",
    "caption": " Social metrics at equilibrium. (A) Utilitarian (B) Equality (C) Sustainability",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three metrics (Utilitarian, Equality, Sustainability) has the smallest difference between the \"All Cooperators\" and \"Equilibrium Points\" scenarios for all the social dilemmas considered?",
    "answer": "Sustainability.",
    "rationale": "Looking at the bar charts, the gap between the bars for \"All Cooperators\" and \"Equilibrium Points\" is consistently smaller for Sustainability (panel C) compared to Utilitarian (panel A) and Equality (panel B). This indicates that Sustainability is less sensitive to the presence of non-cooperating agents compared to the other two metrics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.07777v1",
    "pdf_url": null
  },
  {
    "instance_id": "dd1db1f283ab4c0e98e18553424ee3cc",
    "figure_id": "2208.10227v1-Figure7-1",
    "image_file": "2208.10227v1-Figure7-1.png",
    "caption": " Number of unsat clauses in the best found solution on our MAX-5-SAT test instances. We plot the average over 50 instances as a function in the number of search steps t. We compare ANYCSP and its two ablation versions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best in terms of finding the solution with the least number of unsat clauses?",
    "answer": "ANYCSP",
    "rationale": "The figure shows that the ANYCSP algorithm consistently finds solutions with fewer unsat clauses than the other two algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.10227v1",
    "pdf_url": null
  },
  {
    "instance_id": "acfe19d1d2934dcda935468810aefec1",
    "figure_id": "1902.09359v1-Figure4-1",
    "image_file": "1902.09359v1-Figure4-1.png",
    "caption": " Total distance saved (km) for various values of minW ,maxW ,q. (4a, left) Pragmatic scenario, (4a, right) Requests become critical in just one time-step, (4b) Various levels of waiting time (q) assuming no bounds, i.e.,minW = 0,maxW = ∞, (double log. scale).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm saves the most distance in the pragmatic scenario when q = 0.1?",
    "answer": "ALMA.",
    "rationale": "In Figure (a), the left panel shows the total distance saved for various algorithms in the pragmatic scenario when q = 0.1. The bars for ALMA are the highest, indicating that it saves the most distance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.09359v1",
    "pdf_url": null
  },
  {
    "instance_id": "362a9eea98644cf6aef173030d9b995e",
    "figure_id": "2103.07552v1-Figure3-1",
    "image_file": "2103.07552v1-Figure3-1.png",
    "caption": " Improvement from data augmentation for different dataset sizes (results averaged over HUFF and FEWREL datasets).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which curriculum leads to the largest improvement with data augmentation?",
    "answer": "Gradual curriculum.",
    "rationale": "The blue line in the plot shows the improvement for the gradual curriculum, which is higher than the other two lines for all values of Nc.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.07552v1",
    "pdf_url": null
  },
  {
    "instance_id": "1a28f8f7268d44f9836e644180270603",
    "figure_id": "1909.13404v1-Figure9-1",
    "image_file": "1909.13404v1-Figure9-1.png",
    "caption": " Results for search algorithm experiments. Left: Relation between the performance of the best architecture found and the number of architectures sampled. Right: Histogram of validation accuracies for the architectures encountered by each search algorithm.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which search algorithm finds the best architectures with the least number of samples?",
    "answer": "SMBO",
    "rationale": "The left plot shows the best validation accuracy found by each search algorithm as a function of the number of architectures sampled. SMBO reaches a high validation accuracy with fewer samples than the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.13404v1",
    "pdf_url": null
  },
  {
    "instance_id": "3a01d7d5810b4e4ebadc5d83ebfb230c",
    "figure_id": "2210.14494v1-Figure9-1",
    "image_file": "2210.14494v1-Figure9-1.png",
    "caption": " The distribution of the percentage of selected code lines in code.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common percentage of selected lines in the code?",
    "answer": "Around 10%.",
    "rationale": "The figure shows that the frequency of code selections is highest for percentages between 0.1 and 0.2. This suggests that most code selections are relatively small, comprising around 10% of the total code lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.14494v1",
    "pdf_url": null
  },
  {
    "instance_id": "17053901183f47719247c175e3b9f472",
    "figure_id": "2110.04111v1-Figure7-1",
    "image_file": "2110.04111v1-Figure7-1.png",
    "caption": " Silhouette score.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the silhouette score, what is the optimal number of clusters for this dataset?",
    "answer": "3",
    "rationale": "The silhouette score is a metric that measures how well each data point is assigned to its cluster. A higher silhouette score indicates that the data points are well-clustered. In this case, the silhouette score is highest when the number of clusters is 3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.04111v1",
    "pdf_url": null
  },
  {
    "instance_id": "eb0707acf3534a12811b67e69498ec54",
    "figure_id": "1911.09336v4-Figure5-1",
    "image_file": "1911.09336v4-Figure5-1.png",
    "caption": " Ablation study on NAS-Bench-201.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which BONAS variant achieves the highest accuracy on NAS-Bench-201?",
    "answer": "BONAS_LSTM_EA",
    "rationale": "The plot in (a) shows the accuracy of different BONAS variants as a function of the number of evaluated samples. BONAS_LSTM_EA has the highest accuracy for all numbers of evaluated samples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.09336v4",
    "pdf_url": null
  },
  {
    "instance_id": "d20997f171ec4c418b94c426b6af1c94",
    "figure_id": "2202.06406v1-Figure10-1",
    "image_file": "2202.06406v1-Figure10-1.png",
    "caption": " Clarinet and guitar with silent violin. The first row is localization map of clarinet, the second row is localization map of guitar, the third row is localization map of silent violin, note that the activation value in silent area should be suppressed.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which instrument produces the least amount of sound based on the image?",
    "answer": "The silent violin.",
    "rationale": "The third row of the image shows the localization map of the silent violin. The activation values in the silent area are suppressed, indicating that the silent violin produces less sound than the clarinet and guitar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.06406v1",
    "pdf_url": null
  },
  {
    "instance_id": "090e8aa060d94f8584aeec18b5f43044",
    "figure_id": "2305.16739v1-Figure3-1",
    "image_file": "2305.16739v1-Figure3-1.png",
    "caption": " The performance of ALIGNSCORE-base using different classification heads. ALIGNSCORE-REG and ALIGNSCORE-BIN indicate the regression head and the binary classification head, respectively. ALIGNSCORE is our proposed setting (see Section 3.2).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which classification head achieved the best performance for AlignScore-base on the TRUE metric?",
    "answer": "AlignScore-REG",
    "rationale": "The figure shows the performance of AlignScore-base on different metrics, using different classification heads. For the TRUE metric, the bar corresponding to AlignScore-REG is the highest, indicating that it achieved the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16739v1",
    "pdf_url": null
  },
  {
    "instance_id": "9562ba41e79e4be39487c8bb04039b6c",
    "figure_id": "2211.11082v3-Figure6-1",
    "image_file": "2211.11082v3-Figure6-1.png",
    "caption": " Qualitative comparisons on the Nvidia dataset [75].",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method produces the most realistic rendering of the scene?",
    "answer": " Ours",
    "rationale": " The figure shows a comparison of different rendering methods. The \"Ours\" column shows the most realistic rendering of the scene, as it is the closest to the ground truth (GT) image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11082v3",
    "pdf_url": null
  },
  {
    "instance_id": "1435e30b6aac4939aef231049634215e",
    "figure_id": "1904.12181v1-Figure1-1",
    "image_file": "1904.12181v1-Figure1-1.png",
    "caption": " Sample adversarial attacks on SLSDeep (Sarker et al. 2018), NWCN (Hwang and Park 2017), CDNN (Yuan 2017) and our NLCEN. The input chest radiograph and its ground-truth segmentation are shown in the first row. Adversarial perturbations and images generated for models by the Iterative FGSM attack method (Kurakin, Goodfellow, and Bengio 2016) with adversarial intensity set to 16 are shown in the second and third rows respectively. Segmentation results on the adversarial images and the input image are shown in the fourth and fifth rows respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most robust to adversarial attacks?",
    "answer": "NLCEN.",
    "rationale": "The figure shows the segmentation results of different methods on adversarial images. The segmentation results of NLCEN are most similar to the ground truth segmentation, which indicates that NLCEN is most robust to adversarial attacks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.12181v1",
    "pdf_url": null
  },
  {
    "instance_id": "95605a363f7b47399293aa51c8910e51",
    "figure_id": "2305.19998v1-Figure1-1",
    "image_file": "2305.19998v1-Figure1-1.png",
    "caption": " Heatmaps of explanation scores of an example from Yelp-Polarity based on two runs of KernelSHAP (KS) using different random seeds. KS is run on a fine-tuned BERT model using 200 samples per instance (approx. 3.47s per instance on average using a single A100 GPU, more than 150 times slower than one forward inference of the BERT model). The darker each token is, the higher its explanation score. Clearly, interpretation results are significantly different when using different seeds.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How do the heatmaps for Seed-1 and Seed-2 differ?",
    "answer": "The heatmaps for Seed-1 and Seed-2 are identical.",
    "rationale": "The caption states that KernelSHAP was run twice with different random seeds, but the resulting heatmaps are identical. This suggests that the choice of random seed does not significantly affect the explanation scores in this case.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19998v1",
    "pdf_url": null
  },
  {
    "instance_id": "6443db0b8a8a409ea5917e0f80d42cac",
    "figure_id": "2012.08740v2-Figure5-1",
    "image_file": "2012.08740v2-Figure5-1.png",
    "caption": " On DBLP-E data, TRNNGCN and RNNGCN outperform the static GCN and spectral clustering methods and show better performance over time, indicating that they can optimize their decay rates to account for historical information. TRNNGCN slightly outperforms RNNGCN.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed best overall on the DBLP-E dataset?",
    "answer": "TRNNGCN",
    "rationale": "The bar chart on the right shows that TRNNGCN has the highest bars for all three metrics (Accuracy, AUC, and F1).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.08740v2",
    "pdf_url": null
  },
  {
    "instance_id": "4f4e8c427dbc4105b2debe6172f1c1a1",
    "figure_id": "1811.03925v1-Figure4-1",
    "image_file": "1811.03925v1-Figure4-1.png",
    "caption": " The entity annotation scheme for the example sentence in Figure 1 when the agent predicts a relation type parent-children between Steve Belichick and Bill Belichick. In this example, New England Patriots and Annapolis are not-concerned entities with respect to relation type parent-children.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which entities in the sentence are not concerned with the relation type parent-children?",
    "answer": "New England Patriots and Annapolis",
    "rationale": "The figure shows that the entities New England Patriots and Annapolis are labeled as O, which indicates that they are not concerned with the relation type parent-children.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.03925v1",
    "pdf_url": null
  },
  {
    "instance_id": "16e54102a7184731a4d359146e00dbc4",
    "figure_id": "2005.06901v1-Figure5-1",
    "image_file": "2005.06901v1-Figure5-1.png",
    "caption": " Skeleton-level evaluation F1 (%) results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best overall?",
    "answer": "The +GAT-encoder model performs the best overall.",
    "rationale": "The figure shows the F1 scores for different models on different tasks. The +GAT-encoder model has the highest F1 score for all tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.06901v1",
    "pdf_url": null
  },
  {
    "instance_id": "e3291370fec64741815f9a5a5e5b9506",
    "figure_id": "2103.15213v1-FigureA.1-1",
    "image_file": "2103.15213v1-FigureA.1-1.png",
    "caption": "Figure A.1: Visual illustrations on how we equip the standard neural architectures with the temporal kernel using the random Fourier feature with invertible neural network (the RFF-INN blocks).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three architectures in Figure A.1 uses a causal mask to ensure that the model only attends to past information?",
    "answer": "T-(Self)Attention",
    "rationale": "The T-(Self)Attention architecture includes a causal mask, which is a matrix that is used to prevent the model from attending to future information. This is important for time series forecasting, where the model should only be able to use past information to predict future values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.15213v1",
    "pdf_url": null
  },
  {
    "instance_id": "ce68ee6fc79144f58de89691e950b2e3",
    "figure_id": "2006.07529v2-Figure7-1",
    "image_file": "2006.07529v2-Figure7-1.png",
    "caption": " Confusion matrices of standard CE training and using DU@5x on CIFAR-10-LT with ρ = 100.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on CIFAR-10-LT with ρ = 100, standard CE training or CE with unlabeled data DU@5x?",
    "answer": "CE with unlabeled data DU@5x performs better.",
    "rationale": "The confusion matrix for CE with unlabeled data DU@5x shows higher values on the diagonal, indicating that the model is more accurate in predicting the correct class. In contrast, the confusion matrix for standard CE training shows more off-diagonal values, indicating that the model is making more mistakes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.07529v2",
    "pdf_url": null
  },
  {
    "instance_id": "d7e976a84db64f8ebcb1b96100758423",
    "figure_id": "2109.03413v2-Figure3-1",
    "image_file": "2109.03413v2-Figure3-1.png",
    "caption": " Statistics of the YouRefIt dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most frequently referenced object in the YouRefIt dataset?",
    "answer": "Chair.",
    "rationale": "Figure (a) shows the frequency of the top-20 referenced objects. The chair is the tallest bar, indicating it is the most frequently referenced object.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.03413v2",
    "pdf_url": null
  },
  {
    "instance_id": "e90576567d654f6db4fe1a4a4c57f94a",
    "figure_id": "1909.00161v1-Figure2-1",
    "image_file": "1909.00161v1-Figure2-1.png",
    "caption": " Performance of different situation classes in label-fully-unseen, predicted by the ensemble model.",
    "figure_type": "\"plot\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which situation type has the highest F1 score?",
    "answer": "\"None\"",
    "rationale": "The figure shows the F1 scores for different situation types. The \"None\" situation type has the highest F1 score, which is about 55%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.00161v1",
    "pdf_url": null
  },
  {
    "instance_id": "159a834eaab244dd9b1952668eb66b36",
    "figure_id": "1906.06693v4-Figure10-1",
    "image_file": "1906.06693v4-Figure10-1.png",
    "caption": " Arithmetic operations in latent space.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens when you subtract the chair without arms from the chair with arms?",
    "answer": "You get the arms of the chair.",
    "rationale": "The figure shows that when you subtract the chair without arms (green) from the chair with arms (blue and pink), you get the arms of the chair (pink).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.06693v4",
    "pdf_url": null
  },
  {
    "instance_id": "7ce89fa29e6645b993ca02161d1570d5",
    "figure_id": "2103.01338v2-Figure13-1",
    "image_file": "2103.01338v2-Figure13-1.png",
    "caption": " ResNet-18/CIFAR-10 training with batch size 8192 and a repeated T = 8 fractal Chebyshev schedule. Left: Training loss curves. Right: Learning rates; the schedule pokes through the edge of stability (magenta and red) without destabilizing training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the learning rate and the training loss?",
    "answer": "The learning rate affects the training loss.",
    "rationale": "The plot on the left shows the training loss for different learning rates. The plot on the right shows the learning rates themselves. The fractal learning rate, which pokes through the edge of stability, results in a lower training loss than the constant learning rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.01338v2",
    "pdf_url": null
  },
  {
    "instance_id": "5cb74c7e725b44999f8e774447e3d4c1",
    "figure_id": "1902.00139v4-Figure1-1",
    "image_file": "1902.00139v4-Figure1-1.png",
    "caption": " The figure summarizes the main results of this paper for the spiked matrix-tensor model with p = 3 (left) and p = 4 (right). As a function of the tensor-noise parameter ∆p on the x-axes, we plot the values of 1/∆2 above which the following happens (from above): Above ∆triv",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of achieving the smallest value of 1/∆2 for a given value of ∆p?",
    "answer": "The Bayes optimal method.",
    "rationale": "The Bayes optimal curve is the lowest of all the curves in the plot, which means that it achieves the smallest value of 1/∆2 for any given value of ∆p. This implies that the Bayes optimal method is the most efficient in terms of utilizing the information contained in the data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.00139v4",
    "pdf_url": null
  },
  {
    "instance_id": "a541b2189faa484297f256e4fddbdb30",
    "figure_id": "2207.05621v3-Figure1-1",
    "image_file": "2207.05621v3-Figure1-1.png",
    "caption": " Left. Trade-off between performance vs: number of parameters on CSD [15] testing dataset. Right. Trade-off between performance vs: computational cost calculated on 256 × 256 resolution.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest PSNR while having the lowest number of parameters?",
    "answer": "MSP-Former.",
    "rationale": "The left plot shows the trade-off between PSNR and the number of parameters for different methods. MSP-Former is located at the top right corner of the plot, indicating that it achieves the highest PSNR while having the lowest number of parameters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.05621v3",
    "pdf_url": null
  },
  {
    "instance_id": "0dece0dd50324df8bd20e4c40aeaf4af",
    "figure_id": "1904.03589v2-Figure11-1",
    "image_file": "1904.03589v2-Figure11-1.png",
    "caption": " Illustrative examples of PACG dataset: we first annotate all existing semantic attributes and colors based on captions and manually checking. Counterfactual (CF) attributes and queries are then generated based on that.",
    "figure_type": "\"photograph(s)\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attribute in the first example image was not annotated based on the ground truth caption?",
    "answer": "\"woman\"",
    "rationale": "The ground truth caption describes the person as \"a man in a yellow shirt\", however the annotated attribute for the person is \"woman\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03589v2",
    "pdf_url": null
  },
  {
    "instance_id": "b3aee417e49d4a5fa35c3e8d566acaac",
    "figure_id": "2006.04139v2-Figure8-1",
    "image_file": "2006.04139v2-Figure8-1.png",
    "caption": " Comparison between TTSR trained without (top) and with (bottom) transferal perceptual loss.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two images in the figure has better resolution?",
    "answer": "The bottom image has better resolution.",
    "rationale": "The bottom image has sharper edges and more details than the top image. This is because the bottom image was generated using TTSR trained with transferal perceptual loss, which helps to improve the resolution of the generated image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04139v2",
    "pdf_url": null
  },
  {
    "instance_id": "0c19fd4d5aac43348ab869d0a40da6f1",
    "figure_id": "2106.07802v1-Figure7-1",
    "image_file": "2106.07802v1-Figure7-1.png",
    "caption": " Left: Examples of generated structures. For every model, we show the best generated conformer, i.e. with the smallest RMSD to the shown ground truth. More examples are in appendix M. Right/top: Number of rotatable bonds per DRUGS test molecule versus COV Recall (95% confidence intervals). Right/bottom: conformer generation times for each model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model generated the most accurate conformers for molecules with a high number of rotatable bonds?",
    "answer": "GeoMol",
    "rationale": "The top right plot shows that GeoMol has the highest recall coverage for molecules with a high number of rotatable bonds. This means that GeoMol was able to generate more conformers that were close to the ground truth than the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07802v1",
    "pdf_url": null
  },
  {
    "instance_id": "fd80674991ce434bbe662b3430afc3c7",
    "figure_id": "2210.05499v2-Figure2-1",
    "image_file": "2210.05499v2-Figure2-1.png",
    "caption": " The box plots of document length distribution on Qasper and HotpotQA-Doc.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has a longer average document length?",
    "answer": "HotpotQA-Doc",
    "rationale": "The box plot for HotpotQA-Doc is higher than the box plot for Qasper, indicating that the median document length is greater in HotpotQA-Doc.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.05499v2",
    "pdf_url": null
  },
  {
    "instance_id": "50fe8c7ff2324e3f9ee01eeea94ad348",
    "figure_id": "2011.10379v3-Figure10-1",
    "image_file": "2011.10379v3-Figure10-1.png",
    "caption": " Qualitative results on reconstruction and novel scene arrangements of a scene from the KITTI dataset [11] for SRN [32], NeRF [22], a modified NeRF with a time parameter, and our neural scene graphs. Reconstruction here refers to the reproduction of a frame seen during training, and novel scene arrangements render a scene not seen in the training set. SRN learns to average all frames in the training set. NeRF and the modified variant struggle to learn dynamic parts of the scene adequately. Our neural scene graph method achieves high-quality view synthesis results regardless of this dynamicity, both for dynamic scene reconstruction and novel scene generation.",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method is able to reconstruct the scene most accurately? ",
    "answer": " Our neural scene graph method. ",
    "rationale": " The figure shows that the other methods (SRN, NeRF, and NeRF + Time) all produce blurry or distorted images, while our method produces a clear and accurate image that is very similar to the reference image. \n\n**Figure type:** photograph(s)",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.10379v3",
    "pdf_url": null
  },
  {
    "instance_id": "8000110b70ea4b929b4c71651205f34c",
    "figure_id": "2210.05845v7-Figure5-1",
    "image_file": "2210.05845v7-Figure5-1.png",
    "caption": " ConSpec improves performance on delayed (a) Pong, (b) Bowling, and (c) AirRaid.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does ConSpec compare to PPO in terms of performance on the delayed Pong, Bowling, and AirRaid tasks?",
    "answer": "ConSpec outperforms PPO on all three tasks.",
    "rationale": "The figure shows the learning curves for ConSpec and PPO on the three tasks. ConSpec (green line) consistently achieves higher rewards than PPO (orange line) on all three tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.05845v7",
    "pdf_url": null
  },
  {
    "instance_id": "468834dc1fe04441a8de1ed9856cfdfe",
    "figure_id": "2210.12353v3-Figure8-1",
    "image_file": "2210.12353v3-Figure8-1.png",
    "caption": " Prompt example for the COPA dataset.",
    "figure_type": "Other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Why wouldn't people bring boats to the pond?",
    "answer": "Because the pond is frozen.",
    "rationale": "The passage states that the pond froze over for the winter. This means that the water in the pond is solid ice, and boats would not be able to float on it.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12353v3",
    "pdf_url": null
  },
  {
    "instance_id": "cc31c09a3cd541f4a1562e66aa8932f1",
    "figure_id": "1711.08195v3-Figure4-1",
    "image_file": "1711.08195v3-Figure4-1.png",
    "caption": " Visualization of co-attention for three examples. Each example is comprised of four things: (1) image and visual attentions; (2) ground truth tags and semantic attention on predicted tags; (3) generated descriptions; (4) ground truth descriptions. For the semantic attention, three tags with highest attention scores are highlighted. The underlined tags are those appearing in the ground truth.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following tags is not present in the ground truth for the first example?",
    "answer": "pneumonia",
    "rationale": "The figure shows the ground truth tags for the first example, and \"pneumonia\" is not among them.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.08195v3",
    "pdf_url": null
  },
  {
    "instance_id": "66ce355f6e6e49308ef25d39b1b8a67f",
    "figure_id": "2010.09893v1-Figure7-1",
    "image_file": "2010.09893v1-Figure7-1.png",
    "caption": " Samples of generated images from categories with mode collapse in Baseline BigGAN and its corresponding images generated from LTBigGAN model. The 6 blocks of images corresponds to ImageNet classes: (a) digital clock, (b) parachute, (c) nematode, (d) anemone fish ,(e) theater curtain, (f) daisy. In each block (that comprises of 4 rows of images), the top part (1st and 2nd row) corresponds to images generated using Baseline (BigGAN) model and the bottom part (3rd and 4th row corresponds to images produced using our approach LT-BigGAN.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following images shows the most diversity in the generated images?",
    "answer": "(f) daisy.",
    "rationale": "The images in (f) show a variety of different daisies, with different colors, shapes, and sizes. The other images show less diversity, with the images in (a), (b), and (c) being very similar to each other.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.09893v1",
    "pdf_url": null
  },
  {
    "instance_id": "0db1d0dc48954c148c90cb9a179359c0",
    "figure_id": "2303.12735v1-Figure1-1",
    "image_file": "2303.12735v1-Figure1-1.png",
    "caption": " MODL’s instabilities against perturbations to input data, the measurement sampling rate, and the number of unrolling steps used at testing time shown on an image from the fastMRI [22] dataset. We refer readers to Sec. 4 for more experiment details. (a) MODL reconstruction from benign (i.e., clean) measurement with 4× acceleration (i.e., 25% sampling rate) and 8 unrolling steps. (b) MODL reconstruction from adversarial input of perturbation strength ε = 0.002 (other settings are same as (a)). (c) MODL reconstruction from clean measurement with 2× acceleration (i.e., 50% sampling rate) and using 8 unrolling steps. (d) MODL reconstruction from clean measurement with 4× acceleration and using 16 unrolling steps.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the images shows the MODL reconstruction from clean measurement with 4× acceleration (i.e., 25% sampling rate) and 8 unrolling steps?",
    "answer": "(a)",
    "rationale": "The caption states that (a) shows the MODL reconstruction from benign (i.e., clean) measurement with 4× acceleration (i.e., 25% sampling rate) and 8 unrolling steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.12735v1",
    "pdf_url": null
  },
  {
    "instance_id": "eeabde1336524e77b5655cdbb0c8bd45",
    "figure_id": "2209.10740v1-Figure4-1",
    "image_file": "2209.10740v1-Figure4-1.png",
    "caption": " Momentum error (ME) of Node, Gnode, CGnode, CDGnode, and MCGnode (top palette), and Lgn, Hgn, MCGnode (bottom palette) for spring (top row of both the palettes) and pendulum (bottom row of both the palettes) systems with 3, 4, and 5 links. The shaded region represents the 95% confidence interval based on 100 forward simulations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest momentum error for the spring system with 3 links?",
    "answer": "NODE",
    "rationale": "The figure shows the momentum error for different models and systems. The top row of the figure shows the results for the spring system, and the first column shows the results for 3 links. The NODE model has the highest momentum error, as its line is the highest in the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.10740v1",
    "pdf_url": null
  },
  {
    "instance_id": "b77ac4e723ec46969f7172af5a9a536f",
    "figure_id": "1902.04620v1-Figure3-1",
    "image_file": "1902.04620v1-Figure3-1.png",
    "caption": " Training curves and final loss comparison for a convex problem with synthetic data. Left: Population loss for each optimizer vs. iteration number. Right: Final loss vs. optimizer parameter count.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer converges to the lowest loss in the least number of iterations?",
    "answer": "AdaGrad",
    "rationale": "The left plot shows that AdaGrad (red line) reaches the lowest loss value first.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.04620v1",
    "pdf_url": null
  },
  {
    "instance_id": "c3472a92a67e4a0384244038cbf7a193",
    "figure_id": "2209.15439v2-Figure9-1",
    "image_file": "2209.15439v2-Figure9-1.png",
    "caption": " Five subjects and three backgrounds are used in IhD-1 dataset. For maintaining anonymity, we cover the subject faces.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many subjects are shown in the image?",
    "answer": "Five.",
    "rationale": "The caption states that five subjects are shown in the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.15439v2",
    "pdf_url": null
  },
  {
    "instance_id": "437d6a1379314a74a766b7f37d0b8dc5",
    "figure_id": "1902.04783v4-Figure5-1",
    "image_file": "1902.04783v4-Figure5-1.png",
    "caption": " The crime risk prediction scenario—the number of participants matched with each notion of fairness (y-axis) along with the likelihood levels (x-axis). Demographic parity captures the choices made by the majority of participants.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of participants believe that a fair crime risk prediction system should have equal false positive rates across different groups?",
    "answer": "96%",
    "rationale": "The green bar in the figure shows that 96% of participants chose the fairness notion of equal false positive rates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.04783v4",
    "pdf_url": null
  },
  {
    "instance_id": "dc0fe9bd22024ee8b600536c25d0886c",
    "figure_id": "2205.02068v1-Figure1-1",
    "image_file": "2205.02068v1-Figure1-1.png",
    "caption": " A sample utterance and its semantic parse tree in the Topv2 dataset, where nodes starting with “IN:” are intents and nodes starting with “SL:” are slots. A parse tree always has an intent node at the root. An intent typically corresponds to a verb and can be viewed as an action, with a sequence of slots as its arguments. A slot may have additional intents nested in it, recursively. A linearized parse tree that can be processed by seq2seq models is shown at the bottom.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the intent of the utterance \"find the nearest parking to S Beritania Street\"?",
    "answer": "The intent of the utterance is \"GET_LOCATION\".",
    "rationale": "The semantic parse tree shows that the root node is \"IN:GET_LOCATION\", which indicates that the user wants to find a location.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.02068v1",
    "pdf_url": null
  },
  {
    "instance_id": "26222886b93a4c4a82831d8a380f76a5",
    "figure_id": "2009.08552v2-Figure5-1",
    "image_file": "2009.08552v2-Figure5-1.png",
    "caption": " All models’ performance in (a) Structure Euclidean Distance (SED) and (b) Structure CrossEntropy (SCE) in four dialogue domains.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best in the \"restaurant\" domain according to the Structure Euclidean Distance (SED) metric?",
    "answer": "The VRNN-LinearCRF-BERT model.",
    "rationale": "The bar corresponding to VRNN-LinearCRF-BERT is the shortest among all the models in the \"restaurant\" domain for the SED metric. A shorter bar indicates a lower SED value, which implies better performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.08552v2",
    "pdf_url": null
  },
  {
    "instance_id": "e39affe266c14199a24a2cd39a3d9f62",
    "figure_id": "2010.05466v1-Figure1-1",
    "image_file": "2010.05466v1-Figure1-1.png",
    "caption": " An example of cocktail-party scenario, which contains sounding guitar, sounding cello and silent saxophone. We aim to discriminatively localize the sounding instruments and filter out the silent ones. Video: https://www.youtube. com/watch?v=ebugBtNiDMI.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which instrument is silent in the image?",
    "answer": "The saxophone.",
    "rationale": "The image shows a band playing music, with the guitar and cello making sound, while the saxophone has a \"no sound\" symbol over it.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.05466v1",
    "pdf_url": null
  },
  {
    "instance_id": "b28415007d88472bba3cd72d4375553a",
    "figure_id": "2004.11935v1-Figure8-1",
    "image_file": "2004.11935v1-Figure8-1.png",
    "caption": " Copying Task",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model converges faster, NTM or VBB?",
    "answer": "NTM converges faster than VBB.",
    "rationale": "The plot shows that the cost per sequence for NTM decreases more rapidly than the cost per sequence for VBB.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.11935v1",
    "pdf_url": null
  },
  {
    "instance_id": "6bd138c0d4e24915a9ac12430eff84b7",
    "figure_id": "2108.07797v1-Figure6-1",
    "image_file": "2108.07797v1-Figure6-1.png",
    "caption": " A comparison of different methods in scatter plot. Each point in the figure represents a video in the test set. The red line indicates the prefect predictions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better, Ours-CoRe+GART* or I3D+MLP*?",
    "answer": "Ours-CoRe+GART* performs better than I3D+MLP*.",
    "rationale": "The scatter plot shows the relationship between the predicted score and the true label for each video in the test set. The red line indicates the perfect predictions. The closer the points are to the red line, the better the method is performing. We can see that the points for Ours-CoRe+GART* are closer to the red line than the points for I3D+MLP*, which means that Ours-CoRe+GART* is making more accurate predictions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.07797v1",
    "pdf_url": null
  },
  {
    "instance_id": "6a8706270172484799827ba87db03619",
    "figure_id": "2211.13974v4-Figure5-1",
    "image_file": "2211.13974v4-Figure5-1.png",
    "caption": " Correlation between IoU and MI. The negative correlation suggests that reducing MI to increase independence contributes to segmentation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the strongest negative correlation between IoU and MI?",
    "answer": "CUB",
    "rationale": "The figure shows the correlation coefficient for each dataset. The correlation coefficient for CUB is -0.978, which is the closest to -1, indicating the strongest negative correlation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.13974v4",
    "pdf_url": null
  },
  {
    "instance_id": "d9d799554b8d4695b31b4d48f32d5b2f",
    "figure_id": "2012.08791v2-Figure9-1",
    "image_file": "2012.08791v2-Figure9-1.png",
    "caption": "Mean rank for PPV vs PPV and globalmax pooling.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pooling method has the highest mean rank for PPV?",
    "answer": "PPV + max",
    "rationale": "The figure shows that the PPV + max line is higher than the PPV (DEFAULT) line. This means that the mean rank for PPV is higher when using PPV + max pooling than when using the default pooling method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.08791v2",
    "pdf_url": null
  },
  {
    "instance_id": "b0826901d77449a3b473906d783dd036",
    "figure_id": "2205.01089v1-Figure1-1",
    "image_file": "2205.01089v1-Figure1-1.png",
    "caption": " Non-visual properties like mass and charge govern the interaction between objects and lead to different motion trajectories. a) Objects attract and repel each other according to the (sign of) charge they carry. b) Mass determines how much an object’s trajectory is perturbed during an interaction. Heavier objects have more stable motion.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the red object in Figure (a) when it has the same charge as the blue object?",
    "answer": "The red object moves away from the blue object.",
    "rationale": "The figure shows two objects with the same charge repelling each other. The red object is initially close to the blue object, but it moves away from the blue object over time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.01089v1",
    "pdf_url": null
  },
  {
    "instance_id": "4baff009ffab43cba74581f5fe1ef4d1",
    "figure_id": "1907.08027v2-Figure2-1",
    "image_file": "1907.08027v2-Figure2-1.png",
    "caption": " Observations from DMLab are first person views.",
    "figure_type": "photograph",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the key and the gate in the image?",
    "answer": "The key is likely used to open the gate.",
    "rationale": "The key is located in close proximity to the gate, suggesting that it is meant to be used to unlock it.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.08027v2",
    "pdf_url": null
  },
  {
    "instance_id": "ddcd4cd368ac4b5fa23ed04cd818cbee",
    "figure_id": "2310.17940v4-Figure9-1",
    "image_file": "2310.17940v4-Figure9-1.png",
    "caption": " Bivariate kernel density estimation visualization on the representations of source, target and latent segment.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which representation is most similar to the target representation?",
    "answer": " The latent segment representation.",
    "rationale": " The figure shows that the latent segment representation (green) overlaps more with the target representation (red) than the source representation (blue). This suggests that the latent segment representation is more similar to the target representation than the source representation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.17940v4",
    "pdf_url": null
  },
  {
    "instance_id": "d27fa33e69a945bfa1a653073852622d",
    "figure_id": "2103.02458v2-Figure14-1",
    "image_file": "2103.02458v2-Figure14-1.png",
    "caption": " Convergence. Inception score on CIFAR-10 during training with three WGANs different methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method resulted in the highest Inception score?",
    "answer": "Ours.",
    "rationale": "The red line, which represents the \"Ours\" method, has the highest Inception score at the end of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.02458v2",
    "pdf_url": null
  },
  {
    "instance_id": "ef3f1baf7e5b4dd4898fdfddeeca0af6",
    "figure_id": "1909.08830v1-Figure7-1",
    "image_file": "1909.08830v1-Figure7-1.png",
    "caption": " Power spectra of PGD perturbations on CIFAR10. Magnitudes in low-frequency and high-frequency domains are located near center and edge of each figure, respectively. They are normalized as in (0, 1) after logarithmic transform.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of perturbation produces the most high-frequency noise?",
    "answer": "SNC",
    "rationale": "The SNC perturbation has the most high-frequency noise, as shown by the bright colors near the edge of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.08830v1",
    "pdf_url": null
  },
  {
    "instance_id": "8e3995b05e81492eb70b88209f5df0cf",
    "figure_id": "1905.12198v1-Figure4-1",
    "image_file": "1905.12198v1-Figure4-1.png",
    "caption": " An example of extracting head-modifier template from type description by dependency parsing using Stanford CoreNLP toolkit.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the role of the word \"in\" in the sentence \"street in paris\"?",
    "answer": "The word \"in\" is a modifier.",
    "rationale": "The figure shows that the word \"in\" has a dependency relation of \"case\" with the noun \"paris\", which indicates that it is a modifier of the noun.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12198v1",
    "pdf_url": null
  },
  {
    "instance_id": "6d82fee0fe2b473c999538a485e39b3d",
    "figure_id": "1912.01326v3-Figure6-1",
    "image_file": "1912.01326v3-Figure6-1.png",
    "caption": " Per-class results (goals). A prediction of class goal is a true positive (TP) with tolerance δ when it is located at most δ/2 seconds from a ground-truth goal. The baseline results are obtained from the best model of [21]. Our model spots most goals within 10 seconds around the ground truth (δ = 20 seconds).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is better at identifying goals within a tolerance of 10 seconds, ours or the baseline?",
    "answer": "Our model.",
    "rationale": "The figure shows that our model has a higher number of true positives (TP) than the baseline model for all tolerance values up to 10 seconds. This means that our model is more accurate at identifying goals within this tolerance range.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.01326v3",
    "pdf_url": null
  },
  {
    "instance_id": "2307e9a9fa4941fab247e764e5995a8b",
    "figure_id": "2203.06127v1-Figure2-1",
    "image_file": "2203.06127v1-Figure2-1.png",
    "caption": " Examples of spatial heatmaps produced by ResNet-50 on MS-COCO, in the last training epoch, with ImageNet pretraining (best viewed in color).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the most diverse set of predicted classes?",
    "answer": "The image with the car, bus, and traffic light.",
    "rationale": "The heatmap for this image shows a variety of different colors, indicating that the model is predicting a variety of different classes. The other images have heatmaps that are mostly dominated by one color, indicating that the model is predicting a single class with high confidence.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.06127v1",
    "pdf_url": null
  },
  {
    "instance_id": "7c464a254af3483cae0d2f533f0c05fb",
    "figure_id": "2204.06160v1-FigureD.19-1",
    "image_file": "2204.06160v1-FigureD.19-1.png",
    "caption": "Figure D.19. Additional results of appearance control.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different types of garments are shown in the figure?",
    "answer": "Five.",
    "rationale": "The figure shows five different types of garments: a black lace top, a white lace top, a floral tank top, a white t-shirt, and a pink blouse.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.06160v1",
    "pdf_url": null
  },
  {
    "instance_id": "8c7efe96ff8642d4806349640d20f1c9",
    "figure_id": "1906.00675v2-Figure3-1",
    "image_file": "1906.00675v2-Figure3-1.png",
    "caption": " Curves of Top-1 training error (dashed line) and test error (solid line) of the ResNet-18 models trained on the ImageNet classification dataset. Compared with the baseline model, simple auxiliary classifiers (added after the block Conv3 x and Conv4 x) lead to 1.17% drop in Top-1 accuracy and complex designs bring a 0.60% improvement, while our method achieves 2.38% gain. Remarkably, our method converges with the lowest accuracy on training set but achieves the best accuracy on test set, showing better capability in suppressing over-fitting.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the lowest training error and the highest test accuracy?",
    "answer": "DKS with complex aux. classifiers.",
    "rationale": "The figure shows that the DKS with complex aux. classifiers method has the lowest training error and the highest test accuracy. This is because the DKS method is able to suppress over-fitting, which is the phenomenon where a model performs well on the training data but poorly on the test data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.00675v2",
    "pdf_url": null
  },
  {
    "instance_id": "1f1e4bfbba5446ba87a86ee54c6cf29d",
    "figure_id": "2002.07376v2-Figure2-1",
    "image_file": "2002.07376v2-Figure2-1.png",
    "caption": " The gradient norm of ResNet32 after pruning on CIFAR-100 of various pruning ratios. Shaded area is the 95% confidence interval calculated with 10 trials.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pruning method preserves the gradient norm of ResNet32 the best at a pruning ratio of 95%?",
    "answer": "GraSP",
    "rationale": "The figure shows that the relative gradient norm of ResNet32 pruned with GraSP is the closest to 1 at a pruning ratio of 95%. This indicates that GraSP preserves the gradient norm of the network better than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.07376v2",
    "pdf_url": null
  },
  {
    "instance_id": "8d5c595487204170af478a45217b27db",
    "figure_id": "2201.12126v2-Figure7-1",
    "image_file": "2201.12126v2-Figure7-1.png",
    "caption": " Visualisation of the class tree constructed from a subset of entities appearing in the Textworld Commonsense environment. The subclass relations are extracted from WordNet.",
    "figure_type": "** Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the relationship between the concepts of \"kitchen utensil\" and \"spatula\"?",
    "answer": " A spatula is a type of kitchen utensil.",
    "rationale": " The figure shows that \"kitchen utensil\" is a parent node of \"spatula,\" indicating that \"spatula\" is a subclass of \"kitchen utensil.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.12126v2",
    "pdf_url": null
  },
  {
    "instance_id": "96f3036dbb4a425293346223ccc00996",
    "figure_id": "1906.08042v1-Figure3-1",
    "image_file": "1906.08042v1-Figure3-1.png",
    "caption": " Low-resource performances on different datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which machine learning algorithm performed the best on the DBLP-Scholar dataset?",
    "answer": "Deep Transfer Active",
    "rationale": "The figure shows the F1 scores of different machine learning algorithms on different datasets. The DBLP-Scholar dataset is shown in subfigure (b). The Deep Transfer Active algorithm achieved the highest F1 score on this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.08042v1",
    "pdf_url": null
  },
  {
    "instance_id": "d2783d3e4f9546109724b06e1b65db4b",
    "figure_id": "2305.19043v1-Figure3-1",
    "image_file": "2305.19043v1-Figure3-1.png",
    "caption": " Embeddings of 2000 differentiating cells from embryoid body [21] over 28 days. UMAP and t-SNE do not capture the continuous manifold representing the cells’ evolution.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which embedding method seems to best capture the continuous manifold representing the cells’ evolution?",
    "answer": "HeatGeo",
    "rationale": "UMAP and t-SNE do not capture the continuous manifold representing the cells’ evolution, according to the caption. HeatGeo shows a continuous progression of cell states, while the other methods show more discrete clusters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19043v1",
    "pdf_url": null
  },
  {
    "instance_id": "13a65a54b20e44f49c270d6907b44cf6",
    "figure_id": "2308.11894v1-Figure6-1",
    "image_file": "2308.11894v1-Figure6-1.png",
    "caption": " Visualization of STOP sign attacks with systemdriven design. S1: with S1 only; S2: with S2 only; S1 + S2: with both S1 and S2; TV∗: S1 + S2 with TV loss (§5.1).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following images shows the STOP sign with the most adversarial noise?",
    "answer": "(c) FTE-Y5",
    "rationale": "The image in (c) FTE-Y5 shows the STOP sign with the most adversarial noise because it is the most difficult to read. The text is distorted and the colors are blurred.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.11894v1",
    "pdf_url": null
  },
  {
    "instance_id": "b83752bfa8dc480e8b3eeb407932307d",
    "figure_id": "1809.03359v2-Figure1-1",
    "image_file": "1809.03359v2-Figure1-1.png",
    "caption": " Example of an exact DD construction for a MISP instance, following policy π = argmaxaQ π(s, a).",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which state has the largest number of variables assigned to 0?",
    "answer": "State 3.",
    "rationale": "In state 3, there are 4 variables assigned to 0, which is the largest number of variables assigned to 0 in any of the states shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.03359v2",
    "pdf_url": null
  },
  {
    "instance_id": "ea25ec8728cd491781288c8a605fde3e",
    "figure_id": "1905.01067v4-Figure7-1",
    "image_file": "1905.01067v4-Figure7-1.png",
    "caption": " Comparision of Supermask performances in terms of test accuracy on MNIST and CIFAR10 classification tasks. Subfigures are across two network structures (top: FC on MNIST, bottom: Conv4 on CIFAR-10), as well as 1-action treatments (left: weights are at their original initialization, right: weights are converted to signed constants). No training is performed in any network. Within heuristic based Supermasks (excluding learned_mask), the large_final_same_sign mask creates the highest performing Supermask by a wide margin. Note that aside from the five independent runs performed to generate uncertainty bands for this plot, every point on this plot is from the same underlying network, just with different masks. See Figure S6 for performance on all four networks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which initialization method results in the highest test accuracy on the MNIST dataset for the FC network?",
    "answer": "The large_final_same_sign initialization method.",
    "rationale": "The plot in the top left corner shows the test accuracy of different initialization methods on the MNIST dataset for the FC network. The large_final_same_sign line is the highest of all the lines, indicating that it achieves the highest test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.01067v4",
    "pdf_url": null
  },
  {
    "instance_id": "a43c63fc69d441ffb1dcf0e385866361",
    "figure_id": "2107.11027v1-Figure1-1",
    "image_file": "2107.11027v1-Figure1-1.png",
    "caption": " Image inpainting often faces a dilemma of reconstruction and perceptual quality: L1/L2 loss focuses on the reconstruction of global low-frequency structures while adversarial loss focuses on generating high-frequency texture details. State-of-theart approaches implicitly tackle this issue by weighted summing of the two objectives (e.g. in GMCNN [35]) or employing a Coarseto-Fine strategy (e.g. in GC [40]), but tend to produce inconsistent distributions with missing details or artifacts. The proposed WaveFill disentangles images into multiple frequency bands and applies relevant losses to different bands separately, which mitigates interfrequency conflicts and produces more realistic structures and details. The distances between the ground-truth histogram and prediction histograms in both low-frequency (LF) and high-frequency (HF) are evaluated by Earth Mover’s Distance (EMD) [30].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most realistic structures and details, according to the figure?",
    "answer": "WaveFill.",
    "rationale": "The figure shows the Earth Mover's Distance (EMD) between the ground-truth histogram and prediction histograms for different methods. A lower EMD indicates a closer match to the ground truth. WaveFill has the lowest EMD for both low-frequency (LF) and high-frequency (HF) bands, suggesting that it produces the most realistic structures and details.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.11027v1",
    "pdf_url": null
  },
  {
    "instance_id": "96d31b19542b42e9a47ebf4ac1dae47e",
    "figure_id": "1905.12470v1-Figure9-1",
    "image_file": "1905.12470v1-Figure9-1.png",
    "caption": " Visualization of different recommended learning paths for the learning item 642, i.e., completing_the_square_1. REFERENCES",
    "figure_type": "schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which learning path recommends the most diverse set of learning items?",
    "answer": "CSEAL.",
    "rationale": "The CSEAL learning path recommends a more diverse set of learning items, including trigonometry, quadratic equations, and derivatives. This is evident from the figure, which shows that the CSEAL learning path includes a wider variety of learning items than the other two learning paths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12470v1",
    "pdf_url": null
  },
  {
    "instance_id": "9205599ee6cb4b36a20b4653a97f60e6",
    "figure_id": "2012.12964v2-Figure2-1",
    "image_file": "2012.12964v2-Figure2-1.png",
    "caption": " (a) Example applications of the EMBED function. (b) Neural abstract module for +. (c) Neural placeholder module encoding a HOLEwith the context {x = 3}. (d) Neural abstract semantic encoding of the partial program 1 + HOLE with the context {x = 3}.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which subfigure shows how the neural abstract module for + is used to encode the partial program 1 + HOLE with the context {x = 3}?",
    "answer": "Subfigure (d)",
    "rationale": "Subfigure (d) shows how the neural abstract module for + is used to encode the partial program 1 + HOLE with the context {x = 3}. The module takes as input the embeddings of 1 and HOLE, as well as the context {x = 3}. It then outputs the embedding of the partial program 1 + HOLE with the context {x = 3}.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.12964v2",
    "pdf_url": null
  },
  {
    "instance_id": "30e5852e906f482cbacdc2c9b455fdc2",
    "figure_id": "2212.10057v2-Figure3-1",
    "image_file": "2212.10057v2-Figure3-1.png",
    "caption": " : ROC AUC when splitting TRUE’s data according to abstractiveness.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four systems performs best at low levels of abstractiveness?",
    "answer": "WeCheck",
    "rationale": "The figure shows that the WeCheck system has the highest AUC ROC at low levels of abstractiveness.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10057v2",
    "pdf_url": null
  },
  {
    "instance_id": "0ed56584ed4f4d8198d6bca2cceb34b3",
    "figure_id": "1807.03039v2-Figure6-1",
    "image_file": "1807.03039v2-Figure6-1.png",
    "caption": " Manipulation of attributes of a face. Each row is made by interpolating the latent code of an image along a vector corresponding to the attribute, with the middle image being the original image",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the face as we move from left to right in each row of the figure?",
    "answer": "The face becomes more masculine.",
    "rationale": "Each row of the figure shows the effect of interpolating the latent code of an image along a vector corresponding to a specific attribute. The middle image in each row is the original image, and the images to the left and right of it show the effect of increasing and decreasing the value of that attribute, respectively. In this case, the attribute is masculinity, so the images to the right of the middle image in each row show the face becoming more masculine.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1807.03039v2",
    "pdf_url": null
  },
  {
    "instance_id": "5797b3be270241349ff6f3668570940e",
    "figure_id": "2010.04900v2-Figure2-1",
    "image_file": "2010.04900v2-Figure2-1.png",
    "caption": " Code-switching over select countries (with different code-switching profiles) in CodSw.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which country has the highest proportion of tweets containing code-switching with Italian?",
    "answer": "Oman",
    "rationale": "The figure shows the distribution of code-switching with different languages in different countries. The height of the bars represents the proportion of tweets containing code-switching with that language. The bar for Italian in Oman is the highest, indicating that Oman has the highest proportion of tweets containing code-switching with Italian.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.04900v2",
    "pdf_url": null
  },
  {
    "instance_id": "88088ceb52ef4ba9956cc4e33d2fae6b",
    "figure_id": "1804.08069v1-Figure1-1",
    "image_file": "1804.08069v1-Figure1-1.png",
    "caption": " Our proposed models learn a set of discrete variables to represent sentences by either autoencoding or context prediction.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two main tasks performed by the proposed models?",
    "answer": "Autoencoding and context predicting.",
    "rationale": "The figure shows two boxes, one for the Recognition Network and one for the Generation Network. The Recognition Network takes in an input sentence (x) and produces a set of discrete variables (z). The Generation Network takes these variables and uses them to either reconstruct the original sentence (autoencoding) or predict the next sentence in the context (context predicting).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1804.08069v1",
    "pdf_url": null
  },
  {
    "instance_id": "9037758236dc4be7aa6513ef5eda5100",
    "figure_id": "2207.10448v1-Figure4-1",
    "image_file": "2207.10448v1-Figure4-1.png",
    "caption": " Per-category AP@0.5 on THUMOS14.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the SoccerPenalty action class?",
    "answer": "STPT",
    "rationale": "The STPT bar is the highest for the SoccerPenalty action class.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.10448v1",
    "pdf_url": null
  },
  {
    "instance_id": "1f1a1872fdd94a17aa72d6a6b412e400",
    "figure_id": "2004.00566v5-Figure5-1",
    "image_file": "2004.00566v5-Figure5-1.png",
    "caption": " Out-sample prediction performance of module A via Assisted Learning (as measured by RMSE) against the rounds of assistance on Friedman1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model shows the best performance with assisted learning on the Friedman1 dataset?",
    "answer": "Two-layer NNs.",
    "rationale": "The plot shows that the Two-layer NNs model has the lowest testing error with assisted learning.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.00566v5",
    "pdf_url": null
  },
  {
    "instance_id": "61f6da87370341c4b454d2ce4c13f1d4",
    "figure_id": "2004.14592v1-Figure4-1",
    "image_file": "2004.14592v1-Figure4-1.png",
    "caption": " (a) Ranker P@1 learning curves; and (b) Error bars ofmean and standard deviation of ranking loss for different modules of ensembles. Results are calculated on test set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest precision in the early epochs of training?",
    "answer": "RankGAN-D",
    "rationale": "The plot in (a) shows the precision of different models over training epochs. RankGAN-D (red line) has the highest precision in the early epochs of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14592v1",
    "pdf_url": null
  },
  {
    "instance_id": "062dcaccd21e4366a73cb903f6b9befc",
    "figure_id": "1709.06560v3-Figure4-1",
    "image_file": "1709.06560v3-Figure4-1.png",
    "caption": " Performance of several policy gradient algorithms across benchmark MuJoCo environment suites",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which policy gradient algorithm performed the best in the Hopper environment?",
    "answer": "TRPO",
    "rationale": "The figure shows that TRPO achieved the highest average return in the Hopper environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1709.06560v3",
    "pdf_url": null
  },
  {
    "instance_id": "287a841ae8734b58b67a79dd2d9a5e76",
    "figure_id": "2306.01708v2-Figure2-1",
    "image_file": "2306.01708v2-Figure2-1.png",
    "caption": " Different types of conflict and merged outputs produced by either averaging or TIES-MERGING. The parameters causing interference are denoted by dotted arrows.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of conflict is resolved by averaging the values from the two models?",
    "answer": "Redundant conflict.",
    "rationale": "The figure shows that the redundant conflict is resolved by averaging the values from the two models. This is because the two models are producing the same value, so averaging them will not change the value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01708v2",
    "pdf_url": null
  },
  {
    "instance_id": "6c0d310c78a14cdfa0a036744e1e56f8",
    "figure_id": "2206.02663v1-Figure7-1",
    "image_file": "2206.02663v1-Figure7-1.png",
    "caption": " Static results on four benchmarks with 75 trials.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best on average across all four benchmarks?",
    "answer": "TransBO",
    "rationale": "The figure shows the average distance to the minimum (ADTM) for each algorithm on each benchmark. The ADTM is a measure of how well the algorithm performs, with lower values indicating better performance. TransBO has the lowest ADTM on all four benchmarks, indicating that it performs best on average.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.02663v1",
    "pdf_url": null
  },
  {
    "instance_id": "c80c81f8acff4e3387765e25c75b764a",
    "figure_id": "2308.08871v2-Figure8-1",
    "image_file": "2308.08871v2-Figure8-1.png",
    "caption": " We train models on FAUST r and test on SHREC19 r.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model in the image preserves the most detail of the original image?",
    "answer": "UDMSM",
    "rationale": "The UDMSM model is the one that looks the most similar to the original image. The other models have lost some detail, such as the folds in the clothes and the wrinkles in the skin.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.08871v2",
    "pdf_url": null
  },
  {
    "instance_id": "a6d8c5a806dc4b998052bfbd14ea4308",
    "figure_id": "2303.09778v1-Figure3-1",
    "image_file": "2303.09778v1-Figure3-1.png",
    "caption": " Results of SE-GSL with different encoding tree heights.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which encoding tree height resulted in the highest accuracy for the Texas dataset?",
    "answer": "4-SE",
    "rationale": "The figure shows that the 4-SE bar for the Texas dataset is the highest among all the bars for that dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.09778v1",
    "pdf_url": null
  },
  {
    "instance_id": "cb7d11e922a749fdbed7433239a4e747",
    "figure_id": "1906.02768v3-Figure6-1",
    "image_file": "1906.02768v3-Figure6-1.png",
    "caption": " Ablation studies of several classic control games on the effects of late rewinding and iterative pruning. Shaded error bars represent mean ± standard deviation across runs and the gray curve represents performance of the unpruned network.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following pruning methods resulted in the best performance for the LunarLander-v2 game?",
    "answer": "Winning ticket, no lr",
    "rationale": "The figure shows that the \"Winning ticket, no lr\" line is the highest for the LunarLander-v2 game, indicating that this pruning method resulted in the highest ticket reward.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02768v3",
    "pdf_url": null
  },
  {
    "instance_id": "02df8b8c26b347d3a0b5eddbe1d1256d",
    "figure_id": "2302.14719v1-Figure5-1",
    "image_file": "2302.14719v1-Figure5-1.png",
    "caption": " F1 performance of different models for training epochs, aiming to evaluate the quality of pseudo labels.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in terms of F1 score?",
    "answer": "SCD(μ=1.0)",
    "rationale": "The plot shows the F1 score for each model over the number of epochs. The SCD(μ=1.0) model has the highest F1 score for both training scenarios (ID->R and S->D).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.14719v1",
    "pdf_url": null
  },
  {
    "instance_id": "555995ca48e94215ba1135e44b348a71",
    "figure_id": "1906.02547v4-Figure3-1",
    "image_file": "1906.02547v4-Figure3-1.png",
    "caption": " MSE comparison with respect to the number of training samples for the linear dynamics dataset.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method performs best when the number of training samples is low? ",
    "answer": " The Kalman Smoother with GNN messages.",
    "rationale": " The plot shows the MSE of different methods as a function of the number of training samples. The Kalman Smoother with GNN messages has the lowest MSE for small numbers of training samples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02547v4",
    "pdf_url": null
  },
  {
    "instance_id": "df61319d36c240e0a443e512845fe101",
    "figure_id": "2302.06586v3-Figure2-1",
    "image_file": "2302.06586v3-Figure2-1.png",
    "caption": " One Stitchable Neural Network vs. 200 models in Timm model zoo [56]. It shows an example of SN-Net by stitching ImageNet-22K pretrained Swin-Ti/S/B. Compared to each individual network, SN-Net is able to instantly switch network topology at runtime and covers a wide range of computing resource budgets. Larger and darker dots indicate a larger model with more parameters and higher complexity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model in the figure has the highest ImageNet Top-1 accuracy?",
    "answer": "Swin-B",
    "rationale": "The figure shows a scatter plot of ImageNet Top-1 accuracy vs. FLOPs for different models. The model with the highest ImageNet Top-1 accuracy is the one that is furthest to the right on the plot, which is Swin-B*.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.06586v3",
    "pdf_url": null
  },
  {
    "instance_id": "3db79bc16bb44856a05c07c773f805b0",
    "figure_id": "2005.04304v1-Figure4-1",
    "image_file": "2005.04304v1-Figure4-1.png",
    "caption": " The distributions of different temporal dimensions in the collected data.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which temporal dimension is most represented in the data?",
    "answer": "Typical Time.",
    "rationale": "The pie chart shows that Typical Time makes up 52% of the data, which is the largest proportion of any of the temporal dimensions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.04304v1",
    "pdf_url": null
  },
  {
    "instance_id": "e67ca90fb2b544eb8e85545f2e523a58",
    "figure_id": "2003.11818v1-Figure3-1",
    "image_file": "2003.11818v1-Figure3-1.png",
    "caption": " Influence of replacing a certain operation in different parts of the detector on COCO minival. FPN (4clfc head) is taken as a base detector and the input image is of size 320×320. Taking the left figure as an example, each time one layer from backbone is randomly chosen to be replaced by an operation candidate. For one operation candidate, such random process is repeated for 6 times.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which operation candidate performs the best in the head part of the detector?",
    "answer": "lr_k3_d1",
    "rationale": "The boxplot for lr_k3_d1 in the head part of the detector has the highest median value, which indicates that it has the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.11818v1",
    "pdf_url": null
  },
  {
    "instance_id": "6374d4e3e2da424cb23f702b2da5fffe",
    "figure_id": "2307.03110v1-Figure4-1",
    "image_file": "2307.03110v1-Figure4-1.png",
    "caption": " Error EDF captures the quality of a search space. Our algorithm maintains a concentration of better networks across the board.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest probability of finding networks with lower error rates on the NASBench301 benchmark?",
    "answer": "Our algorithm.",
    "rationale": "The figure shows the cumulative probability of finding networks with different error rates for different algorithms on four different benchmarks. On the NASBench301 benchmark, the curve for our algorithm is the steepest, which means that it has the highest probability of finding networks with lower error rates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.03110v1",
    "pdf_url": null
  },
  {
    "instance_id": "7d62d7e64c774c748cfef8ee3d2a98d5",
    "figure_id": "2101.10143v2-Figure8-1",
    "image_file": "2101.10143v2-Figure8-1.png",
    "caption": " ImageNet validation accuracy for the baseline ResNet and VGG models and their windowed counterparts, where all kernels are replaced with 7× 7 Hamming windowed kernels.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest Top-5 ImageNet validation accuracy?",
    "answer": "VGG-16 with Hamming windowed kernels.",
    "rationale": "The figure shows the ImageNet validation accuracy for different models, with and without Hamming windowed kernels. The VGG-16 model with Hamming windowed kernels has the highest Top-5 accuracy, as shown by the red dashed line in the bottom plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.10143v2",
    "pdf_url": null
  },
  {
    "instance_id": "ff718ceb111f42fdb19cced8f71d141a",
    "figure_id": "2310.18593v1-Figure3-1",
    "image_file": "2310.18593v1-Figure3-1.png",
    "caption": " CelebA, Additional results (m = 5).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three attributes is the model most accurate at predicting?",
    "answer": "\"Eyeglasses\"",
    "rationale": "The figure shows that the model is able to accurately predict the presence of eyeglasses in all of the test images. However, the model is less accurate at predicting the other two attributes, as there are several images in which the model incorrectly predicts the presence of a slightly open mouth or a goatee.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.18593v1",
    "pdf_url": null
  },
  {
    "instance_id": "344e88512f3a48269fa5b48936433f45",
    "figure_id": "2110.04456v1-Figure5-1",
    "image_file": "2110.04456v1-Figure5-1.png",
    "caption": " Attainable PSNR comparison between the proposed method and baselines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between PSNR and SNR?",
    "answer": "The figure shows that PSNR increases with SNR.",
    "rationale": "The figure shows a positive correlation between PSNR and SNR. This means that as SNR increases, PSNR also increases. This is expected because PSNR is a measure of image quality, and higher SNR indicates a higher quality image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.04456v1",
    "pdf_url": null
  },
  {
    "instance_id": "bd56cdbb6ca346e4b8f7ac3fc399dd91",
    "figure_id": "2101.05930v2-Figure10-1",
    "image_file": "2101.05930v2-Figure10-1.png",
    "caption": " Erasing all-target BadNets attack.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attack method is more effective at reducing the success rate of the all-targets attack?",
    "answer": "NAD: after erasing.",
    "rationale": "The figure shows that the attack success rate for the \"NAD: after erasing\" method is consistently lower than the baseline \"all-targets attack\" method for all categories. This indicates that the NAD: after erasing method is more effective at reducing the success rate of the attack.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.05930v2",
    "pdf_url": null
  },
  {
    "instance_id": "a66ac15ede774c8cbd1adf7801d6c3df",
    "figure_id": "1810.00143v4-Figure5-1",
    "image_file": "1810.00143v4-Figure5-1.png",
    "caption": " DenseNet on Cifar-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performs best in terms of test accuracy and training loss?",
    "answer": "Adam.",
    "rationale": "The plot on the right shows that the test accuracy of Adam is the highest among the three optimizers. The plot on the left shows that the training loss of Adam is the lowest among the three optimizers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.00143v4",
    "pdf_url": null
  },
  {
    "instance_id": "991a1d15d1c44996a3325211e6fd8811",
    "figure_id": "1908.10546v1-Figure9-1",
    "image_file": "1908.10546v1-Figure9-1.png",
    "caption": " Signing hands detected by the iterative attention detector vs. the off-the-shelf signing hand detector [35], taken from the ChicagoFSWild dev set. In each example, the upper row is from off-the-shelf detector and the lower row is from iterative attention. Signing hands are successfully detected by iterative attention in all cases. Errors made by the off-the-shelf detector: In (1) and (2), bounding boxes are switched between signing and non-signing hand; in (3), the detected signing hand is incomplete; in (4), the non-signing hand is mis-detected as the signing hand. Note that sequence-level smoothing has already been incorporated in the off-the-shelf detector.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which example(s) does the off-the-shelf detector make a mistake by switching bounding boxes between the signing and non-signing hand?",
    "answer": "Examples (1) and (2).",
    "rationale": "The caption states that in examples (1) and (2), the bounding boxes are switched between the signing and non-signing hand. This can be seen in the figure, where the bounding box in the upper row of each example is around the non-signing hand, while the bounding box in the lower row is around the signing hand.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.10546v1",
    "pdf_url": null
  },
  {
    "instance_id": "e96103e200eb43f99ff396fdd52b08aa",
    "figure_id": "1910.02720v2-Figure6-1",
    "image_file": "1910.02720v2-Figure6-1.png",
    "caption": " Energy distributions of different classes of patterns under an Omniglot model. Memories are the patterns written into memory, non-memories are other randomly sampled images and distorted memories are the written patterns distorted as during the retrieval. CIFAR images were produced by binarizing the original RGB images and serve as out-of-distribution samples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of pattern has the lowest energy distribution?",
    "answer": "CIFAR images",
    "rationale": "The histogram shows that the CIFAR images have the lowest energy distribution, as their bars are concentrated on the left side of the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.02720v2",
    "pdf_url": null
  },
  {
    "instance_id": "5688d84fa0e64bf1bd7de5adc2eabd93",
    "figure_id": "2305.01783v1-Figure3-1",
    "image_file": "2305.01783v1-Figure3-1.png",
    "caption": " Trends in allocative bias in using satellite-based poverty predictions to allocate aid. Panel A: Over-allocation of aid to rural areas vs. mean signed error in poverty prediction in rural areas. Panel B: Over-allocation of aid to rural areas when vs. mean rank error in prediction in rural areas. Filled in markers show biases in satellite-based predictions; faded markers show the noised wealth baseline.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which country has the highest over-allocation of aid to rural areas based on signed error?",
    "answer": "Honduras",
    "rationale": "Panel A of the figure shows the over-allocation of aid to rural areas vs. the mean signed error in poverty prediction in rural areas. The data point for Honduras is the highest on the y-axis, indicating that it has the highest over-allocation of aid to rural areas.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.01783v1",
    "pdf_url": null
  },
  {
    "instance_id": "10f7fbac3e814f4b89e808cdcf0450a9",
    "figure_id": "2106.04302v1-Figure1-1",
    "image_file": "2106.04302v1-Figure1-1.png",
    "caption": " Effect of corpus size on the word-embedding quality for ASE best task independent layer and X2STATICpara : In the legend, parent model is indicated in subscript.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on SIMLEX-999 when using the full wikipedia dataset?",
    "answer": "GPT22STATICpara",
    "rationale": "The figure shows the performance of different models on SIMLEX-999 and RW-2034 datasets, with the x-axis representing the fraction of the full wikipedia dataset used and the y-axis representing Spearman's p. On the SIMLEX-999 plot, when the fraction of the wikipedia dataset used is 1, the orange line representing GPT22STATICpara has the highest Spearman's p value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04302v1",
    "pdf_url": null
  },
  {
    "instance_id": "2db109a9a8894c3f87c40065bdae4019",
    "figure_id": "2202.00914v2-Figure21-1",
    "image_file": "2202.00914v2-Figure21-1.png",
    "caption": " 8 discrete skills discovered by LSD for HalfCheetah. Videos are available on our project page.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many distinct behaviors does the LSD algorithm discover for HalfCheetah?",
    "answer": "8.",
    "rationale": "The figure shows 8 different skills, each representing a distinct behavior.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.00914v2",
    "pdf_url": null
  },
  {
    "instance_id": "0f64a9b4e36c4881968e3d7b0af5788d",
    "figure_id": "2106.02993v1-Figure9-1",
    "image_file": "2106.02993v1-Figure9-1.png",
    "caption": " Comparison of PID-GAN and Adaptive PINN-Drop in terms of absolute error and variance on Darcy’s equation.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest variance for k(x1,x2)?",
    "answer": "APINN-Drop.",
    "rationale": "The variance plots for both models show that APINN-Drop has higher variance than PID-GAN, particularly in the region where x1 is between 4 and 6 and x2 is between 4 and 8.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02993v1",
    "pdf_url": null
  },
  {
    "instance_id": "c15d20794303478793dc2f8818f2d27d",
    "figure_id": "2306.00752v3-Figure9-1",
    "image_file": "2306.00752v3-Figure9-1.png",
    "caption": " Relative spectral difference (in %) between between covariance estimators on NASDAQ stock returns over 2021 and 2022. Here, DDCII fails due to out-of-memory errors.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two methods produced the most similar covariance estimates for NASDAQ stock returns in 2021 and 2022?",
    "answer": "DDC MV 99 and DDC MV 95.",
    "rationale": "The figure shows the relative spectral difference between different covariance estimators. The smaller the difference, the more similar the estimates are. DDC MV 99 and DDC MV 95 have the smallest difference (3.2%) compared to other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.00752v3",
    "pdf_url": null
  },
  {
    "instance_id": "042e217c410943ebb3f452110c86e972",
    "figure_id": "2003.12789v1-Figure3-1",
    "image_file": "2003.12789v1-Figure3-1.png",
    "caption": " The visualization of M-R in different data spaces. If M − R is applied other than raw data space, undesirable residuals will appear. For RGB data, we get 3 types of M-R: (1) “ISP M-R” means do M − R on images after ISP. (2) “Gamma M-R” means to use M2.2 − R2.2 to simulate gamma decompression for M and R, which is a common way used in previous methods. (3) “Raw M-R”: do M −R on raw data. For the gray-scale polarization data, we use gamma correction to simulate the ISP, compared with directly on raw.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of M-R results in the least amount of undesirable residuals?",
    "answer": "Raw M-R.",
    "rationale": "The figure shows that when M-R is applied to raw data, there are fewer undesirable residuals than when it is applied to other types of data, such as RGB or ISP data. This is because raw data is not subject to the same processing steps as other types of data, which can introduce artifacts that can interfere with the M-R process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.12789v1",
    "pdf_url": null
  },
  {
    "instance_id": "396c703b012b4ff2993a8d79db328c80",
    "figure_id": "1904.12785v2-Figure8-1",
    "image_file": "1904.12785v2-Figure8-1.png",
    "caption": " Quantitative evaluation of our method and prior work, we estimate the Pareto frontier of the methods evaluated by linearly interpolation (dashed line)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest style score for a given content score?",
    "answer": "Our method.",
    "rationale": "The figure shows that our method achieves the highest style score for a given content score across all three settings.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.12785v2",
    "pdf_url": null
  },
  {
    "instance_id": "4bfc71c5c10445b2b22881c351689acb",
    "figure_id": "2105.04027v2-Figure6-1",
    "image_file": "2105.04027v2-Figure6-1.png",
    "caption": " Test-case (b): Noisy Common Utilities, σ = 0.1, we report the relative difference in social welfare, the Jain index, and the Gini coefficient, for increasing number of resources ([2, 1024], x-axis in log scale), and N = R. For each problem instance, we trained ALMA-Learning for 8192 time-steps.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the highest social welfare in this test case?",
    "answer": "The Hungarian algorithm.",
    "rationale": "The relative difference in social welfare is shown in subfigure (a). The Hungarian algorithm has the smallest relative difference, which means it achieves the highest social welfare.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.04027v2",
    "pdf_url": null
  },
  {
    "instance_id": "2b7087db09bb4464911f4fea79fae8d8",
    "figure_id": "2110.14038v4-FigureD.1-1",
    "image_file": "2110.14038v4-FigureD.1-1.png",
    "caption": "Figure D.1: Empirical error for a point mass perturbation. We reproduce Figure 2 in [18] and add our Soft Median.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which estimator has the highest relative bias when the fraction of outliers is 0.5?",
    "answer": "The sample mean.",
    "rationale": "The relative bias is shown in Figure D.1(b). The red curve, which represents the sample mean, is the highest when the fraction of outliers is 0.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14038v4",
    "pdf_url": null
  },
  {
    "instance_id": "881e9407148e40ba91188f25ac44c432",
    "figure_id": "2307.01972v1-Figure4-1",
    "image_file": "2307.01972v1-Figure4-1.png",
    "caption": " Human assessment of schema quality and interpretability from different aspects. Results for DoubleGAE are shown in red and our approach INCSCHEMA in blue. Double-GAE does not produce event descriptions thus we omit the description helpfulness question.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which schema quality aspect did participants rate the highest for both INCSCHEMA and DoubleGAE?",
    "answer": "Readability",
    "rationale": "The figure shows that the Likert scale scores for Readability are the highest for both INCSCHEMA and DoubleGAE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.01972v1",
    "pdf_url": null
  },
  {
    "instance_id": "cb843a3119cf4002b81a0d5d14a652b5",
    "figure_id": "2110.13100v1-Figure12-1",
    "image_file": "2110.13100v1-Figure12-1.png",
    "caption": " Most similar (left) and dissimilar (right) architectures (in terms of the `2 distance) in the ID-TEST set based on the graph embeddings obtained by the MLP (top row), GHN-1 (middle row) and GHN-2 (bottom row) trained on CIFAR-10.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model seems to have learned the most about the global structure of the architectures?",
    "answer": "GHN-2",
    "rationale": "The figure shows the most similar and dissimilar architectures according to the graph embeddings obtained by the three models. GHN-2 produces embeddings that are more similar for similar architectures and more dissimilar for dissimilar architectures, indicating that it has learned more about the global structure of the architectures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.13100v1",
    "pdf_url": null
  },
  {
    "instance_id": "6b26a1a44c0b4a0d9002348bb661934a",
    "figure_id": "1905.13736v4-Figure2-1",
    "image_file": "1905.13736v4-Figure2-1.png",
    "caption": " SVHN test accuracy for robust training without the extra data, with unlabeled extra (self-training), and with the labeled extra data. Left: Adversarial training and accuracies under `∞ attack with ε = 4/255. Right: Stability training and certified `2 accuracies as a function of perturbation radius. Most of the gains from extra data comes from the unlabeled inputs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest certified accuracy for an L2 radius of 0.3?",
    "answer": "RSTstab (73K+531K)",
    "rationale": "The plot on the right shows the certified accuracy for each model as a function of the L2 radius. At an L2 radius of 0.3, the RSTstab (73K+531K) model has the highest certified accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.13736v4",
    "pdf_url": null
  },
  {
    "instance_id": "d8d0277019f440e9a370f7d646cf0bc4",
    "figure_id": "1812.00420v2-Figure4-1",
    "image_file": "1812.00420v2-Figure4-1.png",
    "caption": " Evolution of LCA during the first ten mini-batches.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method consistently achieves the highest LCA across both datasets?",
    "answer": "A-GEM",
    "rationale": "In both plots, the red line representing A-GEM consistently sits above the other lines, indicating that it achieves the highest LCA across all batches and for both datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.00420v2",
    "pdf_url": null
  },
  {
    "instance_id": "a1264e562aaa4867825b145f2881c649",
    "figure_id": "2202.09514v2-Figure7-1",
    "image_file": "2202.09514v2-Figure7-1.png",
    "caption": " RRL-Stack formulation with different learning algorithms",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which learning algorithm performs the best with an action delay step of 0?",
    "answer": "RRL-Stack + GDA",
    "rationale": "The figure shows the episodic reward for different learning algorithms and action delay steps. At an action delay step of 0, RRL-Stack + GDA has the highest episodic reward.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.09514v2",
    "pdf_url": null
  },
  {
    "instance_id": "bdaa3b16b04b453bbe263d6a53978ab5",
    "figure_id": "2209.06640v2-Figure23-1",
    "image_file": "2209.06640v2-Figure23-1.png",
    "caption": " Caltech101 25-shot.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best?",
    "answer": "M4",
    "rationale": "The error rate of M4 is the lowest among all models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.06640v2",
    "pdf_url": null
  },
  {
    "instance_id": "c7513455209249a088bc0d97349b2667",
    "figure_id": "1910.04861v1-Figure4-1",
    "image_file": "1910.04861v1-Figure4-1.png",
    "caption": " Training loss and testing accuracy curves.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieved the highest testing accuracy?",
    "answer": "PEHAD",
    "rationale": "The testing accuracy curves in Figure (b) show that PEHAD achieved the highest testing accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.04861v1",
    "pdf_url": null
  },
  {
    "instance_id": "e8443bb046b44cc6ad582682d16810dd",
    "figure_id": "2003.12697v3-Figure14-1",
    "image_file": "2003.12697v3-Figure14-1.png",
    "caption": " Qualitative comparison of our model with several label-to-image methods on the ADE20K dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models produced the most realistic images?",
    "answer": "GroupDNet",
    "rationale": "The figure shows that GroupDNet produced images that are most similar to the ground truth images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.12697v3",
    "pdf_url": null
  },
  {
    "instance_id": "12248c8c0a364acdbae7cdbaa431c02d",
    "figure_id": "1906.02830v1-Figure5-1",
    "image_file": "1906.02830v1-Figure5-1.png",
    "caption": " Excesss variance of the private trimmed mean with smooth sensitivity for N(0, 1) data. Here n = 5001 and results are averaged over 106 repetitions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which differentially private mechanism has the lowest excess variance for all values of m?",
    "answer": "The trim non-priv mechanism.",
    "rationale": "The trim non-priv mechanism is the purple dotted line in the figure, and it is always below the other lines, which means it has the lowest excess variance for all values of m.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02830v1",
    "pdf_url": null
  },
  {
    "instance_id": "982857fbb71743c7a3a8caa7cbd413d2",
    "figure_id": "2107.00793v3-Figure13-1",
    "image_file": "2107.00793v3-Figure13-1.png",
    "caption": " Causal diagram G ofM∗ from Example 7",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between X and Y?",
    "answer": "X causes Y.",
    "rationale": "The figure shows a causal diagram, which is a type of graph that represents causal relationships between variables. In this diagram, there is an arrow pointing from X to Y, which indicates that X causes Y.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.00793v3",
    "pdf_url": null
  },
  {
    "instance_id": "0f7117d7325c4777af41dd375d757631",
    "figure_id": "2001.08779v1-Figure6-1",
    "image_file": "2001.08779v1-Figure6-1.png",
    "caption": " Sunburst plot of generated questions for MC-BMN on VQG-COCO dataset,VQG-Bing dataset, VQG-Flickr dataset are shown in Fig-a, Fig-b, Fig-c respectably : The ith ring captures the frequency distribution over words for the ith word of the generated question. While some words have high frequency, the outer rings illustrate a fine blend of words.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the most frequent word in the generated questions for the VQG-COCO dataset?",
    "answer": " \"THE\"",
    "rationale": " The figure shows that the word \"THE\" appears more frequently than any other word in the sunburst plot for the VQG-COCO dataset. This can be seen by observing the size of the wedge corresponding to \"THE\" in the innermost ring of the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.08779v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee942e408c614953a334c512721a4ac3",
    "figure_id": "2011.05604v1-Figure3-1",
    "image_file": "2011.05604v1-Figure3-1.png",
    "caption": " The average differences in F1-scores compared with Vanilla CRF with different training data sizes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods has the largest F1-score difference compared to Vanilla CRF at 100% training data size for chunking?",
    "answer": "D-Qualinear.",
    "rationale": "The figure shows that D-Qualinear has the highest F1-score difference compared to Vanilla CRF at 100% training data size for chunking.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.05604v1",
    "pdf_url": null
  },
  {
    "instance_id": "32ae65ff3a4c4d2db554c82131c7569d",
    "figure_id": "2004.14373v3-Figure4-1",
    "image_file": "2004.14373v3-Figure4-1.png",
    "caption": " TOTTO example with rare topic.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "On what day does the 11417 Pune - Nagpur Humsafar Express arrive in Nagpur Junction?",
    "answer": "Friday",
    "rationale": "The table shows that the 11417 Pune - Nagpur Humsafar Express departs from Pune Junction at 22:00 PM on Thursday and arrives in Nagpur Junction at 13:30 PM on Friday.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14373v3",
    "pdf_url": null
  },
  {
    "instance_id": "476e8a6f6bc4471ba6e7740287e72f31",
    "figure_id": "2210.14215v1-Figure9-1",
    "image_file": "2210.14215v1-Figure9-1.png",
    "caption": " Asymptotic performance of the A3C (Mnih et al., 2016) and a Q-λ variant of the DQN (Mnih et al., 2013) RL algorithms used to produce learning histories for the Dark and Watermaze environments. These curves show the learning histories AD is trained on. The source algorithms plotted in Fig. 4 are the same as in these plots.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which environment appears to be the most difficult for the A3C and Q-λ algorithms to learn?",
    "answer": " The Dark Key-to-Door environment.",
    "rationale": " The Dark Key-to-Door environment has the lowest return of all the environments, indicating that the agents are performing the worst in this environment. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.14215v1",
    "pdf_url": null
  },
  {
    "instance_id": "a1635a53259e43b8a8f735f597f1513f",
    "figure_id": "1904.11045v2-Figure11-1",
    "image_file": "1904.11045v2-Figure11-1.png",
    "caption": " Cross-view image retrieval examples on the OP Dataset. Ground-truth aerial images are shown in green boxes. The number below each aerial image is its distance in meters from the query image. The first three rows present the images from Orlando and the next three rows of images are from Pittsburgh.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which city is represented in the first three rows of images?",
    "answer": "Orlando",
    "rationale": "The caption states that \"The first three rows present the images from Orlando.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.11045v2",
    "pdf_url": null
  },
  {
    "instance_id": "867dd5d294d54dbb93b8bf78c1f0dadc",
    "figure_id": "2308.13133v1-Figure3-1",
    "image_file": "2308.13133v1-Figure3-1.png",
    "caption": " Visualization of occlusion masks during accumulation. White regions denote occluded area.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which frame has the most occlusion?",
    "answer": "Frame 7",
    "rationale": "The white regions in the occlusion masks denote the occluded area. Frame 7 has the largest white region, indicating that it has the most occlusion.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.13133v1",
    "pdf_url": null
  },
  {
    "instance_id": "f4bf237688bd479d9bee080cdccfe407",
    "figure_id": "2203.08414v1-Figure6-1",
    "image_file": "2203.08414v1-Figure6-1.png",
    "caption": " Confusion matrix of STEGO cluster probe predictions on CocoStuff. Classes after the “vehicle” class are “stuff” and classes before are “things”. Rows are normalized to sum to 1.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class is most likely to be misclassified as \"person\"?",
    "answer": "\"furniture\"",
    "rationale": "The confusion matrix shows that the \"person\" class has the highest off-diagonal value in the \"furniture\" row, indicating that a significant number of \"furniture\" instances were incorrectly classified as \"person.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.08414v1",
    "pdf_url": null
  },
  {
    "instance_id": "268a9f4cbdb34c6291e0a59edd319d02",
    "figure_id": "2010.12537v4-Figure5-1",
    "image_file": "2010.12537v4-Figure5-1.png",
    "caption": " An intuitive example of pre-training objectives.",
    "figure_type": "Table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which cancer type has the highest incidence in males and females worldwide?",
    "answer": "Non-melanoma skin cancer",
    "rationale": "The table shows that non-melanoma skin cancer has the highest incidence in both males (637,733) and females (404,323).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12537v4",
    "pdf_url": null
  },
  {
    "instance_id": "c5658639c32d4b9286cdc3c9f6156e62",
    "figure_id": "2103.09716v5-Figure11-1",
    "image_file": "2103.09716v5-Figure11-1.png",
    "caption": " Comparisons between models in model set R. Compare the feature entropy (1) and selective rate (2) of units at different layers between models in the corresponding model set on the exampled class. (3) shows the scatter plot between feature entropy and selective rate of units at the last residual block on the 100 sampled classes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model in model set R has the highest selective rate at the last residual block?",
    "answer": "Model RA",
    "rationale": "The selective rate of units at the last residual block is shown in Figure (2). Model RA has the highest selective rate at the last residual block.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.09716v5",
    "pdf_url": null
  },
  {
    "instance_id": "651a9a7e05a84ae78d9b928d6f0fa6a0",
    "figure_id": "1911.04047v2-Figure2-1",
    "image_file": "1911.04047v2-Figure2-1.png",
    "caption": " Curves of h(s) with different µ’s.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the value of µ affect the curve of h(s)?",
    "answer": "The curve of h(s) increases as µ increases.",
    "rationale": "The figure shows that the curves of h(s) are higher for larger values of µ. This indicates that the value of h(s) increases as µ increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.04047v2",
    "pdf_url": null
  },
  {
    "instance_id": "050364d117c44793bf2f8fb0a63e9c9b",
    "figure_id": "2109.11058v1-Figure7-1",
    "image_file": "2109.11058v1-Figure7-1.png",
    "caption": " Accuracy on individual test suites used in our experiments.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the Garden Path Object test suite when no modifier is used?",
    "answer": "Transformer-Xinhua",
    "rationale": "The figure shows that the Transformer-Xinhua model has the highest accuracy on the Garden Path Object test suite when no modifier is used.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.11058v1",
    "pdf_url": null
  },
  {
    "instance_id": "1e331c247dcd434eb8feac1d64170d55",
    "figure_id": "2106.03765v2-Figure16-1",
    "image_file": "2106.03765v2-Figure16-1.png",
    "caption": " In-sample RMSE of TARNet by standard deviation of factual outcomes in training sample, histogram of in-sample RMSE across runs and histogram for normalized in-sample RMSE across runs for Setup C (top) and Setup D (bottom)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which setup did TARNet perform better?",
    "answer": "Setup C.",
    "rationale": "The plots in the top row show the results for Setup C, and the plots in the bottom row show the results for Setup D. The histograms of in-sample RMSE and normalized in-sample RMSE are both more concentrated around lower values in Setup C than in Setup D. This indicates that TARNet performed better in Setup C.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03765v2",
    "pdf_url": null
  },
  {
    "instance_id": "c9e64c74dcf743908c5605afd715c915",
    "figure_id": "2303.11828v1-Figure4-1",
    "image_file": "2303.11828v1-Figure4-1.png",
    "caption": " Qualitative comparisons on three challenging samples in the BSDS500 test set.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the different edge detection algorithms used in this figure?",
    "answer": "RCF, BDCN, EDTER, and UAED.",
    "rationale": "The figure shows the results of different edge detection algorithms on three challenging samples in the BSDS500 test set. The algorithms are labeled in the figure as (c) RCF [32], (d) BDCN [16], (e) EDTER [41], and (f) UAED (Ours).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.11828v1",
    "pdf_url": null
  },
  {
    "instance_id": "d07634ee061f4bbe8c4d122c1f0d088e",
    "figure_id": "2302.04215v1-Figure4-1",
    "image_file": "2302.04215v1-Figure4-1.png",
    "caption": " Pitch contour for the utterance: “How much variation is there?” from two models within the same speaker.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is better at capturing the pitch variations in the utterance “How much variation is there?”",
    "answer": "MQTTTS.",
    "rationale": "The figure shows that the pitch contour generated by MQTTTS has more variations than the one generated by VITS. This can be seen in the sharper peaks and valleys in the MQTTTS pitch contour, as well as the wider range of pitch values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.04215v1",
    "pdf_url": null
  },
  {
    "instance_id": "36c8e38305aa435aae91b517351370d1",
    "figure_id": "2303.05121v1-Figure6-1",
    "image_file": "2303.05121v1-Figure6-1.png",
    "caption": " Rate-distortion evaluation on the Kodak data set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which compression method achieves the highest RGB-PSNR at a bitrate of 1 bpp?",
    "answer": "iWave++ + YCbCr",
    "rationale": "The plot shows that the iWave++ + YCbCr curve is the highest at a bitrate of 1 bpp.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.05121v1",
    "pdf_url": null
  },
  {
    "instance_id": "1f4a9ff857104bdb9371c443ece16e90",
    "figure_id": "2205.14083v3-Figure2-1",
    "image_file": "2205.14083v3-Figure2-1.png",
    "caption": " Visualizations of loss landscapes [2, 17] of the PyramidNet-110 model on the CIFAR-100 dataset trained with SGD, SAM, our proposed SAF, and MESA. SAF encourages the networks to converge to a flat minimum with zero additional computational overhead.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer encourages the network to converge to a flat minimum with zero additional computational overhead?",
    "answer": "SAF.",
    "rationale": "The figure shows that the loss landscape for SAF is flatter than the loss landscapes for the other optimizers. This indicates that SAF is more likely to converge to a flat minimum.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.14083v3",
    "pdf_url": null
  },
  {
    "instance_id": "0e9dd3c4ddfb498aade11a4497f5b84b",
    "figure_id": "2006.10800v2-Figure5-1",
    "image_file": "2006.10800v2-Figure5-1.png",
    "caption": " Median test win % with 2 exploration schedules on 2 super-hard SMAC maps. Finally, we present results on 2 more super hard SMAC maps in order to show the limitations of our method in Figure 5. In 3s5z_vs_3s6z we observe that the extra exploration is not helpful for any method. Since QMIX is almost able to solve the task, this indicates that both exploration and the restricted function class of QMIX are not limiting factors in this scenario. On corridor we see that only QMIX with an extended exploration schedule manages non-zero performance, showing the importance of sufficient exploration on this map. The poor performance of Weighted QMIX shows that the extra complexity of our method (notably learning Q̂∗) can sometimes harm performance, indicating that closer attention must be paid to the architecture and weighting functions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the \"corridor\" map?",
    "answer": "QMIX-50k ε [5]",
    "rationale": "The figure shows the median test win % for different methods on the \"corridor\" map. QMIX-50k ε [5] is the only method that achieves a median test win % of greater than 0%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.10800v2",
    "pdf_url": null
  },
  {
    "instance_id": "61b708de037d4ef7affde931db63b0e9",
    "figure_id": "1811.12641v5-Figure5-1",
    "image_file": "1811.12641v5-Figure5-1.png",
    "caption": " The qualitative comparisons between DAG and UEA versus Faster-RCNN and SSD300. Please see the texts for details.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on clean images, SSD300 or Faster-RCNN?",
    "answer": "Both models perform similarly on clean images.",
    "rationale": "The figure shows that both SSD300 and Faster-RCNN correctly identify the objects in the clean images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.12641v5",
    "pdf_url": null
  },
  {
    "instance_id": "1f07a938c5a443cfacfbacdb1bd58374",
    "figure_id": "2306.14898v3-Figure7-1",
    "image_file": "2306.14898v3-Figure7-1.png",
    "caption": " Top 30 most frequently occurring bash utilities out of the 66 in InterCode-Bash with their frequencies in log scale.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which bash utility is the most frequently used?",
    "answer": "`cd`",
    "rationale": "The figure shows the top 30 most frequently occurring bash utilities. The utility with the highest frequency is located at the top of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.14898v3",
    "pdf_url": null
  },
  {
    "instance_id": "453ae451bf584753a71b7e88f5b740d4",
    "figure_id": "2010.12621v1-Figure7-1",
    "image_file": "2010.12621v1-Figure7-1.png",
    "caption": " Intensity plots show the soft instruction pointer pt,n at each step of the IPA-GNN during full program execution for four randomly sampled programs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four programs uses a while loop?",
    "answer": "The third program uses a while loop.",
    "rationale": "The third program has the code `while 3 > v5:`, which indicates a while loop.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.12621v1",
    "pdf_url": null
  },
  {
    "instance_id": "825711b96d664fee913f7f7960852801",
    "figure_id": "1909.00025v2-Figure10-1",
    "image_file": "1909.00025v2-Figure10-1.png",
    "caption": " Multi-shot tieredImageNet results. Top: mean learning curves (test classification accuracy) on held-out meta-test tasks. Bottom: mean test classification performance on held-out meta-test tasks during meta-training. Training from scratch omitted as it is not meta-trained.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which meta-learning algorithm performed the best on the held-out meta-test tasks in terms of test accuracy?",
    "answer": "Warp-Leap",
    "rationale": "The figure shows the test accuracy of different meta-learning algorithms on the held-out meta-test tasks. The Warp-Leap algorithm consistently achieves the highest test accuracy across all inner-loop and meta-training steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.00025v2",
    "pdf_url": null
  },
  {
    "instance_id": "7539480e969d4136be92cdd09fc178d5",
    "figure_id": "2309.04800v1-Figure11-1",
    "image_file": "2309.04800v1-Figure11-1.png",
    "caption": " Qualitative Comparison on DeepFashion. We provide the corresponding real image as a reference for each input pose.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the methods shown in the figure is the most accurate at reconstructing the original image?",
    "answer": " Our method.",
    "rationale": " The figure shows that our method produces results that are closest to the original image. This is evident from the fact that the details of the clothing and the body are more accurately reconstructed in our results than in the results of the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.04800v1",
    "pdf_url": null
  },
  {
    "instance_id": "97363cc635ee4c16abeaf2262361cfa1",
    "figure_id": "2104.10317v1-Figure3-1",
    "image_file": "2104.10317v1-Figure3-1.png",
    "caption": " Group-level Automatic metrics on the whole test set of Home & Kitchen. The lower Pairwise BLEU, the more diverse the generated group. Solid markers are the results for the top 3 candidates in the original group, while hollow markers measures the remaining 3 after deduplication. Points located near top-right are preferred as they achieve a good tradeoff between the 2 metrics.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms generated the most diverse set of candidates, according to the figure?",
    "answer": "Deduplicated.",
    "rationale": "The figure shows that the Deduplicated algorithm has the lowest Pairwise BLEU score, which indicates that it generated the most diverse set of candidates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.10317v1",
    "pdf_url": null
  },
  {
    "instance_id": "606899c4557a4e4b8b545ed403f1d98b",
    "figure_id": "2305.10696v1-Figure2-1",
    "image_file": "2305.10696v1-Figure2-1.png",
    "caption": " Comparison of UnbiasedGBM with other baseline methods on four types of datasets. Each value corresponds the normalized test AUC of the best model (on the validation set) after a specific number of tuning iterations, averaged on all the datasets. The shaded area presents the variance of the scores.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms tested performed best on medium-scale datasets with only numerical features?",
    "answer": "CatBoost",
    "rationale": "The figure shows that CatBoost achieves the highest normalized test AUC on medium-scale datasets with only numerical features.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.10696v1",
    "pdf_url": null
  },
  {
    "instance_id": "f3d5b20e5d7d468b8395fb8e00a2ad7f",
    "figure_id": "2012.08072v1-Figure2-1",
    "image_file": "2012.08072v1-Figure2-1.png",
    "caption": " Target error analysis shows that HDMI (with three hypotheses) preserves more transferable source knowledge, as compared with using MI maximization alone (on A→D, Office-31).",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method preserves more transferable source knowledge, as shown by the target error analysis?",
    "answer": "HDMI",
    "rationale": "The target error analysis shows that HDMI (with three hypotheses) preserves more transferable source knowledge, as compared with using MI maximization alone.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.08072v1",
    "pdf_url": null
  },
  {
    "instance_id": "bd43ae50c9a846f99f79b717ee5684f6",
    "figure_id": "2202.00734v1-Figure2-1",
    "image_file": "2202.00734v1-Figure2-1.png",
    "caption": " Two Anchors explainers with different precision threshold parameters, and their performance statistics over an example record from the Adult dataset.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two anchors, π1 or π2, is more precise?",
    "answer": "Anchor π2 is more precise than π1.",
    "rationale": "The precision threshold parameter for π2 is 0.95, which is higher than the threshold of 0.5 for π1. A higher threshold indicates a more precise explanation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.00734v1",
    "pdf_url": null
  },
  {
    "instance_id": "c4ce5a2f74914d03a42a718c3b3d5ecf",
    "figure_id": "2109.14875v1-Figure1-1",
    "image_file": "2109.14875v1-Figure1-1.png",
    "caption": " Average RMSE for ideal case with no perturbation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which imputation method performed the best on the Abalone dataset?",
    "answer": "The NW method.",
    "rationale": "The NW method has the lowest average RMSE for the Abalone dataset. This can be seen in the bar graph on the left side of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.14875v1",
    "pdf_url": null
  },
  {
    "instance_id": "399b44e2bea9469da5b17018895b9641",
    "figure_id": "2005.00701v1-Figure3-1",
    "image_file": "2005.00701v1-Figure3-1.png",
    "caption": " Distribution of testing set based on PCIO.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which category of information about PCIO is most prevalent in the testing set?",
    "answer": "Treatment",
    "rationale": "The figure shows that the largest percentage of the testing set (61.6%) is related to the treatment of PCIO.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.00701v1",
    "pdf_url": null
  },
  {
    "instance_id": "29dee328bc6542d19b22f2919351f232",
    "figure_id": "2210.12531v1-Figure5-1",
    "image_file": "2210.12531v1-Figure5-1.png",
    "caption": " Emotion co-occurrences in COVIDET.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two emotions are most likely to co-occur in the COVIDET dataset?",
    "answer": "Fear and sadness.",
    "rationale": "The darkest blue square in the heatmap is located at the intersection of the \"fear\" and \"sadness\" rows and columns, indicating that these two emotions co-occur more frequently than any other pair of emotions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12531v1",
    "pdf_url": null
  },
  {
    "instance_id": "cf30b2170f1940b3bc41e633a4038a33",
    "figure_id": "2203.06801v1-Figure5-1",
    "image_file": "2203.06801v1-Figure5-1.png",
    "caption": " Collaboration with Other Optimizers",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does the MetaBalance technique consistently improve the performance of the optimizers?",
    "answer": "No.",
    "rationale": "The figure shows that MetaBalance improves the performance of Adagrad for N@10 and R@10, but it reduces the performance of RMSProp for N@10.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.06801v1",
    "pdf_url": null
  },
  {
    "instance_id": "3cde893e3f4d487c8ed9ea206b8cc561",
    "figure_id": "2109.12393v1-Figure8-1",
    "image_file": "2109.12393v1-Figure8-1.png",
    "caption": " Accuracy vs number of attractors with semantically unrelated attractors",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best when there are multiple entities?",
    "answer": "RobertaLarge",
    "rationale": "The figure shows that RobertaLarge has the highest accuracy for all numbers of attractors in the Multiple Entity plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.12393v1",
    "pdf_url": null
  },
  {
    "instance_id": "bc3cf156c73f4c70b6c0e84e78690314",
    "figure_id": "1905.09265v1-Figure8-1",
    "image_file": "1905.09265v1-Figure8-1.png",
    "caption": " Qualitative results on the Eigen test split. The boundary is more clear and accurate as we add flow pairs and the proposed 2-warp consistency during training.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following images shows the most accurate and clear boundary?",
    "answer": "The \"Full Model\" image.",
    "rationale": "The caption states that the boundary becomes more clear and accurate as we add flow pairs and the proposed 2-warp consistency during training. The \"Full Model\" image is the result of using both of these techniques, so it should have the most accurate and clear boundary.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.09265v1",
    "pdf_url": null
  },
  {
    "instance_id": "e378190dbf0848e4853f1f976413549b",
    "figure_id": "1911.01556v2-Figure2-1",
    "image_file": "1911.01556v2-Figure2-1.png",
    "caption": " Analysis for proposition of instances.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of product, clothing or cell phones, has a higher BLEU4 score when the proportion of reviews is 90%?",
    "answer": "Clothing.",
    "rationale": "The top plot in the figure shows the BLEU4 score for different proportions of reviews for clothing and cell phones. The blue line represents clothing, and the red line represents cell phones. At the 90% mark on the x-axis, the blue line is higher than the red line, indicating that clothing has a higher BLEU4 score than cell phones.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.01556v2",
    "pdf_url": null
  },
  {
    "instance_id": "85e32d75a05744f281686a3ab4a3602d",
    "figure_id": "2302.03825v3-Figure2-1",
    "image_file": "2302.03825v3-Figure2-1.png",
    "caption": " Results of different stochastic methods on the orthonormal fair classification networks task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stochastic method performs the best on the MNIST dataset?",
    "answer": "GNSDA",
    "rationale": "The figure shows the loss curves for different stochastic methods on the MNIST dataset. The GNSDA method has the lowest loss curve, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.03825v3",
    "pdf_url": null
  },
  {
    "instance_id": "f82f3f7f63b2488fb8da8adfa51f4771",
    "figure_id": "2005.13117v4-Figure5-1",
    "image_file": "2005.13117v4-Figure5-1.png",
    "caption": " Visualization of rectification with chromatic problems, unbalanced contrast (row 1-2), low brightness (row 3) and shadow (row 4),respectively. Samples here and in Fig. 6 are all from SVT, IC15 and CUTE80 test sets.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most effective at rectifying images with chromatic problems?",
    "answer": "GA-SPIN.",
    "rationale": "The figure shows that GA-SPIN produces the most accurate and realistic results for images with chromatic problems. The other methods, such as STN and SPIN, produce images that are either too blurry or have unnatural colors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.13117v4",
    "pdf_url": null
  },
  {
    "instance_id": "c4da61932ba44ae7b6dd8ba9717d6a21",
    "figure_id": "1902.01506v3-Figure2-1",
    "image_file": "1902.01506v3-Figure2-1.png",
    "caption": " ROC Curve for the weekly risk prediction task comparing the missed call baseline (blue), Random Forest (yellow) and LEAP (green). Numbers under the blue curve give thresholds used to calculate the baseline’s ROC curve.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed the best in the weekly risk prediction task?",
    "answer": "LEAP",
    "rationale": "The AUC (area under the curve) is a measure of how well a model performs. The higher the AUC, the better the model. In this case, LEAP has the highest AUC (0.775), which means it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.01506v3",
    "pdf_url": null
  },
  {
    "instance_id": "79ada27bf70a4da19863cc782823242f",
    "figure_id": "1912.10000v2-Figure5-1",
    "image_file": "1912.10000v2-Figure5-1.png",
    "caption": " Impact of η (eta) and k (embedding size) on the Brier score. We used TransE and the Self-Adversarial loss for all datasets. Best viewed in colors.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which calibration method and dataset combination resulted in the lowest Brier score?",
    "answer": "Isotonic scaling and YAGO39K.",
    "rationale": "The figure shows that the Brier score for the Isotonic scaling method is consistently lower than the other methods for all datasets. Additionally, the YAGO39K dataset has the lowest Brier score among all datasets for all calibration methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.10000v2",
    "pdf_url": null
  },
  {
    "instance_id": "b4fb5bb98040486a981922dfba8f7193",
    "figure_id": "1908.09888v2-Figure2-1",
    "image_file": "1908.09888v2-Figure2-1.png",
    "caption": " Average RMSE on (a) MIMIC-III, (b) CMS, (c) Synthetic datasets using 5 random initializations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the lowest RMSE on the MIMIC-III dataset?",
    "answer": "DPFfacT",
    "rationale": "The figure shows the RMSE of different algorithms on the MIMIC-III dataset. DPFfacT has the lowest RMSE of all the algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.09888v2",
    "pdf_url": null
  },
  {
    "instance_id": "a2db9bbe57c84f1e85c9de9b990f803c",
    "figure_id": "2009.08451v4-Figure3-1",
    "image_file": "2009.08451v4-Figure3-1.png",
    "caption": " ROC-AUC vs time on CICIDS-DoS dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest ROC-AUC score and the lowest running time?",
    "answer": "MStream-PCA",
    "rationale": "The figure shows that MStream-PCA has the highest ROC-AUC score (0.95) and the lowest running time (less than 10 seconds).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.08451v4",
    "pdf_url": null
  },
  {
    "instance_id": "975bae83ea7a46719c465eff30e3223a",
    "figure_id": "1904.05647v1-Figure4-1",
    "image_file": "1904.05647v1-Figure4-1.png",
    "caption": " Five functions defined to control the change of continuation parameter.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which function results in the fastest increase in the continuation parameter?",
    "answer": "The piecewise linear function.",
    "rationale": "The figure shows the continuation parameter as a function of training epochs for five different functions. The piecewise linear function has the steepest slope, which means that the continuation parameter increases the fastest with this function.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.05647v1",
    "pdf_url": null
  },
  {
    "instance_id": "e52f51e3ca634c5ebba896b1f5fbae4b",
    "figure_id": "2003.02920v2-Figure8-1",
    "image_file": "2003.02920v2-Figure8-1.png",
    "caption": " IoU results of the aneurysm part for each fold of networks. These methods are compared with their best performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network performs the best for each fold?",
    "answer": "PointNet performs the best for each fold.",
    "rationale": "The figure shows the IoU results for each fold of the networks. The box plot for PointNet is consistently higher than the other networks, indicating that it has the highest median IoU for each fold.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.02920v2",
    "pdf_url": null
  },
  {
    "instance_id": "fee51975e84c4796a586d9f5c66caa43",
    "figure_id": "1908.06382v2-Figure7-1",
    "image_file": "1908.06382v2-Figure7-1.png",
    "caption": " The results of user studies, comparing our method with SRGAN [22] and ESRGAN [35].",
    "figure_type": "\"plot\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in the user study?",
    "answer": "Our method performed the best in the user study.",
    "rationale": "The figure shows that our method received the highest percentage of votes (75.1%), followed by ESRGAN (24.9%) and SRGAN (33.8%).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.06382v2",
    "pdf_url": null
  },
  {
    "instance_id": "adc8299a28a3449689f02f5c2c3f0e8d",
    "figure_id": "1806.00880v3-Figure4-1",
    "image_file": "1806.00880v3-Figure4-1.png",
    "caption": " (a) Shows intra-class variation in MNIST. Bars show the mean square distance (MSD) within each class of the dataset. On average, DMGAN-PL outperforms WGAN-GP in capturing intra class variation, as measured by MSD, with larger significance on certain classes. (b) Shows the sample quality in MNIST experiment. (c) Shows sample quality in Face-Bed experiment. Notice how DMWGAN-PL outperforms other models due to fewer off real-manifold samples. We run each model 5 times with random initialization, and report average values with one standard deviation intervals in both figures. 10K samples are used for metric evaluations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model captures the most intra-class variation in the MNIST dataset?",
    "answer": "DMGAN-PL",
    "rationale": "Figure (a) shows the mean square distance (MSD) within each class of the MNIST dataset. The bars for DMGAN-PL are generally higher than those for the other models, indicating that it captures more intra-class variation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.00880v3",
    "pdf_url": null
  },
  {
    "instance_id": "4ccb902cfdd249dab8e48dd549cffb17",
    "figure_id": "1909.02421v3-Figure2-1",
    "image_file": "1909.02421v3-Figure2-1.png",
    "caption": " The topology and the equilibrium assignment of the lower bound instance in the proof of part (i) of Theorem 4.1. The big red square represents a clique whose nodes are occupied by red agents only.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the red circle and the blue circles?",
    "answer": "The red circle is the parent of the blue circles.",
    "rationale": "The figure shows a tree structure, with the red circle at the top and the blue circles below it. The lines connecting the circles represent parent-child relationships.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.02421v3",
    "pdf_url": null
  },
  {
    "instance_id": "429dc9257ca64a3495704a0f1464e91e",
    "figure_id": "1906.01569v1-Figure7-1",
    "image_file": "1906.01569v1-Figure7-1.png",
    "caption": " NER results for the 265 languages represented in Pan et al. (2017), FastText, and BPEmb (top), and the 101 languages constituting the intersection of these methods and BERT (bottom). Per-language F1 scores achieved by each method are sorted in descending order from left to right. The data points at rank 1 show the highest score among all languages achieved by the method in question, rank 2 the second-highest score etc.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed best on the NER task for the 265 languages represented in Pan et al. (2017), FastText, and BPEmb?",
    "answer": "MultiBPEmb+char",
    "rationale": "The figure shows the NER F1 scores for different methods, ranked from highest to lowest. The MultiBPEmb+char method has the highest F1 score for the majority of the languages, indicating that it performed best overall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.01569v1",
    "pdf_url": null
  },
  {
    "instance_id": "d570d7b670c749079d0ec3192c5b3ad6",
    "figure_id": "2210.14250v1-Figure3-1",
    "image_file": "2210.14250v1-Figure3-1.png",
    "caption": " The number of votes for HUM vs GTr, GPT-3 vs GTr, and HUM vs GPT-3 along with their corresponding raters’ confidence.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three systems (HUM, GPT-3, GT) had the highest percentage of confident votes when compared to the other two systems?",
    "answer": "HUM",
    "rationale": "The figure shows that when HUM was compared to GT, 85.7% of the votes were confident. When HUM was compared to GPT-3, 52.6% of the votes were confident. These are the highest percentages of confident votes for any of the three systems.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.14250v1",
    "pdf_url": null
  },
  {
    "instance_id": "e36c4cf8d0b348e2a1eb90b69593d707",
    "figure_id": "2303.08240v3-Figure13-1",
    "image_file": "2303.08240v3-Figure13-1.png",
    "caption": " Visualization of completion results on ShapeNet-PCN dataset with different algorithms. We see that our method can generate points that are closer to the underlying surfaces, preserving better shapes. Please enlarge the PDF for more details.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm produced the most accurate results for the airplane object?",
    "answer": "Our method.",
    "rationale": "The figure shows the results of different algorithms for completing a partially observed point cloud of an airplane. The \"Ground Truth\" column shows the true shape of the airplane, and the other columns show the results of different algorithms. Our method produced a point cloud that is closest to the ground truth, indicating that it is the most accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.08240v3",
    "pdf_url": null
  },
  {
    "instance_id": "71d8869342b44702b49301b91a7be5b5",
    "figure_id": "1809.09600v1-Figure4-1",
    "image_file": "1809.09600v1-Figure4-1.png",
    "caption": " Screenshot of our worker interface on Amazon Mechanical Turk.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which Miami Dolphins season had the greatest single-season turnaround in NFL history?",
    "answer": "2008",
    "rationale": "The passage states that the 2008 Miami Dolphins season \"completed the greatest single-season turnaround in NFL history,\" going from a 1-15 record in 2007 to an 11-5 record in 2008.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.09600v1",
    "pdf_url": null
  },
  {
    "instance_id": "9911a2c6155a4ac4857660b056c4ec1c",
    "figure_id": "1811.02483v1-Figure7-1",
    "image_file": "1811.02483v1-Figure7-1.png",
    "caption": " The learned patroller DQN strategy against a parameterized heuristic random walk poacher. Here, the darkness of the square in each cell indicates the animal density.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the patroller and the poacher?",
    "answer": "The patroller is trying to catch the poacher.",
    "rationale": "The figure shows the patroller (blue circle) moving towards the poacher (red circle) over time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.02483v1",
    "pdf_url": null
  },
  {
    "instance_id": "891cb33356eb41e08d58e4001f5fd852",
    "figure_id": "2203.00089v1-Figure16-1",
    "image_file": "2203.00089v1-Figure16-1.png",
    "caption": " ResNet32 on CIFAR-10, using RMSprop. The shaded regions show the min/max values over 4 random restarts.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which learning rate schedule achieved the lowest training loss?",
    "answer": "RMSprop-APO",
    "rationale": "The training loss plot shows that the RMSprop-APO learning rate schedule (green line) reaches the lowest training loss of all the learning rate schedules tested.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.00089v1",
    "pdf_url": null
  },
  {
    "instance_id": "ce96260b54054a16af1835f1cd55fde1",
    "figure_id": "1812.04605v4-Figure4-1",
    "image_file": "1812.04605v4-Figure4-1.png",
    "caption": " Visualization of predicted depth maps on NYU, ScanNet, and SUN3D. On ScanNet and SUN3D (marked with *) we show the results of the model trained only on NYU data.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the NYU dataset?",
    "answer": "Our method performs best on the NYU dataset.",
    "rationale": "The figure shows the predicted depth maps for three different methods (NYU, ScanNet, and SUN3D) on three different datasets (NYU, ScanNet, and SUN3D). The \"ours\" column shows the results of our method. On the NYU dataset, our method produces depth maps that are closest to the ground-truth depth maps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.04605v4",
    "pdf_url": null
  },
  {
    "instance_id": "bc1e8067c33e4796ad49998fa195d1c6",
    "figure_id": "1812.07626v1-Figure11-1",
    "image_file": "1812.07626v1-Figure11-1.png",
    "caption": " Zero-shot performance on the diagonal: Optimality gap forM′ = {w′|w′1 = w′2, w1 ∈ [0, 1]}. These results were averaged over 10 runs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in terms of optimality gap?",
    "answer": "SF&GPL.",
    "rationale": "The figure shows the optimality gap for four different algorithms: USFA over C={(w',w')}, USFA over C=random(5), UVFA, and SF&GPL. The optimality gap is the difference between the best possible performance and the actual performance of the algorithm. The lower the optimality gap, the better the algorithm is performing. In this case, SF&GPL has the lowest optimality gap, indicating that it is the best performing algorithm.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.07626v1",
    "pdf_url": null
  },
  {
    "instance_id": "4f0ab3080db74e2db03b3b399eba17c6",
    "figure_id": "2210.05298v2-Figure10-1",
    "image_file": "2210.05298v2-Figure10-1.png",
    "caption": " Results of our method for single frequency single-tap (SF 1Tap, left) and multi frequency two-tap (MF 2Tap, right). The scenes contain moving objects. First row shows ToF depths, second row shows error maps. Please note that the ToF depths are not phase unwrapped.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces more accurate depth estimates for the scenes with moving objects, SF 1Tap or MF 2Tap?",
    "answer": "MF 2Tap produces more accurate depth estimates.",
    "rationale": "The error maps in the second row of the figure show that the errors for MF 2Tap are smaller than the errors for SF 1Tap.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.05298v2",
    "pdf_url": null
  },
  {
    "instance_id": "9e24574dfffb4441b0706f8808c894bb",
    "figure_id": "1812.00469v2-Figure3-1",
    "image_file": "1812.00469v2-Figure3-1.png",
    "caption": " COCO anchors and box distribution in log scale. Underlying the markers is the kernel density of the ground-truth bounding box widths and heights (images resized to 544x544). Around the figure are the marginal distributions of log(w) and log(h).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which initialization method produces the anchors that are most similar to the ground-truth bounding box widths and heights?",
    "answer": "The optimized initialization method.",
    "rationale": "The optimized initialization method produces anchors that are clustered around the peak of the kernel density of the ground-truth bounding box widths and heights. This indicates that the optimized anchors are more similar to the ground-truth bounding boxes than the anchors produced by the other initialization methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.00469v2",
    "pdf_url": null
  },
  {
    "instance_id": "8172e148b3d040f294b98980b9d0b3fb",
    "figure_id": "2212.10048v3-Figure11-1",
    "image_file": "2212.10048v3-Figure11-1.png",
    "caption": " (a) Test accuracy vs time and (b) Test loss vs time on CIFAR-10 dataset on distributed data hyper-cleaning task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, ADBO, SDBO, or FEDNEST, has the highest test accuracy after 800 seconds?",
    "answer": "FEDNEST",
    "rationale": "The plot in (a) shows that the test accuracy of FEDNEST is higher than that of ADBO and SDBO after 800 seconds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10048v3",
    "pdf_url": null
  },
  {
    "instance_id": "3078948763ca4acd82e1c4adfcd6ed27",
    "figure_id": "2104.07495v2-Figure7-1",
    "image_file": "2104.07495v2-Figure7-1.png",
    "caption": " Zero-shot experiments on the DMC suite. Experimental settings and results adapted from (Sekar et al. 2020).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the Hopper Stand task?",
    "answer": "Plan2Explore (unsup)",
    "rationale": "The plot for the Hopper Stand task shows that the Plan2Explore (unsup) algorithm achieves the highest return of all the algorithms tested.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.07495v2",
    "pdf_url": null
  },
  {
    "instance_id": "ed6d3a52132e43c19b88a75724b3b4ea",
    "figure_id": "2211.01910v2-Figure21-1",
    "image_file": "2211.01910v2-Figure21-1.png",
    "caption": " Zero-shot test accuracy on 6 Instruction Induction tasks. We compare the performance of different templates used to propose instruction. Insert Template 1 is adapted from instruction induction, while Insert Template 2 is from TruthfulQA.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which template performs best on the \"Starting With\" task?",
    "answer": "Insert (Template 1)",
    "rationale": "The bar corresponding to \"Insert (Template 1)\" is the highest for the \"Starting With\" task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.01910v2",
    "pdf_url": null
  },
  {
    "instance_id": "e8a60edf71034e908b2564a4f1e3b0aa",
    "figure_id": "2007.08259v2-Figure17-1",
    "image_file": "2007.08259v2-Figure17-1.png",
    "caption": " Reliability diagrams for the model from Real-World to Product before and after calibration.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does calibration affect the model's reliability?",
    "answer": "Calibration improves the model's reliability.",
    "rationale": "The figure shows that the red line, which represents the model's predicted probabilities, is closer to the diagonal line, which represents the ideal calibration, after calibration than before. This indicates that the model's predicted probabilities are more accurate after calibration.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.08259v2",
    "pdf_url": null
  },
  {
    "instance_id": "0d4158ef6bf5488797f17ddeeeb92311",
    "figure_id": "2302.03555v1-Figure4-1",
    "image_file": "2302.03555v1-Figure4-1.png",
    "caption": " Visualization of learned item embeddings.We plot two dimensions of item representations onMafengwo-S. ConsRec learns the latent properties of items as geographically similar items are close to each other in the embedding space.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method seems to do the best job of grouping items by geographical similarity?",
    "answer": "Our method.",
    "rationale": "The figure shows that the item embeddings learned by our method are more clustered by geographical region than the item embeddings learned by the other methods. This is evident in the fact that the points representing items from the same region are closer together in our method than in the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.03555v1",
    "pdf_url": null
  },
  {
    "instance_id": "7f889ed24182480fa0a00de4366ceb92",
    "figure_id": "2108.03334v1-Figure2-1",
    "image_file": "2108.03334v1-Figure2-1.png",
    "caption": " Probability density function of the signal-to-noise ratio for each parameter of the learned posteriors in the UNIV BARE language models on splits 1 (blue), 2 (red), 3 (green), 4 (gold). The plot is in log-log scale.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which split of the UNIV BARE language models has the highest signal-to-noise ratio?",
    "answer": "Split 1.",
    "rationale": "The blue line in the plot, which represents split 1, is the highest line in the plot. This means that the probability density function of the signal-to-noise ratio is highest for split 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.03334v1",
    "pdf_url": null
  },
  {
    "instance_id": "d3e0547e6934459eb9734baa8fe1f5b2",
    "figure_id": "2012.10043v2-Figure9-1",
    "image_file": "2012.10043v2-Figure9-1.png",
    "caption": " Models were trained using Content Masked Reward where the features were generated using various layers in the VGG-16 model.",
    "figure_type": "** Photographs",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which layer of the VGG-16 model produced the most accurate representation of the target image?",
    "answer": " Layer 31",
    "rationale": " The figure shows that the images generated using features from Layer 31 are the most similar to the target image. This suggests that Layer 31 is the most effective at capturing the important features of the target image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.10043v2",
    "pdf_url": null
  },
  {
    "instance_id": "0ab59f4507c14eafa53fa588e7477d93",
    "figure_id": "1904.10754v2-Figure13-1",
    "image_file": "1904.10754v2-Figure13-1.png",
    "caption": " Gender analogies via OperatorNet and PointNet. Note that though in some cases PointNet also delivers reasonable results (e.g. the ones on the top row), the results of OperatorNet are more natural and semantically meaningful (see, e.g., the discrepancies highlighted in the red dotted boxes).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two models, OperatorNet or PointNet, produces more natural and semantically meaningful results?",
    "answer": "OperatorNet",
    "rationale": "The caption states that \"the results of OperatorNet are more natural and semantically meaningful\". This is further supported by the discrepancies highlighted in the red dotted boxes, where PointNet's results appear less natural and accurate compared to OperatorNet's.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.10754v2",
    "pdf_url": null
  },
  {
    "instance_id": "ee0c2595208a4fc1a8a40c758389caa7",
    "figure_id": "1905.12982v2-Figure4-1",
    "image_file": "1905.12982v2-Figure4-1.png",
    "caption": " Heatmaps of the p-values of the pairwise Mann-Whitney U test on three scenarios. Small p-values should be interpreted as finding evidence that the method in the column outperforms the method in the row. Using tasks from our meta-model lead to results that are close to using the large set of original tasks. Left: results with 1000 real tasks. Middle: subset of only 9 reals tasks. Right: results with 1000 tasks generated from our meta-model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two methods are most similar in terms of performance, based on the p-values of the pairwise Mann-Whitney U test?",
    "answer": "SMAC and TPE.",
    "rationale": "The heatmaps show that the p-values for the comparison between SMAC and TPE are close to 1 in all three scenarios, indicating that there is no significant difference in performance between the two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12982v2",
    "pdf_url": null
  },
  {
    "instance_id": "411e6775fa2d40af9bee5ec37cdd331d",
    "figure_id": "2210.11947v2-Figure3-1",
    "image_file": "2210.11947v2-Figure3-1.png",
    "caption": " Cross-dataset accuracy for GPT-2 (FT and OP+FT) on OUT ( ) and all (•) samples. One plot for each test dataset; the x-axis reports the training dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training dataset results in the highest accuracy for GPT-2 on OUT samples when using the OP+FT training method?",
    "answer": "CADEC",
    "rationale": "The plot shows that the accuracy for GPT-2 on OUT samples is highest for the CADEC training dataset when using the OP+FT training method. This is indicated by the red diamond at the top of the plot for the CADEC dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.11947v2",
    "pdf_url": null
  },
  {
    "instance_id": "d56001a4cb454c2f8121e437a0c47bd7",
    "figure_id": "2303.02760v2-Figure5-1",
    "image_file": "2303.02760v2-Figure5-1.png",
    "caption": " Illustration of how the annotated self-contact points can benefit 3D human mesh recovery. (a), (c), and (e) show the human mesh outputs from three scenes without self-contact optimization. (b), (d), and (f) are optimized mesh results with self-contact points.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following images shows the human mesh output from a scene with self-contact optimization?",
    "answer": "(b), (d), and (f)",
    "rationale": "The caption states that \"(b), (d), and (f) are optimized mesh results with self-contact points.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.02760v2",
    "pdf_url": null
  },
  {
    "instance_id": "369b2bcf6a8f494484a34d59a460d1c9",
    "figure_id": "2110.12997v2-Figure5-1",
    "image_file": "2110.12997v2-Figure5-1.png",
    "caption": " We evaluate our method in 10 (source, target) transition tasks, where the shifts in dynamics are either external (the map pairs and the attacked series) or internal (the broken series) to the robot.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the different types of dynamics shifts that are tested in the experiment?",
    "answer": "External and internal.",
    "rationale": "The caption states that the shifts in dynamics are either external (the map pairs and the attacked series) or internal (the broken series) to the robot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.12997v2",
    "pdf_url": null
  },
  {
    "instance_id": "8cc2f96655764af4a98b5666b962477e",
    "figure_id": "2112.04137v2-Figure2-1",
    "image_file": "2112.04137v2-Figure2-1.png",
    "caption": " The non-convex Pareto fronts of DANN [14] and CDAN [31] on task W→A (Office-31).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, CDAN or DANN, achieves better domain alignment for a given level of source classification loss?",
    "answer": "CDAN",
    "rationale": "The plot shows the domain alignment loss on the y-axis and the source classification loss on the x-axis. The lower the domain alignment loss, the better the domain alignment. For any given level of source classification loss, the CDAN curve is below the DANN curve, indicating that CDAN achieves better domain alignment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.04137v2",
    "pdf_url": null
  },
  {
    "instance_id": "3194f03b73d0494da455b7bd871cc5b8",
    "figure_id": "2109.04787v1-Figure6-1",
    "image_file": "2109.04787v1-Figure6-1.png",
    "caption": " Case study for out-of-text PCR. Target pro-",
    "figure_type": "photograph",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the dialogue and the image, do you think the surfer is about to ride the wave?",
    "answer": "Yes, the surfer is about to ride the wave.",
    "rationale": "The dialogue indicates that the surfer is not yet riding the wave, but that he looks like he is about to. The image shows the surfer crouched on his board, looking towards the wave. This suggests that he is preparing to ride the wave.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.04787v1",
    "pdf_url": null
  },
  {
    "instance_id": "31f13fe3ec9f44348a6f7536802a94f7",
    "figure_id": "2302.12444v3-Figure6-1",
    "image_file": "2302.12444v3-Figure6-1.png",
    "caption": " ResNet18 finetuned on (left to right): CIFAR10, CIFAR100, and MNIST. Note the slower convergence for SS versus RR across datasets. For the smallest learning rate η = 10−3, we observed a separation after 200 epochs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which shuffling strategy, RR or SS, results in faster convergence for ResNet18 on CIFAR10?",
    "answer": "RR",
    "rationale": "The plot shows that the solid lines, which represent RR, converge faster than the dashed lines, which represent SS, for all learning rates.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.12444v3",
    "pdf_url": null
  },
  {
    "instance_id": "26444139e042432bba1895332f814d9a",
    "figure_id": "2305.08283v3-Figure2-1",
    "image_file": "2305.08283v3-Figure2-1.png",
    "caption": " Change in RoBERTa political leaning from pretraining on pre-Trump corpora (start of the arrow) to post-Trump corpora (end of the arrow). Notably, the majority of setups move towards increased polarization (further away from the center) after pretraining on post-Trump corpora. Thus illustrates that pretrained language models could pick up the heightened polarization in news and social media due to socio-political events.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which language model and data set combination shows the largest increase in polarization after pre-training on post-Trump corpora?",
    "answer": "RoBERTa and Reddit right.",
    "rationale": "The figure shows the change in political leaning for RoBERTa and GPT-2 after pre-training on pre-Trump and post-Trump corpora. The arrows represent the direction and magnitude of the change, with the start of the arrow representing the pre-Trump position and the end of the arrow representing the post-Trump position. The RoBERTa and Reddit right combination shows the largest increase in polarization, as the arrow for this combination is the longest and points furthest away from the center of the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.08283v3",
    "pdf_url": null
  },
  {
    "instance_id": "2c7cff34b13c4db69d3aabb5bcf23286",
    "figure_id": "2107.13802v5-Figure4-1",
    "image_file": "2107.13802v5-Figure4-1.png",
    "caption": " Qualitative results on KITTI depth completion test set, including (b) GuideNet [47], (c) FCFRNet [29], and (d) CSPN [5]. Given sparse depth maps and the aligned color images (1st column), depth completion models output dense depth predictions (e.g., 2nd column). We provide error maps borrowed from the KITTI leaderboard for detailed discrimination. Warmer color in error maps refer to higher error.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most accurate depth completion results?",
    "answer": "RigNet",
    "rationale": "The error maps show that RigNet has the least amount of red, indicating that it has the lowest error compared to the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.13802v5",
    "pdf_url": null
  },
  {
    "instance_id": "8754f01be7954a07937d36790e481546",
    "figure_id": "2002.08797v5-Figure3-1",
    "image_file": "2002.08797v5-Figure3-1.png",
    "caption": " Accuracy on MNIST with different initialization schemes including EOC with rescaling, EOC without rescaling, Ordered phase, with varying depth and sparsity. This shows that rescaling to be on the EOC allows us to train not only much deeper but also sparser models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which initialization scheme allows for the training of deeper and sparser models?",
    "answer": "EOC Init & Rescaling",
    "rationale": "The figure shows that the accuracy of the model is higher for EOC Init & Rescaling when the depth and sparsity are increased. This indicates that this initialization scheme allows for the training of deeper and sparser models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.08797v5",
    "pdf_url": null
  },
  {
    "instance_id": "71d8da66233a4c1dad69df1491cad6d7",
    "figure_id": "2207.03160v2-Figure6-1",
    "image_file": "2207.03160v2-Figure6-1.png",
    "caption": " Average local curvature and scatter plot on TwainSwissRoll and StarFruit dataset. The two examples indicate that traditional ML produces distorted embeddings, which affect the performance of downstream tasks. In contrast, DLME can get as flat embeddings as possible by optimizing the local curvature.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest to converge on the TwainSwissRoll dataset?",
    "answer": "PHATE",
    "rationale": "The plot in (a) shows that PHATE has the lowest average local curvature at the end of the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.03160v2",
    "pdf_url": null
  },
  {
    "instance_id": "87ef0f91cef24109be726baa76640dd6",
    "figure_id": "1904.00887v4-Figure4-1",
    "image_file": "1904.00887v4-Figure4-1.png",
    "caption": " Robustness of our model (without adversarial training) against white-box attacks for various perturbation budgets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is the most robust to white-box attacks for a perturbation budget of 0.05 on the CIFAR-10 dataset?",
    "answer": "FGSM",
    "rationale": "The plot shows the robustness of different models against white-box attacks for different perturbation budgets. The FGSM model has the highest robustness for a perturbation budget of 0.05 on the CIFAR-10 dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.00887v4",
    "pdf_url": null
  },
  {
    "instance_id": "213963c661434fdd8a03619a668f6464",
    "figure_id": "2306.13384v2-Figure5-1",
    "image_file": "2306.13384v2-Figure5-1.png",
    "caption": " Results of the survey. Left: Accuracy per pathologist, color-coded by subjective confidence level (A) and years of experience (B). Right: Average accuracy across pathologists for each image patch, color-coded by path-size (C) and veracity (D).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which factor seems to have the most influence on accuracy?",
    "answer": "Confidence level.",
    "rationale": "The boxplot for confidence level shows a clear trend of increasing accuracy with increasing confidence. The other factors do not show such a clear trend.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.13384v2",
    "pdf_url": null
  },
  {
    "instance_id": "18a1f0b647f040efa5af0dc6d8c1d806",
    "figure_id": "2303.01668v1-Figure2-1",
    "image_file": "2303.01668v1-Figure2-1.png",
    "caption": " The prediction error versus different prediction horizons on Alien. We fix the pre-trained representations from different algorithms and append a trainable prediction network (i.e., a two-layer MLP) to predict the future RAM state. We train the prediction network with 10 random seeds and show the mean.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest prediction error for a prediction horizon of 50?",
    "answer": "BC",
    "rationale": "The figure shows the prediction error for different algorithms and prediction horizons. The BC line is the lowest at a prediction horizon of 50.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.01668v1",
    "pdf_url": null
  },
  {
    "instance_id": "afc22178db1c46bdab8d9212f7f4cf15",
    "figure_id": "2210.01776v2-Figure5-1",
    "image_file": "2210.01776v2-Figure5-1.png",
    "caption": " Chemically plausible local structures. TANKBind (blue), EquiBind (cyan), and DIFFDOCK (red) structures for complex 6g2f. EquiBind (without their correction step) produces very unrealistic local structures and TANKBind, e.g., produces non-planar aromatic rings. DIFFDOCK’s local structures are the realistic local structures of RDKit.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, TANKBind, EquiBind, or DIFFDOCK, produces the most realistic local structures for complex 6g2f?",
    "answer": "DIFFDOCK",
    "rationale": "The caption states that \"DIFFDOCK's local structures are the realistic local structures of RDKit.\" The figure shows that the DIFFDOCK structures (red) are more realistic than the TANKBind (blue) and EquiBind (cyan) structures. For example, the TANKBind structures have non-planar aromatic rings, while the EquiBind structures have unrealistic bond lengths and angles.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.01776v2",
    "pdf_url": null
  },
  {
    "instance_id": "fa2d5230f9c442dc9a7911aac4dafce5",
    "figure_id": "2104.07150v1-Figure2-1",
    "image_file": "2104.07150v1-Figure2-1.png",
    "caption": " Performance comparison on synthetic datasets.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which algorithm performs the best in simulation setting 1? ",
    "answer": " oracleLinUCB",
    "rationale": " The figure shows the regret over iterations for different algorithms in three different simulation settings. In simulation setting 1, the oracleLinUCB algorithm has the lowest regret, which means it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.07150v1",
    "pdf_url": null
  },
  {
    "instance_id": "625c6cf1c0a449d880e68ebc295703fb",
    "figure_id": "2007.15255v2-Figure2-1",
    "image_file": "2007.15255v2-Figure2-1.png",
    "caption": " Correlation between manifold density estimates and FID for each class in the ImageNet dataset. Lower values on the x-axis indicate a more dense dataset manifold. Lower values on the y-axis indicate better quality generated samples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three density estimation methods has the highest correlation with FID?",
    "answer": "PPCA.",
    "rationale": "The correlation coefficient for PPCA is 0.804, which is higher than the correlation coefficients for Gaussian (0.77) and KNN Distance (0.774).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.15255v2",
    "pdf_url": null
  },
  {
    "instance_id": "593fa2ee3d2e44d09168b3cd2d6e8958",
    "figure_id": "2001.06111v1-Figure14-1",
    "image_file": "2001.06111v1-Figure14-1.png",
    "caption": " Effect of τ",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four datasets has the highest time cost when τ = 0.2?",
    "answer": "Wiki-it",
    "rationale": "The figure shows the time cost for each dataset as a function of τ. At τ = 0.2, the Wiki-it dataset has the highest time cost.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.06111v1",
    "pdf_url": null
  },
  {
    "instance_id": "e02b6f73c75c44218d0d0c24e7c6c803",
    "figure_id": "1905.12506v3-Figure6-1",
    "image_file": "1905.12506v3-Figure6-1.png",
    "caption": " Rank correlation between various metrics and down-stream accuracy of the abstract visual reasoning models throughout training (i.e. for different number of samples). The results in the top row are based on the worst 50% of the models (according to final accuracy), and those in the bottom row based on the best 50% of the models. Columns correspond to different data sets.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric has the highest correlation with down-stream accuracy for the best 50% of the models on the dSprites dataset?",
    "answer": "BetaVAE Score",
    "rationale": "The figure shows the rank correlation between various metrics and down-stream accuracy. For the best 50% of the models on the dSprites dataset, the BetaVAE Score has the highest correlation (78) with down-stream accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12506v3",
    "pdf_url": null
  },
  {
    "instance_id": "95c0a3ac9b974fff89b5623ff5f0651a",
    "figure_id": "1908.03706v1-Figure5-1",
    "image_file": "1908.03706v1-Figure5-1.png",
    "caption": "Figure 5 – Visual results of depth estimation on the NYU Depth V2 dataset. The top five rows are: RGB inputs, ground truth, the results of baseline, ST-CLSTM and ST-CLSTM+GAN. For better visualization, we present the corresponding zoom-in regions of ground truth and estimations results on the four bottom rows. Here, both ST-CLSTM and ST-CLSTM+GAN are trained with 5 frames inputs. From the results on the last row, we can see that the estimation results generated by STCLSTM+GAN exhibit better temporal consistency than that of 2DCNN and ST-CLSTM.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate depth estimation results?",
    "answer": "ST-CLSTM + GAN",
    "rationale": "The figure shows the depth estimation results of three different methods: Baseline/2DCNN, ST-CLSTM, and ST-CLSTM + GAN. The ground truth is also shown for comparison. The zoom-in regions of the results show that the ST-CLSTM + GAN method produces the most accurate results, as they are the closest to the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.03706v1",
    "pdf_url": null
  },
  {
    "instance_id": "39d9eeeec00b40a4a3b480bff37bfe9d",
    "figure_id": "2207.03113v4-Figure2-1",
    "image_file": "2207.03113v4-Figure2-1.png",
    "caption": " Faithfulness of explanation models at different values of K on IMDB dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which explanation model is the most faithful at K = 10?",
    "answer": "AIM",
    "rationale": "The plot shows that the AIM model has the highest faithfulness score at K = 10.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.03113v4",
    "pdf_url": null
  },
  {
    "instance_id": "9b2e7556dd244bc7bbf0e5b6d7883b26",
    "figure_id": "1910.05789v2-Figure11-1",
    "image_file": "1910.05789v2-Figure11-1.png",
    "caption": " Average episode rewards on each layout during training for PBT agents when paired with each other, averaged across all agents in the population.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What layout appears to be the most challenging for the PBT agents to learn?",
    "answer": "Counter Circuit.",
    "rationale": "The plot for Counter Circuit shows the lowest average episode reward and the most variance in the rewards throughout training, indicating that the agents struggled to learn in this environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.05789v2",
    "pdf_url": null
  },
  {
    "instance_id": "8305c461836442fcb6e2704937831423",
    "figure_id": "2205.14526v1-Figure6-1",
    "image_file": "2205.14526v1-Figure6-1.png",
    "caption": " Comparison of different clustering algorithms in terms of F1 or 1-RAE.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which clustering algorithm performs best on the Openml_589 dataset in terms of F1 score?",
    "answer": "M-Clustering",
    "rationale": "The figure shows the F1 score for different clustering algorithms on four different datasets. The bar for M-Clustering is the highest for the Openml_589 dataset, indicating that it has the highest F1 score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.14526v1",
    "pdf_url": null
  },
  {
    "instance_id": "a17b28b126354c89910cebbca74ac980",
    "figure_id": "2205.00206v1-Figure3-1",
    "image_file": "2205.00206v1-Figure3-1.png",
    "caption": " Estimation visualization of intermediate modules.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the intermediate modules is the most similar to the clean audio?",
    "answer": "Esti",
    "rationale": "The Esti module has the most similar spectrogram to the clean audio, with the same general patterns and frequency distribution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.00206v1",
    "pdf_url": null
  },
  {
    "instance_id": "c484d72639b748539e6aae5e7de7dfbb",
    "figure_id": "2112.08655v2-Figure6-1",
    "image_file": "2112.08655v2-Figure6-1.png",
    "caption": " Study of different numbers of FSWGs and WDIBs.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the number of FSWGs and PSNR?",
    "answer": "The PSNR increases with the number of FSWGs.",
    "rationale": "The top plot in the figure shows that the PSNR increases as the number of FSWGs increases. This is likely because adding more FSWGs allows the network to learn more complex features, which can improve the quality of the reconstructed image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.08655v2",
    "pdf_url": null
  },
  {
    "instance_id": "9740963c834d48008cfc51c4a4025e79",
    "figure_id": "2210.11800v2-Figure3-1",
    "image_file": "2210.11800v2-Figure3-1.png",
    "caption": " Analyzing retrieval ability on Wiki80.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the best performance on Wiki80?",
    "answer": "PURE",
    "rationale": "The figure shows the Micro-F1 scores for different methods on Wiki80. The Micro-F1 score is a measure of how well the method can retrieve relevant documents. The higher the Micro-F1 score, the better the performance. The figure shows that PURE has the highest Micro-F1 score for all percentages of training instances.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.11800v2",
    "pdf_url": null
  },
  {
    "instance_id": "7fd00d1139684322a5f702757c187a59",
    "figure_id": "2205.11876v1-Figure2-1",
    "image_file": "2205.11876v1-Figure2-1.png",
    "caption": " Visualization of the IVIF results from different methods on the TNO and RoadScene datasets when using the FlowNet+STN algorithm as the basic registration model.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the sharpest and most detailed images?",
    "answer": "Ours.",
    "rationale": "The images produced by our method are the sharpest and most detailed, as can be seen in the close-up crops of the images. The other methods produce images that are either blurry or have artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.11876v1",
    "pdf_url": null
  },
  {
    "instance_id": "c93e626f5f5c458ea9c726e3e0da1107",
    "figure_id": "2012.13841v1-Figure21-1",
    "image_file": "2012.13841v1-Figure21-1.png",
    "caption": " The distance from start for networks without batch normalization trained on the original and shuffled labels. The networks move significantly.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture is most affected by shuffling the labels?",
    "answer": "densenet121",
    "rationale": "The figure shows that the distance from the start for densenet121 is much greater than for the other two networks. This suggests that densenet121 is more sensitive to the order of the training data than the other two networks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.13841v1",
    "pdf_url": null
  },
  {
    "instance_id": "8062c67ef25e400dae05cd6fbac40eff",
    "figure_id": "2002.01628v1-Figure2-1",
    "image_file": "2002.01628v1-Figure2-1.png",
    "caption": " classification accuracies of different algorithms with different r",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the highest accuracy on the \"mushrooms\" dataset?",
    "answer": "SRHT-top-f",
    "rationale": "The figure shows that the SRHT-top-f algorithm achieves the highest accuracy on the \"mushrooms\" dataset, with an accuracy of approximately 98%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.01628v1",
    "pdf_url": null
  },
  {
    "instance_id": "eaa9bd28d7264b6eabcdb6feacd60626",
    "figure_id": "2003.13063v1-Figure7-1",
    "image_file": "2003.13063v1-Figure7-1.png",
    "caption": " Results of the user study. Our method performs better than state-of-the-art FSR methods in recovering perceptualpleasant face images.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in recovering perceptually-pleasant face images?",
    "answer": "HR",
    "rationale": "The figure shows that HR has the highest percentage of Rank-1 results, which indicates that it performed the best in recovering perceptually-pleasant face images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.13063v1",
    "pdf_url": null
  },
  {
    "instance_id": "9c402bb4fd764ca0b7dc204e62df683a",
    "figure_id": "2204.04826v1-Figure2-1",
    "image_file": "2204.04826v1-Figure2-1.png",
    "caption": " In the special case of two-player zero-sum games where mixed strategies are used at each iteration is applied, greedy weights (combined with optimism (Syrgkanis et al. 2015)) outperforms many but not all previous methods for minimizing external regret. However, this trick is not feasible for general computation of equilibria or in games with a large number actions where full queries to the payoff matrix are too expensive.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best in terms of minimizing external regret for two-player zero-sum games?",
    "answer": "Greedy Weights with Optimism",
    "rationale": "The figure shows that the Greedy Weights with Optimism algorithm has the lowest regret of all the algorithms tested, as its curve is consistently below the others.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.04826v1",
    "pdf_url": null
  },
  {
    "instance_id": "e6036696d233409fa2ca9bdaedf528aa",
    "figure_id": "2210.06096v2-Figure3-1",
    "image_file": "2210.06096v2-Figure3-1.png",
    "caption": " We perform mask-and-predict pre-training using temporal shuffled videos and raw videos and then transfer them to the downstream task. (a) Masked appearance (i.e.pixels) is well reconstructed without temporal information. (b) Our MME outperforms VideoMAE when both provided temporal information.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pre-training method results in better downstream performance?",
    "answer": "Pre-training with raw videos.",
    "rationale": "The bar chart in (b) shows that the model pre-trained with raw videos achieves higher top-1 accuracy on the SSV2 dataset compared to the model pre-trained with shuffled videos.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06096v2",
    "pdf_url": null
  },
  {
    "instance_id": "427ed77fec974b7db766e796d91b2c70",
    "figure_id": "2308.16825v1-Figure8-1",
    "image_file": "2308.16825v1-Figure8-1.png",
    "caption": " The coarse-to-fine progress in C2F-Seg. The estimated amodal mask is iteratively refined from the visible mask. VM indicates visible mask while AM indicates amodal mask.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which column of images shows the most accurate amodal masks?",
    "answer": "The GT AM column.",
    "rationale": "The GT AM column stands for Ground Truth Amodal Mask, which means that these are the actual masks that the model is trying to predict. The other columns show the model's predictions at different stages of the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.16825v1",
    "pdf_url": null
  },
  {
    "instance_id": "fd8f18b185334496a5ca972110ef648c",
    "figure_id": "1909.00114v1-Figure9-1",
    "image_file": "1909.00114v1-Figure9-1.png",
    "caption": " Training/Test time comparison with the others using different numbers of parameters. Best viewed in color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the fastest training time per iteration for a given number of parameters?",
    "answer": "Ours.",
    "rationale": "The plot on the left shows the training time per iteration for different models, with the number of parameters on the x-axis. The red line, which represents our model, is consistently below the other lines, indicating that our model has the fastest training time per iteration for a given number of parameters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.00114v1",
    "pdf_url": null
  },
  {
    "instance_id": "30e6dcf1525842d1b8150e880ec2d035",
    "figure_id": "2203.04006v1-Figure3-1",
    "image_file": "2203.04006v1-Figure3-1.png",
    "caption": " Statistical analysis of generated instructions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common number of masks per template in the R2R training set?",
    "answer": "1 mask per template",
    "rationale": "The figure shows the distribution of the number of masks per template. The tallest bar in the left plot is for 1 mask per template, indicating that this is the most common number of masks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.04006v1",
    "pdf_url": null
  },
  {
    "instance_id": "858e67c69d4f4ec68b6d6555ed0b73cb",
    "figure_id": "2110.11571v3-Figure2-1",
    "image_file": "2110.11571v3-Figure2-1.png",
    "caption": " The training loss on clean versus backdoor examples crafted by 9 backdoor attacks including BadNets [1], Trojan [23], Blend [11], Dynamic [18], SIG [35], and CL [21], FC [20], DFST [36], and LBA [32]. This experiment is conducted with WideResNet-16-1 [37] on CIFAR-10 under poisoning rate 10%. ASR: attack success rate (on WideResNet-16-1).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which backdoor attack has the highest training loss for clean examples?",
    "answer": "LBA",
    "rationale": "The training loss for clean examples is shown by the blue line in each plot. The LBA plot has the highest blue line, indicating the highest training loss for clean examples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.11571v3",
    "pdf_url": null
  },
  {
    "instance_id": "8473e6dc29c0472dab580da69f64ea5f",
    "figure_id": "1812.03247v2-Figure14-1",
    "image_file": "1812.03247v2-Figure14-1.png",
    "caption": " Shadow Sequences. We report the pose accuracy vs. reprojection error threshold on the following sequences: shadow 1, shadow 2, and shadow 3. The results on shadow sequences indicate that Deep ChArUco is very robust to nuisance factors such as cast shadows. See top of Figure 13 for examples of difficult shadows–Deep ChArUco is relatively unaffected by such shadows while OpenCV rarely detects point IDs behind a shadow.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, Deep ChArUco or OpenCV, is more robust to cast shadows?",
    "answer": "Deep ChArUco.",
    "rationale": "The plots show that Deep ChArUco has a higher fraction of frames with low reprojection error compared to OpenCV, for all three shadow sequences. This indicates that Deep ChArUco is more robust to cast shadows.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.03247v2",
    "pdf_url": null
  },
  {
    "instance_id": "a258b2a0fa60424fa0a942ad7f1c4be2",
    "figure_id": "1902.01554v1-Figure11-1",
    "image_file": "1902.01554v1-Figure11-1.png",
    "caption": " Comparison with other baselines",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm converges to the optimal solution the fastest?",
    "answer": "FC",
    "rationale": "The figure shows the number of steps to terminate for different algorithms. FC has the lowest number of steps to terminate, which means it converges to the optimal solution the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.01554v1",
    "pdf_url": null
  },
  {
    "instance_id": "9b7f10035b9141f6ac1706df1a7086b9",
    "figure_id": "1810.00859v2-Figure1-1",
    "image_file": "1810.00859v2-Figure1-1.png",
    "caption": " Comprehensive motivation illustration. (a) Using larger mini-batch size helps improve throughput until it is compute-bound; (b) Limited memory capacity on a single computing node prohibits the use of large mini-batch size; (c) Neuronal activation dominates the representational cost when mini-batch size becomes large; (d) BN is indispensable for maintaining accuracy; (e) Upper and lower one are the feature maps before and after BN, respectively. However, using BN damages the sparsity through information fusion; (f) There exists such great representational redundancy that more than 80% of activations are close to zero.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network architecture has the highest throughput when the mini-batch size is 1024?",
    "answer": "WRN-18-2",
    "rationale": "Subfigure (a) shows the normalized throughput for different network architectures and mini-batch sizes. We can see that the WRN-18-2 line is the highest at a mini-batch size of 1024.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.00859v2",
    "pdf_url": null
  },
  {
    "instance_id": "a2e5cd69253849308e82a402634b46f2",
    "figure_id": "2310.18526v1-Figure2-1",
    "image_file": "2310.18526v1-Figure2-1.png",
    "caption": " Deletion curves for the CIFAR-10 dataset. Larger DEL− is better since it indicates a method finds more negative impact training samples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method consistently performs the best across all three experiments?",
    "answer": "NTK-final (tracking)",
    "rationale": "In all three subplots, the line for NTK-final (tracking) is consistently above the lines for the other methods, indicating that it achieves the highest DEL− values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.18526v1",
    "pdf_url": null
  },
  {
    "instance_id": "ca1256d05e8f43db9fee96557c8ad7d1",
    "figure_id": "1912.08795v2-Figure3-1",
    "image_file": "1912.08795v2-Figure3-1.png",
    "caption": " 32 ˆ 32 images generated by inverting a ResNet-34 trained on CIFAR-10 with different methods. All images are correctly classified by the network, clockwise: cat, dog, horse, car.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is able to produce images that are most similar to real-world images?",
    "answer": "Adaptive DI (ADI)",
    "rationale": "The images generated by Adaptive DI (ADI) are the most realistic and visually appealing, while the other methods produce images that are more abstract and less recognizable.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.08795v2",
    "pdf_url": null
  },
  {
    "instance_id": "d049d7b64a6f48828daea8da694b3dcf",
    "figure_id": "2303.09165v4-Figure6-1",
    "image_file": "2303.09165v4-Figure6-1.png",
    "caption": " The t-SNE visualization of target domain features extracted by different models on S2RDA-49 (a-b) and S2RDA-MS-39 (c-d).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models shown in the figure seems to be the most effective at separating the different classes of data?",
    "answer": "DisClusterDA",
    "rationale": "The t-SNE visualization of the target domain features extracted by DisClusterDA shows the clearest separation between the different classes of data. The other models show some overlap between the classes, indicating that they are not as effective at separating the data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.09165v4",
    "pdf_url": null
  },
  {
    "instance_id": "00f8ce44bcaa4e798fb0c45fbe7fe53e",
    "figure_id": "2007.01498v2-Figure3-1",
    "image_file": "2007.01498v2-Figure3-1.png",
    "caption": " Learning rate comparison between shaping, shielding, and the baseline in the continuing cart pole task",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in the continuing cart pole task?",
    "answer": "Shaping.",
    "rationale": "The figure shows that shaping has the highest average reward, which means that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.01498v2",
    "pdf_url": null
  },
  {
    "instance_id": "9318a9eb2cac419cba34e0456a47aa5c",
    "figure_id": "2203.11931v1-Figure10-1",
    "image_file": "2203.11931v1-Figure10-1.png",
    "caption": " Baselines and ablations. Mean reward progression of 100 robots from the UNIMAL design space averaged over 3 runs in flat terrain for baselines and ablations described in § 5.2 and § B.1. Shaded regions denote standard deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in the flat terrain environment?",
    "answer": "MetaMorph-NMT",
    "rationale": "The figure shows the mean reward progression of different methods in the flat terrain environment. MetaMorph-NMT has the highest reward at the end of the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.11931v1",
    "pdf_url": null
  },
  {
    "instance_id": "ecefb54c4e1a48fda743c10d59c36d88",
    "figure_id": "1803.03289v2-Figure3-1",
    "image_file": "1803.03289v2-Figure3-1.png",
    "caption": " (a) shows the quantization of two different clusters generated by k-means in AlexNet. Q/C means the value Q that weights to be quantized into is divided by the centroid of the cluster C.The accuracy of the network changes with the change of Q. (b) shows the test accuracy when 10 clusters of AlexNet are quantized respectively. The clusters are sorted in the descending order.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which cluster in (a) has the higher test accuracy?",
    "answer": "Cluster 1 has the higher test accuracy.",
    "rationale": "The blue line in (a) represents the test accuracy of Cluster 1, and the orange line represents the test accuracy of Cluster 2. The blue line is higher than the orange line for all values of Q/C, indicating that Cluster 1 has the higher test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1803.03289v2",
    "pdf_url": null
  },
  {
    "instance_id": "874ddc93cfdb4e0699ccd3659936343d",
    "figure_id": "1809.10505v1-Figure1-1",
    "image_file": "1809.10505v1-Figure1-1.png",
    "caption": " Validating Assumption 1 on various models and datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model and dataset combination shows the largest difference in empirical ξ between TopK [K=1.0%] and TopK [K=10.0%]?",
    "answer": "ResNet110 and synthetic data.",
    "rationale": "In the plot for ResNet110 and synthetic data (Figure 1c), the gap between the blue and orange lines is larger than in the other plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.10505v1",
    "pdf_url": null
  },
  {
    "instance_id": "5eea4ac47eef46edbacd3151a7bb5fe4",
    "figure_id": "2305.04073v1-Figure7-1",
    "image_file": "2305.04073v1-Figure7-1.png",
    "caption": " Cluster Behaviours for Grid-world. The figure shows 3 example high-level behaviours along with the action description and id of the cluster representing such behaviour.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which cluster behavior is the most successful?",
    "answer": "Cluster 1: Achieving Goal in Top right corner",
    "rationale": "This cluster behavior results in the agent reaching the goal in the top right corner, which is the desired outcome. The other two cluster behaviors either result in the agent taking a longer path to the goal or falling into lava.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.04073v1",
    "pdf_url": null
  },
  {
    "instance_id": "49a8233d162d43c6a53c71a354a62be7",
    "figure_id": "2006.06963v2-Figure2-1",
    "image_file": "2006.06963v2-Figure2-1.png",
    "caption": " Convergence for abt-buy over 1000 repeats. The upper panel plots the KL divergence from the proposal to the asymptoticallyoptimal one. The lower panel plots the MSE of the estimated F1 score. 95% bootstrap confidence intervals are included.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method converges to the asymptotically optimal distribution the fastest?",
    "answer": "Ours-8",
    "rationale": "The upper panel of the figure shows the KL divergence from the proposal to the asymptotically optimal distribution. The Ours-8 method has the lowest KL divergence at all label budgets, indicating that it converges to the asymptotically optimal distribution the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06963v2",
    "pdf_url": null
  },
  {
    "instance_id": "aeead8dc054848d4b944701d393d303b",
    "figure_id": "2309.04907v1-Figure1-1",
    "image_file": "2309.04907v1-Figure1-1.png",
    "caption": " Quantitative assessment for various diffusion inversion based methods, using a challenging image editing task of swapping dog to cat in AFHQ test set. Image editing quality are assessed jointly by LPIPS and FID. Lower LPIPS score is preferred for perception similarity with original image and lower FID in reference to the AFHQ cat train set translates to better image editing quality. Circle areas and their outer ring represent average latency time for editing and inversion respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the fastest in terms of average latency time for editing?",
    "answer": "PTP",
    "rationale": "The size of the circle represents the average latency time for editing, and PTP has the smallest circle.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.04907v1",
    "pdf_url": null
  },
  {
    "instance_id": "973c6d9e01834c3a934089368cdcee2e",
    "figure_id": "2308.10305v1-Figure8-1",
    "image_file": "2308.10305v1-Figure8-1.png",
    "caption": " Failure cases in self-contact and challenging pose.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two failure cases shown in the image?",
    "answer": "Self-contact and challenging pose.",
    "rationale": "The image shows two different scenarios where the system fails to track the person's movements accurately. In the first scenario, the person is picking up a basketball, and the system incorrectly identifies the person's hand as being in contact with their knee. In the second scenario, the person is leaning forward while balancing on a rope, and the system incorrectly identifies the person's back as being in contact with the rope.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.10305v1",
    "pdf_url": null
  },
  {
    "instance_id": "07d0ca9e379b4dbca2f1483630b6d200",
    "figure_id": "2206.07136v3-Figure8-1",
    "image_file": "2206.07136v3-Figure8-1.png",
    "caption": " Dependence of the upper bound G on ξ (left) and γ (right). Here the O(1/ √ T ) term is set to 10 and either γ = 0.01 (left) or ξ = 1 (right).",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \nWhich algorithm appears to have the best performance in terms of minimizing the upper bound of the error? ",
    "answer": " \nDP-SGD AUTO-S with ξ = 1. ",
    "rationale": " \nThe left plot shows the upper bound of the error for different values of ξ, with γ fixed at 0.01. The lines for DP-SGD AUTO-S with ξ = 1 are consistently lower than the lines for other algorithms, indicating that it has the best performance in terms of minimizing the upper bound of the error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.07136v3",
    "pdf_url": null
  },
  {
    "instance_id": "e8bd14416d35470ea05606f3716dfeb4",
    "figure_id": "2201.11676v3-Figure3-1",
    "image_file": "2201.11676v3-Figure3-1.png",
    "caption": " Individual explanation that displays the source of uncertainty for one instance. The previous method allowed only for comparison between distributions, now with explainable uncertainty, we are able to account for individual instances. In red, features pushing the uncertainty prediction higher are shown; in blue, those pushing the uncertainty prediction lower.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature has the largest positive impact on the uncertainty prediction?",
    "answer": "GrLivArea",
    "rationale": "The figure shows the SHAP values for each feature, which represent the contribution of each feature to the final prediction. The feature with the largest positive SHAP value is GrLivArea, which has a SHAP value of +10709.03. This means that this feature is pushing the uncertainty prediction higher.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.11676v3",
    "pdf_url": null
  },
  {
    "instance_id": "f67946cc06304f4a9808afe83810396d",
    "figure_id": "2305.04599v1-Figure1-1",
    "image_file": "2305.04599v1-Figure1-1.png",
    "caption": " Example contrastive opinions extracted by our model from hotel reviews, where “Sentiment dist.” stands for the sentiment distribution (‘Positive’ in blue while ‘Negative’ in red) under a given aspect, and “Popularity” stands for the occurrence proportion of the given aspect in the whole corpus.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What aspect of the hotel experience did guests have the most contrasting opinions about?",
    "answer": "The ambient noise level.",
    "rationale": "The table shows that 91.6% of guests found the noise from the outdoor stage to be incredibly noisy, while only 8.4% found the hotel to be quiet. This suggests that guests had very different experiences with the noise level at the hotel.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.04599v1",
    "pdf_url": null
  },
  {
    "instance_id": "b26a2d528ff44e418b075dae0f10780e",
    "figure_id": "2011.13241v1-Figure4-1",
    "image_file": "2011.13241v1-Figure4-1.png",
    "caption": " Qualitative Comparisons on the Boundary Area. We compare the mask results of (a) BlendMask [6] and (b) Our proposed B2Inst. (c) Our learned image boundary representation. We can clearly see that our proposals resolve ambiguities in a challenging scene and produce more accurate masks.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, BlendMask or B2Inst, produces more accurate masks?",
    "answer": "B2Inst produces more accurate masks.",
    "rationale": "The figure shows that B2Inst is able to resolve ambiguities in a challenging scene and produce more accurate masks than BlendMask. For example, in the image of the kite, B2Inst is able to correctly identify the kite as a separate object from the person, while BlendMask incorrectly merges the two objects together.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.13241v1",
    "pdf_url": null
  },
  {
    "instance_id": "599966fa1f414591acbab3a269893307",
    "figure_id": "2312.02405v1-Figure3-1",
    "image_file": "2312.02405v1-Figure3-1.png",
    "caption": " The distributions of the number of right mouse button clicks and movement key presses across tasks in the Demonstrations Dataset, which act as proxies for the number of blocks placed and distance traveled per episode, respectively. Because BuildVillageHouse takes the longest to complete, it is reasonable that it requires more right mouse button clicks and movement key presses.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task requires the most right mouse button clicks and movement key presses?",
    "answer": "BuildVillageHouse",
    "rationale": "The box plots in the figure show the distribution of the number of right mouse button clicks and movement key presses for each task. The box plot for BuildVillageHouse has the highest median and upper quartile, indicating that this task requires the most right mouse button clicks and movement key presses.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2312.02405v1",
    "pdf_url": null
  },
  {
    "instance_id": "0e4b25a9381e476a98a5d9ad79b93ac6",
    "figure_id": "1807.04836v2-Figure3-1",
    "image_file": "1807.04836v2-Figure3-1.png",
    "caption": " Performance of 1:N matching",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network performs best when the gallery size is 5?",
    "answer": "SVHF-Net.",
    "rationale": "The figure shows that the accuracy of SVHF-Net is the highest when the gallery size is 5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1807.04836v2",
    "pdf_url": null
  },
  {
    "instance_id": "d632efd06c254c4ea93e4f129f233dd4",
    "figure_id": "1903.07254v2-Figure2-1",
    "image_file": "1903.07254v2-Figure2-1.png",
    "caption": " Template matching performance comparisons. (a) QATM v.s. SOTA methods on the OTB dataset. (b) QATM performance under varying α on the OTB dataset. (c) QATM v.s. SOTA methods on the MOTB dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tracker performs best on the MOTB dataset?",
    "answer": "QATM",
    "rationale": "The ROC curve for QATM is closest to the top left corner, indicating the highest true positive rate for any given false positive rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.07254v2",
    "pdf_url": null
  },
  {
    "instance_id": "5ecfa02900af46bba854fad1e69b458b",
    "figure_id": "2109.11635v1-Figure9-1",
    "image_file": "2109.11635v1-Figure9-1.png",
    "caption": " Version of Fig. 3 albeit with linear terms for summed unigram log-probability, total character length, and their interaction as predictors.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Brown corpus?",
    "answer": "GPT-2",
    "rationale": "The figure shows the change in log-likelihood for each model as a function of the number of tokens (k). GPT-2 has the highest peak in the Brown corpus.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.11635v1",
    "pdf_url": null
  },
  {
    "instance_id": "5b3d5247f3c64fe8afaafaeaa6ea9216",
    "figure_id": "2010.05338v1-Figure2-1",
    "image_file": "2010.05338v1-Figure2-1.png",
    "caption": " Political ideology for the most frequent topics: elections, immigration, coronavirus, and politics.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which topic is the most polarized, according to the pie charts?",
    "answer": "Immigration",
    "rationale": "The pie chart for immigration shows the largest difference between the percentages of people who identify as left, center, and right.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.05338v1",
    "pdf_url": null
  },
  {
    "instance_id": "3a261bf8d59f4d9080b2fee5b445116c",
    "figure_id": "2205.09638v1-Figure1-1",
    "image_file": "2205.09638v1-Figure1-1.png",
    "caption": " Comparison between certified and empirical error control methods on MS MARCO Passage v1. Coverage: Percentage of 100 independent runs that satisfies a pre-specified MRR@10≥0.35 (the dotted line).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the required coverage for all 100 independent runs?",
    "answer": "Certified Error Control",
    "rationale": "The figure shows that the orange bars, which represent the Certified Error Control method, always reach or exceed the required coverage (dotted line) for all four model configurations. In contrast, the blue bars, representing the Empirical Error Control method, fall short of the required coverage in all cases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.09638v1",
    "pdf_url": null
  },
  {
    "instance_id": "f9bf5805956c47a5a23ab86b83bea3d9",
    "figure_id": "2003.00492v3-Figure1-1",
    "image_file": "2003.00492v3-Figure1-1.png",
    "caption": " Selected results of part segmentation.",
    "figure_type": "schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the objects in the figure is the most accurately predicted by the model?",
    "answer": "The guitar.",
    "rationale": "The error image for the guitar shows the least amount of white, indicating that the predicted shape is very close to the ground truth shape.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.00492v3",
    "pdf_url": null
  },
  {
    "instance_id": "952404ef844d4ed583e3457be7c795da",
    "figure_id": "2306.12251v2-Figure2-1",
    "image_file": "2306.12251v2-Figure2-1.png",
    "caption": " The impact of different number of neighbor aggregation layers on the performance of XGBGraph and RF-Graph.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better with more aggregation layers?",
    "answer": "XGB-Graph.",
    "rationale": "The plot shows that the AUPRC of XGB-Graph increases with the number of aggregation layers, while the AUPRC of RF-Graph stays relatively constant.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.12251v2",
    "pdf_url": null
  },
  {
    "instance_id": "00da9244afb94409852f2feecfdb5e9e",
    "figure_id": "1903.08469v2-Figure1-1",
    "image_file": "1903.08469v2-Figure1-1.png",
    "caption": " Speed-accuracy trade-off for different semantic semantic segmentation approaches on Cityscapes test on GTX1080Ti (except for DG2s which reports only validation performance). Red dots represent our method. Other methods are displayed in green, whereas blue dots show estimated frame rates of the corresponding methods on our GPU (please refer to subsection 4.2 for details). Our submissions achieve the best accuracy and the best speed among all approaches aiming at real-time operation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which semantic segmentation approach has the best speed and accuracy?",
    "answer": "SwiftNetRN-18 (2MPx)",
    "rationale": "The figure shows a scatter plot of speed (x-axis) vs. accuracy (y-axis) for different semantic segmentation approaches. SwiftNetRN-18 (2MPx) is the red dot in the top right corner, indicating it has the highest accuracy and fastest speed.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.08469v2",
    "pdf_url": null
  },
  {
    "instance_id": "4bd10c491d014fc8a2df0c2e66efa1bc",
    "figure_id": "1812.00740v2-Figure12-1",
    "image_file": "1812.00740v2-Figure12-1.png",
    "caption": " L2 attacks of Madry et al. [62] and Carlini and Wagner [14] on FONTS, EMNIST and F-MNIST. In all cases, we plot regular or on-manifold success rate against test error. Independent of the attack, we can confirm that on-manifold robustness is strongly related to generalization, while regular robustness is independent of generalization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method consistently leads to the highest success rate on all datasets and for both types of attacks?",
    "answer": "On-Learned-Manifold Adversarial Training.",
    "rationale": "The plots show that the green line, which represents On-Learned-Manifold Adversarial Training, consistently achieves the highest success rate for all datasets and for both types of attacks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.00740v2",
    "pdf_url": null
  },
  {
    "instance_id": "1a692466aae945dda70f88bdfd094042",
    "figure_id": "2210.13382v4-Figure2-1",
    "image_file": "2210.13382v4-Figure2-1.png",
    "caption": " (A) explains how we intervene on a board tile. Here, we only want to flip one tile, e.g. E6, from white to black. In (B), four views present an Othello game in progression, which can be reliably probed from an internal representation x. The lower left board represents the model’s perceived world state prior to intervention. The upper left board shows the model’s predictions for legal moves given this state. Post-intervention, the model’s world state is updated—E6’s state has been switched from white to black (lower right), leading to a different set of legal move predictions (upper right). Note that two tiles (E6) are highlighted in the world state boards. This is the tile that we “intervene” on, changing from white to black. (C) Shows our proposed intervention scheme. Light blue indicates unmodified activations; dark blue represents activations affected by intervention. Starting from a predefined layer, we intervene at the temporally-last token (shown in (A)). We replace original internal representations with the post-intervention one and resume computation for the next one layer. Part of the misinformation gets corrected (light blue), but we alternate this intervening and computation process until the last layer, from which the next-step prediction is made.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tile is intervened on in the figure?",
    "answer": "Tile E6.",
    "rationale": "Panel B shows the pre- and post-intervention world state boards. The tile highlighted in both boards is E6, which is the tile that is intervened on.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.13382v4",
    "pdf_url": null
  },
  {
    "instance_id": "bb22ea3e23fd44e88bf7a384af679fd1",
    "figure_id": "2308.13504v1-Figure6-1",
    "image_file": "2308.13504v1-Figure6-1.png",
    "caption": " We visualize the trade-off between resource utilization and model accuracy using Pareto frontiers. We compare A2Q (green dots) to baseline QAT with: (1) a fixed accumulator bit width P (red triangles); (2) layer-wise selection of P using data type bounds (blue dots); and (3) post-training minimization (PTM) of P using weight values (yellow squares). We observe that A2Q provides a better trade-off between LUT utilization and task performance than existing baselines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which quantization method achieves the highest accuracy for MobileNetV1 on CIFAR10?",
    "answer": "A2Q",
    "rationale": "The green line in the top left plot shows the accuracy of A2Q for MobileNetV1 on CIFAR10. This line is higher than the other lines in the plot, which represent the accuracy of other quantization methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.13504v1",
    "pdf_url": null
  },
  {
    "instance_id": "93365b95f12a479ab909a2f5749534bf",
    "figure_id": "2110.04391v3-Figure2-1",
    "image_file": "2110.04391v3-Figure2-1.png",
    "caption": " Top-10 categories not in distribution (green) and in-distribution (blue) noise categories in Aura test set. Indistribution relates to categories in the benchmark test set [6].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What percentage of the Aura test set is comprised of musical instrument sounds?",
    "answer": "0%",
    "rationale": "The figure shows the top 10 categories of noise in the Aura test set, and musical instrument sounds are not included in this list. This indicates that the percentage of musical instrument sounds in the Aura test set is very low, likely 0%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.04391v3",
    "pdf_url": null
  },
  {
    "instance_id": "12c3ef3c9a8f4194acbd305cc54b4980",
    "figure_id": "1808.10038v2-Figure5-1",
    "image_file": "1808.10038v2-Figure5-1.png",
    "caption": " Validation of Theorem 3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in the noiseless case?",
    "answer": "LISTA-CPSS.",
    "rationale": "The figure shows the NMSE (Normalized Mean Squared Error) of different algorithms as a function of iterations/layers. In the noiseless case (Figure (a)), LISTA-CPSS has the lowest NMSE, which means it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.10038v2",
    "pdf_url": null
  },
  {
    "instance_id": "db291595516b473d8e251f30563c1718",
    "figure_id": "2011.03677v1-Figure4-1",
    "image_file": "2011.03677v1-Figure4-1.png",
    "caption": " Qualitative comparison on aerial images from HAI Dataset with a city-based scenario",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best at removing haze from the aerial image?",
    "answer": "SkyGAN",
    "rationale": "The SkyGAN image is the most similar to the Ground Truth image, which is the image without haze.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.03677v1",
    "pdf_url": null
  },
  {
    "instance_id": "ffc83f0e7c914fc7b79164b4da345ac4",
    "figure_id": "2211.12005v3-Figure5-1",
    "image_file": "2211.12005v3-Figure5-1.png",
    "caption": " Ablation study on FA and VR with a different number of sub-models in self-ensemble. Results are produced on CIFAR-10 models with the bound of protective perturbations ε = 2/255.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which technique consistently achieves the highest accuracy across all architectures and ensemble sizes?",
    "answer": "SEP",
    "rationale": "The blue dashed line, representing SEP, consistently achieves the highest accuracy across all architectures and ensemble sizes. This is evident from the fact that the blue dashed line is always above the other lines in each plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.12005v3",
    "pdf_url": null
  },
  {
    "instance_id": "3afdc06d3ffc4d0bbcff563c58ae17e2",
    "figure_id": "1902.00179v2-Figure5-1",
    "image_file": "1902.00179v2-Figure5-1.png",
    "caption": " The effect of cleaning on the Count-Min Sketch Tensor and its corresponding optimizer for the MegaFace dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer and cleaning combination leads to the highest accuracy on the MegaFace dataset?",
    "answer": "Adam with Cleaning.",
    "rationale": "The plot shows that the accuracy of Adam with Cleaning is higher than the other combinations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.00179v2",
    "pdf_url": null
  },
  {
    "instance_id": "39b87dbc2ed44f7a9947709474e69f10",
    "figure_id": "2208.14698v5-Figure8-1",
    "image_file": "2208.14698v5-Figure8-1.png",
    "caption": " Efficiency loss paths (i.e., regret plots) forQinit = 40 of OUR-MVNN-MLCA winners (yellow) from Table 4 compared to BOCA winners (green) from Table 3 and to the results presented in (Weissteiner et al. 2022a) of MVNNs (blue) and plain NNs (red). Shown are averages with 95% CIs over 50 instances.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest efficiency loss in the MRVM case?",
    "answer": "OUR-MVNN-MLCA",
    "rationale": "The efficiency loss path for OUR-MVNN-MLCA (yellow line) is consistently below the other methods in the MRVM plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.14698v5",
    "pdf_url": null
  },
  {
    "instance_id": "9c4265d1341b4aa1871de46382a6eb58",
    "figure_id": "2302.10503v2-Figure6-1",
    "image_file": "2302.10503v2-Figure6-1.png",
    "caption": " Ablation studies in OBJ3D. The original design of RSM always demonstrates the superior performance and the accurately predicted future frames compared to the modified versions, including breaking the usages of the CCI in step 2 (RSM!2) and step 3 (RSM!3), randomly selecting mechanisms (RSMk̄), and parallel update of slots (RSMp). Best viewed in video format (See our repository).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the modified versions of RSM most closely resembles the original RSM in terms of performance?",
    "answer": "RSMp",
    "rationale": "The figure shows that RSMp has the lowest LPIPS score among the modified versions of RSM, indicating that it is the closest to the original RSM in terms of performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.10503v2",
    "pdf_url": null
  },
  {
    "instance_id": "34e2cc261e494d4ca08a63b86857a593",
    "figure_id": "1909.09279v1-Figure1-1",
    "image_file": "1909.09279v1-Figure1-1.png",
    "caption": " t-SNE projection of WALS vectors with clustering. Persian (fa) is an example of a poorly performing language that is also far from its cluster center.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language is both poorly performing and far from its cluster center?",
    "answer": "Persian (fa)",
    "rationale": "The caption explicitly states that Persian (fa) is an example of a poorly performing language that is also far from its cluster center. This can also be seen in the figure, where \"fa\" is located far away from the other languages in its cluster.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.09279v1",
    "pdf_url": null
  },
  {
    "instance_id": "8636e04ea66748eeb606b328d7e2b589",
    "figure_id": "2309.15259v1-Figure8-1",
    "image_file": "2309.15259v1-Figure8-1.png",
    "caption": " SLIQ performs competitively against the comparative classification techniques for all datasets, despite classification not being a learned objective of SLIQ.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which technique performs the best on the MNIST dataset?",
    "answer": "SLIQ",
    "rationale": "The figure shows that SLIQ achieves the highest accuracy on the MNIST dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.15259v1",
    "pdf_url": null
  },
  {
    "instance_id": "c72d3306904c4fe1adfe994e8ede3b60",
    "figure_id": "2302.06556v2-Figure7-1",
    "image_file": "2302.06556v2-Figure7-1.png",
    "caption": " Qualitative comparison with AdaBins (Bhat et al., 2021), NeWCRFs (Yuan et al., 2022) on SUN RGB-D test set (Song et al., 2015). All the models are pre-trained on NYU Depth V2 (Silberman et al., 2012) training set.",
    "figure_type": "** \n\nphotograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \n\nWhich of the methods produces the sharpest depth estimation results?",
    "answer": " \n\nOurs (d)",
    "rationale": " \n\nThe depth estimation results produced by our method are the sharpest, as they have the most defined edges and details. This can be seen in the comparison of the results for the different methods in the figure. For example, in the first row, the chair in the image produced by our method has a sharper edge than the chair in the images produced by the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.06556v2",
    "pdf_url": null
  },
  {
    "instance_id": "5ff1be1ba6874542b14743599aec72b9",
    "figure_id": "2211.01452v2-Figure5-1",
    "image_file": "2211.01452v2-Figure5-1.png",
    "caption": " Ratio of training examples versus performance (normalized by performance with ratio = 1.0) on MRPC, RTE, QNLI, and SST-2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the tasks is the most sensitive to the number of training examples?",
    "answer": "QNLI",
    "rationale": "The QNLI line in the figure shows the steepest decline in performance as the training example ratio decreases. This indicates that QNLI is the most sensitive to the number of training examples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.01452v2",
    "pdf_url": null
  },
  {
    "instance_id": "4e449bd77204406cb525a4131ee1c8cf",
    "figure_id": "1912.01196v3-Figure19-1",
    "image_file": "1912.01196v3-Figure19-1.png",
    "caption": " Additional comparison between direct event to SR intensity (ours) and event to image to SR intensity in a hierarchical manner (EV+MISR) on simulated sequences. (In addition to the Fig. 5 of main manuscript)",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the sharpest image?",
    "answer": "The APS method produces the sharpest image.",
    "rationale": "The APS image has the highest resolution and the most detail. This can be seen in the zoomed-in crops of the images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.01196v3",
    "pdf_url": null
  },
  {
    "instance_id": "f4b436520f6341cf9c05fe9dd866b6a0",
    "figure_id": "2210.12696v1-Figure16-1",
    "image_file": "2210.12696v1-Figure16-1.png",
    "caption": " Aligning encoded concepts with human-defined concept (POS ) in base and pre-trained models",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest number of cluster matches for POS alignment in the base setting?",
    "answer": "XLM-R",
    "rationale": "In the base setting, the XLM-R model has the highest number of cluster matches for POS alignment, as seen in the bar chart.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12696v1",
    "pdf_url": null
  },
  {
    "instance_id": "20089601172e4d209d3932635fef1be3",
    "figure_id": "2210.17378v1-Figure5-1",
    "image_file": "2210.17378v1-Figure5-1.png",
    "caption": " Results for BART models fine-tuned on different subsets of the XLSUM dataset, filtered by different factuality models with varying thresholds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best according to the BLANC metric?",
    "answer": "SummFC.",
    "rationale": "The BLANC score for SummFC is the highest at all thresholds, as shown in Figure (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.17378v1",
    "pdf_url": null
  },
  {
    "instance_id": "cda5ce7a3c6f412786da1a8b5f6ae34f",
    "figure_id": "2105.08810v2-Figure23-1",
    "image_file": "2105.08810v2-Figure23-1.png",
    "caption": " Difference between the weight gradients displayed in 3. The maximum relative error between two individual weights is 10.6%",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which weight gradient has the larger magnitude, SparseVW(0) or OrigVW(0)?",
    "answer": "OrigVW(0)",
    "rationale": "The left panel of the figure shows the difference between SparseVW(0) and OrigVW(0). The blue color indicates that OrigVW(0) is larger than SparseVW(0) in most cases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.08810v2",
    "pdf_url": null
  },
  {
    "instance_id": "d6cb9c60cf624e8d8d095d8038978efd",
    "figure_id": "2302.11002v4-Figure3-1",
    "image_file": "2302.11002v4-Figure3-1.png",
    "caption": " (a) Stefan solution profiles at time t = 0.05 with training parameter values u⋆ ∈ A = [0.55, 0.7] and test-time parameter u⋆ = 0.6. ProbConserv-ANP results in a sharper solution profile and the solution is mean-centered around the shock position. (b) The corresponding histogram of the posterior of the shock position computed as the mean plus or minus 3 standard deviations. ProbConserv-ANP reduces the level of underestimation and the induced negative bias at the shock interface to result in more accurate shock position prediction.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods results in the most accurate shock position prediction?",
    "answer": "ProbConserv-ANP",
    "rationale": "Figure (b) shows the posterior of the shock position for each of the four methods. The true shock position is indicated by the black vertical line. ProbConserv-ANP is the only method that accurately predicts the shock position, with the peak of its posterior distribution closely aligned with the true shock position.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.11002v4",
    "pdf_url": null
  },
  {
    "instance_id": "4704fa84d9ee492caf9c9ea072d3f8fd",
    "figure_id": "2210.06732v2-Figure9-1",
    "image_file": "2210.06732v2-Figure9-1.png",
    "caption": " The data distribution pz(x) for each group z ∈ {0, 1}. Samples with (+) sign have the true label y = 1, while samples with (−) sign have the true label y = 0.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the probability of a sample having the true label y = 1 given that it belongs to group z = 0?",
    "answer": "1/4",
    "rationale": "The figure shows the data distribution for each group. The area of the blue rectangle with a (+) sign represents the probability of a sample having the true label y = 1 given that it belongs to group z = 0. The area of this rectangle is 1/4.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06732v2",
    "pdf_url": null
  },
  {
    "instance_id": "8f789ba333d047028876b1300aa71529",
    "figure_id": "1907.12209v2-Figure16-1",
    "image_file": "1907.12209v2-Figure16-1.png",
    "caption": " Reconstructed point clouds. Three scenes are randomly selected from KITTI. For the reconstructed point cloud of each scene, 3 views are selected to demonstrate the quality of the point cloud. (a) Scene 1; (b) Scene 2; (c) Scene 3.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the scenes is most likely to be from a rural area?",
    "answer": "Scene 1.",
    "rationale": "Scene 1 shows a narrow road with buildings on either side, which is more typical of a rural area than the other two scenes. Scene 2 shows a tram track, which is more likely to be found in an urban area. Scene 3 shows a wide road with a wall on one side, which could be found in either an urban or rural area.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.12209v2",
    "pdf_url": null
  },
  {
    "instance_id": "d9d13aa74dfb4700874c9f8afd859c65",
    "figure_id": "1912.06570v1-Figure1-1",
    "image_file": "1912.06570v1-Figure1-1.png",
    "caption": " The minimum gap between θ1 and θ2 permitted by our Theorem 4.2 versus the state-of-the-art bound of (Galhotra et al. 2019) in the unsupervised setting.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which bound is tighter, the one proposed in this paper or the state-of-the-art bound of (Galhotra et al. 2019)?",
    "answer": "The bound proposed in this paper is tighter.",
    "rationale": "The figure shows that the minimum gap between θ1 and θ2 permitted by the bound proposed in this paper is smaller than the minimum gap permitted by the state-of-the-art bound of (Galhotra et al. 2019). This means that the bound proposed in this paper is tighter.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.06570v1",
    "pdf_url": null
  },
  {
    "instance_id": "c830e604607a492584ba3008511d5413",
    "figure_id": "2206.13749v1-Figure5-1",
    "image_file": "2206.13749v1-Figure5-1.png",
    "caption": " A closer look at the effect of multi-view rules.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best?",
    "answer": "AMRule.",
    "rationale": "The AMRule line is the highest on the plot, indicating that it has the highest performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.13749v1",
    "pdf_url": null
  },
  {
    "instance_id": "892e85eeac01433692e3b37f4c153544",
    "figure_id": "1909.01084v1-Figure3-1",
    "image_file": "1909.01084v1-Figure3-1.png",
    "caption": " Performance comparison of node classification tasks on Flickr and Last.fm as a function of the fraction of nodes used for training the node classifier",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the Flickr dataset?",
    "answer": "MEGAN",
    "rationale": "The figure shows the performance of different methods on the Flickr and Last.fm datasets. The MEGAN method has the highest Micro-F1 and Macro-F1 scores on the Flickr dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.01084v1",
    "pdf_url": null
  },
  {
    "instance_id": "2852feff164342a38c594e1aa173ee5e",
    "figure_id": "2110.14457v2-Figure14-1",
    "image_file": "2110.14457v2-Figure14-1.png",
    "caption": " Complement to Fig. 2: Visualization of the policies learned on the Bottleneck Maze for the remaining methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method results in the most efficient navigation of the maze?",
    "answer": "Flat UpSIDE",
    "rationale": "The Flat UpSIDE method results in the most direct paths through the maze, with minimal backtracking or unnecessary movements. This can be seen in the figure, where the paths taken by the agent using Flat UpSIDE are generally straighter and more direct than those taken by the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14457v2",
    "pdf_url": null
  },
  {
    "instance_id": "3facb2f0bccd470aa60ae18516984b13",
    "figure_id": "2107.08221v4-Figure13-1",
    "image_file": "2107.08221v4-Figure13-1.png",
    "caption": " R2-score on in individual factors. Extrapolation performance across models when only a single factor (x-axis) is OOD.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which factor had the highest median R2-score across all models?",
    "answer": "Object size",
    "rationale": "The box plots show the distribution of R2-scores for each factor across all models. The median is represented by the horizontal line within each box. The box plot for object size has the highest median line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.08221v4",
    "pdf_url": null
  },
  {
    "instance_id": "ae0b6d535e054203a0ec12fdfcbae318",
    "figure_id": "2210.06546v1-Figure6-1",
    "image_file": "2210.06546v1-Figure6-1.png",
    "caption": " Convergence of reconstruction error on the CelebA training (a) and testing (b) sets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the lowest reconstruction error on the CelebA test set?",
    "answer": "WAE",
    "rationale": "The figure shows the convergence of reconstruction error for different models on the CelebA training and testing sets. The WAE model has the lowest reconstruction error on the test set, as its curve is the lowest among all the models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06546v1",
    "pdf_url": null
  },
  {
    "instance_id": "5eb9025b67f24f2e880ee40c706fa633",
    "figure_id": "1907.11840v1-Figure3-1",
    "image_file": "1907.11840v1-Figure3-1.png",
    "caption": " The channel pruning results of VGG16. The x-axis is the index of channels. The label convi j in y-axis means the i jth layer, and each layer contains 10 categories of CIFAR-10. Light color means more samples have this channel been dropped, and vice versa. This result is got when α = 0.5, β = 1.0.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the most channels pruned?",
    "answer": "conv3_2",
    "rationale": "The figure shows that the conv3_2 layer has the most light-colored squares, which indicates that more samples have this channel been dropped.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.11840v1",
    "pdf_url": null
  },
  {
    "instance_id": "530dec4c85384296892acc9d0964c847",
    "figure_id": "1902.02907v1-Figure4-1",
    "image_file": "1902.02907v1-Figure4-1.png",
    "caption": " Source learning vs SR-reward decomposition vs pseudo-reward descent (higher lines corresp. to higher γ)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best when the discount factor γ is highest?",
    "answer": "TD Source-SR",
    "rationale": "The figure shows that the lines corresponding to higher γ are higher for TD Source-SR than for the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.02907v1",
    "pdf_url": null
  },
  {
    "instance_id": "560fa53135e1435f9e830f45de4a328f",
    "figure_id": "1812.02822v5-Figure5-1",
    "image_file": "1812.02822v5-Figure5-1.png",
    "caption": " 3D shape interpolation results. 3DGAN, CNNGAN, and IM-GAN are sampled at 643 resolution to show the smoothness of the surface is not just a matter of sampling resolution. Notice that the morphing sequence of IMGAN not only consists of smooth part movements (legs, board), but also handles topology changes.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods shown in the figure produces the smoothest surfaces?",
    "answer": "IM-GAN.",
    "rationale": "The caption states that \"IM-GAN not only consists of smooth part movements (legs, board), but also handles topology changes.\" This suggests that IM-GAN produces smoother surfaces than the other methods. Additionally, the figure shows that the IM-GAN results have fewer jagged edges and artifacts than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.02822v5",
    "pdf_url": null
  },
  {
    "instance_id": "977a8ca074714f819815927259a5f104",
    "figure_id": "2110.06389v2-Figure8-1",
    "image_file": "2110.06389v2-Figure8-1.png",
    "caption": " Correlation between input and output values on ChEMBL molecules.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four input values has the strongest correlation with the recovered value?",
    "answer": "SA score",
    "rationale": "The r^2 value for the SA score is the highest of the four input values, indicating that it has the strongest correlation with the recovered value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.06389v2",
    "pdf_url": null
  },
  {
    "instance_id": "c6ae7ad751e74d51ae784fb084c460d8",
    "figure_id": "1902.10197v1-Figure5-1",
    "image_file": "1902.10197v1-Figure5-1.png",
    "caption": " Histograms of element-wise additions of inversed relation embedding phases on WN18. (k = 500)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which relation has the most uniform distribution of phases?",
    "answer": "member.meronym o member.holonym",
    "rationale": "The figure shows histograms of the element-wise additions of inversed relation embedding phases on WN18. The most uniform distribution is seen in the plot for member.meronym o member.holonym, which shows a relatively flat distribution across the range of phases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.10197v1",
    "pdf_url": null
  },
  {
    "instance_id": "c8993c8f620d493e8327e4391e597ee6",
    "figure_id": "2206.12327v1-Figure4-1",
    "image_file": "2206.12327v1-Figure4-1.png",
    "caption": " The visualized comparison between the generated diffusion sources and the ground truth. Fig. 4a - 4f are visualizations of Karate dataset, and Fig. 4g - 4l are visualizations of Jazz dataset. The predicted and ground truth diffusion sources are marked with green color while the rest of nodes are marked with red color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method most accurately predicted the diffusion sources in the Karate dataset?",
    "answer": "The SL-VAE method.",
    "rationale": "The visualization in Fig. 4e shows that the SL-VAE method correctly predicted all of the diffusion sources in the Karate dataset, while the other methods predicted some incorrect diffusion sources.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.12327v1",
    "pdf_url": null
  },
  {
    "instance_id": "97efd91494ed4f479641c565010e5739",
    "figure_id": "2106.05933v2-Figure61-1",
    "image_file": "2106.05933v2-Figure61-1.png",
    "caption": " Sparsity over layers for wav2vec-base finetuned for Italian it at 30% sparsity.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the sparsity of the wav2vec-base finetuned for Italian it at 30% sparsity?",
    "answer": "The sparsity is 43.343%.",
    "rationale": "The figure shows the sparsity over layers for wav2vec-base finetuned for Italian it at 30% sparsity. The sparsity is shown as a percentage on the y-axis, and the layers are shown on the x-axis. The sparsity of the wav2vec-base finetuned for Italian it at 30% sparsity is 43.343%, which is shown in the last bar of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05933v2",
    "pdf_url": null
  },
  {
    "instance_id": "676c8ee7550d4a4da5801b79fc9cf8a1",
    "figure_id": "2010.07930v2-Figure5-1",
    "image_file": "2010.07930v2-Figure5-1.png",
    "caption": " Segmentation results of surrogate losses for mIoU and BF1.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods produces the most accurate segmentation result?",
    "answer": "mIoU + BF1",
    "rationale": "The segmentation results of the mIoU + BF1 method are closest to the ground-truth image. This can be seen in the figure, where the segmented bottle in (d) is more accurate than the segmented bottles in (b) and (c).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.07930v2",
    "pdf_url": null
  },
  {
    "instance_id": "d8350fec3d81486e9532df0073d49a68",
    "figure_id": "2111.02434v3-Figure14-1",
    "image_file": "2111.02434v3-Figure14-1.png",
    "caption": " For the 2d mixture of Gaussians with an informative prior, we visualize the distribution of 500 chains after various numbers of gradient evaluations, comparing different samplers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is the most efficient in terms of gradient evaluations?",
    "answer": "HMC.",
    "rationale": "The HMC method requires the least number of gradient evaluations to converge to the target distribution. This can be seen in the figure, where the HMC method has already converged after 10 gradient evaluations, while the other methods require more.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.02434v3",
    "pdf_url": null
  },
  {
    "instance_id": "9616cbb3dbd14b329320281f2f475795",
    "figure_id": "2010.11545v1-Figure7-1",
    "image_file": "2010.11545v1-Figure7-1.png",
    "caption": " Images with different filters.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image shows the most detail?",
    "answer": "The original image (a) shows the most detail.",
    "rationale": "The original image is clear and sharp, while the other images have been altered with filters that reduce the amount of detail that can be seen.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.11545v1",
    "pdf_url": null
  },
  {
    "instance_id": "c4980e5e96ec4cb080d9aa5fa26d54e1",
    "figure_id": "1910.12252v1-Figure4-1",
    "image_file": "1910.12252v1-Figure4-1.png",
    "caption": " Mean Shift Experiment: Rejection rates (estimated from 300 trials) for the six tests with α = 0.05 is shown.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which test has the highest empirical True Positive Rate (TPR)?",
    "answer": "RelPSI KSD-U",
    "rationale": "The figure shows the empirical TPR for all six tests. The line for RelPSI KSD-U is consistently higher than the lines for the other tests, indicating that it has the highest TPR.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.12252v1",
    "pdf_url": null
  },
  {
    "instance_id": "2ed588de84074467ac78bb783320da4c",
    "figure_id": "1911.06465v3-Figure3-1",
    "image_file": "1911.06465v3-Figure3-1.png",
    "caption": " Mean normalized reduced spectra: 10242 (left), cropped 7682 (middle), and 2562 (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model produces images with the most similar spectral characteristics to real images?",
    "answer": "StyleGAN2",
    "rationale": "The figure shows the mean normalized reduced spectra for real images and images generated by different models. The StyleGAN2 curve is the closest to the real curve, indicating that it produces images with the most similar spectral characteristics to real images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.06465v3",
    "pdf_url": null
  },
  {
    "instance_id": "11053a2940b2434a8431c13631a37330",
    "figure_id": "2204.11218v2-Figure12-1",
    "image_file": "2204.11218v2-Figure12-1.png",
    "caption": " MLM dev loss and single task downstream performance of BERTBASE subnetworks. The results of TAMT are obtained from the masks along the training process, and the results of IMP and Rand are from different seeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which sparsity level has the best performance on MNLI ACC and SQUAD F1 tasks, across all initialization methods?",
    "answer": "0.6",
    "rationale": "- The MNLI ACC and SQUAD F1 subplots show that the highest values for these metrics are achieved at a sparsity level of 0.6, regardless of the initialization method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.11218v2",
    "pdf_url": null
  },
  {
    "instance_id": "625d9c585f674c79841e8db49c17ea8f",
    "figure_id": "1903.03793v1-Figure6-1",
    "image_file": "1903.03793v1-Figure6-1.png",
    "caption": " Comparison of convergence of importance ratios in some normalization layers across the network. These plots visualize the variance importance ratios in (layer3.0.norm2), (layer3.1.norm1), (layer3.1.norm2), (layer3.2.norm1), (layer3.3.norm1) and (layer3.4.norm1) of ResNet-50 respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which normalization layer has the fastest convergence of importance ratios?",
    "answer": "layer3.4.norm1",
    "rationale": "The figure shows the convergence of importance ratios for different normalization layers in ResNet-50. The x-axis represents the number of epochs, and the y-axis represents the importance ratio. The importance ratio is a measure of how important a particular input is to the output of the network. A higher importance ratio indicates that the input is more important. The figure shows that the importance ratios for layer3.4.norm1 converge to a higher value faster than the other layers. This indicates that layer3.4.norm1 has the fastest convergence of importance ratios.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.03793v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee1f63622a27487a99dc3db15cddbf34",
    "figure_id": "2310.14509v1-Figure8-1",
    "image_file": "2310.14509v1-Figure8-1.png",
    "caption": " Visualization of learned behaviors in GRF corner.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the win rate change over time?",
    "answer": "The win rate increases over time.",
    "rationale": "The plot shows the win rate over time. The win rate starts at around 0.5 and increases to around 0.8 by the end of the simulation. This indicates that the agent is learning to play the game better over time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.14509v1",
    "pdf_url": null
  },
  {
    "instance_id": "76ad7c330f87458b8bf0ff380186d854",
    "figure_id": "2006.07897v4-Figure2-1",
    "image_file": "2006.07897v4-Figure2-1.png",
    "caption": " Left: Test error of ResNet-18 on CIFAR-10. Right: Test error of ResNet-50 on Tiny ImageNet. The curves are averaged over 5 runs. Training data consumed is the same for SGD, rSGD and eSGD. Epochs are rescaled by y for rSGD and by L for eSGD (they are not rescaled for rSGD×y).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer achieved the lowest top-1 test error on CIFAR-10?",
    "answer": "rSGD (y=3)",
    "rationale": "The plot on the left shows the test error of ResNet-18 on CIFAR-10 for different optimizers. The rSGD (y=3) curve has the lowest test error at the end of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.07897v4",
    "pdf_url": null
  },
  {
    "instance_id": "ec051cfe674c43ba93fad3fb3abecf76",
    "figure_id": "2011.12010v1-Figure9-1",
    "image_file": "2011.12010v1-Figure9-1.png",
    "caption": " Calibration plots for the IMDB dataset (corresponding to the right half of Table 1) using N = 10 bins. BBB, VD and ST-τ are all quite well calibrated on this data set. The first run is displayed. Best viewed in colour.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods shown in the figure is the most well-calibrated on the IMDB dataset?",
    "answer": "BBB",
    "rationale": "The calibration plots show that BBB has the smallest gap between the perfect calibration line and the output calibration line. This means that the BBB method is the most accurate in predicting the probability of a sample belonging to a particular class.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.12010v1",
    "pdf_url": null
  },
  {
    "instance_id": "050978f687c14033a555264332630a78",
    "figure_id": "2106.01144v1-Figure4-1",
    "image_file": "2106.01144v1-Figure4-1.png",
    "caption": " The distribution of strategies at different conversation progress.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which strategy is used most frequently at the beginning of a conversation?",
    "answer": "Asking questions (Qu)",
    "rationale": "The figure shows that the proportion of questions asked is highest at the beginning of a conversation and decreases as the conversation progresses.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01144v1",
    "pdf_url": null
  },
  {
    "instance_id": "f87009db3c1a4a69b81da2f27104cef6",
    "figure_id": "2101.02115v1-Figure5-1",
    "image_file": "2101.02115v1-Figure5-1.png",
    "caption": " Accuracy on a common well-classified set under transfer attacks of increasing strength. Both VGG-OPU networks are more robust than the the baseline, and we observe a trade-off between robustness and natural accuracy with our defense. Higher/lower accuracies are ∼ 85%/80% for CIFAR10 and ∼ 72%/68% for CIFAR100. The baseline VGG-16 was trained to reach the higher accuracy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which network is the most robust to PGD attacks?",
    "answer": "VGG-OPU lower acc.",
    "rationale": "The figure shows the accuracy of different networks under PGD attacks of increasing strength. The VGG-OPU lower acc. network has the highest accuracy for all perturbation values, indicating that it is the most robust to PGD attacks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.02115v1",
    "pdf_url": null
  },
  {
    "instance_id": "60df5659191b49e5ae8616552769aeea",
    "figure_id": "2006.02703v3-Figure4-1",
    "image_file": "2006.02703v3-Figure4-1.png",
    "caption": " Speed comparison.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is the fastest for large networks?",
    "answer": "The proposed algorithm.",
    "rationale": "The figure shows that the proposed algorithm has the lowest time consumption for all network sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.02703v3",
    "pdf_url": null
  },
  {
    "instance_id": "60546f2a42fd490eace9b377b908284b",
    "figure_id": "2106.15845v2-Figure17-1",
    "image_file": "2106.15845v2-Figure17-1.png",
    "caption": " 10 generated molecules with the highest GSK3β scores. The numbers are QED, SA, GSK3β, and JNK3 scores, respectively. We highlight the GSK3β score in red among four different scores.",
    "figure_type": "** schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which molecule has the highest GSK3β score? ",
    "answer": " The molecule in the middle of the bottom row.",
    "rationale": " The GSK3β score is highlighted in red. The molecule in the middle of the bottom row has the highest GSK3β score of 0.94. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15845v2",
    "pdf_url": null
  },
  {
    "instance_id": "5e361caf41524f018bd69ef5058db99d",
    "figure_id": "2307.09555v1-Figure7-1",
    "image_file": "2307.09555v1-Figure7-1.png",
    "caption": " Rendered images and depths on the captured dataset for 5 views.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method appears to perform the best in terms of rendering realistic images and depths?",
    "answer": "The proposed method.",
    "rationale": "The proposed method produces images and depths that are most similar to the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.09555v1",
    "pdf_url": null
  },
  {
    "instance_id": "ffeb515f2ff54ba0860ce6b0823d6bf8",
    "figure_id": "1906.03700v4-Figure2-1",
    "image_file": "1906.03700v4-Figure2-1.png",
    "caption": " The Wasserstein distance against the number of iterations for the four considered algorithms, averaged over the three types of datasets ({m, k} = {2, 3}, {8, 9}, {16, 27}). The best learning rate is shown in the legend. 1000 iterations are for illustration purpose. The computational time per iteration is discussed in Table 3.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performs the best for the mixture of Gaussian dataset?",
    "answer": "W/M + DAda",
    "rationale": "The figure shows the Wasserstein distance against the number of iterations for the four considered algorithms, averaged over the three types of datasets. The W/M + DAda algorithm has the lowest Wasserstein distance for the mixture of Gaussian dataset, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.03700v4",
    "pdf_url": null
  },
  {
    "instance_id": "77abf2ec415941d58a05d1ce9615fb62",
    "figure_id": "1907.00921v1-Figure3-1",
    "image_file": "1907.00921v1-Figure3-1.png",
    "caption": " Prepare Lunch Task. Shows performance (test accuracy with standard error) for each AL strategy under different environmentally constrained conditions. Parameters of allocated time and query budget imposed on the learner vary, with: (a) only time constrained [budget: high (500), time: low (40)], (b) neither time nor query budget constrained [budget: high (500), time: high (150)], (c) both time and query budget constrained [budget: low (25), time: low (40)], and (d) only query budget constrained [budget: low (25), time: high (150)].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Under which condition did the DT-task-env algorithm perform the best?",
    "answer": "Unconstrained (Condition 4)",
    "rationale": "The figure shows that the DT-task-env algorithm achieved the highest accuracy in the shortest amount of time under the unconstrained condition.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.00921v1",
    "pdf_url": null
  },
  {
    "instance_id": "2ee3ef7934074108837cfcbd4eb66aee",
    "figure_id": "2006.16495v2-Figure6-1",
    "image_file": "2006.16495v2-Figure6-1.png",
    "caption": " Training and testing accuracy for different models (all samples, 20% noise)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieved the highest accuracy on the training set?",
    "answer": "TbV60000",
    "rationale": "The top plot shows the training accuracy for each model. The green line, which represents TbV60000, is the highest of the three lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.16495v2",
    "pdf_url": null
  },
  {
    "instance_id": "73d95da9270a44818abf25be3c4c6d38",
    "figure_id": "2306.02561v3-Figure3-1",
    "image_file": "2306.02561v3-Figure3-1.png",
    "caption": " The architectures of typical reranking methods. x is an input and yi is a certain candidate, and its score is si. MLM-Scoring is an unsupervised method that uses an external masked LM to score a candidate; SimCLS uses the same encoder to encode x and each candidate yi; SummaReranker instead employs a cross-encoder to encode both x and yi at the same time; PAIRRANKER encodes a pair of candidates (yi, yj) at the same time for pairwisely scoring them, and the final score of each candidate is produced as shown in Fig. 4.",
    "figure_type": "** \n\nSchematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \n\nWhich of the four reranking methods shown in the figure is pairwise?",
    "answer": " \n\nPAIRRANKER",
    "rationale": " \n\nThe figure shows that PAIRRANKER encodes a pair of candidates (yi, yj) at the same time for pairwisely scoring them, and the final score of each candidate is produced as shown in Fig. 4. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.02561v3",
    "pdf_url": null
  },
  {
    "instance_id": "0550dafab4be497fa523b06c6f3c92d3",
    "figure_id": "2001.03615v2-Figure4-1",
    "image_file": "2001.03615v2-Figure4-1.png",
    "caption": " Visualizations of attention maps overlaid on images produced by VQA models [16]. Source images taken from COCO [27] to compare against bottom-up attention [2] on VQA 2.0 [11]. We show questions (Q), ground-truth answers (GT-A), and side-by-side predictions (attention maps, answers) of region (R) and grid (G) features. From left to right: (a) both region and grid features give correct answers, (b) region features give correct answers but grid features fail, (c) region features fail but grid features give correct answers, and (d) both region and grid features fail. Best viewed in color.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which of the four examples shown does the grid feature model give a correct answer while the region feature model gives an incorrect answer?",
    "answer": "Example (c)",
    "rationale": "In example (c), the region feature model incorrectly predicts that the person is texting, while the grid feature model correctly predicts that the person is cutting. This can be seen by comparing the attention maps and predicted answers for each model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.03615v2",
    "pdf_url": null
  },
  {
    "instance_id": "7a0fbbeef83d4c77b321ef3c117bbf25",
    "figure_id": "1904.03483v2-Figure5-1",
    "image_file": "1904.03483v2-Figure5-1.png",
    "caption": " Examples of point clouds aligned by CSDRSAC. From top to bottom: T-rex; Dragon; Armadillo",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the purpose of CSDRSAC?",
    "answer": "CSDRSAC is a method for aligning point clouds.",
    "rationale": "The figure shows three different point clouds that have been aligned using CSDRSAC. The point clouds are of a T-rex, a dragon, and an armadillo. The fact that the point clouds are aligned indicates that CSDRSAC is able to correctly align different point clouds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.03483v2",
    "pdf_url": null
  },
  {
    "instance_id": "900b42d6e00f4ed2a950143532be39f9",
    "figure_id": "2210.03308v1-Figure11-1",
    "image_file": "2210.03308v1-Figure11-1.png",
    "caption": " Top-K performance of baselines after each update for horizon H ∈ {8, 16, 32, 64, 128}.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the best Top-K performance for all horizons?",
    "answer": "GAflowNet.",
    "rationale": "The figure shows the Top-K performance of different algorithms for different horizons. For all horizons, GAflowNet has the highest Top-K performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.03308v1",
    "pdf_url": null
  },
  {
    "instance_id": "4128b4d94ade4ba6b9d05972d9983bc6",
    "figure_id": "2112.06598v2-Figure6-1",
    "image_file": "2112.06598v2-Figure6-1.png",
    "caption": " Comparison of German GPT-2 models trained with WECHSEL, TransInner and TransInnerShuffleEmb between freezing non-embedding parameters at the start and not freezing any parameters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performed best when no parameters were frozen?",
    "answer": "TransInnerShuffleEmb-GPT2",
    "rationale": "The TransInnerShuffleEmb-GPT2 (no freeze) line is the lowest line on the plot, indicating that it had the lowest perplexity, which is a measure of how well a model predicts a sequence of words.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.06598v2",
    "pdf_url": null
  },
  {
    "instance_id": "8dbd4f7ef069482497ed7ef60abb0749",
    "figure_id": "2101.08367v3-Figure1-1",
    "image_file": "2101.08367v3-Figure1-1.png",
    "caption": " Average Kendall’s Tau (±std) (left) and the Jaccard index (±std) (right) calculated from true and estimated influence on ALL, IS, and FID.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which metric is more sensitive to the number of tracing back epochs, Kendall's Tau or the Jaccard index?",
    "answer": " Kendall's Tau. ",
    "rationale": " The figure shows that the standard deviation of Kendall's Tau decreases more rapidly with increasing number of tracing back epochs than the standard deviation of the Jaccard index. This indicates that Kendall's Tau is more sensitive to the number of tracing back epochs. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.08367v3",
    "pdf_url": null
  },
  {
    "instance_id": "7a578604607b4ec0bb17f1c9909c832e",
    "figure_id": "2202.07646v3-Figure7-1",
    "image_file": "2202.07646v3-Figure7-1.png",
    "caption": " We prompt OPT models with data sampled from their training set. We use a prompt length of 100 here. (a) Fraction of sequences extracted as a function of model scale. (b) Fraction of sequences extracted as the number of repetitions of that sequence in the training set increases.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model size extracts the most sequences?",
    "answer": "The 66B model size extracts the most sequences.",
    "rationale": "Figure (a) shows the fraction of sequences extracted as a function of model size. The 66B model size has the highest fraction of sequences extracted, at approximately 0.22.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.07646v3",
    "pdf_url": null
  },
  {
    "instance_id": "4eaca543a8ea4cf69e88bd9f4dc6f94f",
    "figure_id": "2005.04966v5-Figure5-1",
    "image_file": "2005.04966v5-Figure5-1.png",
    "caption": " Adjusted mutual information score between the clusterings generated by PCL and the ground-truth labels for ImageNet training data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the adjusted mutual information (AMI) score change with the number of epochs?",
    "answer": "The AMI score increases with the number of epochs.",
    "rationale": "The figure shows that the AMI score for all three values of K (25000, 50000, and 100000) increases with the number of epochs. This indicates that the clustering generated by PCL becomes more similar to the ground-truth labels as the number of epochs increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.04966v5",
    "pdf_url": null
  },
  {
    "instance_id": "1d570117e37b4ba286516e0bfb4d3e26",
    "figure_id": "1908.07387v1-Figure6-1",
    "image_file": "1908.07387v1-Figure6-1.png",
    "caption": " Precision-Recall curve when filtering noisy data. Each curve represents the filtering performance of PL, NL, NL→SelNL, and NL→SelNL→SelPL (SelNLPL).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest precision for a recall of 0.7?",
    "answer": "NL→SelNL→SelPL",
    "rationale": "The precision-recall curve for NL→SelNL→SelPL is the highest at a recall of 0.7. This means that this method is able to correctly identify more relevant documents than the other methods for a given level of recall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.07387v1",
    "pdf_url": null
  },
  {
    "instance_id": "0bfdeb897cc94d249694adfcbec5edf3",
    "figure_id": "2306.04235v1-Figure10-1",
    "image_file": "2306.04235v1-Figure10-1.png",
    "caption": " Proportions of different operations (except GEMM) on the Transformer-base model.",
    "figure_type": "\"schematic\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the proportion of attention operations in the Transformer-base model?",
    "answer": "Approximately 60%.",
    "rationale": "The figure shows that the proportion of attention operations is the largest among all the operations, and it takes up about 60% of the total operations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.04235v1",
    "pdf_url": null
  },
  {
    "instance_id": "83a7fca236584682a386abc3060ee843",
    "figure_id": "1711.04297v2-Figure1-1",
    "image_file": "1711.04297v2-Figure1-1.png",
    "caption": " (a) and (b) are two different data graphs, but both of them correspond to the same line graph (a triangle). (c) and (d) are two weighting schemes for a complete graph formed by points X1, X2, X3, X4. Solid line means its weight p > 0 while dash line means p = 0. (c): Weight every example equally. (d): Only the two examples in an independent subset get equally non-zero weights and other weights are 0 (dashed line). Note that maxi=1,...,n ∑ j:(i,j)∈E pi,j of these two weighting schemes are the same, but (c) has the tighter risk bounds, as ‖p‖2, ‖p‖max and ‖p‖∞ of (c) are smaller than that of (d) respectively.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which weighting scheme in the figure has the tighter risk bounds?",
    "answer": "Weighting scheme (c) has the tighter risk bounds.",
    "rationale": "The caption states that \"maxi=1,...,n ∑ j:(i,j)∈E pi,j of these two weighting schemes are the same, but (c) has the tighter risk bounds, as ‖p‖2, ‖p‖max and ‖p‖∞ of (c) are smaller than that of (d) respectively.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.04297v2",
    "pdf_url": null
  },
  {
    "instance_id": "5419b354d0104af2bc9317461d819750",
    "figure_id": "1711.08624v1-Figure9-1",
    "image_file": "1711.08624v1-Figure9-1.png",
    "caption": " Cumulative errors distributions on Large dataset. The x-axis is the normalized mean error (NME), and the yaxis indicates the percentage of images on which NMEs are lower than the x value.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the Large dataset?",
    "answer": "SR-DAN-1000.",
    "rationale": "The figure shows the cumulative error distributions for four different methods on the Large dataset. The x-axis is the normalized mean error (NME), and the y-axis indicates the percentage of images on which NMEs are lower than the x value. The SR-DAN-1000 curve is the highest, indicating that it has the lowest NME for the most images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.08624v1",
    "pdf_url": null
  },
  {
    "instance_id": "3c9b34f151654975a76195585b9e0904",
    "figure_id": "1903.00070v4-Figure20-1",
    "image_file": "1903.00070v4-Figure20-1.png",
    "caption": " Each column corresponds to one example from the 3-link snake problem (5d). The top and the bottom rows are the search trees produced by the RRT* and NEXT-KS, respectively. In the figures, obstacles are colored in deep blue, and the rigid bodies are represented with matchsticks. The samples, starting states, and goal states are denoted by yellow, orange, and brown matchsticks, respectively. Edges are colored in green. We set the maximum number of samples to be 500.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm produces a more efficient search tree for the 3-link snake problem?",
    "answer": "NEXT-KS",
    "rationale": "The NEXT-KS search trees are generally less dense and have fewer edges than the RRT* search trees. This indicates that NEXT-KS is able to find a solution to the problem with fewer samples, which is more efficient.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.00070v4",
    "pdf_url": null
  },
  {
    "instance_id": "b288cfe69e51439db5902f519c40ac2c",
    "figure_id": "2303.12703v1-Figure6-1",
    "image_file": "2303.12703v1-Figure6-1.png",
    "caption": " Run time results for N-BF-ADMG and CAM-UV on a 12 variable synthetic ER dataset. The figure shows mean results ± standard deviation across five randomly generated datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has a faster runtime when the number of data points is small?",
    "answer": "N-BF-AFMG-G.",
    "rationale": "The plot shows that the runtime of N-BF-AFMG-G is almost constant, while the runtime of CAM-UV increases as the number of data points increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.12703v1",
    "pdf_url": null
  },
  {
    "instance_id": "0cca575ed9a44afd865c4cbe8220e1d5",
    "figure_id": "2212.13468v1-Figure3-1",
    "image_file": "2212.13468v1-Figure3-1.png",
    "caption": " Performance comparison for the compression (σ ≡ sign) of an isotropic Gaussian source.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieves the lowest population risk for a given rate?",
    "answer": "VAMP",
    "rationale": "The VAMP curve is the lowest of all the curves in the figure, which means that it achieves the lowest population risk for a given rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.13468v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee64590858114a29af346e4f7e83e584",
    "figure_id": "2302.04496v1-Figure4-1",
    "image_file": "2302.04496v1-Figure4-1.png",
    "caption": " Ford-Fulkerson validation loss for MPNN processors.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which MPNN processor has the lowest validation loss?",
    "answer": "MPNN-max (pipe)",
    "rationale": "The figure shows the validation loss for different MPNN processors. The MPNN-max (pipe) processor has the lowest validation loss, as its line is the lowest on the graph.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.04496v1",
    "pdf_url": null
  },
  {
    "instance_id": "fb470ef0d2364830948c2b2a94218d44",
    "figure_id": "2111.11297v2-Figure6-1",
    "image_file": "2111.11297v2-Figure6-1.png",
    "caption": " Performance across lengths of passages in terms of words. First bin contains very little samples to be significant.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between passage length and F1 score?",
    "answer": "The F1 score increases with passage length.",
    "rationale": "The figure shows that the F1 score is highest for the longest passage length bin and lowest for the shortest passage length bin.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.11297v2",
    "pdf_url": null
  },
  {
    "instance_id": "a89d9da847804ee79f6be14181fdc5c4",
    "figure_id": "2306.06368v1-Figure7-1",
    "image_file": "2306.06368v1-Figure7-1.png",
    "caption": " The first three subfigures on the left: The average increase in the truss size of each considered algorithm",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm consistently produces the largest trusses?",
    "answer": "The proposed algorithm.",
    "rationale": "The bars for the proposed algorithm are consistently taller than the bars for the other algorithms in the first three subfigures. This indicates that the proposed algorithm consistently produces larger trusses than the other algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.06368v1",
    "pdf_url": null
  },
  {
    "instance_id": "425bd10626f5494d976a710b4d24b7e4",
    "figure_id": "2109.04825v2-Figure2-1",
    "image_file": "2109.04825v2-Figure2-1.png",
    "caption": " The results of the robustness experiments. Xaxis=GPT-2 model size. Y-axis=Accuracy score.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is the most robust to input perturbations?",
    "answer": "Topological features",
    "rationale": "The figure shows that the accuracy of the Topological features model decreases the least as the size of the GPT-2 model increases. This suggests that the Topological features model is the most robust to input perturbations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.04825v2",
    "pdf_url": null
  },
  {
    "instance_id": "2b0edbf86d094fbb9dd80b5dfbae2b17",
    "figure_id": "1907.08971v2-Figure1-1",
    "image_file": "1907.08971v2-Figure1-1.png",
    "caption": " Relative decrease in error rate (%) between argument length baseline and EviConvNet (reason codes defined in Table 4).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which reason code shows the largest decrease in error rate?",
    "answer": "C9-4",
    "rationale": "The bar corresponding to C9-4 is the highest in the figure, indicating that the error rate for this reason code decreased the most when using EviConvNet compared to the argument length baseline.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.08971v2",
    "pdf_url": null
  },
  {
    "instance_id": "c70ae637a7354f368032b50584134af3",
    "figure_id": "2301.12860v1-Figure5-1",
    "image_file": "2301.12860v1-Figure5-1.png",
    "caption": " JFT-4B ViT memory usage. A black rectangle means that for the given number of MC samples and batch size the method goes out of memory. Plotted in grey are the cells where HET-H and HET-XL fit in memory but HET does not.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods requires the most memory for a batch size of 8 and 8 MC samples?",
    "answer": "HET-XL",
    "rationale": "The black rectangle in the HET-XL plot indicates that the method goes out of memory for a batch size of 8 and 8 MC samples. The other two methods, HET and HET-H, do not go out of memory for these parameters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.12860v1",
    "pdf_url": null
  },
  {
    "instance_id": "6983f0a11a824eacbcd7c8027dd0dc05",
    "figure_id": "2202.06200v2-Figure3-1",
    "image_file": "2202.06200v2-Figure3-1.png",
    "caption": " Performance of NCL on two datasets without structural neighbors and semantic neighbors (Recall@10).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best on the Yelp dataset?",
    "answer": "NCL performs the best on the Yelp dataset.",
    "rationale": "The figure shows the Recall@10 for different methods on the Yelp dataset. NCL has the highest Recall@10, which indicates that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.06200v2",
    "pdf_url": null
  },
  {
    "instance_id": "77c94a6e158d49f19135703859f6b553",
    "figure_id": "2206.09491v3-Figure4-1",
    "image_file": "2206.09491v3-Figure4-1.png",
    "caption": " Performance of the BaRT defense on ImageNette with different numbers of transformations before and after fine-tuning the model. While the model achieves higher invariance, the defense becomes nearly ineffective3, as evident from the top solid red curves in (a) and (c).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the performance of the BaRT defense change after fine-tuning the model?",
    "answer": "The defense becomes nearly ineffective.",
    "rationale": "The top solid red curves in (a) and (c) show that the attack success rate is very high even after fine-tuning the model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.09491v3",
    "pdf_url": null
  },
  {
    "instance_id": "2e99e96a19bd430eae27bdba68fa1405",
    "figure_id": "2010.02656v1-Figure3-1",
    "image_file": "2010.02656v1-Figure3-1.png",
    "caption": " The impact of the number of Bi-LSTM layers and the softmax activation function.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model architecture performs best on the Rest14-hard dataset?",
    "answer": "AC-MIMLLN with 1 layer Bi-LSTM.",
    "rationale": "The figure shows the accuracy of different model architectures on three different datasets. For the Rest14-hard dataset, the AC-MIMLLN model with 1 layer Bi-LSTM achieves the highest accuracy of 65.28%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02656v1",
    "pdf_url": null
  },
  {
    "instance_id": "8cc1e4877f714e68ad7077797a451697",
    "figure_id": "1810.10999v1-Figure20-1",
    "image_file": "1810.10999v1-Figure20-1.png",
    "caption": " Training/validation perplexity for a 2-layer, 600-hidden unit encoder-decoder architecture, with attention over a 60-dimensional slice of the hidden state, and 5 bit forgetting. Left: RevGRU. Right: RevLSTM.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which architecture, RevGRU or RevLSTM, has lower perplexity on the validation set?",
    "answer": "RevLSTM",
    "rationale": "The orange line in the right plot represents the validation perplexity for RevLSTM, and it is lower than the orange line in the left plot, which represents the validation perplexity for RevGRU.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.10999v1",
    "pdf_url": null
  },
  {
    "instance_id": "685abdc050ce44dd86a7838d42c9f1b6",
    "figure_id": "1905.11096v2-Figure6-1",
    "image_file": "1905.11096v2-Figure6-1.png",
    "caption": " Type III error rates in directional 2-tailed tests at α = 0.05. Plots on the same column correspond to the same topic set size, and plots on the same row correspond to the same effectiveness measure. When indiscernible, the t , permutation and bootstrap tests overlap.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which test has the lowest Type III error rate for all topic set sizes and effectiveness measures?",
    "answer": "The Wilcoxon test.",
    "rationale": "The Wilcoxon test (green line) consistently has the lowest Type III error rate across all plots in the figure, regardless of the topic set size or effectiveness measure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11096v2",
    "pdf_url": null
  },
  {
    "instance_id": "bbb389e331d444e0aee8ca2793969aa3",
    "figure_id": "2110.11940v2-Figure9-1",
    "image_file": "2110.11940v2-Figure9-1.png",
    "caption": " Heatmaps showing ANDIL, ANDAIL, their difference, and their relative difference.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four panels shows the largest difference between ANDIL and ANDAIL?",
    "answer": "The bottom right panel.",
    "rationale": "The bottom right panel shows the relative difference between ANDIL and ANDAIL, which is calculated by dividing the absolute value of their difference by the absolute value of ANDIL. This panel has the largest range of values, indicating the largest difference between the two measures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.11940v2",
    "pdf_url": null
  },
  {
    "instance_id": "647f8c4e9de642ec910c13191801a790",
    "figure_id": "2107.06427v1-Figure2-1",
    "image_file": "2107.06427v1-Figure2-1.png",
    "caption": " Comparison for different model variants w.r.t. K.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best for movies when K=4?",
    "answer": "MetaTL",
    "rationale": "The figure shows the MRR for different models on the movie dataset. When K=4, MetaTL has the highest MRR.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.06427v1",
    "pdf_url": null
  },
  {
    "instance_id": "b7e3f7a020a34c1889f60470c2aab8f4",
    "figure_id": "1905.11926v4-Figure6-1",
    "image_file": "1905.11926v4-Figure6-1.png",
    "caption": " The mean IoU, pixel-wise accuracy and training loss on the Cityscapes dataset using DeepLabV3 with a ResNet-50 backbone. In our setting, we modified all the convolution layers to remove batch normalizations and insert deconvolutions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer achieves higher mean IoU on the Cityscapes dataset?",
    "answer": "Decoupled.",
    "rationale": "The figure shows that the orange line (Decoupled) is consistently higher than the blue line (SGD + BN) in the plot of validation mean IoU vs. epochs. This indicates that Decoupled achieves higher mean IoU on the Cityscapes dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.11926v4",
    "pdf_url": null
  },
  {
    "instance_id": "495456797fef4f88b7a8d3e57e8a25f4",
    "figure_id": "1903.03094v1-Figure17-1",
    "image_file": "1903.03094v1-Figure17-1.png",
    "caption": " t-SNE Visualization of Starspace embeddings learned directly from the LIGHT Dataset. Color denotes each element type, either location, character, action, or object. We select four neighborhoods to explore, for each of the base element types: “Dock” (location), “painters” (character), “hug horse” (action), and “pillows” (object).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the four types of elements in the LIGHT dataset?",
    "answer": "location, character, action, object",
    "rationale": "The caption states that \"Color denotes each element type, either location, character, action, or object.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.03094v1",
    "pdf_url": null
  },
  {
    "instance_id": "87d0fbc35e1943c9919b65e0d86d4114",
    "figure_id": "2204.14069v1-Figure4-1",
    "image_file": "2204.14069v1-Figure4-1.png",
    "caption": " TP99 latency in real online CTR prediction system w.r.t length of exposure sequence.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which algorithm has the lowest TP99 latency when the maximum length of the user exposure sequence is 10?",
    "answer": " UIM + Gamma-Avg",
    "rationale": " The figure shows that the TP99 latency for UIM + Gamma-Avg is the lowest among all algorithms when the maximum length of the user exposure sequence is 10.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.14069v1",
    "pdf_url": null
  },
  {
    "instance_id": "d60e2fa91ded42dd8e150d68ae965797",
    "figure_id": "2101.06605v3-Figure4-1",
    "image_file": "2101.06605v3-Figure4-1.png",
    "caption": " Qualitative results on SAPIEN [79] dataset. On the left-most column, we show reference rendering of the objects we tackle. Each method span two columns and the point colors show the motion segmentation. For our method, we additionally show the registration result as the last column, where the darkness of the points shows the point cloud index it comes from.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method seems to have the best results?",
    "answer": "Our method seems to have the best results.",
    "rationale": "The results of our method are shown in the last column of the figure. The point clouds are well-aligned and the segmentation is accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.06605v3",
    "pdf_url": null
  },
  {
    "instance_id": "3afc274202d4445493c8327f415a9438",
    "figure_id": "2010.04230v3-Figure13-1",
    "image_file": "2010.04230v3-Figure13-1.png",
    "caption": " Class-conditional samples from CIFAR10",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " How many classes are represented in the image?",
    "answer": " 10",
    "rationale": " The caption states that the image shows class-conditional samples from CIFAR10, which is a dataset containing 10 classes. \n\n**Figure type:** photograph(s)",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.04230v3",
    "pdf_url": null
  },
  {
    "instance_id": "4b5570f0106449fba6ddd8d903e22cde",
    "figure_id": "1905.12753v1-Figure1-1",
    "image_file": "1905.12753v1-Figure1-1.png",
    "caption": " Construction of a path (dashed line) in P . The triangle nodes are triangle caplets and the square nodes are edge caplets.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many edge caplets are present in the path?",
    "answer": "3",
    "rationale": "The path is indicated by the dashed line. It passes through 3 square nodes, which represent edge caplets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12753v1",
    "pdf_url": null
  },
  {
    "instance_id": "4959f8f98c1047aebcc7781f4d8cae3a",
    "figure_id": "2009.14306v2-Figure10-1",
    "image_file": "2009.14306v2-Figure10-1.png",
    "caption": " Seeker’s chat interface after they finish the chat, and the Recommender sends over the movie. Seeker can choose to accept or reject.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the Seeker's role in this chat?",
    "answer": "The Seeker is looking for a movie trailer.",
    "rationale": "The chat interface shows that the Seeker is looking for a movie trailer and that the Recommender is providing them with recommendations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.14306v2",
    "pdf_url": null
  },
  {
    "instance_id": "5d909da3d7c941d49ce63a4a6f14d293",
    "figure_id": "1809.10877v5-Figure1-1",
    "image_file": "1809.10877v5-Figure1-1.png",
    "caption": " Reliability diagrams of VGG-16 models trained with baseline, CI (ours) and VWCI (ours) losses in Tiny ImageNet dataset. This diagram shows expected accuracy as a function of confidence, i.e., classification score. ECE (Expected Calibration Error) denotes the average gap between confidence and expected accuracy. The proposed algorithm (VWCI) achieves well-calibrated results compared to the baseline and the best estimate by a simpler version of ours (CI).",
    "figure_type": "",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method produces the most accurate predictions? ",
    "answer": " VWCI",
    "rationale": " The figure shows the accuracy of predictions made by three different methods: baseline, CI, and VWCI. The VWCI method has the highest accuracy, as its bars are the tallest in the figure.\n\n**Figure type:** Plot",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.10877v5",
    "pdf_url": null
  },
  {
    "instance_id": "6093919fb20d4be78bb358c80d82802e",
    "figure_id": "2006.06922v1-Figure4-1",
    "image_file": "2006.06922v1-Figure4-1.png",
    "caption": " KKBOX(N)’s item embedding distributions under different learning mechanisms show that, the song embeddings learned by the MTL keep close distances across different groups (genres) on the premise of discriminating different groups. It conforms to the fact that the successive songs in a session may belong to different genres.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three learning mechanisms (Word2Vec, TransH, or MKM-SR) produces the most distinct clusters of music genres?",
    "answer": "MKM-SR",
    "rationale": "The figure shows that MKM-SR produces the most distinct clusters of music genres. This is because the song embeddings learned by MKM-SR keep close distances across different groups (genres) on the premise of discriminating different groups. This can be seen in the figure, where the different music genres are clearly separated from each other.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06922v1",
    "pdf_url": null
  },
  {
    "instance_id": "a01937844c04460aa2b9e7440b9e4242",
    "figure_id": "2202.13843v2-Figure13-1",
    "image_file": "2202.13843v2-Figure13-1.png",
    "caption": " More GradCAM visualization results. The results indicate that our network tends to focus on globally consistent traces.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the generative adversarial networks (GANs) shown in the figure appears to focus on globally consistent traces?",
    "answer": "InfoMaxGAN",
    "rationale": "The figure shows GradCAM visualization results for different GANs. GradCAM is a technique that highlights the regions of an image that are most important for a particular prediction. The results for InfoMaxGAN show that the network is focusing on globally consistent traces, such as the overall shape of the face and the position of the eyes and nose. This suggests that InfoMaxGAN is able to generate images that are more realistic and consistent with the input data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.13843v2",
    "pdf_url": null
  },
  {
    "instance_id": "b3bf381dde134d39b934fd48da0923fd",
    "figure_id": "1906.08387v1-Figure4-1",
    "image_file": "1906.08387v1-Figure4-1.png",
    "caption": " The evolution of TD error (top), timestep difference (middle), and reward of the transition (bottom) with respect to timestep. Timestep difference is the difference between the training timestep and the timestep at which the transition is obtained (that is, low timestep difference means that the agent tends to use recent transitions for the updates). The average values over transitions are plotted for clearer visualization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest reward?",
    "answer": "Vanilla-DDPG",
    "rationale": "The bottom plot shows the reward of the transition with respect to timestep. The Vanilla-DDPG line is the highest of the three lines, indicating that it has the highest reward.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.08387v1",
    "pdf_url": null
  },
  {
    "instance_id": "8fa12a5927ab4e888c2823a52750f887",
    "figure_id": "1612.06915v2-Figure2-1",
    "image_file": "1612.06915v2-Figure2-1.png",
    "caption": " Value estimates for self-play in Leduc hold’em",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which estimator has the lowest standard deviation?",
    "answer": "\\(P_a=\\{p_c, x, y\\}\\)",
    "rationale": "The table shows the standard deviation for each estimator in the second column. The estimator with the lowest standard deviation is \\(P_a=\\{p_c, x, y\\}\\), with a standard deviation of 0.000377.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1612.06915v2",
    "pdf_url": null
  },
  {
    "instance_id": "b1f7a976d32346f1ac6f7e74716a0e7d",
    "figure_id": "1905.08212v1-Figure1-1",
    "image_file": "1905.08212v1-Figure1-1.png",
    "caption": " Development set perplexity vs. training steps. Top left: aze. Top right: bel. Bottom left: glg. Bottom right: slk.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language shows the most stable perplexity across the training steps?",
    "answer": "slk",
    "rationale": "The bottom right plot for slk shows the least amount of variation in perplexity across the training steps.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.08212v1",
    "pdf_url": null
  },
  {
    "instance_id": "25b90bce092a4b699ed7332fb99d7e46",
    "figure_id": "2005.00661v1-Figure1-1",
    "image_file": "2005.00661v1-Figure1-1.png",
    "caption": " Hallucinations in extreme document summarization: the abbreviated article, its gold summary and the abstractive model generated summaries (PTGEN, See et al. 2017; TCONVS2S, Narayan et al. 2018a; and, GPTTUNED, TRANS2S and BERTS2S, Rothe et al. 2020) for a news article from the extreme summarization dataset (Narayan et al., 2018a). The dataset and the abstractive models are described in Section 3 and 4. We also present the [ROUGE-1, ROUGE-2, ROUGE-L] F1 scores relative to the reference gold summary. Words in red correspond to hallucinated information whilst words in blue correspond to faithful information.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model generated the most faithful summary, according to the ROUGE scores?",
    "answer": "BERTS2S.",
    "rationale": "The ROUGE scores for BERTS2S are the highest among the models listed, indicating that its summary is the most similar to the gold summary.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.00661v1",
    "pdf_url": null
  },
  {
    "instance_id": "90110a6bc5ce4788b624d828031299e5",
    "figure_id": "1905.10496v2-Figure4-1",
    "image_file": "1905.10496v2-Figure4-1.png",
    "caption": " Posterior Triggering Kernels Inferred By VBHP and Gibbs Hawkes. Results of Gibbs Hawkes are obtained in 2000 iterations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, Gibbs-Hawkes or VBHP, more accurately estimates the true triggering kernel?",
    "answer": "VBHP.",
    "rationale": "The green dotted line in the figure represents the true triggering kernel. The red and blue lines represent the triggering kernels estimated by VBHP and Gibbs-Hawkes, respectively. We can see that the red line is closer to the green line than the blue line, indicating that VBHP provides a more accurate estimate of the true triggering kernel.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10496v2",
    "pdf_url": null
  },
  {
    "instance_id": "b4d3a51f7158456fa879932fef89396d",
    "figure_id": "2210.00093v1-Figure6-1",
    "image_file": "2210.00093v1-Figure6-1.png",
    "caption": " Design of Shockwave showing how the different components interact with each other to derive a schedule.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two main inputs to the Bayesian Predictive Model for Dynamic Batch Size Scaling?",
    "answer": "Training progress and batch size history.",
    "rationale": "The figure shows that the Bayesian Predictive Model receives information about the training progress of a job and its batch size history. This information is used to predict the optimal batch size for the job.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.00093v1",
    "pdf_url": null
  },
  {
    "instance_id": "206d396ccb144eb8bdc0d9bc232219b9",
    "figure_id": "2006.08598v1-Figure1-1",
    "image_file": "2006.08598v1-Figure1-1.png",
    "caption": " F1-Score vs. Privacy Budget.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest F1 score for the earthquake dataset?",
    "answer": "Priv-PC",
    "rationale": "The figure shows the F1 score for different methods on different datasets. The Priv-PC method achieves the highest F1 score for the earthquake dataset, as shown in the top left panel of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.08598v1",
    "pdf_url": null
  },
  {
    "instance_id": "73a3fe462c5943aaa9a79918aac2e66a",
    "figure_id": "1902.04790v1-Figure2-1",
    "image_file": "1902.04790v1-Figure2-1.png",
    "caption": " First-Come First-Served (FCFS) policy compared to",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In the web preemption scenario, what is the waiting time for Q2?",
    "answer": "The waiting time for Q2 is 5 time units.",
    "rationale": "In the web preemption scenario, Q2 starts execution at time 33 and completes at time 38. However, it had to wait for Q1 to finish execution, which started at time 0 and completed at time 30. Therefore, the waiting time for Q2 is 33 - 30 = 5 time units.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.04790v1",
    "pdf_url": null
  },
  {
    "instance_id": "9126644013fe40498121f1006c9fa264",
    "figure_id": "2109.02099v1-Figure5-1",
    "image_file": "2109.02099v1-Figure5-1.png",
    "caption": " Improvements of AUC and micro-F1 after removing FN in the test set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest AUC and micro-F1 score after removing FN in the test set?",
    "answer": "FAN",
    "rationale": "The figure shows that the FAN model has the highest bars for both AUC and micro-F1 after removing FN in the test set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.02099v1",
    "pdf_url": null
  },
  {
    "instance_id": "764ccdb321774128b645b1f730a98032",
    "figure_id": "1911.09602v2-Figure5-1",
    "image_file": "1911.09602v2-Figure5-1.png",
    "caption": " Two different captions containing the phrase “many people.” In both cases, the VQ4 layer infers the same unit sequence (872, 360, 712, middle transcription) beneath the phrase. The VQ3 units are somewhat noisier, but contain the common subsequence (956, 265, 80, 401, 262, 762, 246, 774, 828, 386, bottom transcription).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer of the model is more robust to noise, VQ3 or VQ4?",
    "answer": "VQ4 is more robust to noise than VQ3.",
    "rationale": "The caption states that the VQ4 layer infers the same unit sequence for the phrase \"many people\" in both cases, while the VQ3 units are somewhat noisier. This suggests that VQ4 is less sensitive to noise than VQ3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.09602v2",
    "pdf_url": null
  },
  {
    "instance_id": "119dc1729e19446eb709414ba5bbd8e0",
    "figure_id": "2308.11551v2-Figure1-1",
    "image_file": "2308.11551v2-Figure1-1.png",
    "caption": " An example case of multi-event videos from ActivityNet [4]. The video depicts a sequence of unrelated and discontinuous events, including the progression “a girl is sitting on the beach” → “a young man is practicing tightrope walking” → “a scene of sunset by the beach.” Each textual caption only corresponds to a fragment of the video. Such short and specific textual captions are prevalent in our everyday video data and constitute a common video-text retrieval scenario.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the three events depicted in the video?",
    "answer": "The three events are unrelated and discontinuous.",
    "rationale": "The figure shows three separate events: a girl sitting on the beach, a young man practicing tightrope walking, and a scene of sunset by the beach. There is no apparent connection between these events, and they are not shown to be happening in any particular order.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.11551v2",
    "pdf_url": null
  },
  {
    "instance_id": "860bf4c7c1614b83a6591bec71401f66",
    "figure_id": "2303.15015v2-Figure5-1",
    "image_file": "2303.15015v2-Figure5-1.png",
    "caption": " The changes of average forgetting (AF) (%) on three datasets with the increased tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Yelp dataset?",
    "answer": "TGN+BiC",
    "rationale": "The figure shows the average forgetting (AF) for different models on three datasets. The AF for TGN+BiC is the lowest on the Yelp dataset, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.15015v2",
    "pdf_url": null
  },
  {
    "instance_id": "2af96094d3cf49f9924fd5fda841d4d5",
    "figure_id": "2210.02177v2-Figure4-1",
    "image_file": "2210.02177v2-Figure4-1.png",
    "caption": " Sequential optimization performance in terms of hypervolume versus function evaluations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the DTLZ7 benchmark with M=3 objectives?",
    "answer": "qParEGO",
    "rationale": "The figure shows the hypervolume achieved by each algorithm as a function of the number of function evaluations. qParEGO achieves the highest hypervolume for all function evaluations on the DTLZ7 benchmark with M=3 objectives.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.02177v2",
    "pdf_url": null
  },
  {
    "instance_id": "d03c1d14223941d89ef8bf4f7fef7b82",
    "figure_id": "1909.02857v1-Figure2-1",
    "image_file": "1909.02857v1-Figure2-1.png",
    "caption": " Differences between cross-lingual vs. monolingual confusion matrices. The last column represents cases of incorrect heads and the other columns represent cases for correct heads, i.e., each row summing to 100%. Blue cells show higher cross-lingual values and red cells show higher monolingual values.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part-of-speech tag is most likely to be confused with \"nsubj\" in the monolingual setting?",
    "answer": "\"obj\"",
    "rationale": "The confusion matrix shows the differences between cross-lingual and monolingual confusion matrices. The color blue indicates that the cross-lingual value is higher, and the color red indicates that the monolingual value is higher. In the row corresponding to the gold label \"nsubj\", the cell corresponding to the predicted label \"obj\" is red, indicating that the monolingual confusion rate is higher than the cross-lingual confusion rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.02857v1",
    "pdf_url": null
  },
  {
    "instance_id": "59ab069650364ef88d75cd4e4d91e4cb",
    "figure_id": "2006.03158v2-Figure5-1",
    "image_file": "2006.03158v2-Figure5-1.png",
    "caption": " Validation sequence loss as α varies (MGS-LM).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the optimal value of α that minimizes the LM loss?",
    "answer": "The optimal value of α is approximately 1.",
    "rationale": "The figure shows that the LM loss decreases as α increases, reaching a minimum at approximately 1. After this point, the LM loss begins to increase again.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.03158v2",
    "pdf_url": null
  },
  {
    "instance_id": "7652db77199e4157949850a67a1c4178",
    "figure_id": "2104.08646v3-Figure3-1",
    "image_file": "2104.08646v3-Figure3-1.png",
    "caption": " The artifact statistics of the original BoolQ (above) and IMDb (below) samples are plotted in red, compared to the artifact statistics over the edited instances, plotted in green.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows a larger change in artifact statistics after local edits are applied?",
    "answer": "IMDb.",
    "rationale": "The green points in the IMDb plot are much more spread out than the green points in the BoolQ plot, indicating that the local edits had a larger effect on the artifact statistics for IMDb.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.08646v3",
    "pdf_url": null
  },
  {
    "instance_id": "f42d55df487140d88aab51356d36a059",
    "figure_id": "2201.10075v2-Figure3-1",
    "image_file": "2201.10075v2-Figure3-1.png",
    "caption": " Qualitative comparison of our proposed approach with two representative methods on a sample from the XTEST-2K [56] test dataset. While these sophisticated interpolation methods are unable to handle this challenging scenario with the utility pole subject to large motion, our comparatively simple approach is able to generate a plausible result. Please consider our supplementary for more results.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most plausible result in this challenging scenario?",
    "answer": "Ours.",
    "rationale": "The figure shows that the other two methods, SoftSplat and XVFI, are unable to handle the challenging scenario of a utility pole subject to large motion. Our method, however, is able to generate a plausible result.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.10075v2",
    "pdf_url": null
  },
  {
    "instance_id": "d2d4f780e3754c57af2dcd02657321ba",
    "figure_id": "2306.03403v1-Figure3-1",
    "image_file": "2306.03403v1-Figure3-1.png",
    "caption": " Visualization comparison of SGAT4PASS and Trans4PASS+. The rotation of the pitch / roll / yaw axis is 5◦ / 5◦ / 180◦. SGAT4PASS gains the better results of semantic class “door” and “sofa” (highlighted by red dotted line boxes).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces better results for the semantic class \"door\"?",
    "answer": "SGAT4PASS",
    "rationale": "The figure shows that SGAT4PASS produces better results for the semantic class \"door\" than Trans4PASS+. This is evident in the red dotted line boxes in the figure, which highlight the areas where SGAT4PASS performs better.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.03403v1",
    "pdf_url": null
  },
  {
    "instance_id": "5352c6f8284a49cea2f77c83340aa422",
    "figure_id": "1812.03910v3-Figure6-1",
    "image_file": "1812.03910v3-Figure6-1.png",
    "caption": " Example results of regular style transfer produced by different methods. First two columns show the pairs of content/style images; third to last columns present the results from Gatys [5], AdaIN [9], our two-stage model, and our endto-end model respectively. We observe that our models are able to generate results with quality comparable to the baselines.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most realistic results?",
    "answer": "The end-to-end model.",
    "rationale": "The end-to-end model produces results that are most similar to the original content image, while still incorporating the style of the style image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.03910v3",
    "pdf_url": null
  },
  {
    "instance_id": "05a65a6829474e0a9823dd060bca9915",
    "figure_id": "2006.14884v2-Figure12-1",
    "image_file": "2006.14884v2-Figure12-1.png",
    "caption": " FCT across different flow sizes on W4 for SRPT and LAS.",
    "figure_type": "Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which queueing discipline performs better in terms of FCT for all flow sizes?",
    "answer": "SRPT.",
    "rationale": "SRPT has lower FCT than LAS for all flow sizes and loads, as shown in the figure. This means that SRPT can complete requests faster than LAS.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.14884v2",
    "pdf_url": null
  },
  {
    "instance_id": "aba312ccc3ed49b9b163b896c0a99cd0",
    "figure_id": "2104.13755v2-Figure27-1",
    "image_file": "2104.13755v2-Figure27-1.png",
    "caption": " We conduct an ablation study on the multigrid hyperparameters, including the relaxation method (left), the number of relaxation iterations (middle), and the coarsening ratio (right). ©model by Oliver Laric under CC BY-NC-SA.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which relaxation method converges the fastest?",
    "answer": "SOR",
    "rationale": "The left panel of the figure shows the convergence rate of different relaxation methods. The SOR method has the steepest slope, which indicates that it converges the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.13755v2",
    "pdf_url": null
  },
  {
    "instance_id": "219041135ee447dfb3f8361a5b105898",
    "figure_id": "2209.06405v1-Figure4-1",
    "image_file": "2209.06405v1-Figure4-1.png",
    "caption": " Comparison of RG-CACHE w/ RS with state-of-the-art image enhancement (see supplementary material [32] for more examples).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image enhancement technique is most effective at preserving the details of the original image?",
    "answer": "RG-CACHE w/ RS",
    "rationale": "The RG-CACHE w/ RS technique produces images that are most similar to the original images, while still enhancing the contrast and brightness. This can be seen in the images of the pyramids, the house on the cliff, and the still life. The other techniques either produce images that are too dark or too bright, or that have lost some of the details of the original image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.06405v1",
    "pdf_url": null
  },
  {
    "instance_id": "34b8357b7055428fa5f526b5f3085a1b",
    "figure_id": "2107.06106v2-Figure1-1",
    "image_file": "2107.06106v2-Figure1-1.png",
    "caption": " CODAC obtains conservative estimates of the true return quantiles (black); it penalizes out-of-distribution actions, µ(a | s), more heavily than indistribution actions, πβ̂(a | s).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, which policy is more conservative, the CODAC policy or the true return quantiles?",
    "answer": "The CODAC policy is more conservative.",
    "rationale": "The figure shows that the CODAC policy (red line) consistently underestimates the true return quantiles (black line). This means that the CODAC policy is more cautious and avoids taking actions that could lead to large losses.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.06106v2",
    "pdf_url": null
  },
  {
    "instance_id": "f200320f96d248d28a6351e54f898680",
    "figure_id": "2208.04537v1-Figure3-1",
    "image_file": "2208.04537v1-Figure3-1.png",
    "caption": " Four working modes. Dark orange square means partially labeled data, light orange square means unlabeled data.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which mode is the data used for pretraining?",
    "answer": "Pretraining Testing Mode",
    "rationale": "The figure shows that in the Pretraining Testing Mode, there is a data set labeled \"Data for pretraining\" that is fed into the model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.04537v1",
    "pdf_url": null
  },
  {
    "instance_id": "2a1a9401bdbd475aa0162412667f0f76",
    "figure_id": "2304.03544v1-Figure2-1",
    "image_file": "2304.03544v1-Figure2-1.png",
    "caption": " Cosine distance between the topic representations of words over the course of training. The results show that while the topic representations degenerate into similar values in NMTM (Wu et al. 2020a), our InfoCTM successfully avoids degenerate topic representations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is more successful at avoiding degenerate topic representations?",
    "answer": "InfoCTM",
    "rationale": "The figure shows that the cosine distance between the topic representations of words decreases much more rapidly for NMTM than for InfoCTM. This indicates that the topic representations in NMTM are becoming more similar to each other, which is a sign of degeneracy. In contrast, the topic representations in InfoCTM remain relatively distinct, suggesting that it is more successful at avoiding degenerate topic representations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.03544v1",
    "pdf_url": null
  },
  {
    "instance_id": "8fab4ee82b6c415c896d8768fb8ec416",
    "figure_id": "2002.08046v1-Figure5-1",
    "image_file": "2002.08046v1-Figure5-1.png",
    "caption": " Training data size and training/inference time analysis.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in terms of BLEU score?",
    "answer": "TreeTransformer",
    "rationale": "The BLEU score is a measure of how well a machine translation system performs. In this figure, we can see that TreeTransformer has the highest BLEU score for all training data sizes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.08046v1",
    "pdf_url": null
  },
  {
    "instance_id": "b75dead39a9a49ea9a52d0233b87787f",
    "figure_id": "2106.10989v4-Figure6-1",
    "image_file": "2106.10989v4-Figure6-1.png",
    "caption": " The MMD distance between source dataset and target dataset v.s. DR of fine-tuned model. The embeddings are extracted on the pre-trained model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the largest MMD distance from the source dataset?",
    "answer": "CIFAR10",
    "rationale": "The MMD distance is shown on the y-axis of the plot. The CIFAR10 dataset has the highest point on the plot, indicating that it has the largest MMD distance from the source dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.10989v4",
    "pdf_url": null
  },
  {
    "instance_id": "a05086df43934f15a622c0fb0101a74e",
    "figure_id": "2205.14358v2-Figure5-1",
    "image_file": "2205.14358v2-Figure5-1.png",
    "caption": " LCUL results on the CreditCard dataset. (a):PoF, (b):Δcolor, (c):Δpoints/label ,(d):Δcenter/label.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four metrics is most sensitive to the number of clusters?",
    "answer": "Δpoints/label",
    "rationale": "The figure shows that Δpoints/label changes the most as the number of clusters increases, while the other metrics remain relatively stable.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.14358v2",
    "pdf_url": null
  },
  {
    "instance_id": "8bce8ec4d6824a81b21db68c084a7605",
    "figure_id": "2206.04890v2-Figure4-1",
    "image_file": "2206.04890v2-Figure4-1.png",
    "caption": " Illustration of the averaged response curves in task",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on the task, according to the plot?",
    "answer": "GALILEO",
    "rationale": "The plot shows the averaged response curves for different methods. The higher the curve, the better the performance. GALILEO's curve is consistently higher than the other methods, indicating that it performs best on the task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.04890v2",
    "pdf_url": null
  },
  {
    "instance_id": "6ac34606f7684563b37ee2ad252adc51",
    "figure_id": "2204.11144v2-Figure3-1",
    "image_file": "2204.11144v2-Figure3-1.png",
    "caption": " Comparison of CPINN and PINN on the nonlinear Schrödinger equation 21 in terms of relative errors. After 200 000 training iterations, PINN cannot reduce the L2 error further, plateauing about 4×10−3 , whereas CPINN reduces the error to 6×10−4 under a smaller computational budget.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performs better for the nonlinear Schrödinger equation in terms of achieving a lower L2 error?",
    "answer": "CPINN + ACCGD performs better than PINN + Adam.",
    "rationale": "The figure shows the relative L2 error for both optimizers as a function of iterations and forward passes. After 200,000 iterations, PINN + Adam plateaus at an error of about 4×10−3, while CPINN + ACCGD reduces the error to 6×10−4 in fewer iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.11144v2",
    "pdf_url": null
  },
  {
    "instance_id": "3574eca183064e68bbddfb7db32d6bdc",
    "figure_id": "1908.07222v1-Figure7-1",
    "image_file": "1908.07222v1-Figure7-1.png",
    "caption": " The results of the user study, comparing SROBB (ours) with RCAN [44], SRGAN [20], ESRGAN [36] and SFTGAN [35] methods. Our method produces visual results that are the preferred choice for the users by a large margin in terms of: (a) percentage of votes, (b) percentage of winning images by majority of votes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most visually appealing results according to the user study?",
    "answer": "SROBB",
    "rationale": "The figure shows that SROBB received the highest percentage of votes (38.3%) and the highest percentage of winning images by majority vote (42.9%).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.07222v1",
    "pdf_url": null
  },
  {
    "instance_id": "4c9c32f1c1de4838ae55266e402a694f",
    "figure_id": "2009.07364v2-Figure2-1",
    "image_file": "2009.07364v2-Figure2-1.png",
    "caption": " The “difference of accuracy” (Hewitt and Liang, 2019) and the “difference of loss” (Pimentel et al., 2020) criteria against weight decay on model configurations, on UD English. For each configuration, the learning rate leading to the highest accuracy is selected.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model configuration performs best across all weight decay values?",
    "answer": "layer0 dim10",
    "rationale": "The plot shows that layer0 dim10 consistently achieves the highest accuracy and lowest loss across all weight decay values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.07364v2",
    "pdf_url": null
  },
  {
    "instance_id": "a8a863b3b60040bf925960925928be3d",
    "figure_id": "1909.07588v1-Figure4-1",
    "image_file": "1909.07588v1-Figure4-1.png",
    "caption": " Convergence of the loss function (logistic regression)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four optimization methods converges the fastest in terms of number of iterations?",
    "answer": "LAQ",
    "rationale": "The plot in (a) shows the loss residual versus the number of iterations for each of the four optimization methods. LAQ has the steepest descent, meaning it reaches a lower loss residual with fewer iterations than the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.07588v1",
    "pdf_url": null
  },
  {
    "instance_id": "104c32f672b74d309bebc7d5fca226ba",
    "figure_id": "2110.14430v1-Figure9-1",
    "image_file": "2110.14430v1-Figure9-1.png",
    "caption": " The proportion (± std over 5 random runs) of different classes in predictions by a benign model, another benign one trained on an unbalanced training set, and two backdoored models. The first benign model and the SIG model are based on a balanced training set, while the second benign model and the Badnets model are based on an unbalanced training set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models is most likely to be backdoored?",
    "answer": "The Badnets model is most likely to be backdoored.",
    "rationale": "The Badnets model shows a sharp increase in the proportion of predictions for Class 0 as the perturbation budget increases. This suggests that the model is designed to predict Class 0 for certain inputs, regardless of the actual class of the input. This behavior is consistent with backdoor attacks, which aim to manipulate the model's predictions for specific inputs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14430v1",
    "pdf_url": null
  },
  {
    "instance_id": "62ce1ee41c7b41dc9e81711d057dc1ec",
    "figure_id": "2210.12487v1-Figure4-1",
    "image_file": "2210.12487v1-Figure4-1.png",
    "caption": " Performances on each logical operator.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which logical operator has the best overall performance?",
    "answer": "Negation",
    "rationale": "The figure shows that negation has the highest F1 score across all models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12487v1",
    "pdf_url": null
  },
  {
    "instance_id": "6304173e0d1b4deaa43c9206c5e26203",
    "figure_id": "2112.12845v5-Figure7-1",
    "image_file": "2112.12845v5-Figure7-1.png",
    "caption": " Case study for early stopping approximation: NDCG@10 of each epoch during training",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which set of parameters appears to have converged the fastest?",
    "answer": "Set 4.",
    "rationale": "The figure shows the NDCG@10 metric for four different sets of parameters as a function of the number of epochs. Set 4 is the only set that appears to have converged, as its NDCG@10 value has stopped increasing and is relatively stable after approximately 10 epochs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.12845v5",
    "pdf_url": null
  },
  {
    "instance_id": "0f1b998033aa4497855a73bd9112eeed",
    "figure_id": "2106.04488v2-Figure13-1",
    "image_file": "2106.04488v2-Figure13-1.png",
    "caption": " Comparison with StyleSpace [37] on mouth attributes. Zoom in for details.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method produces more realistic images, StyleSpace or Ours?",
    "answer": " Ours",
    "rationale": " The images produced by our method are more realistic and natural-looking than those produced by StyleSpace. For example, in the first row, the image produced by StyleSpace has a very unnatural smile, while the image produced by our method has a more natural and believable smile.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04488v2",
    "pdf_url": null
  },
  {
    "instance_id": "645384ca45e74eefb4365603cff3d5ac",
    "figure_id": "1812.07671v2-Figure8-1",
    "image_file": "1812.07671v2-Figure8-1.png",
    "caption": " Results on crawler experiments. Left: Online recognition of latent task probabilities for alternating periods of normal/crippled experience. Right: MOLe improves from seeing the same tasks multiple times.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed best in terms of cumulative rewards?",
    "answer": "MOLE",
    "rationale": "The plot on the right shows that MOLE (green line) has the highest cumulative reward.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.07671v2",
    "pdf_url": null
  },
  {
    "instance_id": "ec7e809db9d04b2c85ade1e9a9e30c40",
    "figure_id": "2103.03335v4-Figure1-1",
    "image_file": "2103.03335v4-Figure1-1.png",
    "caption": " The relationship between MRR and the number of training queries.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest MRR for all numbers of training queries?",
    "answer": "BM25",
    "rationale": "The figure shows the MRR for different datasets as a function of the number of training queries. The BM25 line is consistently above the other lines, indicating that it has the highest MRR for all numbers of training queries.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.03335v4",
    "pdf_url": null
  },
  {
    "instance_id": "71fab06f21ef42aa9981acc1dd0465ff",
    "figure_id": "2010.07611v2-Figure5-1",
    "image_file": "2010.07611v2-Figure5-1.png",
    "caption": " Layerwise statistics of VGG-16 iteratively pruned on CIFAR-10. Top: Layerwise survival rate for models with {51.2%, 26.2%, 13.4%, 6.87%, 3.52%} weights surviving. Bottom: Number of nonzero weights for models with {3.52%, 1.80%, 0.92%, 0.47%, 0.24%} weights surviving.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pruning method results in the most gradual decrease in the number of nonzero weights across layers?",
    "answer": "The Erdős-Rényi kernel pruning method.",
    "rationale": "The bottom row of the figure shows the number of nonzero weights for different pruning methods. The Erdős-Rényi kernel method has a more gradual decrease in the number of nonzero weights across layers compared to the other methods. This can be seen by comparing the slopes of the lines in the bottom row of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.07611v2",
    "pdf_url": null
  },
  {
    "instance_id": "6e28498df73e497cad197b2ce06f4bb0",
    "figure_id": "2009.14521v2-Figure6-1",
    "image_file": "2009.14521v2-Figure6-1.png",
    "caption": " (Left) Mean expected utility against CLQR (dashed) and BR (solid) over 100 games from set 2 of EFGs with different value of the rationality parameter λ. (Right) Expected utility of different algorithms against CLQR (dashed) and BR (solid) in Leduc Hold’em. p is fixed for both regret minimization approaches and QNE is the value of COMB or RQR with p = 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best in Leduc Hold'em when the mixing parameter p is 0.5?",
    "answer": "NASH",
    "rationale": "The right plot shows the expected utility of different algorithms against CLQR and BR in Leduc Hold'em for different values of the mixing parameter p. When p is 0.5, the NASH line is the highest, indicating that it has the highest expected utility.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.14521v2",
    "pdf_url": null
  },
  {
    "instance_id": "0f2f4a359a4e4839a341965b00284f1b",
    "figure_id": "2110.14782v3-Figure3-1",
    "image_file": "2110.14782v3-Figure3-1.png",
    "caption": " |∆(BZ−BS)| for Ttrans under different conditions on the source of original and derived language pre-training corpora (hereon, corpora) (§ 4.3), averaged over four languages. Larger values imply worse zero-shot transfer. The breakdown of scores for different languages is in Appendix D. (1) Non-parallel (diff) (green bar), which uses corpora from different domains is worse than (2) Non-parallel (same) (orange bar), which uses different sets of sentences sampled from the same domain, which is in turn worse than (3) Parallel, which uses the same sentences. Having pre-training corpora from the same domain like Wikipedia (Nonparallel (same)) gives performance boosts between 2 points for QA to 17 points for NER when compared to Non-parallel (diff).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which condition resulted in the worst zero-shot transfer performance for NER?",
    "answer": "Non-parallel (diff)",
    "rationale": "The figure shows the absolute difference in BLEU scores between the baseline and back-translated systems (|∆(BZ−BS)|) for different tasks and conditions. Larger values imply worse zero-shot transfer performance. For NER, the green bar, which represents the Non-parallel (diff) condition, is the highest, indicating the worst performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14782v3",
    "pdf_url": null
  },
  {
    "instance_id": "e01c491e1bea449cb50f8719eba7890c",
    "figure_id": "2302.05083v1-Figure4-1",
    "image_file": "2302.05083v1-Figure4-1.png",
    "caption": " The convergence curve of α learned by DRGCN on Cora (left), Citeseer (middle) and Pubmed (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What can you say about the convergence of α learned by DRGCN on the three datasets?",
    "answer": "α converges quickly on all three datasets.",
    "rationale": "The figure shows that the initial residual weight (α) decreases rapidly with increasing epochs on all three datasets. This indicates that α converges quickly.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.05083v1",
    "pdf_url": null
  },
  {
    "instance_id": "57f37c05f4924d69abcfdc5a853a3513",
    "figure_id": "2002.10248v4-Figure13-1",
    "image_file": "2002.10248v4-Figure13-1.png",
    "caption": " Prediction confidence for (Fashion-)MNIST ambiguous examples. For each class combination, the lower triangle shows the the confidence for the digit denoted on the horizontal axis, and the upper triangle shows the confidence for the digit on the vertical axis. For example, for the MNIST class combination 9v0, the classifier confidence in class 0 is 0.477 (the bottom left entry) while the classifier confidence in class 9 is 0.474 (the top right entry). Diagonal entries are blank since they have the same class on row and column. Off-diagonal blank entries indicate that BAYES-TREX does not find ambiguous samples for that particular class pair.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the classifier's confidence in class 0 for the Fashion-MNIST class combination \"T-shirt vs. Pullover\"?",
    "answer": "0.488",
    "rationale": "The classifier's confidence in class 0 for the Fashion-MNIST class combination \"T-shirt vs. Pullover\" is shown in the bottom left entry of the corresponding cell in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.10248v4",
    "pdf_url": null
  },
  {
    "instance_id": "0aacc7c5a7e747efb7cc65888dc8bcc9",
    "figure_id": "2306.09368v1-Figure8-1",
    "image_file": "2306.09368v1-Figure8-1.png",
    "caption": " AUROC of the four-layer Warpformer on the PhysioNet dataset with different ?̃? (𝑛) settings.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four models has the highest AUROC?",
    "answer": "The model with the setting ̃_η(𝑛) = 0.1.",
    "rationale": "The AUROC is a measure of how well a model can distinguish between positive and negative examples. The higher the AUROC, the better the model is at distinguishing between positive and negative examples. In the figure, the model with the setting ̃_η(𝑛) = 0.1 has the highest AUROC, which is indicated by the darker blue color in the 3D plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.09368v1",
    "pdf_url": null
  },
  {
    "instance_id": "b2aad76701b74635bf37f4186c1dadcb",
    "figure_id": "1903.10661v1-Figure6-1",
    "image_file": "1903.10661v1-Figure6-1.png",
    "caption": " Training loss of the baseline, Semantic Alignment without updating observation (SA w/o update) and Semantic Alignment (SA). The training starts at a roughly converged model (trained using human annotations only) using 300W training set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the lowest training loss?",
    "answer": "Semantic Alignment (SA)",
    "rationale": "The figure shows the training loss for three models: baseline, Semantic Alignment without updating observation (SA w/o update), and Semantic Alignment (SA). The SA curve is consistently below the other two curves, indicating that it achieves the lowest training loss.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.10661v1",
    "pdf_url": null
  },
  {
    "instance_id": "b65342c3494747fea236df940fb712b6",
    "figure_id": "2109.09861v2-Figure4-1",
    "image_file": "2109.09861v2-Figure4-1.png",
    "caption": " Mean and SD of success for each model in each scenario across all agent types.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the lowest mean success rate across all agent types?",
    "answer": "dLK(λ)",
    "rationale": "The figure shows the mean success rate for each model in each scenario across all agent types. The dLK(λ) model has the lowest mean success rate in all three scenarios.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.09861v2",
    "pdf_url": null
  },
  {
    "instance_id": "12b2854bb30c468e86f1b23b9108b09e",
    "figure_id": "2104.01495v1-Figure8-1",
    "image_file": "2104.01495v1-Figure8-1.png",
    "caption": " (a) Recall@K score on the test split of CIFAR-100. (b) Metric weight distribution of OAHU on CIFAR-100.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric model performs best on the CIFAR-100 dataset according to the Recall@K score?",
    "answer": "OAHU",
    "rationale": "The plot in (a) shows the Recall@K score for different metric models on the CIFAR-100 dataset. OAHU has the highest Recall@K score for all values of K, indicating that it performs best on this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.01495v1",
    "pdf_url": null
  },
  {
    "instance_id": "cf4e85ea18e745e1940c8f2ae84713be",
    "figure_id": "2111.07535v1-Figure5-1",
    "image_file": "2111.07535v1-Figure5-1.png",
    "caption": " Comparison of different predictors for ranking prediction in validation set with 25 uniformly sampled configurations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which predictor is the most accurate?",
    "answer": "The accuracy predictor is the most accurate.",
    "rationale": "The accuracy predictor has the highest R-squared value, which indicates that it is the most accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.07535v1",
    "pdf_url": null
  },
  {
    "instance_id": "39a8f8f1519f4f469891631f94c7de74",
    "figure_id": "2303.16047v3-Figure6-1",
    "image_file": "2303.16047v3-Figure6-1.png",
    "caption": " Precision of the approximated Rashomon sets as a function of θ. Our method always dominates other baselines. hessian is our starting point.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest precision for all values of θ and across all datasets?",
    "answer": "Ours",
    "rationale": "The plot shows that the red line, which represents our method, is always above the other lines, indicating that our method has the highest precision for all values of θ and across all datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.16047v3",
    "pdf_url": null
  },
  {
    "instance_id": "f92721c25fc8429194c974bd9cacef01",
    "figure_id": "2011.15084v2-Figure3-1",
    "image_file": "2011.15084v2-Figure3-1.png",
    "caption": " Model trajectory forecasts at two separate frames in the same scene. K = 5 predicted trajectories are shown in red, and the true recorded future trajectory from the dataset is shown in green. LDS predicts more diverse and plausible trajectories than both baselines.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model predicts more diverse and plausible trajectories?",
    "answer": "LDS.",
    "rationale": "The figure shows that LDS predicts more diverse and plausible trajectories than both baselines. This is evident in the fact that the red trajectories predicted by LDS are more spread out and cover a wider range of possible future paths than the trajectories predicted by the other two models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.15084v2",
    "pdf_url": null
  },
  {
    "instance_id": "f8088ad5978443fdae3519ed03adbbf0",
    "figure_id": "2210.00764v2-Figure9-1",
    "image_file": "2210.00764v2-Figure9-1.png",
    "caption": " Example solutions.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the difference between zendo3 and zendo4?",
    "answer": "Zendo3 has a color of blue, while zendo4 does not.",
    "rationale": "This can be seen in the figure where zendo3 is listed as having a color of blue, while zendo4 does not have a color listed.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.00764v2",
    "pdf_url": null
  },
  {
    "instance_id": "ce37a8eab4924b949eaca21f5e797fc1",
    "figure_id": "1811.09813v1-Figure4-1",
    "image_file": "1811.09813v1-Figure4-1.png",
    "caption": " Top: Correlation between magnetization and estimated marginal probabilities for the same problem instance as we add streamlining constraints. Bottom: Histogram of variables magnetizations. As streamlining constraints are added, the average confidence of assignments increases.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the addition of streamlining constraints affect the average confidence of assignments?",
    "answer": "The average confidence of assignments increases.",
    "rationale": "The histograms in the bottom row of the figure show that the distribution of variable magnetizations becomes more peaked as streamlining constraints are added. This indicates that the variables are more likely to be assigned to a single value, which means that the average confidence of assignments increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.09813v1",
    "pdf_url": null
  },
  {
    "instance_id": "01376ee0c0414b0b8365df8dc4dc264c",
    "figure_id": "2103.02805v1-Figure4-1",
    "image_file": "2103.02805v1-Figure4-1.png",
    "caption": " The distribution of the Webface-OCC dataset for various mask types.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of mask is the most common in the Webface-OCC dataset?",
    "answer": "Normal",
    "rationale": "The figure shows that the largest portion of the pie chart is blue, which corresponds to the \"Normal\" category. This indicates that the majority of images in the dataset are of people wearing no masks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.02805v1",
    "pdf_url": null
  },
  {
    "instance_id": "c9c62c4cffec400fb129f54d29546c36",
    "figure_id": "2211.16784v1-Figure5-1",
    "image_file": "2211.16784v1-Figure5-1.png",
    "caption": " c versus MRE curves for entropy approximation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest MRE for a given eigenvalue decay?",
    "answer": "GRP",
    "rationale": "The GRP curve is the lowest of all the curves in the plot, which means that it has the lowest MRE for a given eigenvalue decay.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.16784v1",
    "pdf_url": null
  },
  {
    "instance_id": "7a8130af2ce84226be6288c5aa576500",
    "figure_id": "2203.00089v1-Figure4-1",
    "image_file": "2203.00089v1-Figure4-1.png",
    "caption": " A comparison of various optimizers on optimization tasks known to be difficult for first-order methods. Left: synthetic data for poorly-conditioned regression; Right: deep autoencoder on MNIST.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which optimizer performs the best on the MNIST Autoencoder task?",
    "answer": "APO-Precond",
    "rationale": "The figure on the right shows the training loss for various optimizers on the MNIST Autoencoder task. The APO-Precond optimizer has the lowest training loss, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.00089v1",
    "pdf_url": null
  },
  {
    "instance_id": "3b92b962da7a41f2942ddb885b56c2c3",
    "figure_id": "2303.02760v2-Figure10-1",
    "image_file": "2303.02760v2-Figure10-1.png",
    "caption": " Statistical analyses on the visibility for all keypoints comparing our 20 scenes with MSCOCO [32] (left) and the distribution map of image sources of Human-Art (right). A.2. Keypoints Attribute",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of scenario is most likely to have all keypoints visible?",
    "answer": "MSCOCO.",
    "rationale": "The left side of the figure shows the percentage of keypoints that are visible, occluded, and invisible for each scenario category. MSCOCO has the highest percentage of visible keypoints.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.02760v2",
    "pdf_url": null
  },
  {
    "instance_id": "47ccea266a0f4533b466ee0b148fde8c",
    "figure_id": "2009.08965v3-Figure5-1",
    "image_file": "2009.08965v3-Figure5-1.png",
    "caption": " Feature divergence between pairs of datasets. Features are extracted by a standard and an AdvBN fine-tuned ResNet50.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pair of datasets has the highest feature divergence?",
    "answer": "ImageNet vs. ImageNet-Ins.",
    "rationale": "The figure shows the feature divergence between pairs of datasets. The feature divergence is highest for ImageNet vs. ImageNet-Ins, as shown in subfigure (a).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.08965v3",
    "pdf_url": null
  },
  {
    "instance_id": "f3d2684ca7224fde95faa809052b5a14",
    "figure_id": "1904.00962v5-Figure2-1",
    "image_file": "1904.00962v5-Figure2-1.png",
    "caption": " The figure shows that adam-correction has the same effect as learning rate warmup. We removed adam-correction from the LAMB optimizer. We did not observe any drop in the test or validation accuracy for BERT and ImageNet training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does the figure show that the learning rate warmup is better than adam-correction?",
    "answer": "No.",
    "rationale": "The figure shows that both adam-correction and learning rate warmup result in the learning rate reaching a similar level after a few thousand iterations. The caption states that the authors removed adam-correction from the LAMB optimizer and did not observe any drop in the test or validation accuracy for BERT and ImageNet training. This suggests that adam-correction and learning rate warmup are equally effective.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.00962v5",
    "pdf_url": null
  },
  {
    "instance_id": "07bc9808f3864b9ba4e8ed86951c1480",
    "figure_id": "2210.11925v3-Figure2-1",
    "image_file": "2210.11925v3-Figure2-1.png",
    "caption": " Comparison between n-BHMC and CRHMC on the hypercube (left) and the simplex (right).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better on the hypercube in terms of variability?",
    "answer": "CRHMC",
    "rationale": "The box plots for CRHMC on the hypercube (left) are narrower than those for n-BHMC, indicating less variability in the results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.11925v3",
    "pdf_url": null
  },
  {
    "instance_id": "99c443f744d04495bfdd4fd5729a0a56",
    "figure_id": "2301.02184v2-Figure12-1",
    "image_file": "2301.02184v2-Figure12-1.png",
    "caption": " Average active mapping performance over 3 random data splits vs. episode step.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four approaches to active mapping has the highest average performance at the end of the training period?",
    "answer": "The \"Ours\" approach.",
    "rationale": "The figure shows the average performance of four different approaches to active mapping, measured by the mean F1 score, as a function of the episode step. The \"Ours\" approach has the highest mean F1 score at the end of the training period (episode step 16).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.02184v2",
    "pdf_url": null
  },
  {
    "instance_id": "b6ce7c3934374e59b85a57d90c2fa46a",
    "figure_id": "1812.01855v2-Figure1-1",
    "image_file": "1812.01855v2-Figure1-1.png",
    "caption": " The flowchart of using the proposed XNMs reasoning over scene graphs, which can be represented by detected one-hot class labels (left) or RoI feature vectors (colored bars on the right). Feature colors are consistent with the bounding box colors. XNMs have 4 meta-types. Red nodes or edges indicate attentive results. The final module assembly can be obtained by training an off-the-shelf sequence-to-sequence program generator [13].",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the different types of X Neural Modules shown in the figure?",
    "answer": "AttendNode, AttendEdge, Transfer, and Logic.",
    "rationale": "The figure shows four different types of X Neural Modules, which are used to reason over scene graphs. These modules are AttendNode, AttendEdge, Transfer, and Logic.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.01855v2",
    "pdf_url": null
  },
  {
    "instance_id": "23fb5022d60349e191e31a7890928a3f",
    "figure_id": "2010.04296v2-Figure10-1",
    "image_file": "2010.04296v2-Figure10-1.png",
    "caption": " Evaluation scores, for pushing, picking, pick and place and stacking2 baselines, from top to bottom respectively. Each protocol was evaluated for 200 episodes and each bar is averaged over five models with different random seeds [bp block pose, bm block mass, bs block size, gp goal pose, ff floor friction].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which baseline algorithm performed the best on the pick and place task?",
    "answer": "SAC(0)",
    "rationale": "The figure shows the fractional success of different baseline algorithms on various tasks. The pick and place task is the third task from the top. SAC(0) is the only algorithm that achieved a fractional success of 1.0 on this task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.04296v2",
    "pdf_url": null
  },
  {
    "instance_id": "9dd393c591694d5e8b5df2f0e8b76c5d",
    "figure_id": "1906.07132v1-Figure4-1",
    "image_file": "1906.07132v1-Figure4-1.png",
    "caption": " A single-hop HotpotQA example that cannot be fixed with our adversary.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which film was directed by Jennifer Kent in her directorial debut?",
    "answer": "The Babadook",
    "rationale": "The figure shows that \"The Babadook\" is a 2014 Australian psychological horror film written and directed by Jennifer Kent in her directorial debut.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.07132v1",
    "pdf_url": null
  },
  {
    "instance_id": "9c36a8f3c30a4ed4bf26b74efe79139c",
    "figure_id": "2110.14508v1-Figure6-1",
    "image_file": "2110.14508v1-Figure6-1.png",
    "caption": " Convergence of iterative algorithm in misdemeanor semi-synthetic set-up. See Figure 5 for description.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model appears to be the most stable across different numbers of agents?",
    "answer": "Decision tree",
    "rationale": "The decision tree plots show the least amount of variance in AUC across iterations, regardless of the number of agents.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14508v1",
    "pdf_url": null
  },
  {
    "instance_id": "a930d9529caf41858c38438b9e3590ee",
    "figure_id": "2305.19148v3-Figure12-1",
    "image_file": "2305.19148v3-Figure12-1.png",
    "caption": " The Macro-F1 scores of GPT-J (8-shots) on TweetEval-hate with different label names. DC (English): DC with random English words; DC (Indomain): DC with random in-domain words. Using less task-relevant label names mitigates domain-label bias but limits the model’s performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the \"neutral vs. hate\" task?",
    "answer": "The original model.",
    "rationale": "The original model has the highest Macro-F1 score on the \"neutral vs. hate\" task, as shown in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19148v3",
    "pdf_url": null
  },
  {
    "instance_id": "5d721594b0a44e9fa706cb5cc54bfdc7",
    "figure_id": "2104.05882v1-Figure10-1",
    "image_file": "2104.05882v1-Figure10-1.png",
    "caption": " Full results of all models over English with average pooling on all tasks except in EDU segmentation (with the only differences over Figure 2 being for BERT and ALBERT, where we originally used [CLS] embeddings on two-text classification probing tasks).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the EDU segmentation task?",
    "answer": "BART.",
    "rationale": "The figure shows the F1-macro score for each model on the EDU segmentation task. BART has the highest F1-macro score of all the models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.05882v1",
    "pdf_url": null
  },
  {
    "instance_id": "f9409ae03f294a068d7e56bff1e90ef7",
    "figure_id": "2301.00015v1-Figure5-1",
    "image_file": "2301.00015v1-Figure5-1.png",
    "caption": " Visualization of the original graph of Cora and learned graphs by Graph-PRI, Pro-GNN, IDGL, and PRI-GSL.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method is most effective at separating the different classes of nodes in the Cora dataset?",
    "answer": " PRI-GSL.",
    "rationale": " The figure shows the original graph of the Cora dataset and the learned graphs by four different methods. PRI-GSL is the only method that clearly separates the different classes of nodes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.00015v1",
    "pdf_url": null
  },
  {
    "instance_id": "734ac1622b2f47d3896352f32b499719",
    "figure_id": "2110.13957v4-Figure7-1",
    "image_file": "2110.13957v4-Figure7-1.png",
    "caption": " Comparison among our proposed models on different embedding models. The left columns shows the unbiasedness (attribute prediction) and the right columns shows the utility (link prediction).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best for link prediction on the node2vec embedding model?",
    "answer": "UGE-C",
    "rationale": "The rightmost column of the figure shows the link prediction performance for different models on the node2vec embedding model. UGE-C has the highest bar, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.13957v4",
    "pdf_url": null
  },
  {
    "instance_id": "889675b5b6ea43909aec05740b99e57d",
    "figure_id": "2007.05515v3-Figure6-1",
    "image_file": "2007.05515v3-Figure6-1.png",
    "caption": " Evaluation of the weak tag distributions. (a/b) Number of times each tag appears in the dataset from the agglomerative clustering or affinity propagation. (c/d) Number of tags in each video. Videos have between 0 and 65 tags, most have 1-8 tags.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method of clustering results in a more even distribution of tags across videos?",
    "answer": "Affinity propagation",
    "rationale": "Figure (d) shows that the distribution of tags per video is more even when using affinity propagation than when using agglomerative clustering (Figure (c)).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.05515v3",
    "pdf_url": null
  },
  {
    "instance_id": "52d569d71a5c45c0b2bedec0a4a5785b",
    "figure_id": "2003.01090v2-Figure5-1",
    "image_file": "2003.01090v2-Figure5-1.png",
    "caption": " FGSM attack on CIFAR-100 with different epsilons for the l∞ ball on ResNet-V2(18).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which defense method is most effective against FGSM attacks on CIFAR-100 with ResNet-V2(18)?",
    "answer": "AdvBNN",
    "rationale": "The figure shows the accuracy of different defense methods against FGSM attacks with different epsilons. AdvBNN has the highest accuracy for all epsilons, indicating that it is the most effective defense method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.01090v2",
    "pdf_url": null
  },
  {
    "instance_id": "1fc02bfccf1745a4afe1fd4b06fbde2c",
    "figure_id": "2208.06102v2-Figure12-1",
    "image_file": "2208.06102v2-Figure12-1.png",
    "caption": " Relative cumulative energy consumption of Zeus across all jobs, w.r.t. the early-stopping threshold β.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is the most energy-efficient across all jobs for an early-stopping threshold of 3?",
    "answer": "ResNet-50",
    "rationale": "The figure shows the relative cumulative energy consumption of different models for different early-stopping thresholds. The line for ResNet-50 is the lowest at an early-stopping threshold of 3, which indicates that it is the most energy-efficient model for that threshold.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.06102v2",
    "pdf_url": null
  },
  {
    "instance_id": "e2b416da659f463eb52c02e74ac060fb",
    "figure_id": "2206.03740v1-Figure5-1",
    "image_file": "2206.03740v1-Figure5-1.png",
    "caption": " Hyperparameter effect of LL-Ct on COCO dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the mAP change as the Δrel increases?",
    "answer": "The mAP decreases as the Δrel increases.",
    "rationale": "The figure shows that the mAP decreases as the Δrel increases. This is because the Δrel is a hyperparameter that controls the amount of data augmentation, and increasing the Δrel results in more data augmentation, which can lead to overfitting and a decrease in performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.03740v1",
    "pdf_url": null
  },
  {
    "instance_id": "ffe258bbd66547559090081b9ba5fa1e",
    "figure_id": "1912.09957v3-Figure2-1",
    "image_file": "1912.09957v3-Figure2-1.png",
    "caption": " Decentralized PrSBC with 7 robots. Robots 6 and 7 marked in black serve as passive moving obstacles without interaction to other robots.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which robots are passive moving obstacles?",
    "answer": "Robots 6 and 7 are passive moving obstacles.",
    "rationale": "The caption states that \"Robots 6 and 7 marked in black serve as passive moving obstacles without interaction to other robots.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.09957v3",
    "pdf_url": null
  },
  {
    "instance_id": "79d709cdc2b74ae882419780d3f79e59",
    "figure_id": "2210.02177v2-Figure8-1",
    "image_file": "2210.02177v2-Figure8-1.png",
    "caption": " Cumulative wall time of experiments shown in Fig. 4. All computations were performed on an Intel Xeon Silver 4110 CPU. The mean and 2 standard errors over 5 trials are reported.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm is the fastest on average for the Car Side Impact, M=4 task?",
    "answer": "qParEgo",
    "rationale": "The figure shows the runtime of different algorithms on the Car Side Impact, M=4 task. The qParEgo line is the lowest on average, which indicates that it is the fastest algorithm.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.02177v2",
    "pdf_url": null
  },
  {
    "instance_id": "f974a0ef2c1f45dfb014974a9492be57",
    "figure_id": "2105.13385v1-Figure24-1",
    "image_file": "2105.13385v1-Figure24-1.png",
    "caption": " EXP3 with human-comet loss.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm appears to converge the fastest?",
    "answer": "UEDIN 6334.",
    "rationale": "The figure shows the weights of different algorithms over the number of iterations. The UEDIN 6334 algorithm's weights decrease rapidly and appear to reach a stable value within the first 200 iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.13385v1",
    "pdf_url": null
  },
  {
    "instance_id": "3423796434804036a541be2c0e9a48e7",
    "figure_id": "2303.15469v1-Figure2-1",
    "image_file": "2303.15469v1-Figure2-1.png",
    "caption": " System Overview. Our framework mainly consists of a CVAE-based planner module and an optimization-based synthesizer module. Given the generation condition as the input, the planner first generates a per-stage CAMS representation containing contact reference frames and sequences of finger embedding. Then the synthesizer optimizes the whole manipulation animation based on the CAMS embedding.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two main modules of the framework?",
    "answer": "The planner module and the synthesizer module.",
    "rationale": "The figure shows that the framework consists of two main modules: the planner module (green box) and the synthesizer module (orange box). The planner module generates a per-stage CAMS representation, while the synthesizer module optimizes the whole manipulation animation based on the CAMS embedding.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.15469v1",
    "pdf_url": null
  },
  {
    "instance_id": "b49950d9278447a4a1d6da6d387af9d8",
    "figure_id": "1906.05253v1-Figure9-1",
    "image_file": "1906.05253v1-Figure9-1.png",
    "caption": " Does SoRB Generalize? After training on 100 SUNCG houses, we collect random data in held-out houses to use for search in those new environments. Whether using depth images or RGB images, SoRB generalizes well to new houses, reaching almost 80% of goals 10 steps away, while goal-conditioned RL reaches less than 20% of these goals. Transparent lines correspond to average success rate across 22 held-out houses for each of three random seeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does the performance of SoRB compare to RL when using depth images?",
    "answer": "SoRB outperforms RL when using depth images.",
    "rationale": "The figure shows that the success rate of SoRB is higher than RL for all distances when using depth images. For example, at a distance of 10, SoRB has a success rate of almost 80%, while RL has a success rate of less than 20%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.05253v1",
    "pdf_url": null
  },
  {
    "instance_id": "451073efd9cf4b31bf4ce1721fdd8ae7",
    "figure_id": "2010.02140v1-Figure2-1",
    "image_file": "2010.02140v1-Figure2-1.png",
    "caption": " Survival function per system estimated for each domain.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which system performed the best on the Empathetic Dialogues domain?",
    "answer": "Human",
    "rationale": "The figure shows that the Human line is the highest for the Empathetic Dialogues domain, which means that humans had the highest survival probability.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02140v1",
    "pdf_url": null
  },
  {
    "instance_id": "f9e40e26f63f46a6b42a52ce8310532e",
    "figure_id": "2101.05779v3-Figure5-1",
    "image_file": "2101.05779v3-Figure5-1.png",
    "caption": " Percentage of output sentences presenting different kinds of errors, when training with a variable portion of the CoNLL04 training dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of error decreases the most as the percentage of training data increases?",
    "answer": "Replication errors",
    "rationale": "The figure shows that the percentage of replication errors decreases the most as the percentage of training data increases. This is because replication errors are caused by the model generating the same output for different inputs, and this problem can be mitigated by training on more data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.05779v3",
    "pdf_url": null
  },
  {
    "instance_id": "20f330ffb56b4645a78b68e3ace1b96a",
    "figure_id": "2108.05517v2-Figure4-1",
    "image_file": "2108.05517v2-Figure4-1.png",
    "caption": " Mispronunciation correction results.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which TTS system resulted in the highest preference score?",
    "answer": "MaskedAU",
    "rationale": "The figure shows that MaskedAU has the highest preference score (44.33%).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.05517v2",
    "pdf_url": null
  },
  {
    "instance_id": "0c70e281735c43d88523cb51fe7ae875",
    "figure_id": "2012.03137v1-Figure5-1",
    "image_file": "2012.03137v1-Figure5-1.png",
    "caption": " Testing loss over synthetic datasets.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the testing loss for Clayton according to ACNet?",
    "answer": "-0.9171",
    "rationale": "The table shows the testing loss for Clayton according to both the Ground Truth and ACNet. The testing loss for Clayton according to ACNet is -0.9171.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.03137v1",
    "pdf_url": null
  },
  {
    "instance_id": "2b94a2d88d2e4a0693030e93d1f7a939",
    "figure_id": "2305.04835v3-Figure7-1",
    "image_file": "2305.04835v3-Figure7-1.png",
    "caption": " Performance under different complexity settings (on high-level combinations).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best for code with low complexity?",
    "answer": "PhraReco",
    "rationale": "The figure shows that PhraReco has the highest accuracy for code with low complexity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.04835v3",
    "pdf_url": null
  },
  {
    "instance_id": "3b811ec0d1984f029174884aaa628985",
    "figure_id": "2305.10825v3-Figure1-1",
    "image_file": "2305.10825v3-Figure1-1.png",
    "caption": " Examples of text editing. DiffUTE achieves the best result among existing models.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models produced the most accurate text editing result?",
    "answer": "DiffUTE",
    "rationale": "The caption states that \"DiffUTE achieves the best result among existing models.\" This can also be seen in the figure, where the \"Ours\" column shows the most accurate text editing results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.10825v3",
    "pdf_url": null
  },
  {
    "instance_id": "4b3dcabd990b4dbda048d97d5ae80d59",
    "figure_id": "2004.09484v1-Figure2-1",
    "image_file": "2004.09484v1-Figure2-1.png",
    "caption": " Illustration of our translation method with three domains.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which domain is the target domain?",
    "answer": "Y",
    "rationale": "The target domain is the domain that we are trying to translate to. In this case, the target domain is Y, as indicated by the orange arrow pointing to it.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.09484v1",
    "pdf_url": null
  },
  {
    "instance_id": "17f5b81f044a47b8b86641553d4752d3",
    "figure_id": "2210.16422v1-Figure3-1",
    "image_file": "2210.16422v1-Figure3-1.png",
    "caption": " Section segmentation results evaluated by F1 (higher is better) and WinDiff (lower is better). Results are reported for PubMed and arXiv. Best performance is achieved with our Lodoss-full model. ‘-LG’ means a Longformer-large model is used to encode the input document.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the arXiv dataset according to the F1 metric?",
    "answer": "Lodoss-full-LG",
    "rationale": "The figure shows the F1 scores for different models on the arXiv dataset. The Lodoss-full-LG model has the highest F1 score, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.16422v1",
    "pdf_url": null
  },
  {
    "instance_id": "af35c1baec2349e1aea97dcf2a264df5",
    "figure_id": "2210.12429v1-Figure8-1",
    "image_file": "2210.12429v1-Figure8-1.png",
    "caption": " Summary of results for each generative model prompted as being non-human rated on 40 utterances. While this is visually similar Figure 2 they are not exactly comparable as it is with a different population of utterances within the survey.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model had the highest mean Likert score for \"Possible\"?",
    "answer": "text-babbage-001",
    "rationale": "The mean Likert score for \"Possible\" is shown in the second column of the figure. The highest mean Likert score is 3.8, which is for text-babbage-001.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12429v1",
    "pdf_url": null
  },
  {
    "instance_id": "964e2c1374764702b63ef1e808d9edd5",
    "figure_id": "2002.11891v1-Figure4-1",
    "image_file": "2002.11891v1-Figure4-1.png",
    "caption": " Scatter plots and regression curves of (a) Baugh [16], (b) Wang [11], (c) BBAND, versus MOS on banding dataset [11].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, Baugh [16], Wang [11], or BBAND, has the best fit to the data?",
    "answer": "BBAND",
    "rationale": "The figure shows that the BBAND method has the closest fit to the data points, as indicated by the orange line.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11891v1",
    "pdf_url": null
  },
  {
    "instance_id": "79601588c8504d27bc59c86c89a946d1",
    "figure_id": "1905.10989v4-Figure3-1",
    "image_file": "1905.10989v4-Figure3-1.png",
    "caption": " Quality for comparative sampling.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest average quality across all three dimensions?",
    "answer": "Q'modo-σ",
    "rationale": "The figure shows the quality of each method for each of the three dimensions. The average quality for Q'modo-σ is higher than the average quality for any other method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10989v4",
    "pdf_url": null
  },
  {
    "instance_id": "ffeb94ab4e4d4297ba4ad3f7fe12219f",
    "figure_id": "2209.10077v2-Figure11-1",
    "image_file": "2209.10077v2-Figure11-1.png",
    "caption": " Image attributions extracted by the Integrated Gradients method [41]. We observe that the network is mostly sensitive to the penumbra regions, where most biometric information seems to lie.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the network appear to be most sensitive to, according to the image attributions?",
    "answer": "The penumbra regions.",
    "rationale": "The image attributions show that the network is most sensitive to the penumbra regions of the iris images. This is evident by the bright yellow and green colors in the penumbra regions of the overlaid images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.10077v2",
    "pdf_url": null
  },
  {
    "instance_id": "ba35a8c937b14374b175bddd7a377032",
    "figure_id": "1808.08946v3-Figure2-1",
    "image_file": "1808.08946v3-Figure2-1.png",
    "caption": " Accuracy of different NMT models on the subject-verb agreement task.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the subject-verb agreement task when the distance between the subject and verb is greater than 15 words?",
    "answer": "The RNN-bideep model.",
    "rationale": "The figure shows that the RNN-bideep model has the highest accuracy for distances greater than 15 words.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.08946v3",
    "pdf_url": null
  },
  {
    "instance_id": "17c01d9bab3b4645a37e40d178f9f8d6",
    "figure_id": "2209.07007v2-Figure6-1",
    "image_file": "2209.07007v2-Figure6-1.png",
    "caption": " Reconstructed images in CelebA (Liu et al., 2015). The images denote original data samples (top rows), reconstructed images (middle rows), and zoomed reconstructions (bottom rows). Each column corresponds to one data instance in the test set.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the five methods produced the most realistic reconstructions? ",
    "answer": " GWAE.",
    "rationale": " The reconstructed images in the middle row of (e) are the most similar to the original images in the top row. This indicates that GWAE was able to capture the most important features of the original data and reconstruct them accurately.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.07007v2",
    "pdf_url": null
  },
  {
    "instance_id": "f253498bd5154d7089bfa42ac6cd0056",
    "figure_id": "1907.12271v1-Figure5-1",
    "image_file": "1907.12271v1-Figure5-1.png",
    "caption": " Accuracy of all modes in the neutral setting, broken down by question type. The types and/or/progression/union reflect the type of relationship across the nine images, while attribute/object/counting correspond to the type of visual properties to which the relationship applies. Each group of bars corresponds to the methods MLP-cat-2, MLP-cat-4, MLP-cat-6, MLP-sum-2, MLP-sum-4, MLP-sum-6, GRU, GRU-shared, VQA-like, RN without panel IDs, and RN. See supplementary material for numbers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best for \"And\" questions?",
    "answer": "ResNet + aux.loss",
    "rationale": "The figure shows the accuracy of different methods for different types of questions. The bars for ResNet + aux.loss are the highest for \"And\" questions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.12271v1",
    "pdf_url": null
  },
  {
    "instance_id": "516bb1b74709466d886a30d42c0862d7",
    "figure_id": "2302.07317v3-Figure28-1",
    "image_file": "2302.07317v3-Figure28-1.png",
    "caption": " Kuzushiji-49, Number of Pulls of The Most Frequent Selection",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest selection frequency for the most frequent selection?",
    "answer": "TAILOR Div.",
    "rationale": "The figure shows that the TAILOR Div. algorithm has the highest selection frequency for the most frequent selection. This can be seen by the blue line being higher than the other lines in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.07317v3",
    "pdf_url": null
  },
  {
    "instance_id": "15b85efeb7fb44de82ab1f84b771c2a2",
    "figure_id": "1810.03292v3-Figure1-1",
    "image_file": "1810.03292v3-Figure1-1.png",
    "caption": " Saliency maps for some common methods compared to an edge detector. Saliency masks for 3 different inputs for an Inception v3 model trained on ImageNet. We see that an edge detector produces outputs that are strikingly similar to the outputs of some saliency methods. In fact, edge detectors can also produce masks that highlight features which coincide with what appears to be relevant to a model’s class prediction. Interestingly, we find that the methods that are most similar to an edge detector, i.e., Guided Backprop and its variants, show minimal sensitivity to our randomization tests.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which saliency method produces outputs that are most similar to those of an edge detector?",
    "answer": " Guided Backprop and its variants.",
    "rationale": " The figure shows that the saliency maps produced by Guided Backprop and its variants are very similar to the edge maps produced by an edge detector. This suggests that these methods may be relying heavily on edge information to identify salient regions in the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.03292v3",
    "pdf_url": null
  },
  {
    "instance_id": "d8dbf208478b427ca698ab463754f166",
    "figure_id": "2303.16716v2-Figure11-1",
    "image_file": "2303.16716v2-Figure11-1.png",
    "caption": " We cluster the edges of the simplicial complex S depicted in Figure 1, our toy example. Its first Betti number B1(S) is 6, corresponding to a 6-dimensional zeroeigenspace of L1. We show a projection of the 6-dimensional feature space X1 to 3-dimensional space. There are six different subspace clusters: three 1-dimensional lines in purple, green, and pink corresponding to ordinary loops in the point cloud. Furthermore, there are two 2-dimensional subspaces marked in brown and orange. They represent the edges in the two tori of our data set. Finally, there is one 0-dimensional cluster corresponding to the edges without a contribution to homology in the two cubes marked in blue.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many clusters are there in the 6-dimensional feature space X1?",
    "answer": "There are six different subspace clusters.",
    "rationale": "The caption states that \"There are six different subspace clusters: three 1-dimensional lines in purple, green, and pink corresponding to ordinary loops in the point cloud. Furthermore, there are two 2-dimensional subspaces marked in brown and orange. They represent the edges in the two tori of our data set. Finally, there is one 0-dimensional cluster corresponding to the edges without a contribution to homology in the two cubes marked in blue.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.16716v2",
    "pdf_url": null
  },
  {
    "instance_id": "d24d054720d848e2ad48702612eb9369",
    "figure_id": "2110.09131v2-Figure1-1",
    "image_file": "2110.09131v2-Figure1-1.png",
    "caption": " An example AMR graph for the sentence You told me to wash the dog.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the words \"you\" and \"tell\"?",
    "answer": "The word \"you\" is the agent of the verb \"tell\".",
    "rationale": "The figure shows a directed edge from \"you\" to \"tell\", indicating that \"you\" is the agent of the verb \"tell\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.09131v2",
    "pdf_url": null
  },
  {
    "instance_id": "009bc9923d9b4caf9f5300dd639a0f60",
    "figure_id": "2005.01096v1-Figure2-1",
    "image_file": "2005.01096v1-Figure2-1.png",
    "caption": " Generation process of our approach. Segment end symbol $ is ignored when updating the state of the decoder. Solid arrows indicate the transition model and dashed arrows indicate the generation model. Every segment so is generated by attending only to the corresponding data record c(so).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the figure represents the generation model?",
    "answer": "The dashed arrows.",
    "rationale": "The caption states that \"dashed arrows indicate the generation model.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.01096v1",
    "pdf_url": null
  },
  {
    "instance_id": "b3d21a87418c4478a28261410b012d0d",
    "figure_id": "2306.02623v1-Figure2-1",
    "image_file": "2306.02623v1-Figure2-1.png",
    "caption": " In the Do-GOOD benchmark, each document image is extracted from the training domain. We studied five distribution shifts acting on the three modalities respectively to generate the test domain. The five distribution shift acting includes: two image distribution shifts with (a) the distorted image background or (b) the natural image background; (c) text distribution shift with Bert-Attack and Word Swap; two layout distribution shifts by (d) merging and (e) moving the layouts.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the five distribution shifts is most likely to affect the performance of a model that relies on the spatial relationships between words?",
    "answer": "Moving the bounding boxes of entities.",
    "rationale": "This shift changes the spatial relationships between words, which can make it difficult for a model to correctly identify entities and their relationships. The figure shows how the bounding boxes of entities are moved in the test domain, which can disrupt the model's ability to learn the spatial relationships between words.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.02623v1",
    "pdf_url": null
  },
  {
    "instance_id": "2f04c9b2e10640629647fee769706ecf",
    "figure_id": "2109.03160v2-Figure6-1",
    "image_file": "2109.03160v2-Figure6-1.png",
    "caption": " Targeted downstream tasks evaluation. Dependency parsing UAS and LAS evolution.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which RoBERTa model has the highest UAS score at the final checkpoint?",
    "answer": "roberta-base-1B-3",
    "rationale": "The figure shows the UAS score for different RoBERTa models over the course of training. The UAS score for roberta-base-1B-3 is the highest at the final checkpoint.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.03160v2",
    "pdf_url": null
  },
  {
    "instance_id": "47eadcd506c2488ab1c6e7919ae88c6e",
    "figure_id": "1903.00780v1-Figure2-1",
    "image_file": "1903.00780v1-Figure2-1.png",
    "caption": " We find some gaps in overall pairwise accuracy that are improved through the pairwise regularization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which group has the higher pairwise accuracy at level 4 of engagement after pairwise regularization?",
    "answer": "Not subgroup",
    "rationale": "The figure shows that the \"Not subgroup\" group has a higher bar at level 4 of engagement after pairwise regularization than the \"Subgroup\" group.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.00780v1",
    "pdf_url": null
  },
  {
    "instance_id": "30f844702f1d454e91a881aaa06bdd11",
    "figure_id": "2112.08594v2-Figure9-1",
    "image_file": "2112.08594v2-Figure9-1.png",
    "caption": " Cluster Hierarchy for Military Vehicles",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two military vehicles are most similar to each other according to the cluster hierarchy?",
    "answer": "\"pilot_fly\" and \"woman_fly\"",
    "rationale": "The figure shows a cluster hierarchy, which is a way of grouping similar items together. The two items that are closest to each other on the tree are the most similar. In this case, the two items that are closest to each other are \"pilot_fly\" and \"woman_fly\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.08594v2",
    "pdf_url": null
  },
  {
    "instance_id": "0b81d99caa9940fc8db6174ead8b8d25",
    "figure_id": "2005.07371v2-Figure2-1",
    "image_file": "2005.07371v2-Figure2-1.png",
    "caption": " Lifelong MAPF instances with replanning period h = 2. Solid (dashed) circles represent the current (goal) locations of the agents.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to agent α3 after it reaches its goal location at timestep 2?",
    "answer": "It is assigned a new goal location.",
    "rationale": "The figure shows that at timestep 0, agent α3 has a goal location of (1, 1). At timestep 2, it reaches this goal location. The figure also shows that at timestep 2, agent α3 has a new goal location of (2, 1). This indicates that the agent is assigned a new goal location after it reaches its previous goal location.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.07371v2",
    "pdf_url": null
  },
  {
    "instance_id": "b1f21cbf3f6846b1a8927118c3f7da1f",
    "figure_id": "2301.11556v2-Figure5-1",
    "image_file": "2301.11556v2-Figure5-1.png",
    "caption": " Pinball loss functions on test-augmented hold-out data for three alternative regression models, M1,M2 and M3, as a function of the place-holder outcome y for the test point. The CES method utilizes the best model for each possible value of y, which is identified by the lower envelope of these three pinball loss functions. In this case, the lower envelope has a single finite knot at k2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "At which point do the loss functions for the models M1, M2 and M3 have the same value?",
    "answer": "At y=k2.",
    "rationale": "The three loss functions intersect at the black dot which is located at y=k2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.11556v2",
    "pdf_url": null
  },
  {
    "instance_id": "0ffe2623dc904347900334bd38d80168",
    "figure_id": "2001.07311v1-Figure4-1",
    "image_file": "2001.07311v1-Figure4-1.png",
    "caption": " Number of electricity thieves in each month.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which month did the highest number of electricity thefts occur?",
    "answer": "January",
    "rationale": "The bar for January is the tallest, indicating that the most electricity thefts occurred in that month.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.07311v1",
    "pdf_url": null
  },
  {
    "instance_id": "aa923741a47f4279ae38cab423458069",
    "figure_id": "2204.05610v1-Figure5-1",
    "image_file": "2204.05610v1-Figure5-1.png",
    "caption": " F1 of DTR and StyleDGPT (positive), and SOTA KDG models in different evaluation settings.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the KDG task with style?",
    "answer": "DTR(Gold-K)",
    "rationale": "The figure shows that DTR(Gold-K) achieves the highest F1 score (33.62%) on the KDG task with style.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.05610v1",
    "pdf_url": null
  },
  {
    "instance_id": "84e0ea7b87f64496b67927b8aae4ecaf",
    "figure_id": "2004.15014v2-Figure6-1",
    "image_file": "2004.15014v2-Figure6-1.png",
    "caption": " tSNE feature visualizations (for classes in split-1) for MAP vectors computed for foreground and background form neat clusters.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which class has the highest mIoU?",
    "answer": "bus",
    "rationale": "The mIoU values are shown below each plot. The plot for the \"bus\" class has the highest mIoU value of 84.76.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.15014v2",
    "pdf_url": null
  },
  {
    "instance_id": "e693236908be4c139644a5f04c53bc2b",
    "figure_id": "1909.09552v2-Figure8-1",
    "image_file": "1909.09552v2-Figure8-1.png",
    "caption": " Examples of different search techniques. From left to right: 1) the original input image, 2) the plot of input gradient, 3) face with ROA location identified using gradient-based search, 4) face with ROA location identified using exhaustive search. Each row is a different example.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which search technique is more accurate, gradient-based search or exhaustive search?",
    "answer": "Exhaustive search is more accurate.",
    "rationale": "The figure shows that exhaustive search correctly identifies the ROA location in both examples, while gradient-based search only correctly identifies the ROA location in the first example.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.09552v2",
    "pdf_url": null
  },
  {
    "instance_id": "55532a3a5f92470994b9ba8201677b3e",
    "figure_id": "2011.10427v1-Figure4-1",
    "image_file": "2011.10427v1-Figure4-1.png",
    "caption": " Precision and Recall on Synthetic",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest precision for all values of k?",
    "answer": "D3L",
    "rationale": "The figure shows that the blue line, which represents D3L, is consistently above the other two lines for all values of k.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.10427v1",
    "pdf_url": null
  },
  {
    "instance_id": "0f113f28f0f04e63a1a86911c1e6180c",
    "figure_id": "2202.03092v1-Figure3-1",
    "image_file": "2202.03092v1-Figure3-1.png",
    "caption": " Case Study.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many shares of Hongtu Co., Ltd. did Sanbao Co., Ltd. hold as of the disclosure date of this announcement?",
    "answer": "253212320 shares",
    "rationale": "The text in the top half of the image states that \"As of the disclosure date of this announcement, Sanbao Co., Ltd holds 253212320 shares of the company, accounting for 21.46% of the total share capital of the company.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.03092v1",
    "pdf_url": null
  },
  {
    "instance_id": "0d7ace56307f4e0a85e938528332e23b",
    "figure_id": "1904.04433v1-Figure5-1",
    "image_file": "1904.04433v1-Figure5-1.png",
    "caption": " We show the curves of the average distortions (MSEs) of the adversarial images generated by the evolutionary method with different dimensions of the search space over the number of queries. We perform dodging and impersonation attacks against SphereFace, CosFace, and ArcFace on face verification.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attack method and face verification model combination results in the lowest distortion for the generated adversarial images?",
    "answer": "The dodging attack against SphereFace.",
    "rationale": "The plot in the top left corner shows that the dodging attack against SphereFace has the lowest distortion values across all queries compared to the other attack and model combinations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.04433v1",
    "pdf_url": null
  },
  {
    "instance_id": "0a6e47aa32d94b13b69ba2d03c246cb1",
    "figure_id": "1903.03096v4-Figure6-1",
    "image_file": "1903.03096v4-Figure6-1.png",
    "caption": " Accuracy on the test datasets, when training on ILSVRC only or All datasets (same results as shown in the main tables). The bars display 95% confidence intervals.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the Aircraft dataset when trained on all datasets?",
    "answer": "fo-MAML",
    "rationale": "The bar for fo-MAML on the Aircraft dataset is the highest when trained on all datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.03096v4",
    "pdf_url": null
  },
  {
    "instance_id": "bcaa39fa6e074b1ebdc31f2f345f714c",
    "figure_id": "2311.04943v2-Figure5-1",
    "image_file": "2311.04943v2-Figure5-1.png",
    "caption": " MathNAS v.s. SOTA ViT and CNN models on ImageNet-1K.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the highest accuracy?",
    "answer": "MathNAS-T5",
    "rationale": "The figure shows the accuracy of different models on the y-axis and the number of FLOPs on the x-axis. MathNAS-T5 is the highest point on the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2311.04943v2",
    "pdf_url": null
  },
  {
    "instance_id": "1e49c4018a654976bbfa57bfb1dc7147",
    "figure_id": "2006.06069v3-Figure2-1",
    "image_file": "2006.06069v3-Figure2-1.png",
    "caption": " Practical Effect vs. Recall for different detectors against ensemble attacks on YelpNYC and YelpZip.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which detector has the highest practical effect for a given recall on YelpNYC?",
    "answer": "SpEagle",
    "rationale": "The plot shows the practical effect vs. recall for different detectors against ensemble attacks on YelpNYC. The SpEagle curve is the highest for all values of recall, indicating that it has the highest practical effect.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.06069v3",
    "pdf_url": null
  },
  {
    "instance_id": "4db4d832466044789e8c811ddba3f20b",
    "figure_id": "2012.06979v1-Figure1-1",
    "image_file": "2012.06979v1-Figure1-1.png",
    "caption": " The function g(x) defined in Active estimation for a single feature section",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the value of g(x) at x = 0.5?",
    "answer": "Approximately 0.1",
    "rationale": "The plot shows the function g(x) as a purple line. We can see that at x = 0.5, the value of g(x) is approximately 0.1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.06979v1",
    "pdf_url": null
  },
  {
    "instance_id": "5b8d06b52f9c4679b675374afeda5a00",
    "figure_id": "2105.02544v2-Figure3-1",
    "image_file": "2105.02544v2-Figure3-1.png",
    "caption": " Proportions of test examples that the predictions of generator overlap with the predictions of selector. Here only the top-1 predictions of generator and selector are used.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, SG or SGGG, consistently achieves a higher overlap between the predictions of the generator and the selector?",
    "answer": "SGGG",
    "rationale": "The bar graphs show that the percentage of test examples where the predictions of the generator and selector overlap is higher for SGGG than for SG across all four datasets for both keyphrase generation and title generation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.02544v2",
    "pdf_url": null
  },
  {
    "instance_id": "f96a079e4afd41eaa61af37ec367cdeb",
    "figure_id": "2110.02871v1-Figure6-1",
    "image_file": "2110.02871v1-Figure6-1.png",
    "caption": " Evaluation metrics for a subset of the models studied in the ablation study and presented in Table 1—models trained without pseudo labels or with DADA for the Masker are excluded—as well as the two baselines for comparison. The solid symbols indicate the median of the distribution and the error lines the bootstrapped 99 % confidence intervals. The shaded area highlights the best model—7.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best according to the F0.5 score?",
    "answer": "Model 7.",
    "rationale": "The figure shows the distribution of F0.5 scores for different models. The shaded area highlights the model with the highest median F0.5 score, which is Model 7.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02871v1",
    "pdf_url": null
  },
  {
    "instance_id": "182ccbd01668456f9efe2cb4a8ecb1ff",
    "figure_id": "2301.09254v1-Figure8-1",
    "image_file": "2301.09254v1-Figure8-1.png",
    "caption": " (a) ReLU sensitivity per non-layer layer meeting different target ReLU budget, (b) ReLU count for different layers, evaluated following the sensitivity. We used ResNet18 on CIFAR-100 for this analysis.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the highest ReLU sensitivity for the 49.5k ReLU budget?",
    "answer": "Layer 17",
    "rationale": "The plot in (a) shows the ReLU sensitivity for each layer for different ReLU budgets. The line for the 49.5k ReLU budget is the highest at layer 17.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.09254v1",
    "pdf_url": null
  },
  {
    "instance_id": "328d572783644286b307f3d36ed08ce4",
    "figure_id": "2205.15269v2-Figure17-1",
    "image_file": "2205.15269v2-Figure17-1.png",
    "caption": " Outdoor→ church (128× 128) translation with NOT with costs C2,0, C2,γ , Ck,γ .",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most realistic translation of the outdoor image to a church image?",
    "answer": "C_k,γ",
    "rationale": "The images in column (c) appear the most realistic and similar to actual churches, while the images in columns (a) and (b) are less realistic and contain artifacts.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.15269v2",
    "pdf_url": null
  },
  {
    "instance_id": "03e42d52da75493d989dacca7d6c6c9e",
    "figure_id": "2212.11541v2-Figure5-1",
    "image_file": "2212.11541v2-Figure5-1.png",
    "caption": " Additional qualitative results (5).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following techniques preserves the color of the original image best?",
    "answer": "ColTran (2)",
    "rationale": "The figure shows the results of different techniques applied to the same image. The ColTran (2) technique preserves the color of the original image best, while the other techniques either change the color or make it grayscale.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.11541v2",
    "pdf_url": null
  },
  {
    "instance_id": "cd0ac790e624435b94b165430dc31e2d",
    "figure_id": "2004.00448v2-Figure10-1",
    "image_file": "2004.00448v2-Figure10-1.png",
    "caption": " Comparison of the generalization ability on the color Gaussian denoising task. Both methods are trained on severely distorted dataset (σ = 70) and tested on the mild case (σ = 30). The baseline over-smooths the inputs or generates artifacts while ours successfully reconstructs the fine structures.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is better at reconstructing fine structures in images with mild distortion?",
    "answer": "The proposed method.",
    "rationale": "The figure shows that the baseline method over-smooths the inputs or generates artifacts, while the proposed method successfully reconstructs the fine structures. This is evident in the details of the images, such as the woman's hair, the boat's rigging, and the trees in the background.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.00448v2",
    "pdf_url": null
  },
  {
    "instance_id": "4585cc253ff04a7bbc5497f882dd3619",
    "figure_id": "1904.07235v1-Figure2-1",
    "image_file": "1904.07235v1-Figure2-1.png",
    "caption": " Evolution of the image of warped events (IWE, (3)) as the focus loss is optimized, showing how the IWE sharpens as the motion parameters θ are estimated. Motion blur (left) due to event misalignment (in the example, dominantly in horizontal direction) decreases as warped events become better aligned (right). Top row: without polarity (bk = 1 in (3)); Bottom row: with polarity (bk = pk in (3)). The last column shows the histograms of the images; the peak at zero corresponds to the pixels with no events (white in the top row, gray in the bottom row).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of polarity on the sharpness of the image of warped events (IWE)?",
    "answer": "Polarity improves the sharpness of the IWE.",
    "rationale": "The top row of the figure shows the IWE without polarity, while the bottom row shows the IWE with polarity. The bottom row images are sharper than the top row images, indicating that polarity improves the sharpness of the IWE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.07235v1",
    "pdf_url": null
  },
  {
    "instance_id": "74a6019312e3495ca170f1f95bab3049",
    "figure_id": "1902.10667v2-Figure3-1",
    "image_file": "1902.10667v2-Figure3-1.png",
    "caption": " Sample sentence with a discontinuous occurrence of an English MWE, make an effort.",
    "figure_type": "schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which words in the sentence are part of the multi-word expression (MWE)?",
    "answer": "\"make\" and \"effort\".",
    "rationale": "The figure shows that the words \"make\" and \"effort\" are connected by a green arc, indicating that they form a MWE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.10667v2",
    "pdf_url": null
  },
  {
    "instance_id": "3e9c08f3075440be82c75329a057d170",
    "figure_id": "2212.00116v1-Figure2-1",
    "image_file": "2212.00116v1-Figure2-1.png",
    "caption": " Performance evaluation with random activity pattern.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest normalized mean squared error (NMSE) for a pilot length of 12?",
    "answer": "Algorithm 1",
    "rationale": "The NMSE for Algorithm 1 is approximately 10^0 for a pilot length of 12, which is higher than the NMSE for any other algorithm at that pilot length.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.00116v1",
    "pdf_url": null
  },
  {
    "instance_id": "eb4c85576a384df4bbcff469e88c1da4",
    "figure_id": "2212.11185v1-Figure4-1",
    "image_file": "2212.11185v1-Figure4-1.png",
    "caption": " Improvements in CDR model log-likelihood from including ATTN-N+NAE, ATTNRL-N+MD, and both on the held-out partition of Natural Stories self-paced reading data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which combination of NAE and MD results in the largest improvement in CDR model log-likelihood?",
    "answer": "NAE & MD",
    "rationale": "The figure shows that the combination of NAE & MD results in the largest improvement in CDR model log-likelihood, with a p-value of < 0.001. This is evident from the height of the bars in the figure, which represent the ΔLL values for each combination of NAE and MD.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.11185v1",
    "pdf_url": null
  },
  {
    "instance_id": "577fb136b7bd4e27b5e50b0fc55bf5a4",
    "figure_id": "2006.07897v4-Figure1-1",
    "image_file": "2006.07897v4-Figure1-1.png",
    "caption": " Normalized local entropy ΦLE as a function of the squared distance d (left), training error difference δEtrain as a function of perturbation intensity σ (center) and test error distribution (right) for a committee machine as defined in Eq. 7, trained with various algorithms on the reduced version of the Fashion-MNIST dataset. Results are obtained using 50 random restarts for each algorithm.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest test error?",
    "answer": "SGD slow",
    "rationale": "The test error distribution plot (right) shows that SGD slow has the highest frequency of test errors in the range of 0.07 to 0.08.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.07897v4",
    "pdf_url": null
  },
  {
    "instance_id": "30595cca76a242a69539cd552dbf563f",
    "figure_id": "1903.10104v1-Figure3-1",
    "image_file": "1903.10104v1-Figure3-1.png",
    "caption": " Distribution of proposition type per venue.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which venue has the highest percentage of evaluation propositions?",
    "answer": "ICLR",
    "rationale": "The figure shows that ICLR has the highest percentage of evaluation propositions, at around 40%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.10104v1",
    "pdf_url": null
  },
  {
    "instance_id": "fe306deefd214effb540abb1ece4c229",
    "figure_id": "2106.15535v2-Figure14-1",
    "image_file": "2106.15535v2-Figure14-1.png",
    "caption": " Results on OGBN-Arxiv. Test accuracy disparity across subgroups by geodesic distance. The experiment and plot settings are the same as Figure 10, except for the aggregated-feature distance is replaced by the geodesic distance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, GCN or GraphSAGE, has higher accuracy on average across the subgroups?",
    "answer": "GCN.",
    "rationale": "The average height of the bars in Figure 10(a) is higher than the average height of the bars in Figure 10(b). This indicates that GCN has higher accuracy on average across the subgroups.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15535v2",
    "pdf_url": null
  },
  {
    "instance_id": "73d2984f6aa9492889b51055eeb6c9e5",
    "figure_id": "2211.11620v1-Figure6-1",
    "image_file": "2211.11620v1-Figure6-1.png",
    "caption": " An example of the update order for Algorithm 1 with κt = 2 and Kmax = 3. Applications of T̂ ?t and T̂κtt are denoted, respectively, by magenta and blue nodes, while dashed arrows represent the bootstrap persistence.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many times is the operator T̂ applied in the figure?",
    "answer": "5 times.",
    "rationale": "The figure shows the update order for Algorithm 1 with κt = 2 and Kmax = 3. The operator T̂ is applied once for each of the magenta nodes and once for each of the blue nodes. There are 3 magenta nodes and 2 blue nodes, so the operator T̂ is applied a total of 5 times.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.11620v1",
    "pdf_url": null
  },
  {
    "instance_id": "812e06551c0e43409a3eb863e34eafdf",
    "figure_id": "1911.08718v2-Figure7-1",
    "image_file": "1911.08718v2-Figure7-1.png",
    "caption": " Ablation study on our network structure. We plot the results of our full methods with the input in yellow region. The baseline network CAN can remove the shadow but still have color in-consistency, and our feature LA(+LA) and dual LA(+DLA) can further remove the artifacts.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following methods is most effective at removing shadows and color inconsistencies from images?",
    "answer": "+DLA",
    "rationale": "The figure shows the results of different methods for removing shadows and color inconsistencies from images. The +DLA method produces the most accurate results, as it removes both the shadows and the color inconsistencies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.08718v2",
    "pdf_url": null
  },
  {
    "instance_id": "564f767f864f46dbbb3563e26802867d",
    "figure_id": "2208.13298v4-Figure9-1",
    "image_file": "2208.13298v4-Figure9-1.png",
    "caption": " Complete ContinuousSeek Results for d = 10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following combinations of parameters resulted in the highest success rate? \n(a) ReenGAGE(α=0.1) + HER, batch = 128 \n(b) ReenGAGE(α=0.3) + HER, batch = 512 \n(c) DDPG+HER, batch = 256 \n(d) ReenGAGE(α=0.2) + HER, batch = 128",
    "answer": "(b) ReenGAGE(α=0.3) + HER, batch = 512",
    "rationale": "The figure shows the success rate of different combinations of parameters for the ContinuousSeek task. The highest success rate is achieved by ReenGAGE(α=0.3) + HER, batch = 512, as shown by the green line in the bottom right plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.13298v4",
    "pdf_url": null
  },
  {
    "instance_id": "f7b8c38a8316459ea99c03caf0ad3d89",
    "figure_id": "1909.00295v1-Figure3-1",
    "image_file": "1909.00295v1-Figure3-1.png",
    "caption": " Examples of non-local covariant attention heatmaps with different viewpoints. The green points in each heatmap are the reference points and the red points are the top related points. We can see that when the reference points (green) are located within the body region, their highly related red points are also in the body region capturing salient features such as logos on the shoes or watches. The background reference points are more related to background points.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Based on the figure, what are the most salient features that the non-local covariant attention heatmaps capture when the reference points are located within the body region?",
    "answer": "Logos on the shoes or watches.",
    "rationale": "The figure shows that when the reference points are located within the body region, the highly related red points are also in the body region, capturing salient features such as logos on the shoes or watches.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.00295v1",
    "pdf_url": null
  },
  {
    "instance_id": "fb286bc4cbfc4d99a0ec97a6f7a8c8a4",
    "figure_id": "1809.01696v2-Figure2-1",
    "image_file": "1809.01696v2-Figure2-1.png",
    "caption": " Distribution of question types based on answer types.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common type of question asked based on the answer types?",
    "answer": "Abstract (what)",
    "rationale": "The pie chart shows that 21.5% of the questions are of the type \"Abstract (what)\". This is the largest percentage of any category, making it the most common type of question asked.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.01696v2",
    "pdf_url": null
  },
  {
    "instance_id": "64d5a374081949b39f6ff91c4ed34918",
    "figure_id": "2305.18729v3-Figure8-1",
    "image_file": "2305.18729v3-Figure8-1.png",
    "caption": " RIVAL extended to self-example image inpainting. We use the same image as the example to fill the masked area. Results are compared with a well-trained inpainting SOTA method [25].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most realistic inpainting results for the lynx image?",
    "answer": "RIVAL",
    "rationale": "The RIVAL method produced an inpainting result that was very similar to the original image, while the Paint-by-example method produced a result that was more blurry and less detailed. The RIVAL-variation 2 method produced a result that was slightly different from the original image, but still very realistic.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.18729v3",
    "pdf_url": null
  },
  {
    "instance_id": "ea7cd90cc1554efe8d4690f1a41ce3d7",
    "figure_id": "2207.12534v3-Figure8-1",
    "image_file": "2207.12534v3-Figure8-1.png",
    "caption": " Training curves during retraining with ResNet56 on CIFAR10 at different pruning ratios (PRs). We can observe that at large PRs (0.9, 0.95), TPP significantly accelerates the optimization in the comparison to L1 (Li et al., 2017), because of better trainability preserved before retraining.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pruning method, L1 or TPP, performs better at large pruning ratios?",
    "answer": "TPP performs better at large pruning ratios.",
    "rationale": "The figure shows that at large pruning ratios (0.9, 0.95), TPP significantly accelerates the optimization in comparison to L1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.12534v3",
    "pdf_url": null
  },
  {
    "instance_id": "4cb3fb35050a482e8830265939fda323",
    "figure_id": "1912.06126v2-Figure4-1",
    "image_file": "1912.06126v2-Figure4-1.png",
    "caption": " Representation efficiency. F-score vs. model complexity. Curves show varying M for constant N . Other methods marked as points. Top: F-score vs. count of decoder parameters. The N = 32,M = 32 configuration (large dot) reaches >90% F-score with <1% of the parameters of OccNet, and is used as the benchmark configuration in this paper. Bottom: F-score vs. shape vector dimension (|Θ|+ |Z| for DSIF). DSIF achieves similar reconstruction accuracy to OccNet at the same dimensionality, and can use additional dimensions to further improve accuracy.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model configuration achieves the highest F-score with the fewest decoder parameters?",
    "answer": "The DSIF model with N = 32 and M = 32.",
    "rationale": "The top plot shows that the DSIF model with N = 32 and M = 32 achieves an F-score of over 90% with fewer than 1% of the parameters of OccNet.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.06126v2",
    "pdf_url": null
  },
  {
    "instance_id": "67ed0c8a29d043c9bef3889968d3c361",
    "figure_id": "2009.05169v4-Figure1-1",
    "image_file": "2009.05169v4-Figure1-1.png",
    "caption": " An illustration of sparse attention matrices assuming a three-layer encoder and decoder (separated by the dashed line). The blue color reflects the memory consumption of self-attention (encoder) and cross-attention (decoder). (A) The complete input consumed at once. (B) Memory reduced with blockwise attention and (C) pooling applied after the encoder. (D) Gradual reduction of memory by pooling after every layer.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four attention mechanisms depicted in the figure uses the least memory?",
    "answer": "Pyramiding (D).",
    "rationale": "The blue color in the figure represents the memory consumption of self-attention (encoder) and cross-attention (decoder). In (D), the memory is gradually reduced by pooling after every layer, resulting in the least memory consumption overall.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.05169v4",
    "pdf_url": null
  },
  {
    "instance_id": "f2c59a6ef1a34aaa9da4ab1068e16472",
    "figure_id": "2301.05033v1-Figure7-1",
    "image_file": "2301.05033v1-Figure7-1.png",
    "caption": " Visualizing learned patches on sub-point clouds of 2048 points.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does each color represent in the figure?",
    "answer": "Each color represents a different learned patch.",
    "rationale": "The figure shows a visualization of learned patches on sub-point clouds of 2048 points. Each point in the sub-point cloud is colored according to the learned patch that it belongs to.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.05033v1",
    "pdf_url": null
  },
  {
    "instance_id": "28b394d7e83e4c648d1a875730835e62",
    "figure_id": "2212.03827v1-Figure11-1",
    "image_file": "2212.03827v1-Figure11-1.png",
    "caption": " Linear regression performance when using the hidden states across different layers, using all six models we consider in the paper. This is the ceiling of all possible methods, and is supervised.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best according to the plot?",
    "answer": "RoBERTa performs the best according to the plot.",
    "rationale": "The plot shows the accuracy of different models across different layers. RoBERTa has the highest accuracy across most of the layers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.03827v1",
    "pdf_url": null
  },
  {
    "instance_id": "2a945117a8ec4055bdaf5b004ce261f6",
    "figure_id": "2006.04026v1-Figure5-1",
    "image_file": "2006.04026v1-Figure5-1.png",
    "caption": " Qualitative results on the test set of the Make3D dataset [38]. In the top row, some far tree structures that are missing in the depth map predicted by GASDA were better captured on using the SharinGAN module. For the bottom row, GASDA wrongly predicts the depth map of the houses behind the trees to be far, which is correctly captured by the SharinGAN.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces a more accurate depth map, GASDA or SharinGAN?",
    "answer": "SharinGAN.",
    "rationale": "The caption states that SharinGAN better captures far tree structures that are missing in the depth map predicted by GASDA. Additionally, GASDA wrongly predicts the depth map of the houses behind the trees to be far, which is correctly captured by SharinGAN.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04026v1",
    "pdf_url": null
  },
  {
    "instance_id": "b68768e6f3e84531b29b9142751ec948",
    "figure_id": "2012.15562v3-Figure3-1",
    "image_file": "2012.15562v3-Figure3-1.png",
    "caption": " Sample efficiency. For \"MF\" we leverage the MF10 KMEANS-LEX setting.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four types of data has the highest F1 score at a dataset size of 10^4?",
    "answer": "MF",
    "rationale": "The figure shows that the blue line, which represents the MF data, is the highest at a dataset size of 10^4.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.15562v3",
    "pdf_url": null
  },
  {
    "instance_id": "19a92e8347384162ac38e969820c1def",
    "figure_id": "2106.08942v1-Figure1-1",
    "image_file": "2106.08942v1-Figure1-1.png",
    "caption": " Change in probability for gold tokens to belong to each rank before and after RL on in-domain data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method resulted in the largest decrease in probability for gold tokens ranked 100 or higher?",
    "answer": "The \"no baseline\" method.",
    "rationale": "The figure shows that the \"no baseline\" method (orange bars) has the largest negative change in probability for gold tokens ranked 100 or higher.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.08942v1",
    "pdf_url": null
  },
  {
    "instance_id": "d919ae8473b644a7b520328e986b48af",
    "figure_id": "2205.02959v6-Figure2-1",
    "image_file": "2205.02959v6-Figure2-1.png",
    "caption": " Dynamic Bayesian network for the behavior of a 2-agent team with time-invariant latent states depicted using plate notation.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many latent states are there in the model?",
    "answer": "Two.",
    "rationale": "The figure shows two latent states,  and . These states are represented by the circles labeled and , respectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.02959v6",
    "pdf_url": null
  },
  {
    "instance_id": "b0641c8f018a41e798997a31a8891c46",
    "figure_id": "2108.06885v1-Figure4-1",
    "image_file": "2108.06885v1-Figure4-1.png",
    "caption": " Comparison of NADAR to WRN32-10 backbone and randomly dilated hybrid networks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three network architectures has the highest median adversarial validation accuracy?",
    "answer": "NADAR-A/B",
    "rationale": "The box plot for NADAR-A/B has the highest median line, which indicates that the median adversarial validation accuracy for this architecture is higher than that of the other two architectures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.06885v1",
    "pdf_url": null
  },
  {
    "instance_id": "056c35a1d5d54e02bcbfcd09b295c972",
    "figure_id": "2010.14851v1-Figure2-1",
    "image_file": "2010.14851v1-Figure2-1.png",
    "caption": " Qualitative Example of the Displacement Probability Distribution with Different Kinds of Matching Costs. The intersection of two yellow lines shows the ground truth location. ‘MLP’ indicates predicting the matching cost with a three-layer multilayer perceptron.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which matching cost function seems to produce the most accurate displacement probability distribution?",
    "answer": "DICL",
    "rationale": "The figure shows the displacement probability distribution for different matching cost functions. The ground truth location is indicated by the intersection of the two yellow lines. The DICL function produces a distribution that is most concentrated around the ground truth location, indicating that it is the most accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.14851v1",
    "pdf_url": null
  },
  {
    "instance_id": "bfab1617890f4493ac9d8ec885915900",
    "figure_id": "2306.02671v1-Figure3-1",
    "image_file": "2306.02671v1-Figure3-1.png",
    "caption": " The duration required to train one epoch on synthetic datasets with different length (x = S = T ). Thick and shallow lines are fitted curves based on time complexities of vNQ2, EModel and PModel, i.e., O(x6), O(x3) and O(x5).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is the most efficient in terms of training time?",
    "answer": "P Model",
    "rationale": "The P Model has the lowest time complexity, O(x^3), which means that it requires the least amount of time to train for a given dataset length. This is also evident from the plot, where the green line representing the P Model is consistently below the other lines.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.02671v1",
    "pdf_url": null
  },
  {
    "instance_id": "aeb9117f6b1849708573f48e9b97dc45",
    "figure_id": "2010.03250v3-Figure1-1",
    "image_file": "2010.03250v3-Figure1-1.png",
    "caption": " Illustration of 𝐹A (·) with 𝐾 = 2 for an example academic network (best viewed in color). Hidden weight matrices and non-linearity are omitted for ease of illustration. Here author (A) is the target node type related to the evaluation task which predicts research areas, and the derived meta graph does not propagate information from I to A. Candidate edge types for each link in the search space are shown with task-dependent type constraint.",
    "figure_type": "** Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the target node type in the figure? ",
    "answer": " Author (A). ",
    "rationale": " The caption states that \"author (A) is the target node type related to the evaluation task which predicts research areas\". ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.03250v3",
    "pdf_url": null
  },
  {
    "instance_id": "d54661eeb9f14367994c709a2c3a3fb0",
    "figure_id": "2310.00697v1-Figure6-1",
    "image_file": "2310.00697v1-Figure6-1.png",
    "caption": " Graph correlation obtained from our learned propagation distributions. Similar graphs are more correlated, such as Cora is closer to CiteSeer than Texas.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which two graphs are the most similar according to the L2S metric?",
    "answer": "Cora and CiteSeer.",
    "rationale": "The L2S heatmap shows the correlation between the different graphs. The darker the color, the higher the correlation. The cell corresponding to Cora and CiteSeer is the darkest, indicating that these two graphs are the most similar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.00697v1",
    "pdf_url": null
  },
  {
    "instance_id": "08eacabc7858483ca1c643762a6f16d3",
    "figure_id": "2308.11025v1-Figure10-1",
    "image_file": "2308.11025v1-Figure10-1.png",
    "caption": " Visualization of Stability with High Frequency.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method(s) produce the most realistic reconstruction of the rabbit?",
    "answer": "Ours(HF-NeuS) and Ours(NeuS)",
    "rationale": "The figure shows four different reconstructions of a rabbit, each generated using a different method. The two reconstructions generated using our methods (Ours(HF-NeuS) and Ours(NeuS)) are the most realistic, as they accurately capture the rabbit's shape and features. The other two reconstructions (HF-NeuS and NeuS) are less realistic, as they produce artifacts and distortions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.11025v1",
    "pdf_url": null
  },
  {
    "instance_id": "c816869ad0f1439bb1c6119a09aa2eb3",
    "figure_id": "2304.08971v1-Figure4-1",
    "image_file": "2304.08971v1-Figure4-1.png",
    "caption": " Qualitative comparisons of rendering quality. The 1-2 column are the results of no per-scene optimization. And the left three columns are per-scene optimization results. It is obvious that with or without per-scene optimization, our method can still achieve the most photorealistic rendering results. Since PointNeRF without a depth refinement network relies on captured depths only, it cannot handle the scenario that depth is incomplete. However, our method can tackle this problem with a lightweight depth refinement network. ADOP can produce geometrically correct results, but its color is less photorealistic.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most photorealistic rendering results, according to the caption?",
    "answer": "The author's method.",
    "rationale": "The caption states that \"with or without per-scene optimization, our method can still achieve the most photorealistic rendering results.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.08971v1",
    "pdf_url": null
  },
  {
    "instance_id": "d1c3bcb0c3ea49b38ed7ba4a514e5187",
    "figure_id": "1912.12179v1-Figure4-1",
    "image_file": "1912.12179v1-Figure4-1.png",
    "caption": " Mutual Information heatmaps allow to understand which local patches contributed the most to the final representation. For each heatmap we plot the absolute values in the rightmost plot and the superposition of the heatmap and the original image on the left, to increase interpretability. Yellow corresponds to higher scores.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the most interpretable representations, according to the Mutual Information heatmaps?",
    "answer": "Fully Supervised.",
    "rationale": "The Fully Supervised method produced heatmaps with the highest scores, as indicated by the yellow color. This suggests that the representations learned by this method are more interpretable than the representations learned by the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.12179v1",
    "pdf_url": null
  },
  {
    "instance_id": "20088574218f46d2b4351bea21f7037a",
    "figure_id": "2210.03809v2-Figure1-1",
    "image_file": "2210.03809v2-Figure1-1.png",
    "caption": " OK-VQA contains questions whose answer cannot be found within the image.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the animal in the image?",
    "answer": "A cat.",
    "rationale": "The image shows a cat lying on a windowsill.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.03809v2",
    "pdf_url": null
  },
  {
    "instance_id": "542371e97e8f4f6a8f80a007c81d22ee",
    "figure_id": "2001.06838v2-Figure2-1",
    "image_file": "2001.06838v2-Figure2-1.png",
    "caption": " Plot of batch statistics from layer1.0.bn1 in ResNet-50 during training. The formulation of these batch statistics (µB, σ2 B, gB, ΨB) have been shown in Section 3.1. Blue line represents the small batch statistic (|B| = 2) to compute, while orange line represents the regular batch statistics(|B| = 32). The x-axis represents the iterations, while the y-axis represents the l2 norm of these statistics in each figures. Notice the mean of g and Ψ is close to zero, hence l2 norm of gB and ΨB essentially represent their standard deviation.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which batch statistic has the highest standard deviation for both small and regular batch sizes?",
    "answer": " ΨB",
    "rationale": " The figure shows that the l2 norm of ΨB is the highest for both small and regular batch sizes. Since the mean of Ψ is close to zero, the l2 norm of ΨB essentially represents its standard deviation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.06838v2",
    "pdf_url": null
  },
  {
    "instance_id": "831db2019f2148c89612159580646047",
    "figure_id": "2212.10236v1-Figure5-1",
    "image_file": "2212.10236v1-Figure5-1.png",
    "caption": " Qualitative analysis of pair, unpair, and Self-Pair. True positives (TP), false positives (FP), and false negatives (FN) are represented as green, red, and blue, respectively.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, Pair, Unpair, or Self-Pair, is the most accurate in detecting changes in the images?",
    "answer": "Self-Pair is the most accurate method.",
    "rationale": "The Self-Pair method has the fewest false positives and false negatives, as shown by the red and blue pixels in the image. This means that it is more accurate in detecting changes in the images than the other two methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10236v1",
    "pdf_url": null
  },
  {
    "instance_id": "69841d29c3a14781bfbdf389235e5630",
    "figure_id": "1909.09569v3-Figure12-1",
    "image_file": "1909.09569v3-Figure12-1.png",
    "caption": " More test accuracy (%) curves of DARTS, ENAS, AmoebaNet, NASNet and their random variants of operations on CIFAR-10 during training.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which NAS method achieves the highest test accuracy on CIFAR-10?",
    "answer": "NASNet.",
    "rationale": "The figure shows the test accuracy curves of different NAS methods on CIFAR-10 during training. NASNet has the highest curve, indicating that it achieves the highest test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.09569v3",
    "pdf_url": null
  },
  {
    "instance_id": "a102efd443044754a59dc3ba37c50b48",
    "figure_id": "2112.08340v3-Figure5-1",
    "image_file": "2112.08340v3-Figure5-1.png",
    "caption": " Training and validation loss curves for different initialization of our model. GenIE starts from a random initialization, GenIE – PLM fine-tunes a BART pre-trained language model, while GenIE - GENRE is initialized with a pre-trained autoregressive entity linking model by De Cao et al. (2021b).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which initialization method resulted in the lowest validation loss?",
    "answer": "GenIE - PLM",
    "rationale": "The validation loss curves show that the GenIE - PLM model has the lowest validation loss of the three models. This can be seen by comparing the final points of the three curves in the validation plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.08340v3",
    "pdf_url": null
  },
  {
    "instance_id": "5d216580b22b431eb329c5aaa0e1df3f",
    "figure_id": "2005.00052v3-Figure13-1",
    "image_file": "2005.00052v3-Figure13-1.png",
    "caption": " Mean F1 scores of XLM-RLarge for cross-lingual transfer on NER.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which target language has the highest mean F1 score for cross-lingual transfer on NER with XLM-RLarge?",
    "answer": "gn",
    "rationale": "The figure shows the mean F1 scores for cross-lingual transfer on NER with XLM-RLarge. The highest score is for the target language gn, which has a score of 64.9.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.00052v3",
    "pdf_url": null
  },
  {
    "instance_id": "5183b6fb1f204fb4abc4b3d78ffe72a6",
    "figure_id": "1905.10498v2-Figure10-1",
    "image_file": "1905.10498v2-Figure10-1.png",
    "caption": " SVM error rates for various values of the regularization parameter C (left plot) and the RBF kernel parameter g (right plot) after training on the QMNIST training set. Red circles: testing on MNIST. Blue triangles: testing on its QMNIST counterpart. Green stars: testing on the 50,000 new QMNIST testing examples.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which value of the RBF kernel parameter g results in the lowest test error on MNIST when C=10?",
    "answer": "g=0.02",
    "rationale": "The right plot shows the test error for different values of g when C=10. The red circles represent the test error on MNIST. The red circle at g=0.02 has the lowest y-value, indicating the lowest test error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10498v2",
    "pdf_url": null
  },
  {
    "instance_id": "524091ab09c34afd867953fe54f6bc47",
    "figure_id": "2305.01876v5-Figure7-1",
    "image_file": "2305.01876v5-Figure7-1.png",
    "caption": " An example to illustrate how to perform the pointer network.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the predicted concept with the highest confidence score according to the pointer network?",
    "answer": "novelist",
    "rationale": "The figure shows the predicted concept with the highest confidence score is novelist, with a score of 0.35.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.01876v5",
    "pdf_url": null
  },
  {
    "instance_id": "7950dba2e52b4941b31964335625696a",
    "figure_id": "2306.07117v1-Figure1-1",
    "image_file": "2306.07117v1-Figure1-1.png",
    "caption": " Gaussian kernel estimates of the distributions of agreed prices among successful negotiations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which negotiation strategy resulted in a higher average agreed price?",
    "answer": "Alternating offers.",
    "rationale": "The figure shows that the distribution of agreed prices for alternating offers is shifted to the right compared to the distribution for natural language. This indicates that, on average, alternating offers resulted in higher agreed prices.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.07117v1",
    "pdf_url": null
  },
  {
    "instance_id": "3a4d8a4a2a70468cbbdb3c0da80f4c99",
    "figure_id": "2001.01408v1-Figure13-1",
    "image_file": "2001.01408v1-Figure13-1.png",
    "caption": " Example successful predictions.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the predicted products is the most similar to the ground truth?",
    "answer": "The third predicted product.",
    "rationale": "The third predicted product has a similarity score of 1.0, which is higher than the other predicted products. This means that it is the most similar to the ground truth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.01408v1",
    "pdf_url": null
  },
  {
    "instance_id": "7fe02e1237274d558b873ce7dc487f57",
    "figure_id": "1911.03642v3-Figure2-1",
    "image_file": "1911.03642v3-Figure2-1.png",
    "caption": " Proportion of sentences corresponding to a given relation over total sentences in WikiGenderBias for each entity. This demonstrates that, of the entities we sampled to create WikiGenderBias, the spouse relation is expressed more often relative to the birthdate, birthplace, and hypernym relations in articles about female entities than in articles about male entities. Additionally, hypernym is mentioned more often relative to the other relations in articles about male entities than in articles about female entities.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What relation is mentioned most often in articles about male entities compared to female entities? ",
    "answer": " Hypernym. ",
    "rationale": " The figure shows that the proportion of sentences corresponding to the hypernym relation is higher for male entities than for female entities. This suggests that hypernyms are mentioned more often in articles about male entities than in articles about female entities. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.03642v3",
    "pdf_url": null
  },
  {
    "instance_id": "cdaee98077634bc6a3ac8eec2f5d870e",
    "figure_id": "2006.16913v3-Figure6-1",
    "image_file": "2006.16913v3-Figure6-1.png",
    "caption": " Datasets for HOC and Karel tasks.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tasks have the greatest depth?",
    "answer": "H5, H6, and K9 all have the greatest depth of 3.",
    "rationale": "The C_depth column of the table shows the depth of each task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.16913v3",
    "pdf_url": null
  },
  {
    "instance_id": "f54dce91c14a4033ad544c86813f05d9",
    "figure_id": "2303.06856v1-Figure3-1",
    "image_file": "2303.06856v1-Figure3-1.png",
    "caption": " Graph Representation of Task-adaptive Sub-network The finalized sub-network topologies (M = 7) trained with NYUv2 datasets is illustrated as graph. (a-c) The task-adaptive subnetwork of semantic segmentation, depth estimation, and surface normal, respectively. (d) The adjacency matrix where color represents the discretized value for the activated edge of each task.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which task uses the most complex sub-network?",
    "answer": "The semantic segmentation task.",
    "rationale": "The adjacency matrix in Figure (d) shows that the semantic segmentation task (purple) has the most activated edges, indicating a more complex sub-network compared to the other tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.06856v1",
    "pdf_url": null
  },
  {
    "instance_id": "61a772180ffe4cfdabd9de4009ed9747",
    "figure_id": "2206.10926v2-Figure3-1",
    "image_file": "2206.10926v2-Figure3-1.png",
    "caption": " Mean role locations of each formation group with the proportion (%) in terms of playing time.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which formation group is most likely to have players located on the right side of the field?",
    "answer": "Formation group 4-1-3-2.",
    "rationale": "The figure shows the mean role locations of each formation group. In formation group 4-1-3-2, the majority of players are located on the right side of the field. This can be seen by looking at the location of the points in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.10926v2",
    "pdf_url": null
  },
  {
    "instance_id": "8bad303326604795b518f6884e410b6d",
    "figure_id": "2010.10505v1-Figure6-1",
    "image_file": "2010.10505v1-Figure6-1.png",
    "caption": " Qualitative results from PASCAL3D+ reconstruction. Compared to the two baseline methods, SDF-SRN recovers significantly more accurate 3D shapes and topologies from the images. Both CMR and DVR struggle to associate meaningful shape regularities within category, while CMR additionally suffers from topological limitations due to its mesh-based nature, as with SoftRas.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods tested produced the most accurate 3D shapes and topologies?",
    "answer": "SDF-SRN",
    "rationale": "The caption states that \"SDF-SRN recovers significantly more accurate 3D shapes and topologies from the images.\" This can also be seen by comparing the reconstructed shapes to the ground truth (GT) shapes in the figure. The SDF-SRN shapes are much closer to the GT shapes than the CMR and DVR shapes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.10505v1",
    "pdf_url": null
  },
  {
    "instance_id": "b90481ae1b8d442cbe42f7bf6ade5daf",
    "figure_id": "1906.07987v1-Figure20-1",
    "image_file": "1906.07987v1-Figure20-1.png",
    "caption": " Atari. Unnormalized MSVE results for TD(λ). Confidence intervals over 20 seeds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest MSVE for Breakout?",
    "answer": "Adaptive TDC(3)",
    "rationale": "The plot for Breakout shows that the Adaptive TDC(3) line is consistently below the other lines, indicating that it has the lowest MSVE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.07987v1",
    "pdf_url": null
  },
  {
    "instance_id": "955b95c295b946b485b13bc4c24db4e2",
    "figure_id": "2204.02877v2-Figure3-1",
    "image_file": "2204.02877v2-Figure3-1.png",
    "caption": " Adaptation performance of different algorithms in four domains. Experimental results of algorithm generalization in new environment. Our results are even better than PPO in some environments, which proves that our method has a better generalization ability by learning in the training environment. For the PDVF algorithm, we use the source code provided by the authors.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best in the Ant-Wind environment?",
    "answer": "PPO",
    "rationale": "The figure shows the average return for each algorithm in each environment. In the Ant-Wind environment, PPO has the highest average return.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.02877v2",
    "pdf_url": null
  },
  {
    "instance_id": "bca2b1521e054a66bc36faa04e6c706e",
    "figure_id": "2110.11202v2-Figure6-1",
    "image_file": "2110.11202v2-Figure6-1.png",
    "caption": " The number of unique states visited (measured by a hashing) in Atari games as a function of the number of game episodes. Each plot corresponds to a different intrinsic reward scheme, and PPO is being trained only to maximize this intrinsic reward.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which intrinsic reward scheme leads to the agent visiting the most unique states in the Breakout game?",
    "answer": "ACB",
    "rationale": "The ACB line in the Breakout plot is the highest of the three lines, indicating that the agent visits more unique states when trained with ACB than with RND or ICM.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.11202v2",
    "pdf_url": null
  },
  {
    "instance_id": "64243aaf20b94f6da3f14f2ea2babc4a",
    "figure_id": "2210.01558v1-Figure6-1",
    "image_file": "2210.01558v1-Figure6-1.png",
    "caption": " Comparison of qualitative results on ScanNet-v2 validation set. H denotes the entropy visualization.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate reconstruction of the scene?",
    "answer": "H-GaLA.",
    "rationale": "The figure shows the ground truth (GT) reconstruction of the scene, as well as the reconstructions produced by several different methods. The H-GaLA reconstruction is the most similar to the ground truth, indicating that it is the most accurate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.01558v1",
    "pdf_url": null
  },
  {
    "instance_id": "c516c7a18ad04c0c9a2e2ed5acb53580",
    "figure_id": "2011.02556v1-Figure5-1",
    "image_file": "2011.02556v1-Figure5-1.png",
    "caption": " Dynamic Address Pool.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which cluster has the most available addresses?",
    "answer": "Cluster 1",
    "rationale": "Cluster 1 has two available addresses, while Clusters 2 and 3 each have only one available address. This can be seen in the figure by looking at the number of addresses in each cluster that are not marked as \"Deleted.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.02556v1",
    "pdf_url": null
  },
  {
    "instance_id": "f5d66f3391d74b158d79075dcf33177b",
    "figure_id": "2108.01899v2-Figure6-1",
    "image_file": "2108.01899v2-Figure6-1.png",
    "caption": " Proxy task search.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs better in terms of Spearman's Rho with increasing number of samples?",
    "answer": "Regularized Evolutionary Algorithm.",
    "rationale": "The plot shows that the blue line (Regularized Evolutionary) is consistently above the orange line (Random Search), indicating a higher Spearman's Rho for the Regularized Evolutionary algorithm across the range of sample numbers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.01899v2",
    "pdf_url": null
  },
  {
    "instance_id": "78e061d8ec144cd1bfd52992814e2c48",
    "figure_id": "1908.11044v1-Figure5-1",
    "image_file": "1908.11044v1-Figure5-1.png",
    "caption": " Reconstruction error for motion capture data under different conditions. Reported averages over 20 executions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most robust to noise?",
    "answer": "DLOE with independent images",
    "rationale": "The figure shows that DLOE with independent images has the lowest reconstruction error for all noise levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.11044v1",
    "pdf_url": null
  },
  {
    "instance_id": "e3f0a75eb1624061bf7630ef2130eb71",
    "figure_id": "1906.02506v2-Figure15-1",
    "image_file": "1906.02506v2-Figure15-1.png",
    "caption": " Histograms of predictive entropy for out-of-distribution tests for AlexNet trained on CIFAR-10 with data augmentation. Going from left to right, the inputs are: the in-distribution dataset (CIFAR-10), followed by out-of-distribution data: SVHN, LSUN (crop), LSUN (resize). Also shown are the AUROC metric (higher is better) and FPR at 95% TPR metric (lower is better), averaged over 3 runs. The standard deviations are very small and so not reported here.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the best performance for out-of-distribution detection on the LSUN (resize) dataset?",
    "answer": "MC-Dropout",
    "rationale": "The figure shows the FPR at 95% TPR metric for each method on the LSUN (resize) dataset. MC-Dropout has the lowest FPR at 95% TPR, which indicates the best performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.02506v2",
    "pdf_url": null
  },
  {
    "instance_id": "95d2810abbe743a49075f45dbf26ac4e",
    "figure_id": "1909.01060v1-Figure4-1",
    "image_file": "1909.01060v1-Figure4-1.png",
    "caption": " (a) Sequential primary change (b) ad change (c) Shortcutting change (d) Extending change.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of change is most likely to occur when a species is introduced to a new environment?",
    "answer": "Extending change.",
    "rationale": "Extending change is when a new interaction is added to an existing network. This is most likely to occur when a species is introduced to a new environment because it will be exposed to new species and resources that it did not interact with in its previous environment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.01060v1",
    "pdf_url": null
  },
  {
    "instance_id": "2043bfef611947d293c333165bc18bc2",
    "figure_id": "2110.14807v1-Figure10-1",
    "image_file": "2110.14807v1-Figure10-1.png",
    "caption": " Compare scalability with prior protocols [20, 17].",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which protocol has the highest accuracy for the VGG8 model on the C10 dataset?",
    "answer": "MixedTrn",
    "rationale": "The MixedTrn bar for the C10/VGG8 model is the tallest, indicating it has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14807v1",
    "pdf_url": null
  },
  {
    "instance_id": "305fe3057f3249b8a1811d96a218a03c",
    "figure_id": "2212.08985v1-Figure5-1",
    "image_file": "2212.08985v1-Figure5-1.png",
    "caption": " In the top figure, we show the predicted image caption, ground truth (GT) captions, and our predicted visual concepts. In the bottom figure, we exhibit the channel attention weights of the first three concepts (i.e., Dessert, Cake, and Spoon).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three most important visual concepts in the image, according to the model?",
    "answer": "Dessert, Cake, and Spoon.",
    "rationale": "The bottom figure shows the channel attention weights for the first three concepts, which are Dessert, Cake, and Spoon. The higher the attention weight, the more important the concept is to the model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.08985v1",
    "pdf_url": null
  },
  {
    "instance_id": "0c003317a8d34bd59eeac99d69aaea74",
    "figure_id": "2010.08321v3-Figure7-1",
    "image_file": "2010.08321v3-Figure7-1.png",
    "caption": " Each curve shows the rate savings at different PSNR quality levels relative to BPG. Our full model outperforms BPG by 21% at low bit rates.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in terms of bitrate savings at low PSNR values?",
    "answer": "Full Model",
    "rationale": "The red curve, which represents the Full Model, is consistently higher than the other curves, indicating that it achieves the highest bitrate savings at all PSNR values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.08321v3",
    "pdf_url": null
  },
  {
    "instance_id": "1441263657494b54a75203c6d8dfb5cb",
    "figure_id": "2202.13843v2-Figure11-1",
    "image_file": "2202.13843v2-Figure11-1.png",
    "caption": " Setup-II experiment results of empirical study for architecture classification.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the accuracy of the model on patch 16?",
    "answer": "99.35%",
    "rationale": "The figure shows the accuracy of the model on each patch. The accuracy on patch 16 is shown in the bottom right corner of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.13843v2",
    "pdf_url": null
  },
  {
    "instance_id": "99ed6631996b494f85326f44594b9064",
    "figure_id": "1809.00653v1-Figure2-1",
    "image_file": "1809.00653v1-Figure2-1.png",
    "caption": " Three of the sixteen trees with nonzero probability for an SST test example. Flat representations, such as the first tree, perform well on this task, as reflected by the baselines. The second tree, marked with X, agrees with the off-line parser.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three trees is most likely to be correct, according to the passage?",
    "answer": "The first tree.",
    "rationale": "The passage states that \"flat representations, such as the first tree, perform well on this task,\" and the first tree is the only flat representation among the three.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.00653v1",
    "pdf_url": null
  },
  {
    "instance_id": "6b2acbb6fb224b97806f36bc091d3b1c",
    "figure_id": "2306.01708v2-Figure13-1",
    "image_file": "2306.01708v2-Figure13-1.png",
    "caption": " T5-Base with increasing number of task being merged. Average performance when merging a different number of tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods performs the best when merging a different number of tasks?",
    "answer": "TIES",
    "rationale": "The figure shows that the TIES method has the highest average normalized performance for all numbers of tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01708v2",
    "pdf_url": null
  },
  {
    "instance_id": "a14048d999f6431daffff6eb7827dbd9",
    "figure_id": "2202.09177v2-Figure7-1",
    "image_file": "2202.09177v2-Figure7-1.png",
    "caption": " Ranking analysis for different model families on different datasets. The top is dataset of node classification task and the bottom is datasets of link prediction task. Lower is better.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which model family performs the best on the link prediction task for the Freebase dataset? ",
    "answer": " HGBN. ",
    "rationale": " The figure shows the ranking analysis for different model families on different datasets. The bottom part of the figure shows the results for the link prediction task. For the Freebase dataset, HGBN has the lowest average ranking, indicating that it performs the best. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2202.09177v2",
    "pdf_url": null
  },
  {
    "instance_id": "35980f7e6f644344808df1c387ccc1f1",
    "figure_id": "2101.05834v1-Figure23-1",
    "image_file": "2101.05834v1-Figure23-1.png",
    "caption": " Advection-Diffusion system: Predictions at t = 0, 80, 160 and 1000 obtained without zt and with the model of Equation (31).",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \nWhich prediction is closest to the reference solution?",
    "answer": " \nThe prediction at t = 1000.",
    "rationale": " \nThe plot at t = 1000 shows that the posterior mean (blue line) is very close to the reference solution (orange line). This is not the case for the other plots, where the posterior mean deviates significantly from the reference solution.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.05834v1",
    "pdf_url": null
  },
  {
    "instance_id": "20d2a57096e04def8c1f695f7b51ed7b",
    "figure_id": "2004.14999v1-Figure2-1",
    "image_file": "2004.14999v1-Figure2-1.png",
    "caption": " Layer probing results",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature in the English model has the highest value in layer 5?",
    "answer": "`tttype`",
    "rationale": "The figure shows the values of different features in different layers of the English and German models. The `tttype` feature has the highest value in layer 5 of the English model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.14999v1",
    "pdf_url": null
  },
  {
    "instance_id": "3fe10c4e31ad440b8a0f0ca30dc46245",
    "figure_id": "1905.02249v2-Figure3-1",
    "image_file": "1905.02249v2-Figure3-1.png",
    "caption": " Error rate comparison of MixMatch to baseline methods on SVHN for a varying number of labels. Exact numbers are provided in table 6 (appendix). “Supervised” refers to training with all 73257 training examples and no unlabeled data. With 250 examples MixMatch nearly reaches the accuracy of supervised training for this model.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest test error rate when trained on 4000 labeled data points?",
    "answer": "MixMatch.",
    "rationale": "The figure shows the test error rate for each method as a function of the number of labeled data points. The MixMatch line is the lowest at 4000 labeled data points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.02249v2",
    "pdf_url": null
  },
  {
    "instance_id": "989989da6fc24f2cbde5ce35ba003809",
    "figure_id": "2110.07700v2-Figure5-1",
    "image_file": "2110.07700v2-Figure5-1.png",
    "caption": " Training stochastic VAEs to generate MNIST digits with f -HNCA with Baseline with different aspects ablated. We omit the single-layer VAE as the ablations are not meaningful in this case. No child pruning refers to unnecessarily multiplying by ρΦ(φ) even when no children have downstream connections to a function component, that is Equation 12. Full reward, does the same as no child pruning, in addition to unnecessarily including upstream function components in the estimator. For full reward, these additional function components are also included in the moving average baseline. Each line represents the average of 5 random seeds with error bars showing 95% confidence interval. Final values at the end of training are written near each line in matching color. The top row shows the online training ELBO. The bottom row shows the natural logarithm of the mean gradient variance. Mean gradient variance is computed as the mean over parameters and batches of the per-parameter empirical variance over examples in a training batch of 50. It appears that unnecessarily including children has a significant negative impact on f -HNCA with baseline, while the impact of including upstream function components is negligible.",
    "figure_type": "\"plot\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which ablation has the most negative impact on the training ELBO of f-HNCA with Baseline?",
    "answer": "\"No child pruning\"",
    "rationale": "The plot shows that the training ELBO for \"HNCA with Baseline (no child pruning)\" is consistently lower than the other ablated versions of f-HNCA with Baseline.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.07700v2",
    "pdf_url": null
  },
  {
    "instance_id": "95b09ccb25b64f30b5a666a381ede427",
    "figure_id": "2010.01893v2-Figure4-1",
    "image_file": "2010.01893v2-Figure4-1.png",
    "caption": " Convergence analysis of the hard transfer. The convergence is faster with more hard-transferred modules.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model converges the fastest?",
    "answer": "RegDG + Encoder.",
    "rationale": "The figure shows the perplexity (PPL) of three different models as a function of training step. The lower the perplexity, the better the model is performing. The RegDG + Encoder model has the lowest perplexity at the end of training, indicating that it converges the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.01893v2",
    "pdf_url": null
  },
  {
    "instance_id": "d132ee1002c0425abd3f54c9137b7c9f",
    "figure_id": "2212.03467v2-Figure1-1",
    "image_file": "2212.03467v2-Figure1-1.png",
    "caption": " Plot of the tight upper bound of the simultaneous approximation ratio for ck and cp (denoted as function f) with respect to p k . When k and p are similar this factor is small, and in fact when p ≤ 4k they can both be approximated within a factor of 2. As p k becomes larger, the worst-case approximation approaches 1 + √ 2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the worst-case approximation ratio for ck and cp when p k becomes larger?",
    "answer": "1 + √ 2",
    "rationale": "The figure shows that the worst-case approximation ratio for ck and cp approaches 1 + √ 2 as p k becomes larger. This is indicated by the horizontal dashed line at y = 1 + √ 2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.03467v2",
    "pdf_url": null
  },
  {
    "instance_id": "72ff8d2fe8a94611af93bdf6b29b7d06",
    "figure_id": "2107.13034v3-Figure2-1",
    "image_file": "2107.13034v3-Figure2-1.png",
    "caption": " Robustness to neural network variations. KIP ConvNet images (trained with fixed labels) are tested on variations of the ConvNet neural network, including those which have various normalization layers (layer, instance, batch). A similar architecture to ConvNet, the Myrtle5 architecture (without normalization layers) [Shankar et al., 2020], which differs from the ConvNet architecture by having an additional convolutional layer at the bottom and a global average pooling that replaces the final local average pooling at the top, is also tested. Finally, mean-square error is compared with cross-entropy loss (left versus right). Settings: CIFAR-10, 500 images, ZCA, no label learning.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which normalization layer performs the best on the KIP ConvNet images according to the plot?",
    "answer": "Batch-Norm",
    "rationale": "The plot shows the test accuracy of different normalization layers on the KIP ConvNet images. The Batch-Norm line is the highest, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.13034v3",
    "pdf_url": null
  },
  {
    "instance_id": "08f9fa39998349cda74ba747b95d6a6c",
    "figure_id": "1907.00921v1-Figure4-1",
    "image_file": "1907.00921v1-Figure4-1.png",
    "caption": " Pack Lunchbox Task. Shows performance (test accuracy with standard error) on a separate task, under the most constrained experimental condition: both time and query budget constrained.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed best in the constrained time and budget condition?",
    "answer": "DT-task-env",
    "rationale": "The figure shows that DT-task-env (red line) achieved the highest accuracy of all the methods tested.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.00921v1",
    "pdf_url": null
  },
  {
    "instance_id": "defb3b40da7e46bba492a6866bd7c75d",
    "figure_id": "1905.10756v4-Figure4-1",
    "image_file": "1905.10756v4-Figure4-1.png",
    "caption": " (a) Convergence analysis on SVHN10→MNIST5. (b) Learning curve on SVHN10→MNIST5. (c) Source classwise retention probability learned by policy network on SVHN10→MNIST5. (d) Class-wise selected ratio and filtered ratio evaluated by RDS trained on SVHN10→MNIST5. (e) The accuracy curve of varying the number of target classes on A→W.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which model achieves the highest test accuracy on SVHN10→MNIST5? ",
    "answer": " RTNNet",
    "rationale": " The test accuracy of each model is shown in Figure (a). The RTNNet curve is the highest among all the models, indicating that it achieves the highest test accuracy. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10756v4",
    "pdf_url": null
  },
  {
    "instance_id": "7c8b2adb7edc452fbfacb7a3e1a6cfb0",
    "figure_id": "2007.05086v2-Figure3-1",
    "image_file": "2007.05086v2-Figure3-1.png",
    "caption": " Adversarial robustness and thickness. (a) Increasing boundary thickness improves robust accuracy in adversarial training. (b) Increasing boundary thickness reduces overfitting (measured by robust accuracy gap between training and testing). (c) Thickness can differentiate models of different robust levels (dark to light blue), while margin cannot (dark to light red and dark to light green). Results are obtained for ResNet-18 trained on CIFAR10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How does increasing boundary thickness affect overfitting?",
    "answer": "Increasing boundary thickness reduces overfitting.",
    "rationale": "The robust accuracy gap, which measures the difference between training and testing robust accuracy, decreases as the boundary thickness increases, as shown in panel (b) of the figure. This suggests that increasing boundary thickness reduces overfitting.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.05086v2",
    "pdf_url": null
  },
  {
    "instance_id": "bb1771b8a3a047029eb6eb498a330465",
    "figure_id": "2110.13309v2-Figure10-1",
    "image_file": "2110.13309v2-Figure10-1.png",
    "caption": " Failure cases in R2R val unseen split. The instruction is “Go stand underneath the stairs, next to the liquor shelf. ” (id: 36968_2). Though HAMT correctly goes towards the direction, it fails to recognize the liquor shelf and results in exploring further the room until reaching the maximum number of navigation steps.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Why did HAMT fail to follow the instruction in this case?",
    "answer": "HAMT failed to recognize the liquor shelf.",
    "rationale": "The figure shows the ground truth trajectory (a) and the predicted trajectory by HAMT (b). In the predicted trajectory, HAMT goes towards the stairs but does not stop at the liquor shelf. Instead, it continues to explore the room until it reaches the maximum number of navigation steps. This suggests that HAMT failed to recognize the liquor shelf.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.13309v2",
    "pdf_url": null
  },
  {
    "instance_id": "ee72f2e48d514c8e8522038f7fa551d3",
    "figure_id": "2205.06350v2-Figure4-1",
    "image_file": "2205.06350v2-Figure4-1.png",
    "caption": " Performance vs the minimum costs for different languages. The performance function considered is AMUE . For c = 0.1 case refer to Fig. 13 in appendix.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the highest performance for a given cost?",
    "answer": "Arabic (ar)",
    "rationale": "The plot shows the performance of different languages as a function of the minimum cost. The line for Arabic is consistently above the lines for the other languages, indicating that it has the highest performance for a given cost.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.06350v2",
    "pdf_url": null
  },
  {
    "instance_id": "e5bab72e465d452294f934faa847763a",
    "figure_id": "2010.02172v3-Figure3-1",
    "image_file": "2010.02172v3-Figure3-1.png",
    "caption": " Contextual uncertainty versus lexical ambiguity in a selection of languages. Each plot contains the scatter points (representing each word type), a robust linear regression and kernel density estimate regions. (From left to right; Top) WordNet: Arabic, English, Indonesian; (Bottom) BERT: Arabic, English, Malayalam, Tagalog.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the strongest negative correlation between contextual uncertainty and lexical ambiguity?",
    "answer": "Arabic.",
    "rationale": "The figure shows scatter plots of contextual uncertainty versus lexical ambiguity for different languages. The slope of the regression line indicates the strength of the correlation between the two variables. The steeper the slope, the stronger the negative correlation. In this case, Arabic has the steepest slope, indicating the strongest negative correlation.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.02172v3",
    "pdf_url": null
  },
  {
    "instance_id": "b42825f306384d7bb3afc170c903a322",
    "figure_id": "2212.08158v2-Figure1-1",
    "image_file": "2212.08158v2-Figure1-1.png",
    "caption": " We display image-sentence alignment scores (ISA) and the textual degree T-SHAP that measures how much models focus on text rather than the image (with 100−T-SHAP% the corresponding visual degree) for 3 VL models. Blue/red highlights on text tokens and image tokens (patches) contribute towards higher/lower ISA. Note: CLIP’s ISA is an absolute score, while ALBEF and LXMERT predict ISA probabilities. See Section 4.4 for more details on this figure; App. C for more detailed analysis of this instance and more samples.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model focuses the most on the text rather than the image?",
    "answer": "CLIP",
    "rationale": "The T-SHAP score for CLIP is the highest at 67%, which indicates that CLIP focuses the most on the text rather than the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.08158v2",
    "pdf_url": null
  },
  {
    "instance_id": "64c30bf077ef4470b7e8a4d597f7bdc6",
    "figure_id": "2303.14871v1-Figure9-1",
    "image_file": "2303.14871v1-Figure9-1.png",
    "caption": " The best synchronized BN-AN pair in each layer in “Shapes”). The number in the brackets is the PCC.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the highest PCC?",
    "answer": "Layer 3.",
    "rationale": "The PCC is shown in brackets below each BN-AN pair. The highest PCC is 0.55266, which is found in layer 3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.14871v1",
    "pdf_url": null
  },
  {
    "instance_id": "dd40c634d6fa433a9fcb8654a108658e",
    "figure_id": "2203.14987v1-Figure5-1",
    "image_file": "2203.14987v1-Figure5-1.png",
    "caption": " Entity distribution for E-PKG.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the highest number of entities in the \"compatible_phones\" category?",
    "answer": "EN (English)",
    "rationale": "The figure shows the distribution of entities for each language in different categories. The height of each colored bar represents the number of entities in that category for the corresponding language. The red bar for \"compatible_phones\" is the highest for EN.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.14987v1",
    "pdf_url": null
  },
  {
    "instance_id": "164c873f2bf44e67963caac4d9973de5",
    "figure_id": "2109.15047v2-Figure13-1",
    "image_file": "2109.15047v2-Figure13-1.png",
    "caption": " Bitrate saving when using different channel dimensions for context. The anchor is 3-Dim (dimension is 3) model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dimension of context has the largest bitrate saving for HEVC Class D?",
    "answer": "256-Dim",
    "rationale": "The figure shows the bitrate saving for different channel dimensions for context, and the 256-Dim bar is the highest for HEVC Class D.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.15047v2",
    "pdf_url": null
  },
  {
    "instance_id": "3684d880b89746e1b3798148b7622fec",
    "figure_id": "2210.03308v1-Figure5-1",
    "image_file": "2210.03308v1-Figure5-1.png",
    "caption": " Comparison of GAFlowNets and baselines in GridWorld with increasing sizes corresponding to each column (left: small, middle: medium, right: large). The first and second rows correspond to empirical L1 error and the number of discovered modes, respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method consistently discovers the most modes in all three GridWorld sizes?",
    "answer": "GAFlowNet",
    "rationale": "The plots in the second row show the number of modes discovered by each method as a function of the number of updates. In all three GridWorld sizes, GAFlowNet consistently discovers the most modes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.03308v1",
    "pdf_url": null
  },
  {
    "instance_id": "96bb0910e3854743812809cd36913d5a",
    "figure_id": "1905.13453v1-Figure1-1",
    "image_file": "1905.13453v1-Figure1-1.png",
    "caption": " A 2D-visualization of the similarity between different datasets using the force-directed placement algorithm. We mark datasets that use web snippets as context with triangles, Wikipedia with circles, and Newswire with squares. We color multi-hop reasoning datasets in red, trivia datasets in blue, and factoid RC datasets in green.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset is most similar to WikiHop?",
    "answer": "HotpotQA",
    "rationale": "The figure shows that WikiHop and HotpotQA are closest together in the 2D space, which indicates that they are most similar.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.13453v1",
    "pdf_url": null
  },
  {
    "instance_id": "767fae11ebc546cc8a04cf79f22d60b4",
    "figure_id": "1905.04722v1-Figure2-1",
    "image_file": "1905.04722v1-Figure2-1.png",
    "caption": " All the variations based on rotation and flipping of the left-most case. Ideally, a RL model should handle all these cases equally well.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different variations of the intersection are shown in the figure?",
    "answer": "8",
    "rationale": "The figure shows four different rotations of the intersection (0 degrees, 90 degrees, 180 degrees, and 270 degrees) and two different flipping options (no flipping and west-east flipping). This results in a total of 8 different variations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.04722v1",
    "pdf_url": null
  },
  {
    "instance_id": "5ead6629167f4e5587cfb974ba3e1eb2",
    "figure_id": "2105.01879v1-Figure13-1",
    "image_file": "2105.01879v1-Figure13-1.png",
    "caption": " Average others scores for all OOD datasets",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest average others score for the \"Artifact\" category?",
    "answer": "iNaturalist",
    "rationale": "The bar for \"Artifact\" in the iNaturalist plot is the tallest among all the plots, indicating that it has the highest average others score for this category.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.01879v1",
    "pdf_url": null
  },
  {
    "instance_id": "4269249df98447688c6457e710090bed",
    "figure_id": "2006.10455v2-Figure12-1",
    "image_file": "2006.10455v2-Figure12-1.png",
    "caption": " Scatter plots for CIFAR10 Experiments A. Models fine-tuned on real labels (TOP and CENTER) and on random labels (BOTTOM). Each dot corresponds to one group of experiments. Points below the orange x = y line on TOP correspond to experiments where models pre-trained with random labels train faster downstream (with real labels) compared to models trained from scratch with same hyperparameters. CENTER AND RIGHT columns contain zoomed in versions of the plots from LEFT column.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs better on CIFAR10: the simple CNN or VGG16?",
    "answer": "VGG16 performs better than the simple CNN.",
    "rationale": "The plot in the bottom right corner of the figure shows that the AUC for VGG16 is generally higher than the AUC for the simple CNN. This indicates that VGG16 is more accurate than the simple CNN.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.10455v2",
    "pdf_url": null
  },
  {
    "instance_id": "6b4fd6d87e584e129055b730959970de",
    "figure_id": "1911.12511v1-Figure3-1",
    "image_file": "1911.12511v1-Figure3-1.png",
    "caption": " Fraction of tasks solved by each method at the end of training for 1.3 million steps. The tabular agents, which do not take history into account, perform quite poorly. LI stands for “look, inventory” (see text for details).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best at solving tasks?",
    "answer": "The SC method performed the best at solving tasks.",
    "rationale": "The figure shows that the SC method had the highest average fraction of total subtasks solved across all levels.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.12511v1",
    "pdf_url": null
  },
  {
    "instance_id": "56a6f1db42b94ccbace9d071b220f1b4",
    "figure_id": "2208.10378v3-Figure1-1",
    "image_file": "2208.10378v3-Figure1-1.png",
    "caption": " An illustration of how new entities emerge in real life, excerpted from Wikidata [27]. Solid glowing links are new facts as new entities emerge, while dashed glowing links are queries, e.g., (Omicron variant, instance of, ?), to be inferred in the inductive KG reasoning task.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which variant of SARS-CoV-2 was first discovered in South Africa?",
    "answer": "The Omicron variant.",
    "rationale": "The figure shows that the Omicron variant is linked to South Africa by a \"location of discovery\" relationship.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.10378v3",
    "pdf_url": null
  },
  {
    "instance_id": "607475ed4c47445faac7305b4391ae02",
    "figure_id": "2308.16900v3-Figure5-1",
    "image_file": "2308.16900v3-Figure5-1.png",
    "caption": " Wine attributes. WineSensed contains attributes about the geolocation of production (country, region) and the grape composition of each wine. Furthermore, the dataset includes information on the average price of the wine, alcohol percentage, average rating on the Vivino platform, and the year of production. The histograms show the distribution of these attributes.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which country produces the most wine according to the WineSensed dataset?",
    "answer": "Italy.",
    "rationale": "The histogram in (a) shows that Italy has the highest frequency of wines in the dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.16900v3",
    "pdf_url": null
  },
  {
    "instance_id": "df98660b155b4b2b817e33c5943d2fde",
    "figure_id": "1903.10128v1-Figure9-1",
    "image_file": "1903.10128v1-Figure9-1.png",
    "caption": " Visual results on Vid4 for 4× scaling factor. Zoom in to see better visualization.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the best visual results?",
    "answer": "RBPN/6-PF.",
    "rationale": "The figure shows the original image and the results of applying different super-resolution methods to it. The RBPN/6-PF method produces the sharpest and most detailed image, which is closest to the ground truth (GT) image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.10128v1",
    "pdf_url": null
  },
  {
    "instance_id": "b6a47dfdf28e43868231880812b74037",
    "figure_id": "2210.10592v2-Figure11-1",
    "image_file": "2210.10592v2-Figure11-1.png",
    "caption": " The benefits of disentanglement in terms of training resources.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best with the least amount of training data and the fewest number of MLP layers?",
    "answer": "HTGN-DyTed without a discriminator.",
    "rationale": "The figure shows the AUC (Area Under the Curve) for different models and configurations. The model with the highest AUC and the fewest number of MLP layers is HTGN-DyTed without a discriminator.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.10592v2",
    "pdf_url": null
  },
  {
    "instance_id": "b14d4954d60a4dd296825225aaa1019c",
    "figure_id": "2306.09308v1-Figure7-1",
    "image_file": "2306.09308v1-Figure7-1.png",
    "caption": " F1 of DistilGPT2 and MLMINI attributors under varying dataset size, relative to original dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attributor performs best on the IMDB dataset when the dataset size is 20% of the original size?",
    "answer": "DistilGPT2",
    "rationale": "The figure shows that the F1 score for DistilGPT2 on the IMDB dataset is higher than the F1 score for MLMINI on the IMDB dataset when the dataset size is 20% of the original size.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.09308v1",
    "pdf_url": null
  },
  {
    "instance_id": "16ee3c82678847c7b7ab0f5e17420be6",
    "figure_id": "1812.00408v3-Figure1-1",
    "image_file": "1812.00408v3-Figure1-1.png",
    "caption": " The CASIA-SURF dataset. It is a large-scale and multimodal dataset for face anti-spoofing, consisting of 492, 522 images with 3 modalities (i.e., RGB, Depth and IR).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the three modalities of data included in the CASIA-SURF dataset?",
    "answer": "RGB, Depth, and IR.",
    "rationale": "The figure shows that the dataset includes three types of data for each video: RGB, Depth, and IR. This is indicated by the labels on the left side of the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.00408v3",
    "pdf_url": null
  },
  {
    "instance_id": "c00eb1d2021e4296b3fb0f92c77a30cc",
    "figure_id": "2007.14864v1-Figure1-1",
    "image_file": "2007.14864v1-Figure1-1.png",
    "caption": " Running example of a toy Knowledge Graph of academic collaborations and a corresponding graph pattern query.",
    "figure_type": "** schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Who are the co-authors of Ramakrishnan?",
    "answer": " Sudarshan, Gehrke, and Ooi.",
    "rationale": " The knowledge graph in (a) shows that Ramakrishnan has co-author relationships with Sudarshan, Gehrke, and Ooi.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.14864v1",
    "pdf_url": null
  },
  {
    "instance_id": "988fed5517bc4368b786df9f199a5df9",
    "figure_id": "2010.04091v1-Figure2-1",
    "image_file": "2010.04091v1-Figure2-1.png",
    "caption": " Average computation time per decision vs. averaged cumulative regret for (a) Figure 1(a); (b) Figure 1(b); (c) Figure 1(c); (d) Figure 1(d).",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest average final cumulative regret and the highest computation time?",
    "answer": "LinTS.",
    "rationale": "The figure shows that LinTS has the lowest average final cumulative regret and the highest computation time. This can be seen by looking at the position of the LinTS marker (purple triangle) in each of the subplots. In all cases, the LinTS marker is located near the bottom of the plot (indicating low regret) and near the right side of the plot (indicating high computation time).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.04091v1",
    "pdf_url": null
  },
  {
    "instance_id": "4dab46e204524a0887cbbf8520875656",
    "figure_id": "1805.00145v3-Figure12-1",
    "image_file": "1805.00145v3-Figure12-1.png",
    "caption": " Examples of users interacting with the proposed dialog manager system. User feedbacks are shown below the corresponding images. “Unlike the provided image, the ones I want\" is omitted from each sentence for brevity.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following shoes has the most sporty design?",
    "answer": "The white and blue sneakers.",
    "rationale": "The image shows a pair of white and blue sneakers, which are typically associated with athletic activities and have a sporty design.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.00145v3",
    "pdf_url": null
  },
  {
    "instance_id": "c9d54eebae544813a5e16f0593d1ece8",
    "figure_id": "1908.03919v3-Figure4-1",
    "image_file": "1908.03919v3-Figure4-1.png",
    "caption": " Part A: Illustration of the GAN-Tree training procedure over MNIST+Fashion-MNIST dataset. Part B: Effectiveness of our mode-split procedure (with bagging) against the baseline deep-clustering technique (without bagging) on MNIST root node. Our approach divides the digits into two groups in a much cleaner way (at iter=11k). Part C: We evaluate the GAN-Tree and iGAN-Tree algorithms against the prior incremental training method AdaGAN [35]. We train up to 13 generators and evaluate their mean JS Divergence score (taken over 5 repetitions). Part D: Incremental GAN-Tree training procedure (i) Base GAN-Tree, trained over digits 0-4 (ii) GAN-Tree after addition of digit 5, with dσ0 = 4 (iii) GAN-Tree after addition of digit 5, with dσ0 = 9.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more effective in dividing the digits into two groups, our approach or the baseline deep-clustering technique?",
    "answer": "Our approach.",
    "rationale": "Part B of the figure shows the effectiveness of our mode-split procedure (with bagging) against the baseline deep-clustering technique (without bagging) on the MNIST root node. At iter=11k, our approach has clearly divided the digits into two groups, while the baseline deep-clustering technique has not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.03919v3",
    "pdf_url": null
  },
  {
    "instance_id": "eb9dff614be5440ca37d9a65c3843b46",
    "figure_id": "2306.00006v3-Figure10-1",
    "image_file": "2306.00006v3-Figure10-1.png",
    "caption": " The ROC curve and precision-recall curve of our method TAM on all the six datasets used. (a)(b) BlogCatalog; (c)(d) ACM; (e)(f) Amazon; (h)(i) Facebook; (j)(k) Reddit; (l)(m) YelpChi",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the best performance according to the ROC curve?",
    "answer": "Facebook",
    "rationale": "The area under the ROC curve (AUROC) is a measure of the performance of a binary classifier. The higher the AUROC, the better the performance. In this figure, the Facebook dataset has the highest AUROC of 0.911.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.00006v3",
    "pdf_url": null
  },
  {
    "instance_id": "7fc5223aa25e402db75bb7cf07ced032",
    "figure_id": "2106.05010v2-Figure4-1",
    "image_file": "2106.05010v2-Figure4-1.png",
    "caption": " Out of distribution performances",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on MNIST and CIFAR10?",
    "answer": "MAP",
    "rationale": "The figure shows the out-of-distribution performance of different methods on MNIST and CIFAR10. The MAP method consistently has the highest accuracy on both datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05010v2",
    "pdf_url": null
  },
  {
    "instance_id": "219efb2b361c43c18f98be11bc133f87",
    "figure_id": "1808.06218v2-Figure2-1",
    "image_file": "1808.06218v2-Figure2-1.png",
    "caption": " The median location of summary n-grams in the multi-document input (and the lower/higher quartiles). The n-grams come from the 1st/2nd/3rd/4th/5th summary sentence and the location is the source sentence index. (TAC-11)",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which summary sentence has the most n-grams located in the first half of the multi-document input? ",
    "answer": " The 1st summary sentence.",
    "rationale": " The figure shows the median location of n-grams from each summary sentence in the multi-document input. The median location for the 1st summary sentence is around 4, which is in the first half of the multi-document input. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.06218v2",
    "pdf_url": null
  },
  {
    "instance_id": "d4a2dd7f14ec4cd2a7ea2b604f1e8f4c",
    "figure_id": "2303.06919v2-Figure16-1",
    "image_file": "2303.06919v2-Figure16-1.png",
    "caption": " Qualitative evaluation of the improvement over two SOTA NeRF models on Tanks and Temples [25].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two SOTA NeRF models compared in the figure?",
    "answer": "TensorRF [7] and DIVER [6]",
    "rationale": "The figure shows the qualitative evaluation of the improvement over two SOTA NeRF models, TensorRF [7] and DIVER [6], on Tanks and Temples [25].",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.06919v2",
    "pdf_url": null
  },
  {
    "instance_id": "61d8b2f5d78e47e2a1b5a50263b0f96a",
    "figure_id": "2201.09863v1-Figure10-1",
    "image_file": "2201.09863v1-Figure10-1.png",
    "caption": " BidirectionalWalker-v0",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which direction does the walker move?",
    "answer": "The walker moves in both directions.",
    "rationale": "The arrows on the figure point in both directions, indicating that the walker can move in either direction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.09863v1",
    "pdf_url": null
  },
  {
    "instance_id": "5120d48d90ca4d22920a3c3fed02a8a2",
    "figure_id": "2010.11918v2-Figure7-1",
    "image_file": "2010.11918v2-Figure7-1.png",
    "caption": " Performance of AF by the number of dropped AF layers. We show the results for AF and the used adapters (both with and without AdapterDrop), and compare the performance with a standard single task adapter.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training method achieves the best performance on the RTE task when all AF layers are dropped?",
    "answer": "Standard Fusion Training.",
    "rationale": "The figure shows the performance of different training methods on the RTE task as a function of the number of dropped AF layers. When all AF layers are dropped, the Standard Fusion Training method has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.11918v2",
    "pdf_url": null
  },
  {
    "instance_id": "8066b4fd6b9d49d9bb949af0d67f3d73",
    "figure_id": "2207.05704v2-Figure4-1",
    "image_file": "2207.05704v2-Figure4-1.png",
    "caption": " Qualitative comparison of our method, the original RAFT-3D, as well as the two top-performing approaches from the literature for two scenes using the visualizations provided by the KITTI benchmark [21]. From left to right: Target disparity visualization, corresponding D2 error plot, optical flow visualization, corresponding Fl error plot, combined SF error plot.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four methods is the most accurate?",
    "answer": "M-FUSE",
    "rationale": "The figure shows the results of four different methods for estimating depth and flow. The D2 error plot shows the error in the estimated depth, the Fl error plot shows the error in the estimated flow, and the SF error plot shows the combined error. M-FUSE has the lowest error in all three plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.05704v2",
    "pdf_url": null
  },
  {
    "instance_id": "527384ea5d46419ababc1c6de6fc8f82",
    "figure_id": "2302.05915v1-Figure1-1",
    "image_file": "2302.05915v1-Figure1-1.png",
    "caption": " The top 15 policies and percentage of instances that use each policy (sorted by the percentage of instances).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which policy is used by the most instances?",
    "answer": "Others",
    "rationale": "The bar for Others is the longest in the instances group, meaning it is used by the most instances.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.05915v1",
    "pdf_url": null
  },
  {
    "instance_id": "63f027b97e39419d9f69647f2b4c9792",
    "figure_id": "2012.10043v2-Figure11-1",
    "image_file": "2012.10043v2-Figure11-1.png",
    "caption": " The proportion of paintings produced by each model that had faces detected by a model (Schroff, Kalenichenko, and Philbin 2015) within the specified number of brush strokes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is most likely to produce a painting with a face detected within the first 100 strokes?",
    "answer": "CM + L1",
    "rationale": "The plot shows that the CM + L1* line is the highest at the 100 stroke mark on the x-axis. This means that this model is most likely to produce a painting with a face detected within the first 100 strokes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.10043v2",
    "pdf_url": null
  },
  {
    "instance_id": "250c0983216d44a5a37fd05109b3f9e4",
    "figure_id": "2104.11178v3-Figure4-1",
    "image_file": "2104.11178v3-Figure4-1.png",
    "caption": " The average node activation across the Modality-Agnostic-Medium VATT while feeding a multimodal video-audio-text triplet to the model.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which modality has the earliest activation?",
    "answer": "Text",
    "rationale": "The figure shows that the text modality has the earliest activation, as indicated by the red dashed box in the bottom left corner. This means that the text modality is processed first by the model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.11178v3",
    "pdf_url": null
  },
  {
    "instance_id": "5104dea839a84bdea2057903792564de",
    "figure_id": "2206.04477v2-Figure4-1",
    "image_file": "2206.04477v2-Figure4-1.png",
    "caption": " Learning curves for RHIRL and other methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the Hopper-v2 environment?",
    "answer": "GAIL",
    "rationale": "The plot for the Hopper-v2 environment shows that the GAIL algorithm (green line) achieves the highest reward among all the algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.04477v2",
    "pdf_url": null
  },
  {
    "instance_id": "3cbbf5c219c942b2bdc3066ed3731fa4",
    "figure_id": "2302.06353v2-Figure9-1",
    "image_file": "2302.06353v2-Figure9-1.png",
    "caption": " IoU of RITM (per click) and IoU of our method (a single contour) on UserContours.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves higher IoU with fewer clicks?",
    "answer": "Our contour-based method.",
    "rationale": "The plot shows that our method (green dashed line) achieves an IoU of approximately 0.95 with only one contour, while RITM (red solid line) requires approximately 15 clicks to achieve the same IoU.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.06353v2",
    "pdf_url": null
  },
  {
    "instance_id": "8b6a56417c2747a7903624e8b38da5c4",
    "figure_id": "2104.05374v1-Figure5-1",
    "image_file": "2104.05374v1-Figure5-1.png",
    "caption": " Qualitative comparison in 3D reconstruction between our JDACS and SOTA supervised method(CVPMVSNet) on DTU dataset. From left to right: ground truth, results of supervised CVP-MVSNet, our results.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two methods, JDACS or CVP-MVSNet, produces 3D reconstructions that are more faithful to the ground truth?",
    "answer": "JDACS.",
    "rationale": "The figure shows three 3D reconstructions for each of three scans: the ground truth, the reconstruction produced by CVP-MVSNet, and the reconstruction produced by JDACS. For all three scans, the JDACS reconstruction is visually closer to the ground truth than the CVP-MVSNet reconstruction. For example, in the scan of the coffee pot, the JDACS reconstruction accurately captures the shape of the pot and the details of the spilled coffee, while the CVP-MVSNet reconstruction is more blurry and less detailed.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.05374v1",
    "pdf_url": null
  },
  {
    "instance_id": "4c2cde8570ef4872b1d79e18db6bf69e",
    "figure_id": "1905.01059v1-Figure4-1",
    "image_file": "1905.01059v1-Figure4-1.png",
    "caption": " LORD-CI intervals for sign-determining LORD-CI procedure, representation by time. Top panel is for selection with the symmetric CI, bottom is for MQC. The horizontal axis is “time\" (order of appearance) and the vertical position of the circles is the Xi values. When a CI is not constructed, only a circle is shown; for selected parameters, a CI is shown—green for a covering CI and red for a noncovering CI. The MQC-equipped procedure makes 17 more selections (an increase of 13%).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which selection procedure resulted in more selections?",
    "answer": "The MQC-equipped procedure.",
    "rationale": "The caption states that the MQC-equipped procedure made 17 more selections than the symmetric CI procedure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.01059v1",
    "pdf_url": null
  },
  {
    "instance_id": "1b71926a8d5c472d827a63cac00326e8",
    "figure_id": "2002.11297v2-Figure2-1",
    "image_file": "2002.11297v2-Figure2-1.png",
    "caption": " An example scheme of semantic shift and nonsemantic shift. It is illustrated with DomainNet [31] images. The setting with two splits (A and B) will be used in our experiments, where only real-A is the in-distribution data.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following types of images is most likely to be considered in-distribution data in this experiment?",
    "answer": "Real images from Split A.",
    "rationale": "The caption states that \"only real-A is the in-distribution data.\" This means that the images in Split A that are labeled as \"real\" are the only ones that are considered to be part of the in-distribution data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11297v2",
    "pdf_url": null
  },
  {
    "instance_id": "ea85f065180d4e118f18ffbb5bb1694c",
    "figure_id": "2105.09511v3-Figure2-1",
    "image_file": "2105.09511v3-Figure2-1.png",
    "caption": " Effective receptive fields of 3 models, indicated by nonnegligible gradients in blue blobs and light-colored dots. Gradients are back-propagated from the center of the image. Segtran has nonnegligible gradients dispersed across the whole image (light-colored dots). U-Net and DeepLabV3+ have concentrated gradients. Input image: 576× 576.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models has the largest effective receptive field?",
    "answer": "Segtran.",
    "rationale": "The effective receptive field is the area of the image that a model can \"see\" and use to make predictions. In the figure, the effective receptive field is indicated by the blue blobs and light-colored dots. Segtran has nonnegligible gradients dispersed across the whole image, which means that it has the largest effective receptive field.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.09511v3",
    "pdf_url": null
  },
  {
    "instance_id": "3b0ffd9843474a7ab37e668571968640",
    "figure_id": "2308.15081v1-Figure9-1",
    "image_file": "2308.15081v1-Figure9-1.png",
    "caption": " Distribution maps for the UAV hyperspectral datasets. The maps with the best F1-score are displayed for five experiments.",
    "figure_type": "photographs",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest F1 score on the HanChuan dataset?",
    "answer": "The T-HOneCls method.",
    "rationale": "The distribution maps for the HanChuan dataset show that the T-HOneCls method has the highest F1 score, as it is the only method that produces a map with a large amount of green, which indicates a high F1 score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.15081v1",
    "pdf_url": null
  },
  {
    "instance_id": "4ee729b60ce54e6d8ca64ee028122d9c",
    "figure_id": "2210.06456v2-Figure6-1",
    "image_file": "2210.06456v2-Figure6-1.png",
    "caption": " Results on all sentiment analysis ID-OOD settings when comparing zero-shot prompting, prompt-based fine-tuning, and standard fine-tuning.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in the zero-shot setting on the SST-2 dataset?",
    "answer": "GPT-3 (175B)",
    "rationale": "The figure shows the accuracy of different models on the SST-2 dataset in the zero-shot setting. GPT-3 (175B) achieves the highest accuracy of around 0.85.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.06456v2",
    "pdf_url": null
  },
  {
    "instance_id": "cfc3f3832fa24269b33a4ab66a3ad688",
    "figure_id": "1909.02729v5-Figure2-1",
    "image_file": "1909.02729v5-Figure2-1.png",
    "caption": " Mean accuracy of transductive fine-tuning for different query shot, way and support shot. Fig. 2a shows that the mean accuracy improves with query shot if the support shot is low; this effect is minor for Tiered-ImageNet. The mean accuracy for query shot of 1 is high because transductive fine-tuning can specialize to those queries. Fig. 2b shows that the mean accuracy degrades logarithmically with way for fixed support shot and query shot (15). Fig. 2c suggests that the mean accuracy improves logarithmically with the support shot for fixed way and query shot (15). These trends suggest thumb rules for building few-shot systems.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows a more significant improvement in mean accuracy with increasing query shot when the support shot is low?",
    "answer": "Mini-ImageNet",
    "rationale": "Figure 2a shows that the mean accuracy of Mini-ImageNet improves more significantly with increasing query shot when the support shot is low compared to Tiered-ImageNet.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.02729v5",
    "pdf_url": null
  },
  {
    "instance_id": "0b2142d38cd5465b9cb8affe7b65d9f5",
    "figure_id": "1706.03607v2-Figure1-1",
    "image_file": "1706.03607v2-Figure1-1.png",
    "caption": " Illustration of the one2all construction proof with ρ = 1. The data points X are in black. The points in M are colored red. We show the respective Voronoi partition and for each cluster, we show circles centered at the respective m ∈M (red) point with radius ∆m. The points in blue are a set Q. The points x ∈ X are labeled A if dxQ < 2dxM (and we apply Lemma 4.1). Otherwise, when there is a point m such that dxQ > dxm, the point is labeled B when dmQ ≥ 2∆m (Lemma 4.2) and is labeled C otherwise (Lemma 4.3).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which points in the figure are guaranteed to be within a distance of 2∆m from a point in M?",
    "answer": "Points labeled B.",
    "rationale": "The caption states that points labeled B are those for which there exists a point m such that dxQ > dxm and dmQ ≥ 2∆m. This means that the distance between the point and Q is greater than the distance between the point and m, and the distance between m and Q is at least 2∆m. Therefore, the point must be within a distance of 2∆m from m.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1706.03607v2",
    "pdf_url": null
  },
  {
    "instance_id": "3c85527308e442a4b8c3e05b1eaa2978",
    "figure_id": "2209.13446v5-Figure2-1",
    "image_file": "2209.13446v5-Figure2-1.png",
    "caption": " Privacy risk comparison between the raw output data and the data subject to 2-anonymization under the strategy of CF-K. For all metrics, lower is better. l-Diversity is evaluated on 2 most sensitive attributes.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which anonymization strategy consistently results in lower privacy risk according to all four metrics shown in the figure?",
    "answer": "CF-K.",
    "rationale": "The figure shows that the bars representing CF-K (green) are consistently shorter than the bars representing the raw data (grey) for all four metrics: 1-diversity (1), 1-diversity (2), validity loss, and 1-map. This indicates that CF-K consistently reduces privacy risk compared to the raw data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.13446v5",
    "pdf_url": null
  },
  {
    "instance_id": "b7deaa81353d4eb5ad3980ecab313394",
    "figure_id": "2002.11930v2-Figure4-1",
    "image_file": "2002.11930v2-Figure4-1.png",
    "caption": " (a) MAP@1000 results with extremely short code lengths on CIFAR-10. (b) 16-bit normalized reconstruction errors of TBH and its variants on CIFAR-10. (c) and (d) Effects of the hyper-parameters λ and L on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest MAP@1000 on CIFAR-10 with 8 bits?",
    "answer": "TBH",
    "rationale": "In subfigure (a), the red line representing TBH reaches the highest point on the y-axis at 8 bits.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11930v2",
    "pdf_url": null
  },
  {
    "instance_id": "2aa770baf9314fcf88f557f7baf9c727",
    "figure_id": "2203.15529v2-Figure4-1",
    "image_file": "2203.15529v2-Figure4-1.png",
    "caption": " (a) With proposed TLT and CPS dataset, neural saliency methods can be extended to visual pattern from inference. Take the top row as an example, using TLT, guided grad-CAM [Selvaraju et al., 2017] can be more aligned with the concise human-interpretable giraffe patterns instead of forest texture and edges. More correlation analyses between saliency and labels in NICO and CPS are given in supplement C. (b) Visualization of learned manifolds of q(z) by tSNE [Maaten and Hinton, 2008], proposed CTR’s results largest intra-cluster pair-wise sample distances between additive noise (adversarial) (t = 1) and vanilla (t = 0) image samples from CPS1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, CVAE*, CEVAE*, or TTT, produces the most visually distinct clusters in the latent space?",
    "answer": "TTT",
    "rationale": "The figure shows the t-SNE visualizations of the latent space for each method. The TTT visualization shows two distinct clusters, one for each class of images (giraffe and non-giraffe). The CVAE* and CEVAE* visualizations show less distinct clusters, with some overlap between the two classes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.15529v2",
    "pdf_url": null
  },
  {
    "instance_id": "13094b61a4694a1794ff6555fe037ac0",
    "figure_id": "2308.10896v1-Figure18-1",
    "image_file": "2308.10896v1-Figure18-1.png",
    "caption": " General setup of the face reconstruction experiment. The face model is placed in front of a planar shadow receiver and the camera (blue) is placed between them, focusing the receiver (left). For the experiments with three shadows, we add two additional lights with small horizontal offsets (middle, right).",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many light sources are used in the experiment?",
    "answer": "Three.",
    "rationale": "The figure shows a camera and three light sources, one in the center and two with small horizontal offsets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.10896v1",
    "pdf_url": null
  },
  {
    "instance_id": "7847130028c546a988fb2e8f03ce3c99",
    "figure_id": "2306.03310v2-Figure13-1",
    "image_file": "2306.03310v2-Figure13-1.png",
    "caption": " Comparison of different algorithms using the RESNET-T policy architecture. The y-axis represents the success rate, while the x-axis shows the agent’s performance on each of the 10 tasks in a given task suite during the course of learning. For example, the plot in the upper-left corner depicts the agent’s performance on the first task as it learns the 10 tasks sequentially.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which algorithm performed the best on Task 10? ",
    "answer": " ResNet-T ",
    "rationale": " The figure shows the success rate of different algorithms on each of the 10 tasks. The plot in the bottom-right corner depicts the agent's performance on the 10th task. The ResNet-T line is the highest on this plot, indicating it performed the best on this task. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.03310v2",
    "pdf_url": null
  },
  {
    "instance_id": "e0aa693c75784e9d9cff8253960b414d",
    "figure_id": "2211.14563v2-Figure6-1",
    "image_file": "2211.14563v2-Figure6-1.png",
    "caption": " Total number of occurrences of pronouns in Coreferenced Image Narratives .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pronoun is the most frequent in the dataset?",
    "answer": "\"he\"",
    "rationale": "The figure shows that the bar for \"he\" is the highest, indicating that it is the most frequent pronoun in the dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.14563v2",
    "pdf_url": null
  },
  {
    "instance_id": "542b8384781f4469ad76af1a8fa15de9",
    "figure_id": "2204.02937v2-Figure2-1",
    "image_file": "2204.02937v2-Figure2-1.png",
    "caption": " Feature learning and simplicity bias. ResNet-20 ERM classifiers trained on Dominoes data with varying levels of spurious correlation between core and spurious features. We show worst-group test accuracy for Original data, data with only core features present (Core-Only), and accuracy of decoding the core feature from the latent representations of the Original data with logistic regression. We additionally report optimal accuracy: the accuracy of a model trained and evaluated on the Core-Only data. Even in cases when the model achieves 0% accuracy on the Original data, the core features can still be decoded from latent representations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest worst-group test accuracy on the MNIST - Fashion dataset?",
    "answer": "The Optimal Acc. model.",
    "rationale": "The figure shows that the Optimal Acc. model achieves 95% worst-group test accuracy on the MNIST - Fashion dataset, which is higher than the other models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.02937v2",
    "pdf_url": null
  },
  {
    "instance_id": "afba972d7943413ca0d82d98f72ac7cb",
    "figure_id": "2104.05160v2-Figure2-1",
    "image_file": "2104.05160v2-Figure2-1.png",
    "caption": "Figure 2 – Overview of our proposed FDRL method. (a) The backbone network (ResNet-18) that extracts basic CNN features; (b) A Feature Decomposition Network (FDN) that decomposes the basic feature into a set of facial action-aware latent features; (c) A Feature Reconstruction Network (FRN) that learns an intra-feature relation weight and an inter-feature relation weight for each latent feature, and reconstructs the expression feature. FRN contains two modules: an Intra-feature Relation Modeling module (Intra-RM) and an Inter-feature Relation Modeling module (Inter-RM); (d) An Expression Prediction Network (EPN) that predicts an expression label.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the FDRL method is responsible for learning the relationships between different facial action-aware latent features?",
    "answer": "The Feature Reconstruction Network (FRN)",
    "rationale": "The FRN contains two modules: an Intra-feature Relation Modeling module (Intra-RM) and an Inter-feature Relation Modeling module (Inter-RM). The Intra-RM learns the relationships between different elements within each latent feature, while the Inter-RM learns the relationships between different latent features.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.05160v2",
    "pdf_url": null
  },
  {
    "instance_id": "2ba3804c8ba44b269eb38a9c5876186c",
    "figure_id": "2111.01007v1-Figure21-1",
    "image_file": "2111.01007v1-Figure21-1.png",
    "caption": " Uncurated Results for AFHQ-Cat (5122). The images are selected randomly given one global random seed. We recommend zooming in for comparison.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GAN model produced the most realistic cat images?",
    "answer": "Projected GAN (ours)",
    "rationale": "The FID score, which measures the distance between the generated images and the real images, is lowest for Projected GAN (ours). This means that the images generated by Projected GAN are more similar to real images than the images generated by the other two models. Additionally, the Recall score, which measures the percentage of real images that are correctly identified as real, is highest for Projected GAN (ours). This means that the images generated by Projected GAN are more likely to be mistaken for real images than the images generated by the other two models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.01007v1",
    "pdf_url": null
  },
  {
    "instance_id": "45fabe5ac7fb465db4291cdb43919a9a",
    "figure_id": "1905.10138v2-Figure2-1",
    "image_file": "1905.10138v2-Figure2-1.png",
    "caption": " Several types of pruning granularity. In the conventional sparse formats, as a sparse matrix becomes more structured to gain parallelism in decoding, pruning rate becomes lower in general.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of pruning granularity has the highest decoding parallelism according to the proposed method?",
    "answer": "Unstructured pruning.",
    "rationale": "The figure shows that the proposed method achieves higher decoding parallelism than the conventional method for all types of pruning granularities. Additionally, the figure shows that unstructured pruning has the highest pruning rate, which corresponds to the highest decoding parallelism according to the proposed method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10138v2",
    "pdf_url": null
  },
  {
    "instance_id": "5880257bc8cb4d468cc00f948052a452",
    "figure_id": "1910.09284v1-Figure3-1",
    "image_file": "1910.09284v1-Figure3-1.png",
    "caption": " Accuracy for different number of snapshots N .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest accuracy for N = 20?",
    "answer": "CovNet with N_train = N",
    "rationale": "The figure shows the accuracy of different methods for different numbers of snapshots N. The CovNet with N_train = N curve is the highest at N = 20.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.09284v1",
    "pdf_url": null
  },
  {
    "instance_id": "11947f26751e405299c0fa1bce4eea59",
    "figure_id": "2001.11659v2-Figure6-1",
    "image_file": "2001.11659v2-Figure6-1.png",
    "caption": " Best-feasible CIFAR10 test-set accuracy by each iteration for the D = 36 constrained NAS problem, showing mean and two standard errors over 100 repeated runs. ALEBO was a best-performing method, and the only embedding method that outperformed random search.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieved the highest best-feasible CIFAR10 test-set accuracy after 50 function evaluations?",
    "answer": "ALEBO",
    "rationale": "The plot shows the best-feasible CIFAR10 test-set accuracy achieved by each algorithm as a function of the number of function evaluations. ALEBO is the only algorithm that consistently achieves an accuracy of greater than 93% after 50 function evaluations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.11659v2",
    "pdf_url": null
  },
  {
    "instance_id": "0212fb5965fa433198092b4637ffcb78",
    "figure_id": "2110.12381v1-Figure2-1",
    "image_file": "2110.12381v1-Figure2-1.png",
    "caption": " Parameter Analysis.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the effect of increasing the value of gamma (γ) on the negative log-likelihood (NLL) for the Yahoo dataset?",
    "answer": " The NLL decreases as the value of γ increases.",
    "rationale": " The figure shows the NLL for different values of γ on the Yahoo dataset. As γ increases, the NLL decreases, indicating that the model is able to better fit the data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.12381v1",
    "pdf_url": null
  },
  {
    "instance_id": "18cb4262d6d54df1aa90c19b9586eaa1",
    "figure_id": "2106.03760v3-Figure3-1",
    "image_file": "2106.03760v3-Figure3-1.png",
    "caption": " Expert weights of the DSelect-k gates on the recommender system.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which engagement task and satisfaction task pair had the highest expert weight?",
    "answer": "Engagement task 3 and satisfaction task 2.",
    "rationale": "The expert weight is indicated by the color of the boxes in the figure. The darkest red boxes have the highest weight, and the lightest orange boxes have the lowest weight. The box corresponding to engagement task 3 and satisfaction task 2 is the darkest red, indicating that this pair had the highest expert weight.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03760v3",
    "pdf_url": null
  },
  {
    "instance_id": "d9d3b105acca4d8e8945bd55eb36d235",
    "figure_id": "2011.02610v1-Figure4-1",
    "image_file": "2011.02610v1-Figure4-1.png",
    "caption": " Times of event words that are predicted correctly in TimeBank TestWSJ set in unsupervised setting (only shows most frequent 15 event words)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which event word is predicted correctly the most often in the TimeBank TestWSJ set in the unsupervised setting?",
    "answer": "\"loss\"",
    "rationale": "The bar for \"loss\" is the highest in the figure, indicating that it is the most frequently predicted event word.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2011.02610v1",
    "pdf_url": null
  },
  {
    "instance_id": "42d4acbc5f5d44729e8879250c538ab6",
    "figure_id": "2310.19859v1-Figure7-1",
    "image_file": "2310.19859v1-Figure7-1.png",
    "caption": " Qualitative results of SD v1.5, DreamBooth, existing tuning strategies, and our Res-Tuning on Oxford Flowers and Food-101 fine-grained dataset with the same generated seed. We frame our results in green and others in red.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tuning strategy appears to be the most effective for generating realistic images of balloon flowers and hotdogs?",
    "answer": "Res-Tuning",
    "rationale": "The figure shows that Res-Tuning produces the most realistic images of balloon flowers and hotdogs, compared to the other tuning strategies. For example, the balloon flowers generated by Res-Tuning have the correct number of petals and are bell-shaped, while the hotdogs have the correct toppings and are visually appealing.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.19859v1",
    "pdf_url": null
  },
  {
    "instance_id": "32103785144a40bf8cf41be48ff1e5c5",
    "figure_id": "2305.19148v3-Figure13-1",
    "image_file": "2305.19148v3-Figure13-1.png",
    "caption": " Although domain-label bias is modeldependent, we observe a high correlation of domainlabel bias between LLMs on the evaluation datasets.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which two language models have the most similar domain-label bias?",
    "answer": " GPT-3 (6.7B) and GPT-J (6B)",
    "rationale": " The figure shows a heatmap of the correlation of domain-label bias between different language models. The darker the color, the higher the correlation. The correlation between GPT-3 (6.7B) and GPT-J (6B) is 0.84, which is the highest correlation among all pairs of models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.19148v3",
    "pdf_url": null
  },
  {
    "instance_id": "b492c1704a3b4ba9b64bc7a9611988f1",
    "figure_id": "2110.14354v1-Figure2-1",
    "image_file": "2110.14354v1-Figure2-1.png",
    "caption": " The convergence of different clustering methods over time (seconds) on Rossmann dataset. The optimization objective of MixARMA is maximized by EM algorithm, and the optimization objective of MixSeq and DTCR are minimized by gradient descent with Adam.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three clustering methods converges the fastest?",
    "answer": "DTCR",
    "rationale": "The figure shows the convergence of the three clustering methods over time. DTCR has the steepest slope, which means it converges the fastest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.14354v1",
    "pdf_url": null
  },
  {
    "instance_id": "02109f7cffed4303978f09c70ac15787",
    "figure_id": "2101.00408v2-Figure1-1",
    "image_file": "2101.00408v2-Figure1-1.png",
    "caption": " An example illustrating OpenQA pipeline.",
    "figure_type": "\"other\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Where is the Bowling Hall of Fame located?",
    "answer": "Arlington, Texas",
    "rationale": "The answer can be found in Document 3. It states that the World Bowling Writers (WBW) International Bowling Hall of Fame was established in 1993 and is located on the International Bowling campus in Arlington, Texas.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.00408v2",
    "pdf_url": null
  },
  {
    "instance_id": "bfe45085fb7e4cd898b1052723ea1954",
    "figure_id": "2203.12971v1-Figure4-1",
    "image_file": "2203.12971v1-Figure4-1.png",
    "caption": " Relation Accuracy of BAP and DEPPROBE compared for all 13 in-language targets, grouped according to the Universal Dependencies taxonomy (de Marneffe et al., 2014).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which relation target has the highest accuracy for both BAP and DepProbe?",
    "answer": "'cc'",
    "rationale": "The figure shows the relation accuracy of BAP and DepProbe for all 13 in-language targets. The target with the highest accuracy for both models is 'cc', which is a coordinating conjunction.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.12971v1",
    "pdf_url": null
  },
  {
    "instance_id": "d7eabb01d26a43e4a27e08d5631bf5e7",
    "figure_id": "2001.11101v1-Figure3-1",
    "image_file": "2001.11101v1-Figure3-1.png",
    "caption": " Performances in demographic attribute prediction based on both SVR (black bar) and PCA+LR (hatching bar). See abbreviation description in Experiments-Baseline Models section.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which city had the best performance for average household income (AHI) based on SVR?",
    "answer": "Bay Area",
    "rationale": "The bar for Bay Area in the SVR category is the highest among all cities for AHI.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.11101v1",
    "pdf_url": null
  },
  {
    "instance_id": "675b5de0f540443b935f5b98832c8d12",
    "figure_id": "2006.04492v2-Figure14-1",
    "image_file": "2006.04492v2-Figure14-1.png",
    "caption": " NAS performance of Random Search (RS) in combined with final validation accuracy (Final Val Acc), early-stop validation accuracy (ES Val Acc) and our estimator TSE-EMA on NASBench-201. TSE-EMA enjoys competitive convergence as ES Val Acc and both are faster than using Final Val Acc.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three datasets is the most challenging for Random Search (RS) in terms of finding the best architecture?",
    "answer": "RS-IMAGENET-16-120.",
    "rationale": "The figure shows the best test error for RS on three different datasets: RS-CIFAR10, RS-CIFAR100, and RS-IMAGENET-16-120. The best test error for RS-IMAGENET-16-120 is the highest, which indicates that it is the most challenging dataset for RS to find the best architecture.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04492v2",
    "pdf_url": null
  },
  {
    "instance_id": "fb6e26dc18b442f3990ee5678d326a46",
    "figure_id": "1905.10307v4-Figure3-1",
    "image_file": "1905.10307v4-Figure3-1.png",
    "caption": " The four-stage experimental protocol for multi-task curriculum training. The same input module (CNN) and output module (MLP) are used for the PrediNet and all baseline architectures; only the central module varies. Task identifiers are appended to the central module’s output vector.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which stage of the experimental protocol involves pre-training the input and central modules of the network?",
    "answer": "Stage 4",
    "rationale": "The figure shows that in Stage 4, both the input network (CNN) and the central module are pre-trained. This is indicated by the yellow \"Frozen\" boxes around these modules.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.10307v4",
    "pdf_url": null
  },
  {
    "instance_id": "735200f0b36d4a1a94af6ebcae95a957",
    "figure_id": "2203.02557v3-Figure4-1",
    "image_file": "2203.02557v3-Figure4-1.png",
    "caption": " Attention. Attention heatmap generated by the attention weights from the 12 Transformer encoder blocks in the pixel-wise ViT. The attention heatmap demonstrates the amount of attention different locations of an image receive.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which block of the Transformer encoder pays the most attention to the eyes of the subjects in the image?",
    "answer": "Block 10.",
    "rationale": "The attention heatmap for block 10 shows the most intense blue color around the eyes of the subjects, indicating that this block is paying the most attention to that area of the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.02557v3",
    "pdf_url": null
  },
  {
    "instance_id": "f8aa4443912e4eacbe4710c1151a8241",
    "figure_id": "2211.15974v2-Figure3-1",
    "image_file": "2211.15974v2-Figure3-1.png",
    "caption": " Average preference scores (%) of ABX tests on speech quality between NSPP and its ablated variants, where N/P stands for “no preference” and p denotes the p-value of a t-test between two models.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which NSPP ablated variant had the highest preference score?",
    "answer": "NSPP wo IP.",
    "rationale": "The figure shows the average preference scores (%) of ABX tests on speech quality between NSPP and its ablated variants. The NSPP wo IP bar is the tallest among the NSPP ablated variants, indicating it has the highest preference score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.15974v2",
    "pdf_url": null
  },
  {
    "instance_id": "f795af75fe9a4a2b83509c6c781164b6",
    "figure_id": "2106.05956v4-Figure13-1",
    "image_file": "2106.05956v4-Figure13-1.png",
    "caption": " ResNet-56 without SkipInit",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in terms of test accuracy?",
    "answer": "BN.",
    "rationale": "The figure shows the test accuracy of different methods as a function of the number of epochs. The BN method has the highest test accuracy at the end of training.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05956v4",
    "pdf_url": null
  },
  {
    "instance_id": "3f412d7a6f414535ba20c52fb032c08c",
    "figure_id": "2305.13987v1-Figure5-1",
    "image_file": "2305.13987v1-Figure5-1.png",
    "caption": " Two graphs that can be distinguished by SPD-SEGWL but not WL.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two graphs is more likely to be generated by a process that exhibits power-law behavior?",
    "answer": "The graph on the left.",
    "rationale": "The graph on the left has a heavier tail than the graph on the right. This is consistent with power-law behavior, which is characterized by a distribution that decays slowly as a function of the variable.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.13987v1",
    "pdf_url": null
  },
  {
    "instance_id": "72b2cdf594ee4190985d8b2dbc4c6521",
    "figure_id": "2305.10696v1-Figure4-1",
    "image_file": "2305.10696v1-Figure4-1.png",
    "caption": " Comparison of different feature importance methods in feature selection. We report the AUC on the test set of the model using top k% selected features according to the feature importance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which feature importance method performs the best when selecting the top 30% of features?",
    "answer": "Unbiased Gain",
    "rationale": "The figure shows that the Unbiased Gain method has the highest AUC on the test set when selecting the top 30% of features. This is evident from the height of the bars in the figure, which represent the AUC for each method at different values of k.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.10696v1",
    "pdf_url": null
  },
  {
    "instance_id": "082eacbfa5214fb784997141ba51550b",
    "figure_id": "2111.05498v2-Figure4-1",
    "image_file": "2111.05498v2-Figure4-1.png",
    "caption": " Histogram showing learned β coefficients for all Attention heads across layers for the 5 translation tasks used in [22]. We plot the β values for Attention that approximate the different d∗ definitions showing how the βs are interpolating between them. βCD is optimal for critical distance (maximum noise for each query); βSNR is optimal for Signal-to-Noise ratio. This assumes there is no query noise and SDM wants to minimize noise from other queries. βMem maximizes memory capacity and also assumes no query noise.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which beta coefficient is most commonly learned by the attention heads?",
    "answer": "BetaMem",
    "rationale": "The histogram shows that the majority of the learned beta coefficients are between 30 and 35, which corresponds to the BetaMem value.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.05498v2",
    "pdf_url": null
  },
  {
    "instance_id": "c7cc0bde727e452da202de689fbac4ee",
    "figure_id": "1905.04405v2-Figure1-1",
    "image_file": "1905.04405v2-Figure1-1.png",
    "caption": " In this work, we create context-aware representations for objects by sending messages between relevant objects in a dynamic way that depends on the input language. In the left example, the first round of message passing updates object 2 with features of object 3 based on the woman holding a blue umbrella (green arrow), and the second round updates object 1 with object 2’s features based on person to the left (red arrow). The final answer prediction can be made by a single attention hop over the most relevant object (blue box).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which object is updated with features of object 3 in the first round of message passing?",
    "answer": "Object 2.",
    "rationale": "The figure shows that the green arrow goes from object 3 to object 2 in the first round of message passing.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.04405v2",
    "pdf_url": null
  },
  {
    "instance_id": "12d3002d94c24db9bd1e8426f2cd3a6f",
    "figure_id": "2206.01829v2-Figure15-1",
    "image_file": "2206.01829v2-Figure15-1.png",
    "caption": " Additional Unconditional generation results from DooD.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the datasets in the figure appears to have the most diversity in terms of the types of images it contains?",
    "answer": "Omniglot",
    "rationale": "The Omniglot dataset contains images of characters from a variety of different languages, including English, Japanese, and Chinese. This can be seen in the figure, where the Omniglot images include a wide range of different symbols and characters.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.01829v2",
    "pdf_url": null
  },
  {
    "instance_id": "9b9f8fe147ee4cfaa3dcb42b5911e4e8",
    "figure_id": "2304.05170v2-Figure4-1",
    "image_file": "2304.05170v2-Figure4-1.png",
    "caption": " Visualization of re-ID features from sampled videos in MOT17, SportsMOT and DanceTrack dataset using t-SNE [35]. The same object is coded by the same color. It indicates that object appearance of SportsMOT is less distinguishable than that of MOT17, while more distinguishable than that of DanceTrack. We expect the appearance model to capture more discriminative and extensive representation for object association.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the most distinguishable object appearances?",
    "answer": "MOT17",
    "rationale": "The figure shows the visualization of re-ID features from sampled videos in MOT17, SportsMOT, and DanceTrack datasets using t-SNE. The same object is coded by the same color. In MOT17, the different colored points are clearly separated from each other, indicating that the object appearances are very distinguishable. In SportsMOT, the different colored points are more clustered together, indicating that the object appearances are less distinguishable. In DanceTrack, the different colored points are even more clustered together, indicating that the object appearances are the least distinguishable.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.05170v2",
    "pdf_url": null
  },
  {
    "instance_id": "4f050c96f8794d0eb53641f1b96ef287",
    "figure_id": "2207.08922v1-Figure11-1",
    "image_file": "2207.08922v1-Figure11-1.png",
    "caption": " Overall effects measures by the Delta Relative Improvement (DRI) and the Effect Ratio (ER) of AP.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in terms of Delta Relative Improvement (DRI) and Effect Ratio (ER)?",
    "answer": "Robust05",
    "rationale": "The figure shows the DRI and ER for each method. Robust05 has the highest DRI and ER, indicating that it performed the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.08922v1",
    "pdf_url": null
  },
  {
    "instance_id": "656dc86e75ce42c29f540ab81b2ee0c8",
    "figure_id": "2109.14723v1-Figure2-1",
    "image_file": "2109.14723v1-Figure2-1.png",
    "caption": " The four configurations we evaluate. In (B), the contraint-solver (SAT solver) is run over all model M answers so far. In (C), current beliefs are fed back as context for new questions. (D) combines the two.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the configurations in the figure uses both constraint-solving and feedback?",
    "answer": "Configuration (D)",
    "rationale": "The figure shows four configurations of a system. Configuration (D) includes both the constraint-solver (SAT solver) and feedback loop, which are not present in the other configurations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.14723v1",
    "pdf_url": null
  },
  {
    "instance_id": "f96e566743594299a08322ab7f8ab2dd",
    "figure_id": "2109.05198v1-Figure11-1",
    "image_file": "2109.05198v1-Figure11-1.png",
    "caption": " Evolution of the testing accuracy (top row) and the maximum accuracy (bottom row) of OASIS, AdGD and AdaHessian for non-linear least square. From left to right: ijcnn1, rcv1, news20, covtype and real-sim.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm achieved the highest maximum accuracy on the news20 dataset?",
    "answer": "AdaHessian",
    "rationale": "The bottom row of the figure shows the maximum accuracy achieved by each algorithm on each dataset. The bar for AdaHessian on the news20 dataset is the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.05198v1",
    "pdf_url": null
  },
  {
    "instance_id": "eb3ae7d1c54443af94eae5686cc8b765",
    "figure_id": "1906.04659v3-Figure14-1",
    "image_file": "1906.04659v3-Figure14-1.png",
    "caption": " Comparison: eLhist of the discriminator for pairs of samples from the real distribution on CIFAR10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of GAN has the highest Inception Score?",
    "answer": "Stable GAN with projection discriminator.",
    "rationale": "The Inception Score is shown on the y-axis of the figure. The purple dashed line shows the maximum Inception Score, which is achieved by the Stable GAN with projection discriminator.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04659v3",
    "pdf_url": null
  },
  {
    "instance_id": "2d3ca3aac1164bba9d5785b2dea49bb6",
    "figure_id": "2205.02835v1-Figure8-1",
    "image_file": "2205.02835v1-Figure8-1.png",
    "caption": " Runtime of each method. We draw the wall-time in seconds on the x-axis and the corresponding Wasserstein-1 distance loss values on the y-axis. Each data point represents the lowest loss achieved for that method at the end of a stage.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the lowest loss for the airplane dataset?",
    "answer": "CPD Deform",
    "rationale": "The figure shows that the blue line, which represents CPD Deform, has the lowest loss values for the airplane dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.02835v1",
    "pdf_url": null
  },
  {
    "instance_id": "e3cd63b524f148d79ef475dcdd2567f1",
    "figure_id": "2105.09447v1-Figure7-1",
    "image_file": "2105.09447v1-Figure7-1.png",
    "caption": " Average episode lengths of VTNet and VTNet without pre-training during training. We compare VTNet with VTNet without pre-training scheme. Blue and orange curves represent VTNet and VTNet w/o pre-training, respectively.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the effect of pre-training on the performance of VTNet?",
    "answer": "Pre-training improves the performance of VTNet.",
    "rationale": "The figure shows that the average episode length of VTNet with pre-training is shorter than that of VTNet without pre-training. This means that VTNet with pre-training is able to learn faster and achieve better performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.09447v1",
    "pdf_url": null
  },
  {
    "instance_id": "b4bb57c0996a40f3a9cd7df54c5d1518",
    "figure_id": "1901.02840v2-Figure7-1",
    "image_file": "1901.02840v2-Figure7-1.png",
    "caption": " Ablation study of CCDNet on GIF-Faces dataset. (a) U-Net is a more effective building block for CCDNet than others. (b) It is critical to include the loss defined on the gradient values, and using adversarial loss yields more realistic images. (c, d) It is beneficial to unfold CCDNet by multiple steps.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which loss function leads to the highest PSNR and SSIM values when using the U-Net architecture?",
    "answer": "All Losses",
    "rationale": "The plot in Figure (b) shows the PSNR and SSIM values for different loss functions. The \"All Losses\" configuration, which includes all the available loss functions, achieves the highest values for both metrics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.02840v2",
    "pdf_url": null
  },
  {
    "instance_id": "d808b3afc1604c23b439ab133ba24c03",
    "figure_id": "2309.16019v1-Figure1-1",
    "image_file": "2309.16019v1-Figure1-1.png",
    "caption": " Attention maps by different encoders. On top: RGB image and depth map by models using CNN-based (ResNet) and transformer-based (MPViT) encoders. At bottom: untextured region highlighted on RGB image and attention maps by ResNet and MPViT based models.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which encoder focuses more on the untextured region of the image?",
    "answer": "MPViT",
    "rationale": "The attention map of the MPViT model shows a higher concentration of attention on the untextured region (green square) compared to the ResNet model.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.16019v1",
    "pdf_url": null
  },
  {
    "instance_id": "8bc8634ed3204b7ba466b9e18559b401",
    "figure_id": "1810.13333v2-Figure16-1",
    "image_file": "1810.13333v2-Figure16-1.png",
    "caption": " Results on the kMNIST dataset. In each row we consider a different metric to generate the triplets. In each column we consider a different proportion of triplets available from 1 to 10%. In each plot we vary the noise level from 0 to 20%.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which metric performs the best when the noise level is high and the proportion of triplets is low?",
    "answer": " Euclidean distance.",
    "rationale": " The figure shows that the Euclidean distance metric (red line) consistently outperforms the other metrics (Cosine and Cityblock) across all noise levels and proportions of triplets. When the noise level is high (e.g., 0.2) and the proportion of triplets is low (e.g., 1%), the Euclidean distance metric maintains a higher test accuracy compared to the other metrics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.13333v2",
    "pdf_url": null
  },
  {
    "instance_id": "d2c3b51ce8874443a03c96be431de5c1",
    "figure_id": "2305.17804v1-Figure6-1",
    "image_file": "2305.17804v1-Figure6-1.png",
    "caption": " Examples of potential operations.",
    "figure_type": "\"schematic\"",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What are the two types of updates that can be made in this interface?",
    "answer": "Local and global updates.",
    "rationale": "The figure shows two buttons labeled \"Update Local\" and \"Update Global\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.17804v1",
    "pdf_url": null
  },
  {
    "instance_id": "db9a4962f0404bdc8240ae54a53da9ad",
    "figure_id": "2304.02797v1-Figure5-1",
    "image_file": "2304.02797v1-Figure5-1.png",
    "caption": " Depth and view synthesis performance on ScanNet (scene 0653 00), with varying latent space shapes (larger values were not considered due to computational constraints). Blue and red lines correspond to predictions decoded from a shared latent space, and green and yellow lines to predictions decoded from latent spaces with a single representation. We observe that sharing the latent space between representations not only does not degrade results, but in fact leads to overall improvements in both view synthesis and depth estimation. These improvements are more noticeable in smaller latent spaces, particularly for depth and light field estimates, indicating that both representations are compatible for multi-task decoding.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does sharing the latent space between representations improve the performance of depth and view synthesis?",
    "answer": "Yes.",
    "rationale": "The figure shows that the blue and red lines, which correspond to predictions decoded from a shared latent space, are generally higher than the green and yellow lines, which correspond to predictions decoded from latent spaces with a single representation. This indicates that sharing the latent space leads to better performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.02797v1",
    "pdf_url": null
  },
  {
    "instance_id": "9f40e78e19fa4aa488ed7140dc277c78",
    "figure_id": "2206.11953v1-Figure1-1",
    "image_file": "2206.11953v1-Figure1-1.png",
    "caption": " The Simulated Spatial Dataset consists of procedurally generated motion data of a virtual agent interacting with an object. In this sequence the agent (red sphere) pushes the object (blue sphere). At t=0 and t=1, the agent approaches the ball. Then, in t=2 and t=3, the agent pushes to ball. Finally, at t=4, the ball is rolling away from the agent.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "At what time does the agent start pushing the ball?",
    "answer": "t=2",
    "rationale": "The figure shows the agent approaching the ball at t=0 and t=1. At t=2, the agent is in contact with the ball, indicating that it has started pushing the ball.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.11953v1",
    "pdf_url": null
  },
  {
    "instance_id": "906e44755c1740798f350c1ca76a251b",
    "figure_id": "2211.01910v2-Figure1-1",
    "image_file": "2211.01910v2-Figure1-1.png",
    "caption": " (a) Our method, Automatic Prompt Engineer (APE), automatically generates instructions for a task that is specified via output demonstrations: it generates several instruction candidates, either via direct inference or a recursive process based on semantic similarity, executes them using the target model, and selects the most appropriate instruction based on computed evaluation scores. (b) As measured by the interquartile mean across the 24 NLP tasks introduced by Honovich et al. (2022), APE is able to surpass human performance when using the InstructGPT model (Ouyang et al., 2022).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better than the human prompt engineer on the 24 NLP tasks?",
    "answer": "APE with the InstructGPT model.",
    "rationale": "The figure shows that APE with the InstructGPT model has a higher interquartile mean than the human prompt engineer on the 24 NLP tasks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.01910v2",
    "pdf_url": null
  },
  {
    "instance_id": "0e0b0fd96230434598126dc66777a6b9",
    "figure_id": "1908.02725v2-Figure6-1",
    "image_file": "1908.02725v2-Figure6-1.png",
    "caption": " Comparison of theoretical variants of SVRG without mini-batching (b = 1) on the ijcnn1 data set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the algorithms converges the fastest in terms of epochs for λ = 10^-1?",
    "answer": "SVRG",
    "rationale": "The figure shows the residual (y-axis) versus epochs (x-axis) for different algorithms. The algorithm that reaches the lowest residual value in the fewest epochs is the fastest to converge. For λ = 10^-1, SVRG reaches the lowest residual value in the fewest epochs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.02725v2",
    "pdf_url": null
  },
  {
    "instance_id": "c7735a0b1db448789799fd365af0efa3",
    "figure_id": "2212.09278v1-Figure3-1",
    "image_file": "2212.09278v1-Figure3-1.png",
    "caption": " Results on the SparC dev set for different T5 pretrained model sizes. * indicates our re-implementation. The results of UNIFIEDSKG come from the original paper.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the SparC dev set in terms of TM accuracy?",
    "answer": "T5-3B",
    "rationale": "The bar graph on the right shows that the T5-3B model achieves the highest TM accuracy (48.6%) among all the models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.09278v1",
    "pdf_url": null
  },
  {
    "instance_id": "2083fa3581874ea7a467fdf8033d132a",
    "figure_id": "2304.11327v2-Figure10-1",
    "image_file": "2304.11327v2-Figure10-1.png",
    "caption": " Saliency map of feature learning on IWILDCAM benchmark. The blue dots are the salient features. A deeper blue color denotes more salient features. It can be found that FeAT is able to learn more meaningful and diverse features than ERM and Bonsai.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods, ERM, Bonsai, or FeAT, is able to learn the most meaningful and diverse features?",
    "answer": "FeAT.",
    "rationale": "The saliency maps show that FeAT is able to learn more meaningful and diverse features than ERM and Bonsai. The blue dots in the saliency maps represent the salient features, and a deeper blue color denotes more salient features. FeAT's saliency maps have more blue dots and deeper blue colors than ERM and Bonsai, indicating that it is able to learn more meaningful and diverse features.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.11327v2",
    "pdf_url": null
  },
  {
    "instance_id": "d6af2ed33db54738af4a40417f8f27d3",
    "figure_id": "2307.03110v1-Figure2-1",
    "image_file": "2307.03110v1-Figure2-1.png",
    "caption": " The top row of graphs demonstrate RWA. The correlation drop-off is steep and the correlations are weak (< .3) at one-third of the total edit distance available. The bottom row of graphs show the AAD. The trends are consistent across all search spaces and tasks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the search spaces and tasks exhibit the weakest correlations at one-third of the total edit distance available?",
    "answer": "ShuffleNetV2, NASBench101, and TransNASBench.",
    "rationale": "The top row of graphs shows that the RWA (Rank Weighted Average) for all three search spaces and tasks drops off steeply and reaches weak correlations (< .3) at one-third of the total edit distance available.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.03110v1",
    "pdf_url": null
  },
  {
    "instance_id": "5801060741f64457a2e3e8e6cb1b93e7",
    "figure_id": "2211.15612v2-Figure13-1",
    "image_file": "2211.15612v2-Figure13-1.png",
    "caption": " The performance of different algorithms on the medium-quality datasets (MPE)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the medium-quality datasets (MPE)?",
    "answer": "Ours",
    "rationale": "The figure shows the performance of different algorithms on the medium-quality datasets (MPE). The y-axis shows the episode return, and the x-axis shows the number of steps. The algorithm \"Ours\" has the highest episode return, which means it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.15612v2",
    "pdf_url": null
  },
  {
    "instance_id": "bd47b50eb38449dea062ac49bc1b6ae8",
    "figure_id": "2104.14403v2-Figure17-1",
    "image_file": "2104.14403v2-Figure17-1.png",
    "caption": " %Attr vs manipulation visibility for all pairs of saliency maps and manipulations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which saliency map method is most sensitive to changes in brightness?",
    "answer": "Gradient.",
    "rationale": "The figure shows that the %Attr for Gradient decreases more rapidly with increasing visibility of the Brightness manipulation than for any other saliency map method.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.14403v2",
    "pdf_url": null
  },
  {
    "instance_id": "578814c95ce749a2ad04f57182e42bf8",
    "figure_id": "2106.03050v1-Figure3-1",
    "image_file": "2106.03050v1-Figure3-1.png",
    "caption": " Exploration ability analysis of double actors on GoldMiner environment.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm explores the environment more effectively, DADDPG or DDPG?",
    "answer": "DADDPG",
    "rationale": "The state visit frequency plot (c) shows that DADDPG visits the right gold mine more frequently than DDPG, which indicates that DADDPG explores the environment more effectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03050v1",
    "pdf_url": null
  },
  {
    "instance_id": "58907e98cd2849638c9caae067e5c989",
    "figure_id": "2107.01057v1-Figure5-1",
    "image_file": "2107.01057v1-Figure5-1.png",
    "caption": " Temporal evolution of accuracy improvement and accumulated negative flips on ImageNet for two scenarios where the models Ct do not exhibit improving performance, but instead arrive in (a) random or (b) adversarial order. As can be seen, our methods are robust in both these cases while introducing much fewer negative flips than the non-probabilistic baselines.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method introduces the least number of negative flips?",
    "answer": "MBME (100)",
    "rationale": "The figure shows the accumulated negative flips for different methods. MBME (100) has the lowest curve in both the random and adversarial order scenarios.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.01057v1",
    "pdf_url": null
  },
  {
    "instance_id": "f0a02c6917f34b1098cb07cd30df87d1",
    "figure_id": "2212.04129v2-Figure8-1",
    "image_file": "2212.04129v2-Figure8-1.png",
    "caption": " Depth of the meta model. We perform modular training with meta models of varying depths. Two ways of implementation, i.e., Module Imitation (Fig. 2b) and Module Incubation (ours, Fig. 2c), are compared.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better for meta-model depths greater than 16?",
    "answer": "Incubation.",
    "rationale": "The figure shows that the Incubation method (red line) has a higher accuracy than the Imitation method (blue line) for meta-model depths greater than 16.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.04129v2",
    "pdf_url": null
  },
  {
    "instance_id": "b84bd926aca0426281e4035d88b4b54f",
    "figure_id": "1904.00923v1-Figure3-1",
    "image_file": "1904.00923v1-Figure3-1.png",
    "caption": " A car, though initially classified correctly with high confidence, is easily changed into being classified as a sofa with only 26 points changed. The original input has been marked with red points to denote all the parts of the input that exist in the critical set.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more efficient at changing the classification of the image, iterative sample occlusion or random sampling?",
    "answer": "Iterative sample occlusion.",
    "rationale": "The figure shows that iterative sample occlusion only needs to change 26 points to change the classification of the image from a car to a sofa, while random sampling needs to change 1905 points to change the classification of the image from a car to a sink.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.00923v1",
    "pdf_url": null
  },
  {
    "instance_id": "96356d5f47984406bd1d4a3cce44f724",
    "figure_id": "2002.11566v1-Figure5-1",
    "image_file": "2002.11566v1-Figure5-1.png",
    "caption": " Analysis of different temperatures of ELM and different ratios of KL-loss on MSR-VTT.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which temperature and KL-loss ratio combination resulted in the highest CIDEr score?",
    "answer": "T-1 and λ=0.1.",
    "rationale": "The figure shows that the CIDEr score is highest for T-1 at all KL-loss ratios. Additionally, the CIDEr score is highest for λ=0.1 across all temperatures. Therefore, the combination of T-1 and λ=0.1 results in the highest CIDEr score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.11566v1",
    "pdf_url": null
  },
  {
    "instance_id": "10b631b1669a45248d199e31dfcf95bf",
    "figure_id": "2301.05217v3-Figure28-1",
    "image_file": "2301.05217v3-Figure28-1.png",
    "caption": " The train and test loss over the course of training with two types of regularization, dropout and ℓ1 regularization. Grokking occurs with some runs for dropout but never for ℓ1 regularization.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does the train loss converge to zero in any of the runs shown in the figure?",
    "answer": "No.",
    "rationale": "The train loss does not converge to zero in any of the runs shown in the figure. This is evident from the fact that the train loss curves do not reach zero at the end of the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2301.05217v3",
    "pdf_url": null
  },
  {
    "instance_id": "8f3c79e257a24700a76943c1b66cda9b",
    "figure_id": "2010.01528v2-Figure2-1",
    "image_file": "2010.01528v2-Figure2-1.png",
    "caption": " Few-shot CIL learning of CUB200 in 11 tasks where each point shows the classification accuracy on all seen classes so far. (Left) Shows ER with and without LRRR using different backbone architectures and saliency map techniques. (Right) Performance of the state-of-the-art existing approaches with and without LRRR on CUB200 including TOPIC (Tao et al., 2020), EEIL (Castro et al., 2018), iCaRL (Rebuffi et al., 2017). Joint training serves as the upper bound. Results for baselines are obtained using their original implementation. All results are averaged over 3 runs and mean and standard deviation values are given in the appendix. Best viewed in color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best in the 10-way 5-shot on CUB200 in 11 tasks setting?",
    "answer": "Joint training performs the best.",
    "rationale": "The figure shows that the Joint training method has the highest accuracy across all numbers of classes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.01528v2",
    "pdf_url": null
  },
  {
    "instance_id": "0d37f344fc584d9a918e524951fcc831",
    "figure_id": "2109.01394v2-Figure19-1",
    "image_file": "2109.01394v2-Figure19-1.png",
    "caption": " dSprites TVAE L = 0, K = 3",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different shapes are present in the image?",
    "answer": "5",
    "rationale": "The image shows a grid of 64 squares, each containing a different shape. There are five different shapes: a square, a triangle, a diamond, a circle, and an ellipse.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.01394v2",
    "pdf_url": null
  },
  {
    "instance_id": "e5db0d1285514ce286370a328fae10ad",
    "figure_id": "2104.13730v2-Figure4-1",
    "image_file": "2104.13730v2-Figure4-1.png",
    "caption": "Figure 4",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between Z1 and Z2?",
    "answer": "Z1 and Z2 are independent variables.",
    "rationale": "The figure shows a directed acyclic graph (DAG), which is a type of graph that is used to represent causal relationships between variables. In this DAG, Z1 and Z2 are not connected by any arrows, which indicates that they are independent of each other.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.13730v2",
    "pdf_url": null
  },
  {
    "instance_id": "2207878c21e1422bb751abe89470910e",
    "figure_id": "1810.04650v2-Figure6-1",
    "image_file": "1810.04650v2-Figure6-1.png",
    "caption": " Sample MultiMNIST images. In each image, one task (task-L) is classifying the digit on the top-left and the second task (task-R) is classifying the digit on the bottom-right.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many digits are there in each image?",
    "answer": "There are two digits in each image.",
    "rationale": "The caption states that each image contains two digits, one for task-L and one for task-R. This can also be seen in the individual crops of the images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1810.04650v2",
    "pdf_url": null
  },
  {
    "instance_id": "221dcb8709464d908ec4895263d28e12",
    "figure_id": "2105.04504v2-Figure8-1",
    "image_file": "2105.04504v2-Figure8-1.png",
    "caption": " Results on the rotated MNIST, FASHION-MNIST and corrupted CIFAR-10, showing the mean and std. dev. of the accuracy (top), and test log-likelihood (TLL) (bottom).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models tested is the most robust to rotation?",
    "answer": "NN+Dropout",
    "rationale": "The NN+Dropout model has the highest accuracy and test log-likelihood across all levels of rotation for all three datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.04504v2",
    "pdf_url": null
  },
  {
    "instance_id": "3418b9aa5dd3489db291210617605c28",
    "figure_id": "1911.12247v2-Figure6-1",
    "image_file": "1911.12247v2-Figure6-1.png",
    "caption": " Abstract state transition graphs per object slot for a trained SWM model without contrastive loss, using instead a loss in pixel space, on the 3D cubes environment. Edge color denotes action type.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the edge color represent in the abstract state transition graphs?",
    "answer": "The edge color denotes the action type.",
    "rationale": "The caption states that \"Edge color denotes action type.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.12247v2",
    "pdf_url": null
  },
  {
    "instance_id": "1a47dc5974db4eb3820fff26eec0714e",
    "figure_id": "2105.07122v3-Figure8-1",
    "image_file": "2105.07122v3-Figure8-1.png",
    "caption": " Annotation interface for phase 2.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the premise of the image?",
    "answer": "The atmosphere is cheerful.",
    "rationale": "The premise of the image is stated in the text box at the top of the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.07122v3",
    "pdf_url": null
  },
  {
    "instance_id": "96f9e7c9d7b8474fa10bd20b7ee21854",
    "figure_id": "2207.05959v2-Figure3-1",
    "image_file": "2207.05959v2-Figure3-1.png",
    "caption": " Performance of FPSR with different _.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the highest performance for FPSR?",
    "answer": "Gowalla",
    "rationale": "The figure shows the Recall@20 and NDCG@20 for different datasets. The Gowalla dataset has the highest values for both metrics.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2207.05959v2",
    "pdf_url": null
  },
  {
    "instance_id": "6d6ab20bec6643649eae45acd7c7a2b3",
    "figure_id": "2105.14953v1-Figure6-1",
    "image_file": "2105.14953v1-Figure6-1.png",
    "caption": " Our method shows smaller errors than others during the almost entire extrapolation testing time in PhysioNet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the smallest error during the extrapolation testing time in PhysioNet?",
    "answer": "ACE-Latent-ODE (ODE enc.)",
    "rationale": "The figure shows the MSE (mean squared error) of three different methods over time. The ACE-Latent-ODE (ODE enc.) method has the lowest MSE for almost the entire testing time.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.14953v1",
    "pdf_url": null
  },
  {
    "instance_id": "389764648a3f4332a98455de922a6474",
    "figure_id": "1911.10492v1-Figure8-1",
    "image_file": "1911.10492v1-Figure8-1.png",
    "caption": " Image cropping results on six images (left column) with different aspect ratios and shapes.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image has the least amount of information cropped out?",
    "answer": "The circular crop.",
    "rationale": "The circular crop preserves the most information from the original image, while the other crops remove information from the sides or top and bottom of the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.10492v1",
    "pdf_url": null
  },
  {
    "instance_id": "ee8d2dd6ac994842b3b3b2935ff23895",
    "figure_id": "1903.10598v1-Figure1-1",
    "image_file": "1903.10598v1-Figure1-1.png",
    "caption": " Accuracy-discrimination trade-off of 4 families of approaches on 3 classification datasets: (a) Default, (b) Adult, and (c) COMPAS. Each dot represents a different sample from 5-fold cross-validation and each shaded area corresponds to the convex hull of the results associated with each approach in accuracy-discrimination space. Same trade-off of 3 families of approaches on the regression dataset Crime is shown in (d).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which approach has the highest accuracy on the Default dataset?",
    "answer": "MIP-DT",
    "rationale": "The figure shows the accuracy-discrimination trade-off of different approaches on different datasets. The x-axis shows the discrimination and the y-axis shows the accuracy. The MIP-DT approach has the highest accuracy on the Default dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.10598v1",
    "pdf_url": null
  },
  {
    "instance_id": "5abd5d5de5cf437887ad9e391ed65a86",
    "figure_id": "2010.14439v2-Figure6-1",
    "image_file": "2010.14439v2-Figure6-1.png",
    "caption": " Distribution of # answers of test questions.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the most common number of answer concepts for test questions?",
    "answer": "5",
    "rationale": "The histogram shows that the most common number of answer concepts is 5, as this is the bar with the highest frequency.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.14439v2",
    "pdf_url": null
  },
  {
    "instance_id": "55533a87732c44a0b182244b6418e815",
    "figure_id": "1808.09442v2-Figure5-1",
    "image_file": "1808.09442v2-Figure5-1.png",
    "caption": " The learning curves of agents (DQN, DDQ, and D3Q) under the full domain setting.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest success rate after 250 epochs?",
    "answer": "D3Q",
    "rationale": "The figure shows the learning curves of four different algorithms. The success rate of each algorithm is plotted on the y-axis, and the number of epochs is plotted on the x-axis. The D3Q algorithm has the highest success rate after 250 epochs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.09442v2",
    "pdf_url": null
  },
  {
    "instance_id": "00313e2a02084a29a70f4e4d0492bce0",
    "figure_id": "2005.00574v1-Figure1-1",
    "image_file": "2005.00574v1-Figure1-1.png",
    "caption": " Examples from the emrQA dataset: Part of a clinical note as context and 2 question-answer pairs. Due to the original emrQA generation issues, oftentimes answers are incomplete or contain irrelevant parts to the questions (the underlined parts are what we think the most relevant to the questions).",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the main reason the patient was prescribed HCTZ?",
    "answer": "For HTN control.",
    "rationale": "The passage states that the patient was given HCTZ and lopressor for HTN control, which sufficiently controlled his BP.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.00574v1",
    "pdf_url": null
  },
  {
    "instance_id": "1d52d9bea2dc40e0a961fe0cbd4d5408",
    "figure_id": "2008.11089v1-Figure9-1",
    "image_file": "2008.11089v1-Figure9-1.png",
    "caption": " The effect of network architectures on transfer performance and robustness. The results of WideRes are consistent with those of the DTN architecture. The FTmodel is more advantageous than the Scratchmodel when the WideRes architecture is adopted under white-box attacks. On the other hand, the FTmodel is more likely to be attacked by the adversarial examples produced by its source model than the Scratchmodel under black-box attacks.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is more robust to black-box attacks when using the WideRes architecture?",
    "answer": "The Scratch model.",
    "rationale": "In Figure (c), which shows the results of black-box attacks, the Scratch model (blue dashed line) has a higher adversarial accuracy than the FT model (orange dotted line) for all values of epsilon. This indicates that the Scratch model is more robust to black-box attacks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.11089v1",
    "pdf_url": null
  },
  {
    "instance_id": "b5ddf67b80a44b3590819f5b47023fc5",
    "figure_id": "2002.12213v1-Figure8-1",
    "image_file": "2002.12213v1-Figure8-1.png",
    "caption": " Visualized comparisons of super-resolution results (×2) with isotropic blur kernel and bicubic subsampling gb1.3.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the super-resolution methods produced the most accurate reconstruction of the eye?",
    "answer": "MZSR",
    "rationale": "The figure shows the original image (GT) and the results of different super-resolution methods. MZSR is the method that produces the closest reconstruction to the original image, as evidenced by the sharpness and clarity of the eye in the MZSR result compared to the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.12213v1",
    "pdf_url": null
  },
  {
    "instance_id": "c2d8d6f5ffeb46dcacd6e5c922945ed6",
    "figure_id": "1904.07539v1-Figure1-1",
    "image_file": "1904.07539v1-Figure1-1.png",
    "caption": " Illustration of the setting: we observe the coverage of news events from a fixed set of sources over several time epochs. Example events are extracted from the GDELT database.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between news sources and news events?",
    "answer": "News sources report on news events.",
    "rationale": "The figure shows that news sources are connected to news events by lines, which indicates that the news sources are reporting on the news events.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.07539v1",
    "pdf_url": null
  },
  {
    "instance_id": "5530331f7dbf4b3ea2dc9ca81f2dc551",
    "figure_id": "2106.02514v2-Figure12-1",
    "image_file": "2106.02514v2-Figure12-1.png",
    "caption": " More qualitative results in SDF compared between Taming and iLAT.",
    "figure_type": "** photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which of the methods, Taming or iLAT, is able to generate images that are more similar to the original pose image?",
    "answer": " Taming.",
    "rationale": " The figure shows that the images generated by Taming are more similar to the original pose image than the images generated by iLAT. This is evident in the details of the clothing and the background. For example, in the first row, the Taming image correctly generates the ice rink and the person's jacket, while the iLAT image does not.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.02514v2",
    "pdf_url": null
  },
  {
    "instance_id": "6317ab29822143738c51dd5098f1bde7",
    "figure_id": "2306.01735v1-Figure7-1",
    "image_file": "2306.01735v1-Figure7-1.png",
    "caption": " The correctness score for every (concept, model) pair for (right to left) ES vs DE, ES vs ID, ES vs JA, and ES vs JA. Languages sharing scripts (ES/DE/ID and JA/ZH) are more correlated than those that don’t (ES/JA).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language pair has the highest cross-consistency score?",
    "answer": "ES-DE.",
    "rationale": "The figure shows that the ES-DE scatter plot has the most points in the upper right corner, indicating that these languages have the highest cross-consistency score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.01735v1",
    "pdf_url": null
  },
  {
    "instance_id": "f0a8d988a31d4d55ba016ee5d019bc49",
    "figure_id": "2007.05608v1-Figure5-1",
    "image_file": "2007.05608v1-Figure5-1.png",
    "caption": " Left: Human evaluation results on the Caption Comparison task. The pie plot shows percentage of votes for different options. There are four options for participants, Option 1: caption 1, Option 2: caption 2, Option 3: equally good, Option 4: equally bad. Right: We count the number of occurrences of words from each subcategory word list in the 5K test split. The pie plot shows the ratio of word occurrences between the two models. We also show two specific examples from the count list, e.g., two and three.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which category of words is more likely to be generated by the top-down model than the Ours: Mod model?",
    "answer": "Spatial words.",
    "rationale": "The pie plot for the Spatial category shows that the Top-Down model generates a higher percentage of spatial words (52%) than the Ours: Mod model (48%).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.05608v1",
    "pdf_url": null
  },
  {
    "instance_id": "94526097c98340148544fbbaf26b1f06",
    "figure_id": "2012.00857v3-Figure2-1",
    "image_file": "2012.00857v3-Figure2-1.png",
    "caption": " An example of T, ∆ and respective dependency graph D. Solid lines represent dependency relations between tokens. StructFormer only allow tokens with dependency relation to attend on each other.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which token has the highest height?",
    "answer": "Token x3.",
    "rationale": "The heights of the tokens are shown in the bottom row of the figure. Token x3 has the highest height of 5.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.00857v3",
    "pdf_url": null
  },
  {
    "instance_id": "0d8993eaa0d942b79fb2426f5bb22cb3",
    "figure_id": "2203.00089v1-Figure8-1",
    "image_file": "2203.00089v1-Figure8-1.png",
    "caption": " Test Accuracy on SVHN using SGDm with fixed learning rate, decayed learning rate schedule, and APO-tuned learning rate.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three learning rate schedules achieved the highest test accuracy on the SVHN dataset?",
    "answer": "SGDm-APO",
    "rationale": "The figure shows the test accuracy of the three learning rate schedules over 160 epochs. The SGDm-APO curve is consistently higher than the other two curves, indicating that it achieved the highest test accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.00089v1",
    "pdf_url": null
  },
  {
    "instance_id": "bb794a3e1cd94bcf9942d88dc826e151",
    "figure_id": "1805.07458v1-Figure3-1",
    "image_file": "1805.07458v1-Figure3-1.png",
    "caption": " Trace plots of cumulative regret for PG-TS and PG-TS-stream (Left), and Laplace-TS (Right) on the simulated data set with Gaussian θ∗ over 100 runs with 1, 000 trials.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the lowest cumulative regret?",
    "answer": "PG-TS-stream.",
    "rationale": "The figure on the left shows the cumulative regret for PG-TS, PG-TS-stream, and Laplace-TS. PG-TS-stream has the lowest cumulative regret because its line is the lowest on the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1805.07458v1",
    "pdf_url": null
  },
  {
    "instance_id": "e4cb7da1a3a2432c8891e2cfaf919261",
    "figure_id": "2105.02685v1-Figure4-1",
    "image_file": "2105.02685v1-Figure4-1.png",
    "caption": " Numerical experiments on multiclass style transfer using categorical labels. Results include: BLEU (Fig. 4a)); style transfer accuracy (Fig. 4b); sentence fluency (Fig. 4c).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieved the highest BLEU score?",
    "answer": "vCLUB-S",
    "rationale": "The figure shows the BLEU score for each method at different values of λ. The vCLUB-S method achieved the highest BLEU score at all values of λ.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2105.02685v1",
    "pdf_url": null
  },
  {
    "instance_id": "f6d97fba43b54cb990ad3b465585accd",
    "figure_id": "2205.11266v2-Figure3-1",
    "image_file": "2205.11266v2-Figure3-1.png",
    "caption": " Visual comparison of our attributions for a VGG-16 network fine-tuned on images from the VOC-2007 dataset and frozen. Our attributions are much more effective at retaining object class regions and discarding the rest. Examples 13 and 14 show cases where our Explainer is inaccurate.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the methods shown in the figure is the most effective at retaining object class regions and discarding the rest?",
    "answer": "Ours",
    "rationale": "The caption states that \"Our attributions are much more effective at retaining object class regions and discarding the rest.\" This is evident in the figure, where the \"Ours\" column shows the most focused and accurate attributions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.11266v2",
    "pdf_url": null
  },
  {
    "instance_id": "e736c6c864f945a9a2a69cf29c619918",
    "figure_id": "2006.01339v1-Figure2-1",
    "image_file": "2006.01339v1-Figure2-1.png",
    "caption": " Performance comparison of super-resolution models using SRZoo. (a) PSNR vs. NIQE (b) PSNR vs. running time on a CPU",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which super-resolution model has the highest PSNR and the lowest NIQE?",
    "answer": "EDSR+",
    "rationale": "The figure shows the performance of different super-resolution models in terms of PSNR, NIQE, and running time. EDSR+ has the highest PSNR and the lowest NIQE, which means it produces the highest quality images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.01339v1",
    "pdf_url": null
  },
  {
    "instance_id": "b8892b8f1d5b4fb9972afcdd8dec8b00",
    "figure_id": "2307.09696v2-Figure7-1",
    "image_file": "2307.09696v2-Figure7-1.png",
    "caption": " Qualitative comparisons on IXI, where we mark maximum values of error maps on each top left. Best view zoomed.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the highest error for the VM-ESC image?",
    "answer": "TMBS-ESC",
    "rationale": "The maximum error value for the VM-ESC image is shown in the top right corner of the corresponding CSE map. For TMBS-ESC, this value is 6.203, which is higher than the maximum error values for the other methods.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.09696v2",
    "pdf_url": null
  },
  {
    "instance_id": "550e569a7fc546cdb5d3a6f34b121725",
    "figure_id": "2106.04763v8-Figure2-1",
    "image_file": "2106.04763v8-Figure2-1.png",
    "caption": " Adaptive instance for d = K − 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best when K is 8 and B is 400?",
    "answer": "BayesGap-exp",
    "rationale": "The figure shows that the accuracy of BayesGap-exp is the highest when K is 8 and B is 400.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.04763v8",
    "pdf_url": null
  },
  {
    "instance_id": "5cbd9e3ea9684d058df840b74f3bad22",
    "figure_id": "2111.03505v1-Figure13-1",
    "image_file": "2111.03505v1-Figure13-1.png",
    "caption": " The emergence of regional patterns through the forward propagation. Coordinates along the vertical axis reflect the discrimination power of the target category.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset shows the most distinct regional patterns in the early layers of the network?",
    "answer": "The CUB dataset.",
    "rationale": "In the CUB dataset, the regional features of each category are clearly separated in the early layers of the network (conv_1, conv_2, etc.). This is evident from the distinct clusters of points in the scatter plots for each category. In contrast, the other datasets show more overlap between the regional features of different categories in the early layers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.03505v1",
    "pdf_url": null
  },
  {
    "instance_id": "8e2d8fccfb444c54b7445e74b370b562",
    "figure_id": "2210.14424v1-Figure3-1",
    "image_file": "2210.14424v1-Figure3-1.png",
    "caption": " Variation of mean citations (per publication) across time for the top-10 publishing countries.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which country has the highest mean citation per paper in 2022?",
    "answer": "United States",
    "rationale": "The line representing the United States is the highest in 2022.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.14424v1",
    "pdf_url": null
  },
  {
    "instance_id": "391f7a2f2ee64f06940f0a9d349435ee",
    "figure_id": "2203.12258v1-Figure5-1",
    "image_file": "2203.12258v1-Figure5-1.png",
    "caption": " The verbalization stabilities of 4 PLMs on all relations, which is measured by the percentage of relation instances whose predictions are unchanged when verbalization varies. We can see that the verbalization stabilities of all 4 PLMs (BERT-large, RoBERTa-large, GPT2-xl, BART-large) are poor.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which PLM has the highest verbalization stability?",
    "answer": "BERT",
    "rationale": "The box plot for BERT is the highest, which indicates that it has the highest median verbalization stability.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.12258v1",
    "pdf_url": null
  },
  {
    "instance_id": "0d627ff6191147299ccc97d1c3075324",
    "figure_id": "2101.09704v1-Figure5-1",
    "image_file": "2101.09704v1-Figure5-1.png",
    "caption": " Sensitivity to the sampling frequency.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more sensitive to the sampling frequency?",
    "answer": "AAPD",
    "rationale": "The plot shows that the F1-score of AAPD increases more rapidly with increasing sampling frequency than the F1-score of RCV.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.09704v1",
    "pdf_url": null
  },
  {
    "instance_id": "2d47f914d66e4f5aa835fbd4ae890435",
    "figure_id": "2006.09239v2-Figure10-1",
    "image_file": "2006.09239v2-Figure10-1.png",
    "caption": " Histograms of the entropy of the predicted categorical distributions for in-distribution (green), out-of-distribution (yellow) and out-of-domain (red) data. The value 2.3026∗ denotes the maximal entropy achievable for a categorical distribution with 10 classes. We use MNIST, FashionMNIST and the unscaled version of FashionMNIST as in-distribution, out-of-distribution and out-of-domain data. PostNet clearly distinguishes between the three types of data with low entropy for in-distribution data and high entropy for out-of-distribution, and close to the maximum possible entropy for out-of-domain data.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of data has the highest entropy?",
    "answer": "Out-of-domain data.",
    "rationale": "The figure shows that the out-of-domain data has the highest entropy, as indicated by the red bars.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.09239v2",
    "pdf_url": null
  },
  {
    "instance_id": "36710c1451d3434e8eb105ff84f3fe56",
    "figure_id": "2106.03765v2-Figure11-1",
    "image_file": "2106.03765v2-Figure11-1.png",
    "caption": " RMSE of CATE estimation by ρ for n0 = n1 = 200, 500, 1000 for setup A and B, using TNet (top row) and TARNet (bottom row) as baseline. Avg. across 10 runs, one standard error shaded.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs the best for setup A with nw = 200?",
    "answer": "DR + (TNet)",
    "rationale": "The figure shows the RMSE of different methods for different setups and values of nw. For setup A with nw = 200, the DR + (TNet) method has the lowest RMSE, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03765v2",
    "pdf_url": null
  },
  {
    "instance_id": "994aaefd48904066b64cba4e2c87a6e3",
    "figure_id": "2212.10549v2-Figure4-1",
    "image_file": "2212.10549v2-Figure4-1.png",
    "caption": " Top: UNITER SLV attention for caption “a few clouds and many wind turbines\", with the bounding box maximally attended to by the token in green; other highly attended boxes in red. Bottom: UNITER SLV attention with bounding boxes labeled with the tokens that maximally attend to them. Note that argmaxes often fail to precisely identify cross-modal equivalence.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the figure show?",
    "answer": "The figure shows how UNITER SLV attention works.",
    "rationale": "The figure shows four images with different bounding boxes. The top two images show the attention for the caption \"a few clouds and many wind turbines.\" The bottom two images show the attention for the caption \"hammering something together.\" The green bounding box is the one that is maximally attended to by the token in green. The other highly attended boxes are in red. The figure shows that UNITER SLV attention is able to attend to different parts of the image depending on the caption.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2212.10549v2",
    "pdf_url": null
  },
  {
    "instance_id": "31a9412638cc4b9b98011d44ad0504e3",
    "figure_id": "1905.12506v3-Figure18-1",
    "image_file": "1905.12506v3-Figure18-1.png",
    "caption": " Additional examples (including answers) of the RPM-like abstract visual reasoning task using dSprites.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many shapes are there in each grid?",
    "answer": "4",
    "rationale": "Each grid in the image contains 4 shapes, regardless of the color or type of shape.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.12506v3",
    "pdf_url": null
  },
  {
    "instance_id": "854503e407b14d2d8a9c9405c9cc01e5",
    "figure_id": "2109.04732v1-Figure17-1",
    "image_file": "2109.04732v1-Figure17-1.png",
    "caption": " Inter-rater consistency of gender base pairs. The word embeddings are trained with SGNS.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which corpus has the highest inter-rater consistency for the OCC16 target word list?",
    "answer": "WikiText-103",
    "rationale": "The box plot for WikiText-103 on the OCC16 target word list is higher than the other two box plots, indicating that the inter-rater consistency is higher for this corpus.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.04732v1",
    "pdf_url": null
  },
  {
    "instance_id": "258fe227d6f9430d9ff0f12b2f01a69f",
    "figure_id": "2006.16531v3-Figure7-1",
    "image_file": "2006.16531v3-Figure7-1.png",
    "caption": " The ICA training curve of test negative LL with different training objectives. y-axis indicates the negative test log-likelihood.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which training objective achieves a lower test negative log-likelihood after 15,000 iterations?",
    "answer": "maxSKS-g",
    "rationale": "The plot shows the test negative log-likelihood for two different training objectives, maxSKS-g and maxSKS-rg. The y-axis shows the negative log-likelihood, and the x-axis shows the number of iterations. After 15,000 iterations, the pink line representing maxSKS-g is lower than the brown line representing maxSKS-rg. This indicates that maxSKS-g achieves a lower test negative log-likelihood after 15,000 iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.16531v3",
    "pdf_url": null
  },
  {
    "instance_id": "075e26370169487a844ad560bc785908",
    "figure_id": "1904.06097v2-Figure7-1",
    "image_file": "1904.06097v2-Figure7-1.png",
    "caption": " Comparison of the PSNR values of SR images with respect to different α values for the partial attack.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best on the BSD100 dataset when α = 32?",
    "answer": "EDSR",
    "rationale": "The figure shows the PSNR values of different SR algorithms on different datasets for different values of α. The PSNR value for EDSR on the BSD100 dataset when α = 32 is the highest among all the algorithms.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.06097v2",
    "pdf_url": null
  },
  {
    "instance_id": "85e105fcbdbd4b5d9538bbc9486afc5e",
    "figure_id": "2106.01793v1-Figure1-1",
    "image_file": "2106.01793v1-Figure1-1.png",
    "caption": " A case extracted from the DocRED dataset. While the document has 6 sentences, only 1 or 2 sentences form the evidence for each relation instance.",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between Espoo Cathedral and Espoon keskus?",
    "answer": "Espoo Cathedral is located in Espoon keskus.",
    "rationale": "The figure shows that the subject \"Espoo Cathedral\" has the relation \"location\" with the object \"Espoon keskus\". This is evidenced by sentences [1] and [2] in the document.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.01793v1",
    "pdf_url": null
  },
  {
    "instance_id": "e7e65aacc96746c5bc7fb0f82848f76a",
    "figure_id": "2302.02601v4-Figure1-1",
    "image_file": "2302.02601v4-Figure1-1.png",
    "caption": " Example of a bi-level knowledge graph consisting of base-level and higher-level triplets in the FBHE dataset. The relation labels are omitted in the base-level triplets.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between David Beckham and the England National Team?",
    "answer": "David Beckham plays for the England National Team.",
    "rationale": "The figure shows a directed edge from David Beckham to the England National Team with the label \"PlaysFor\". This indicates that David Beckham plays for the England National Team.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.02601v4",
    "pdf_url": null
  },
  {
    "instance_id": "db9d22fdc66d42efb4091fd0eae9d0af",
    "figure_id": "2104.07705v2-Figure3-1",
    "image_file": "2104.07705v2-Figure3-1.png",
    "caption": " The performance of 24hBERT on the SST-2 task in few-shot settings (5 seeds for each #examples, with 25% of the examples used for validation), with and without prompts.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does 24hBERT perform better with or without prompts?",
    "answer": "24hBERT performs better with prompts.",
    "rationale": "The plot shows that the performance of 24hBERT with prompts (blue line) is higher than the performance of 24hBERT without prompts (red line) for all numbers of examples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2104.07705v2",
    "pdf_url": null
  },
  {
    "instance_id": "109e89b5fe9f42d5989da0864bf50a6a",
    "figure_id": "1906.11881v2-Figure5-1",
    "image_file": "1906.11881v2-Figure5-1.png",
    "caption": " Reconstructions (left images) and manipulation of latent codes (right images) on MNIST for the three different models: VAE (a), β-VAE (b) and C-VITAE (c). The right images are generated by varying one latent dimension in all models, while keeping the rest fixed. For the C-VITAE model, we have shown this for both the appearance and perspective spaces.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model is better at disentangling the latent space, VAE, β-VAE, or C-VITAE?",
    "answer": "C-VITAE",
    "rationale": "The figure shows that C-VITAE is able to disentangle the latent space better than VAE and β-VAE. This is because when we vary one latent dimension in C-VITAE, the resulting images change in a more meaningful way. For example, in the rightmost column of (c), we can see that varying the latent dimension for perspective changes the perspective of the digit, while keeping the appearance of the digit relatively constant. This is not the case for VAE and β-VAE, where varying a single latent dimension often results in changes to both the appearance and perspective of the digit.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.11881v2",
    "pdf_url": null
  },
  {
    "instance_id": "60208813127c47598f0fad98f152e116",
    "figure_id": "2210.15272v1-Figure2-1",
    "image_file": "2210.15272v1-Figure2-1.png",
    "caption": " A comparison between WVD and PWVD pitch tracking results of “Your letter”. A hann window of 40ms is chosen for PWVD.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pitch tracking method is more accurate, WVD or PWVD?",
    "answer": "PWVD is more accurate.",
    "rationale": "The PWVD pitch tracking results are closer to the ground truth than the WVD pitch tracking results. This can be seen in the figure, where the PWVD pitch track is closer to the red line representing the ground truth than the WVD pitch track.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.15272v1",
    "pdf_url": null
  },
  {
    "instance_id": "968bd7b0ed314ef393c3223d024e11d8",
    "figure_id": "2007.04640v2-Figure11-1",
    "image_file": "2007.04640v2-Figure11-1.png",
    "caption": " Performance of the entropy index as a function of training samples achieved by MEPOL and a random policy in the additional HandReach experiment (95% c.i. over 8 runs, k = 4, T = 50, Ntraj = 100, δ = 0.05).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Does MEPOL perform better than a random policy?",
    "answer": "Yes, MEPOL outperforms the random policy.",
    "rationale": "The plot shows that the entropy index of MEPOL is higher than that of the random policy for all numbers of training samples. A higher entropy index indicates better performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2007.04640v2",
    "pdf_url": null
  },
  {
    "instance_id": "fcaaa6880aae4259a4e9a6fd5b315777",
    "figure_id": "2304.12652v2-Figure12-1",
    "image_file": "2304.12652v2-Figure12-1.png",
    "caption": " The trend of PSNR across different training iterations is shown for the case of sparse points and noisy points. When using image features (i.e., hybrid neural rendering), the performance is better than the neural-3D-feature-only design (i.e., Point-NeRF).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs better for sparse points, hybrid neural rendering or Point-NeRF?",
    "answer": "Hybrid neural rendering.",
    "rationale": "The figure shows that the PSNR for hybrid neural rendering is higher than the PSNR for Point-NeRF for sparse points. This indicates that hybrid neural rendering performs better for sparse points.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.12652v2",
    "pdf_url": null
  },
  {
    "instance_id": "6a8f6b1e9b59450cb8797240d92f853c",
    "figure_id": "2303.10902v3-Figure3-1",
    "image_file": "2303.10902v3-Figure3-1.png",
    "caption": " Sensitivity analysis about the number of nearest neighbors K in memorized spatial local clustering, entropy filter hyperparameter M and trade-off parameter λ (cf . Eq. 7).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which parameter has the largest impact on the accuracy of the model?",
    "answer": "The number of nearest neighbors K.",
    "rationale": "The figure shows that the accuracy of the model varies the most with changes in K.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.10902v3",
    "pdf_url": null
  },
  {
    "instance_id": "bdd66f82c8294ac78c9b02fb9e4fae57",
    "figure_id": "2102.06571v3-FigureA.25-1",
    "image_file": "2102.06571v3-FigureA.25-1.png",
    "caption": "Figure A.25: Box plots of the mean-squared error of Bayesian FCNNs doing regression on UCI datasets. For each temperature, and prior, each box displays the median ±1.5 times the inter-quartile range. Outliers are plotted as ×. We exclude runs where the potential diverges. Temperature 1 is clearly best for all datasets, but otherwise there is no clear trend.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which temperature is the best for all datasets?",
    "answer": "Temperature 1 is the best for all datasets.",
    "rationale": "The box plots for temperature 1 are consistently lower than the box plots for other temperatures, indicating that the mean-squared error is lower for temperature 1.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.06571v3",
    "pdf_url": null
  },
  {
    "instance_id": "98fd91228e5848119c63d5b153035989",
    "figure_id": "2003.02739v4-Figure4-1",
    "image_file": "2003.02739v4-Figure4-1.png",
    "caption": " Differences in performance in terms of accuracy scores on the test set for few-shot X-MAML on XNLI using the Multi-BERT model. Rows correspond to target and columns to auxiliary languages used in X-MAML. Numbers on the off-diagonal indicate performance differences between X-MAML and the baseline model in the same row. The coloring scheme indicates the differences in performance (e.g., blue for large improvement).",
    "figure_type": "table",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which auxiliary language led to the biggest improvement in accuracy for the target language \"es\"?",
    "answer": "\"fr\"",
    "rationale": "The figure shows that the difference in accuracy between X-MAML and the baseline model for \"es\" is highest when \"fr\" is used as the auxiliary language. This is indicated by the dark blue color of the corresponding cell in the figure.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.02739v4",
    "pdf_url": null
  },
  {
    "instance_id": "372daac419bd42d2a22bf929dce6b347",
    "figure_id": "2106.08977v2-Figure7-1",
    "image_file": "2106.08977v2-Figure7-1.png",
    "caption": " Illustration of Weak Label Generation Process for Biomedical NER.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the chemical used to modify the silica-coated Fe3O4 nanoparticles?",
    "answer": "3-mercaptopropionic acid",
    "rationale": "The figure shows that the silica-coated Fe3O4 nanoparticles are modified with 3-mercaptopropionic acid. This is evident from the text in the red box.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.08977v2",
    "pdf_url": null
  },
  {
    "instance_id": "e2ba04a1b01b4f64838042b351f7d014",
    "figure_id": "2001.04559v1-Figure2-1",
    "image_file": "2001.04559v1-Figure2-1.png",
    "caption": " Examples of geometrically identical faces generated for five different IDs. First row shows input faces xi, and the second row shows the corresponding face images x̂i′ .",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many different IDs are represented in the figure?",
    "answer": "Five.",
    "rationale": "The caption states that the figure shows examples of geometrically identical faces generated for five different IDs. This is evident in the figure, which shows five different faces in the top row and five corresponding faces in the bottom row.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2001.04559v1",
    "pdf_url": null
  },
  {
    "instance_id": "64ff6e0e8a39468ba89a4958eb8d9fb9",
    "figure_id": "1901.10124v1-Figure6-1",
    "image_file": "1901.10124v1-Figure6-1.png",
    "caption": " Qualitative examples presenting the Civic Issue Graphs generated by our model. We show the top 3 relations and highlight the ones that are representative of the civic issue along with their bounding regions",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the car and the sidewalk in the second image?",
    "answer": "The car is parked on the sidewalk.",
    "rationale": "The figure shows a car that is partially parked on the sidewalk. The bounding boxes around the car and the sidewalk overlap, which indicates that the two objects are related.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.10124v1",
    "pdf_url": null
  },
  {
    "instance_id": "d4ae2d1c95884f5fa560cfa9bb784dbc",
    "figure_id": "2003.07449v2-Figure2-1",
    "image_file": "2003.07449v2-Figure2-1.png",
    "caption": " Existing models introduce spurious objects not specified in the layout, a failure mode over which our model improves significantly.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model introduces the most spurious objects?",
    "answer": "SOAR-ISG.",
    "rationale": "SOAR-ISG introduces a variety of objects not specified in the layout, such as a bridge, buildings, and people in the first scene, and additional people and structures in the second scene.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.07449v2",
    "pdf_url": null
  },
  {
    "instance_id": "019f7f3f97404afcb938c452defff0b8",
    "figure_id": "1911.03058v2-Figure3-1",
    "image_file": "1911.03058v2-Figure3-1.png",
    "caption": " The Lexicon Induction accuracy generally correlates positively with the GH distance of the source and target language vector spaces to the hub language.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language pair has the highest Lexicon Induction accuracy?",
    "answer": "En-Hi",
    "rationale": "The plot shows the Lexicon Induction accuracy (P@1) for different language pairs. The En-Hi language pair has the highest P@1 value, which indicates that it has the highest Lexicon Induction accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.03058v2",
    "pdf_url": null
  },
  {
    "instance_id": "f9e9637d7d77484097eeb36e57906737",
    "figure_id": "1911.11834v2-Figure4-1",
    "image_file": "1911.11834v2-Figure4-1.png",
    "caption": " Per-attribute improvement of the DOMAININDEPENDENT model over the BASELINE model on the CelebA validation set, as a function of the level of gender imbalance in the attribute. Attributes with high skew (such as “bald”) benefit most significantly.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which attribute benefits the most from the DOMAININDEPENDENT model compared to the BASELINE model?",
    "answer": " Bald",
    "rationale": " The plot shows the improvement of the DOMAININDEPENDENT model over the BASELINE model for different attributes. The attribute \"Bald\" has the highest improvement, as indicated by the green triangle at the top of the plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.11834v2",
    "pdf_url": null
  },
  {
    "instance_id": "c30a739341fa4968834c88316b4909f6",
    "figure_id": "2107.01105v2-Figure1-1",
    "image_file": "2107.01105v2-Figure1-1.png",
    "caption": " LITE enables meta-learners to be trained on large images with one GPU thereby significantly improving performance while retaining their test time computational efficiency. The schematic shows test time efficiency (the number of steps and number of MultiplyAccumulate operations (MACs) needed to learn a new task at test time) and whether the method can be trained on large images (required for good performance). Existing meta-learners are cheap to adapt but trained on small images (multiple GPUs are required for large images), transfer learning methods are expensive to adapt but trainable on large images. Metalearners with LITE get the best of both worlds. Note: SC + LITE is Simple CNAPS [5] trained with LITE.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which meta-learning method requires the fewest steps to adapt to a new task at test time?",
    "answer": "ProtoNets with LITE.",
    "rationale": "The figure shows that ProtoNets with LITE is located furthest to the left on the x-axis, which represents the number of steps to adapt.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.01105v2",
    "pdf_url": null
  },
  {
    "instance_id": "6101549c697543899132daa7d1ae45d6",
    "figure_id": "2306.02338v1-Figure2-1",
    "image_file": "2306.02338v1-Figure2-1.png",
    "caption": " Performance of algorithms for the Amazon Products and Facebook100 datasets.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs best on the Amazon Products dataset?",
    "answer": "Alg. 2",
    "rationale": "The figure shows the performance of four algorithms on the Amazon Products dataset. Alg. 2 has the highest normalized density for all values of the color participation upper bound.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.02338v1",
    "pdf_url": null
  },
  {
    "instance_id": "ddc40f575ebb44ff8694d6e6df3d4492",
    "figure_id": "2108.12961v1-Figure22-1",
    "image_file": "2108.12961v1-Figure22-1.png",
    "caption": " Predictions from ManTraNet and baseline CNN. It is evident that current forensic models are not suitable for the CSTD task.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is more accurate for the CSTD task?",
    "answer": "ManTraNet.",
    "rationale": "The ManTraNet predictions are closer to the ground truth masks than the baseline CNN predictions. This suggests that ManTraNet is more accurate for the CSTD task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.12961v1",
    "pdf_url": null
  },
  {
    "instance_id": "f9dcebc25b714ced990560086c00d8e2",
    "figure_id": "1801.09356v1-Figure6-1",
    "image_file": "1801.09356v1-Figure6-1.png",
    "caption": " Categories sorted by the median location of first guess.",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which category has the highest median location of the first guess?",
    "answer": "Fan.",
    "rationale": "The figure shows the median location of the first guess for each category. The fan category has the highest median location of the first guess.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1801.09356v1",
    "pdf_url": null
  },
  {
    "instance_id": "90f55ed96d0f460a8accd75f23a77b53",
    "figure_id": "2310.12743v2-Figure6-1",
    "image_file": "2310.12743v2-Figure6-1.png",
    "caption": " FID-like validation score training curves. Different colors correspond to different repetitions with both methods, CMF and RNF.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model shows the most consistent performance across repetitions?",
    "answer": "HEMPMASS.",
    "rationale": "The training curves for HEMPMASS are closer together than the training curves for GAS, indicating that the performance of HEMPMASS is more consistent across repetitions.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2310.12743v2",
    "pdf_url": null
  },
  {
    "instance_id": "a37d6fdba5bc4a9a90799c537f2ed00e",
    "figure_id": "2303.03101v1-Figure9-1",
    "image_file": "2303.03101v1-Figure9-1.png",
    "caption": " An illustration of a PCRF structure.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between the query point q and the PCRF?",
    "answer": "The query point q is located inside the PCRF.",
    "rationale": "The figure shows that the query point q is located inside the shaded area, which represents the PCRF.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.03101v1",
    "pdf_url": null
  },
  {
    "instance_id": "304ee98355704f208d6d762baf54d8fa",
    "figure_id": "1909.03194v3-Figure1-1",
    "image_file": "1909.03194v3-Figure1-1.png",
    "caption": " An example of PIT, constructed from a sorted list with three items 3 2 1.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which element in the persistent search tree is the root?",
    "answer": "The element 3.",
    "rationale": "The figure shows that the root of the tree is the element with the largest key, which is 3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.03194v3",
    "pdf_url": null
  },
  {
    "instance_id": "5a13a66bf1bb42e9a7bcd7cda54878c1",
    "figure_id": "2006.04139v2-FigureD.10-1",
    "image_file": "2006.04139v2-FigureD.10-1.png",
    "caption": "Figure D.10. Visual comparison of different SR methods on Manga109 [20] dataset.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which SR method produces the most realistic and detailed results?",
    "answer": "TTSR (Ours)",
    "rationale": "The figure shows the results of different SR methods on two different images from the Manga109 dataset. The TTSR method produces the most realistic and detailed results, as can be seen in the images of the girl's ear and the cat's fur.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04139v2",
    "pdf_url": null
  },
  {
    "instance_id": "cb62279256c048e78bad623e7d8e2d57",
    "figure_id": "2308.08871v2-Figure5-1",
    "image_file": "2308.08871v2-Figure5-1.png",
    "caption": " (a) part of the fixed poses from different individuals in FAUST r; (b) part of the different poses in SCAPE r; (c) shapes in SHREC19 r; (d) 8 categories of humanoid shapes in DT4DH; (e) 5 categories of animals used in training; (f) 3 categories of animals used in test; (g) test animals from TOSCA r.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the datasets shown in the figure contains the most diverse set of shapes?",
    "answer": "DT4DH.",
    "rationale": "The figure shows that DT4DH contains 8 categories of humanoid shapes, while the other datasets contain fewer categories.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.08871v2",
    "pdf_url": null
  },
  {
    "instance_id": "d0bd6628f1cf40f5bdaa13ee830415fc",
    "figure_id": "2210.12367v1-Figure1-1",
    "image_file": "2210.12367v1-Figure1-1.png",
    "caption": " Robust Analysis over four systems on dialogue generation and text summarization.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which system performs the best on the dialogue generation task according to the PPL(Entail.) metric?",
    "answer": "BART.",
    "rationale": "The figure shows the PPL(Entail.) metric for four different systems on the dialogue generation task. BART has the lowest PPL(Entail.) score, which indicates that it performs the best according to this metric.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12367v1",
    "pdf_url": null
  },
  {
    "instance_id": "077941c348984ae8bf12ac967f4c22c4",
    "figure_id": "2003.09541v2-Figure5-1",
    "image_file": "2003.09541v2-Figure5-1.png",
    "caption": " SDEaaS Scalability Study.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which technique achieves the highest throughput when the number of workers is 4 and the number of streams is 500?",
    "answer": "CM",
    "rationale": "In Figure (b), the line representing CM is the highest at the point where the number of workers is 4 and the number of streams is 500.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.09541v2",
    "pdf_url": null
  },
  {
    "instance_id": "e88f08c992a44597a54b1997a3c3a51d",
    "figure_id": "2009.00142v4-Figure5-1",
    "image_file": "2009.00142v4-Figure5-1.png",
    "caption": " K-hop aggregation does necessarily better the discriminatory power. Consider using DE-GNN-1 and DEA-GNN-1-2-hop to learn the structural representation of the nodes colored by black. We choose SPD as DE-1. Both models require at least two layers to distinguish two black nodes. DEA-GNN-1-2-hop cannot decrease the number of layers by a factor of 2.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the minimum number of layers required by DEA-GNN-1-2-hop to distinguish the two black nodes in the figure?",
    "answer": "At least two layers.",
    "rationale": "The figure shows that both DE-GNN-1 and DEA-GNN-1-2-hop require at least two layers to distinguish the two black nodes. This is because the black nodes are only connected to each other through the blue nodes, and the blue nodes are only connected to each other through the light blue nodes. Therefore, the model needs at least two layers to propagate information from one black node to the other.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.00142v4",
    "pdf_url": null
  },
  {
    "instance_id": "e9afdbf2a366454a9483288fc07b93cc",
    "figure_id": "2106.15338v2-Figure2-1",
    "image_file": "2106.15338v2-Figure2-1.png",
    "caption": " Unsupervised key adaptation. Mean IoU vs #clicks with and without key adaptation (KA) on the GrabCut and Berkeley datasets. Probabilistic BoTNets with factored position encodings are evaluated without using KA or using 1 iteration of KA with two different prior precision (Prec.) values of 0.001 or 0.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the GrabCut dataset?",
    "answer": "ProbBoTNet50-FactoredPE-KA1-Prec.0",
    "rationale": "The figure shows that the model ProbBoTNet50-FactoredPE-KA1-Prec.0 has the highest IoU for all number of clicks on the GrabCut dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.15338v2",
    "pdf_url": null
  },
  {
    "instance_id": "afe3cecdd3fd41a6b2ccdacb5cf946b3",
    "figure_id": "2004.07788v1-Figure6-1",
    "image_file": "2004.07788v1-Figure6-1.png",
    "caption": " The structure of our H-GPLVM. Each node Xi produces joint rotations (and translation, if applicable) Yi for the bones with the corresponding colour.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which part of the dog's body is represented by the node X1?",
    "answer": "Tail",
    "rationale": "The figure shows a tree structure with nodes X1 through X9. The node X1 is connected to the node Y1, which is labeled \"Tail.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.07788v1",
    "pdf_url": null
  },
  {
    "instance_id": "d528c016728a491b92d017acd08592b9",
    "figure_id": "1912.05909v1-Figure4-1",
    "image_file": "1912.05909v1-Figure4-1.png",
    "caption": " The cumulative distribution functions (CDF) of the RMSE re-projection errors (horizontal axis) of the estimated homographies on datasets EVD and homogr. Being accurate is interpreted by a curve close to the top.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performs the best on the EVD dataset?",
    "answer": "MAGSAC++",
    "rationale": "The curve for MAGSAC++ is closest to the top on the left plot, which represents the EVD dataset. This indicates that MAGSAC++ has the highest probability of achieving a low RMSE error on this dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1912.05909v1",
    "pdf_url": null
  },
  {
    "instance_id": "b782510208754814a8983a1812f209c8",
    "figure_id": "1909.12051v2-Figure1-1",
    "image_file": "1909.12051v2-Figure1-1.png",
    "caption": " Incremental learning dynamics in deep models. Each panel shows the evolution of the five largest values of σ, the parameters of the induced model. All models were trained using gradient descent with a small initialization and learning rate, on a small training set such that there are multiple possible solutions. In all cases, the deep parameterization of the models lead to “incremental learning”, where the values are learned at different rates (larger values are learned first), leading to sparse solutions. (a) Depth 4 matrix sensing, σ denotes singular values (see section 4.1). (b) Quadratic networks, σ denotes singular values (see section 4.2). (c) Depth 3 diagonal networks, σ denotes feature weights (see section 4.3). (d) Depth 3 circular-convolutional networks, σ denotes amplitudes in the frequency domain of the feature weights (see appendix G).",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which type of deep model exhibits the most pronounced incremental learning behavior?",
    "answer": " Convolutional nets.",
    "rationale": " The figure shows the evolution of the five largest values of σ, the parameters of the induced model, for different types of deep models. The convolutional nets (panel d) show the most pronounced incremental learning behavior, as the values of σ are learned at very different rates, with the largest values being learned much faster than the smaller values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.12051v2",
    "pdf_url": null
  },
  {
    "instance_id": "c76f007eb85842bab4b52f091c32550a",
    "figure_id": "1904.09460v1-Figure5-1",
    "image_file": "1904.09460v1-Figure5-1.png",
    "caption": " Neuron proportion for scales in each SA block of ScaleNets on CIFAR-100 and ImageNet.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which scale contributes the most to the total number of neurons in the last SA block of ScaleNet-50 on ImageNet?",
    "answer": "Scale 2",
    "rationale": "The figure shows the proportion of neurons for each scale in each SA block of ScaleNets on CIFAR-100 and ImageNet. In the last SA block of ScaleNet-50 on ImageNet, the bar for scale 2 is the tallest, indicating that it has the highest proportion of neurons.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.09460v1",
    "pdf_url": null
  },
  {
    "instance_id": "8614fa6d01294a8299c62c6611507b68",
    "figure_id": "2211.05756v1-Figure2-1",
    "image_file": "2211.05756v1-Figure2-1.png",
    "caption": " Language vs. hours of training data for 70 languages.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which language has the most training data?",
    "answer": "English (en)",
    "rationale": "The bar for English (en) is the tallest, which indicates that it has the most training data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.05756v1",
    "pdf_url": null
  },
  {
    "instance_id": "cc19b6e89d7b4919956bc6ba7168a922",
    "figure_id": "2201.09871v2-Figure11-1",
    "image_file": "2201.09871v2-Figure11-1.png",
    "caption": " The wall-clock time of each metric as datasets scale in a single dimension. Activations is the time to extract graph embeddings from GIN on a GPU.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric takes the least amount of time to compute as the number of samples increases?",
    "answer": "Activations.",
    "rationale": "The figure shows that the Activations line is consistently below all other lines, indicating that it takes the least amount of time to compute.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.09871v2",
    "pdf_url": null
  },
  {
    "instance_id": "a5466b4ee9f54fa7a2352a52c7e4c2d4",
    "figure_id": "1911.08142v2-Figure6-1",
    "image_file": "1911.08142v2-Figure6-1.png",
    "caption": " Visual comparison of point cloud part segmentation with the state-of-the-art unsupervised method MAP-VAE. We achieve more accurate segmentation even in tiny parts and transition regions.",
    "figure_type": "Plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces more accurate segmentation results, MAP-VAE or GraphTER?",
    "answer": "GraphTER.",
    "rationale": "The caption states that GraphTER achieves more accurate segmentation, even in tiny parts and transition regions. This is evident in the figure, where the GraphTER results show more distinct and accurate segmentation of the airplane and chair point clouds compared to the MAP-VAE results.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.08142v2",
    "pdf_url": null
  },
  {
    "instance_id": "9bb47b37e1f94ad19803f2e0fcd6a326",
    "figure_id": "2112.07499v1-Figure12-1",
    "image_file": "2112.07499v1-Figure12-1.png",
    "caption": "Figure 12 A circular-arc graph with d(s, t) = 4",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the distance between s and t in the circular-arc graph?",
    "answer": "4",
    "rationale": "The distance between two vertices in a circular-arc graph is defined as the minimum number of arcs that need to be traversed to get from one vertex to the other. In this case, we can see that there are four arcs between s and t.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.07499v1",
    "pdf_url": null
  },
  {
    "instance_id": "66a63d281ca64267967c7e5e1ecd83af",
    "figure_id": "2208.02656v2-Figure4-1",
    "image_file": "2208.02656v2-Figure4-1.png",
    "caption": " Representation results for our Bernoulli entropy model (BinaryBernoulli), our joint density estimation model (BinaryMI), an adversarial classifier (AdvCls) and a fair variational autoencoder (VFAE). The dotted line represent the line of equivalent fairness/accuracy tradeoffs and goes through the model closest (under the L1 norm) to (1, 1). A Random Forest (RF) classifier is trained on the extracted representations T `. In the top row, model AUC is compared with RF 1-ADRG, i.e. the absolute distance to random guess. This is computed by subtracting the RF accuracy to the majority class ratio in the dataset and taking the absolute value. In the bottom row, we compare the model AUC and the RF AUC.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the best fairness-accuracy trade-off on the COMPAS dataset?",
    "answer": "BinaryMI",
    "rationale": "The dotted line in the figure represents the line of equivalent fairness/accuracy trade-offs. The model closest to the point (1, 1) on this line achieves the best trade-off. In the figure for the COMPAS dataset, BinaryMI is the closest to this point.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2208.02656v2",
    "pdf_url": null
  },
  {
    "instance_id": "5cea19cd87f342d2badf54ecee0055d4",
    "figure_id": "2306.07707v1-Figure3-1",
    "image_file": "2306.07707v1-Figure3-1.png",
    "caption": " Three networks with four agents where (b) and (c) can be obtained by one of the agents in (a) hiding her out-edge. The probabilities of each agent being chosen by a generic IC 2-selection mechanism are attached beside the node.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "In which of the three networks is the probability of agent 1 being chosen by a generic IC 2-selection mechanism the highest?",
    "answer": "Network (a).",
    "rationale": "In network (a), agent 1 has two incoming edges, while in networks (b) and (c), agent 1 only has one incoming edge. This means that agent 1 is more likely to be chosen in network (a) than in networks (b) or (c).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.07707v1",
    "pdf_url": null
  },
  {
    "instance_id": "d59ca0d3cccd46c691cf94655b177f1a",
    "figure_id": "2110.08557v2-Figure5-1",
    "image_file": "2110.08557v2-Figure5-1.png",
    "caption": " Analysis of the generated cells on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activation function is the most popular among the generated cells?",
    "answer": "ReLU",
    "rationale": "Figure (a) shows the proportion of different activation functions used in the generated cells. ReLU has the highest proportion (40.5%), indicating that it is the most popular activation function.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.08557v2",
    "pdf_url": null
  },
  {
    "instance_id": "56b23139c6304b19abf2fedd1e30b504",
    "figure_id": "2203.01517v1-Figure5-1",
    "image_file": "2203.01517v1-Figure5-1.png",
    "caption": " Model alignment loss (a) and mutual information (b, c) after training with ERM, Jtt, and CnC. CnC most effectively reduces spurious attribute dependence, and obtains smaller gaps for per-class worst-group versus average error (d), as supported by Theorem 3.1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method is most effective in reducing spurious attribute dependence?",
    "answer": "CnC",
    "rationale": "This can be seen in sub-figure (c), which shows that CnC has the lowest spurious attribute MI for both the Waterbirds and CelebA datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.01517v1",
    "pdf_url": null
  },
  {
    "instance_id": "4a6d0c0288ee473f84b07c9e401637f6",
    "figure_id": "2109.02986v3-Figure1-1",
    "image_file": "2109.02986v3-Figure1-1.png",
    "caption": " A graphical causal model which reveals a generative process of the data which contains instance-dependent label noise, where the shaded variables are observable and the unshaded variables are latent.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between variables Z and X?",
    "answer": "Z is a direct cause of X.",
    "rationale": "The arrow from Z to X in the causal model indicates that Z has a direct causal influence on X.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.02986v3",
    "pdf_url": null
  },
  {
    "instance_id": "162690526aca48e3baf69264857dc480",
    "figure_id": "2302.04542v1-Figure2-1",
    "image_file": "2302.04542v1-Figure2-1.png",
    "caption": " Left and right: Additional empirical memory consumption and running time comparison for different attention mechanisms under various sequence lengths.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attention mechanism has the lowest memory consumption and running time for all sequence lengths?",
    "answer": "FlashAttention.",
    "rationale": "The figure shows the memory consumption and running time of different attention mechanisms for different sequence lengths. FlashAttention has the lowest memory consumption and running time for all sequence lengths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2302.04542v1",
    "pdf_url": null
  },
  {
    "instance_id": "799196bd33ac4f12ba4a0eece0af867d",
    "figure_id": "2205.15580v3-Figure3-1",
    "image_file": "2205.15580v3-Figure3-1.png",
    "caption": " Classification task on MNIST",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the fastest convergence rate for the MNIST classification task?",
    "answer": "baGMM-NP-PAGE with a step size of 0.5.",
    "rationale": "The figure shows the convergence rate of different algorithms for the MNIST classification task. The x-axis shows the number of communication rounds, and the y-axis shows the number of nodes participating. The algorithm with the fastest convergence rate is the one that reaches the lowest number of nodes participating in the fewest communication rounds. In this case, it is baGMM-NP-PAGE with a step size of 0.5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.15580v3",
    "pdf_url": null
  },
  {
    "instance_id": "b1eaf250677746e6a3e410b20c075e9c",
    "figure_id": "2309.02020v1-Figure8-1",
    "image_file": "2309.02020v1-Figure8-1.png",
    "caption": " The quantitative results for our RawHDR model trained on sRGB data and Raw data.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the images is the ground truth image?",
    "answer": "The image labeled \"GT\" is the ground truth image.",
    "rationale": "The ground truth image is the reference image that the other images are compared to. In this case, the ground truth image is the one that is labeled \"GT.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.02020v1",
    "pdf_url": null
  },
  {
    "instance_id": "45abb79e9c4e44b1bbdd2c53fb3b21d9",
    "figure_id": "1904.11622v3-Figure5-1",
    "image_file": "1904.11622v3-Figure5-1.png",
    "caption": " A subset of visual relationships with different levels of complexity as defined by spatial and categorical subtypes. In Section 5.3, we show how this measure is a good indicator of our semi-supervised method’s effectiveness compared to baselines like transfer learning.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which verb has the most spatial subtypes?",
    "answer": "The verb \"look\" has the most spatial subtypes.",
    "rationale": "The bar for \"look\" is the highest in the \"Spatial Subtypes\" plot.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.11622v3",
    "pdf_url": null
  },
  {
    "instance_id": "1ce7cda3fcec4911864b16f657391c50",
    "figure_id": "2201.01251v3-Figure5-1",
    "image_file": "2201.01251v3-Figure5-1.png",
    "caption": " Average episode score throughout training for all ablations on Inhumane. Shaded areas indicate one standard deviation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best according to the average episode score?",
    "answer": "The XTX (full model) performs the best.",
    "rationale": "The XTX (full model) line is consistently higher than the other lines, indicating that it has a higher average episode score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2201.01251v3",
    "pdf_url": null
  },
  {
    "instance_id": "92082d32798b4fc9ae4303604d090c1a",
    "figure_id": "2005.05751v1-Figure6-1",
    "image_file": "2005.05751v1-Figure6-1.png",
    "caption": " Joint embedding of style codes parameters extracted from 3Dmotions as well as directly from 2D videos.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which emotion seems to be the most similar to \"strutting\"?",
    "answer": "Proud.",
    "rationale": "The clusters for \"proud\" and \"strutting\" are very close together in the embedding space, which suggests that they are similar in terms of the style codes that are extracted from the 3D motions and 2D videos.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.05751v1",
    "pdf_url": null
  },
  {
    "instance_id": "03775259f8c5488f94d8cc2c1aa5ec5f",
    "figure_id": "1911.11177v1-Figure7-1",
    "image_file": "1911.11177v1-Figure7-1.png",
    "caption": " Compressing EfficientNet architectures to the size of B0 (5M). Bars are grouped by the architecture being compressed. We compare compressing all layers (blue), with hashing only convolutional layers (orange). There is a small difference - the method isn’t sensitive to this design choice. Note that smaller models benefit from maintaining the fully connected layers untouched, while the larger base models gain from the flexibility to compress all layers.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which base model benefits the most from compressing all layers instead of just the convolutional layers?",
    "answer": "B5",
    "rationale": "The figure shows that the difference in accuracy between compressing all layers and just the convolutional layers is largest for B5.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.11177v1",
    "pdf_url": null
  },
  {
    "instance_id": "84311bd5977a459ab9cff896b9d67348",
    "figure_id": "1909.06168v1-Figure5-1",
    "image_file": "1909.06168v1-Figure5-1.png",
    "caption": " Solution Cost Comparison of PFD and the competing algorithms varying number of agents (scale-free graphs)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest solution cost for 20 agents?",
    "answer": "PFD",
    "rationale": "The figure shows the solution cost of different algorithms for different numbers of agents. For 20 agents, the PFD line is higher than the other lines, indicating that it has the highest solution cost.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.06168v1",
    "pdf_url": null
  },
  {
    "instance_id": "5b0613bbf7df4192b429af2c7fb0d3ff",
    "figure_id": "1901.00063v2-Figure9-1",
    "image_file": "1901.00063v2-Figure9-1.png",
    "caption": " Matterport qualitative results. From top to bottom: ground-truth color and scene geometry, our pose estimation results (two input scans in red and green), baseline results (4PCS, DL, GReg and CGReg), ground-truth scene RGBDNS and completed scene RGBDNS for two input scans. The unobserved regions are dimmed.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the baselines performs the best in terms of accurately estimating the pose of the two input scans?",
    "answer": "GReg.",
    "rationale": "The figure shows the pose estimation results for each of the baselines, as well as the ground-truth pose. GReg's results are the closest to the ground truth, indicating that it performs the best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1901.00063v2",
    "pdf_url": null
  },
  {
    "instance_id": "7f2db2ec24814fe0aecb2298ba7a99aa",
    "figure_id": "2111.05685v1-Figure3-1",
    "image_file": "2111.05685v1-Figure3-1.png",
    "caption": " Top-1 Validation Accuracy and Train-cost Savings on MobileNetV1 on ImageNet. Epochwise Train-cost and Variance Comparison on VGG-19 on CIFAR-10.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves higher accuracy with lower FLOPs on MobileNetV1?",
    "answer": "GrowEfficient",
    "rationale": "The top left plot shows that GrowEfficient (green dashed line) achieves higher accuracy than Ours (red solid line) for the same number of FLOPs.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.05685v1",
    "pdf_url": null
  },
  {
    "instance_id": "680e1b0a70f940608a897fbdc18b25ee",
    "figure_id": "1911.03584v2-Figure2-1",
    "image_file": "1911.03584v2-Figure2-1.png",
    "caption": " Test accuracy on CIFAR-10.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model achieves the highest test accuracy on CIFAR-10?",
    "answer": "ResNet18.",
    "rationale": "The plot shows the test accuracy of four models on the CIFAR-10 dataset. The ResNet18 model has the highest test accuracy, which is approximately 0.95.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1911.03584v2",
    "pdf_url": null
  },
  {
    "instance_id": "74e52dee3545443aaa9c257039e7a9eb",
    "figure_id": "1902.04698v4-Figure4-1",
    "image_file": "1902.04698v4-Figure4-1.png",
    "caption": " Predictions of CNNs on test examples at di erent angles to the training image. The horizontal axis shows the train-test correlation, while the vertical axis indicate the number of hidden layers for the CNNs being evaluated. The heatmap shows the similarity (measured in correlation) between the model prediction and the reference function (the constant or the identity function).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of function has a higher correlation with the predictions of CNNs with more hidden layers?",
    "answer": "The identity function.",
    "rationale": "The heatmap on the right shows that the correlation between the predictions of CNNs and the identity function increases as the number of hidden layers increases. In contrast, the heatmap on the left shows that the correlation between the predictions of CNNs and the constant function decreases as the number of hidden layers increases.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.04698v4",
    "pdf_url": null
  },
  {
    "instance_id": "47207cd2aace426f897d62e944c87504",
    "figure_id": "1809.00898v1-Figure5-1",
    "image_file": "1809.00898v1-Figure5-1.png",
    "caption": " Example of a graph allowing empty positions",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Can the node labeled \"S\" reach the node labeled \"T\" by traversing the graph without visiting any empty nodes?",
    "answer": "Yes",
    "rationale": "The node labeled \"S\" can reach the node labeled \"T\" by traversing the graph through the path S->1->T. This path does not include any empty nodes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1809.00898v1",
    "pdf_url": null
  },
  {
    "instance_id": "6c1e013107bf4c1f82dd80a98c0c72a5",
    "figure_id": "2107.01396v1-Figure3-1",
    "image_file": "2107.01396v1-Figure3-1.png",
    "caption": " We compare perturbation imperceptibility of our Demiguise-C&W with C&W-PSNR and C&W-SSIM. We find that while all three attacks achieve 100% fooling rate, only DemiguiseC&W crafted adversarial examples are able to truly maintain perturbation imperceptibility.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three attacks, C&W-PSNR, C&W-SSIM, or Demiguise-C&W, is able to truly maintain perturbation imperceptibility?",
    "answer": "Demiguise-C&W",
    "rationale": "The caption states that \"only Demiguise-C&W crafted adversarial examples are able to truly maintain perturbation imperceptibility.\"",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2107.01396v1",
    "pdf_url": null
  },
  {
    "instance_id": "0fcc19ec3fec4725a0ce97bdd62c5f40",
    "figure_id": "2106.07135v2-Figure7-1",
    "image_file": "2106.07135v2-Figure7-1.png",
    "caption": " Initialization Comparison on HT. The multiresolution factorization in MTC outperforms other initialization methods. With the multiresolution factorization module, the 5 Jacobi iterations in stage 1 become very powerful, and then the stage 2 Cholesky solver continues to provide a stable and accurate estimation.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which initialization method performed the best for the HT dataset?",
    "answer": "MTC",
    "rationale": "The MTC method is shown to have the highest accuracy after the first 5 iterations, and it continues to perform well in the subsequent iterations.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.07135v2",
    "pdf_url": null
  },
  {
    "instance_id": "de6b4a74cda94501b8bf456819d03372",
    "figure_id": "1902.04187v1-Figure3-1",
    "image_file": "1902.04187v1-Figure3-1.png",
    "caption": " Average depth of top nodes as the number of the selected top nodes varies.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model generally results in the lowest average depth of top nodes?",
    "answer": "BERT",
    "rationale": "The figure shows that the BERT line is generally below the other lines, indicating that it has the lowest average depth of top nodes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.04187v1",
    "pdf_url": null
  },
  {
    "instance_id": "681496b3d862497cb80ce3b96732752b",
    "figure_id": "2110.02488v2-Figure1-1",
    "image_file": "2110.02488v2-Figure1-1.png",
    "caption": " Sequence-to-sequence decoding speed (top) and memory consumption (bottom) varying sequence lengths. Greedy decoding is used, with batch size 16.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which decoding method is the fastest for all sequence lengths?",
    "answer": "RFA",
    "rationale": "The figure shows the decoding speed for different decoding methods as a function of sequence length. RFA is the highest line on the plot, which means it has the highest decoding speed for all sequence lengths.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.02488v2",
    "pdf_url": null
  },
  {
    "instance_id": "ba514714a617459f9a2edaeb3b2816af",
    "figure_id": "2101.05930v2-Figure12-1",
    "image_file": "2101.05930v2-Figure12-1.png",
    "caption": " The attention maps derived by 5 different attention functions are shown for (a) BadNet, (b) Finetuned BadNet by 5% clean training data, and (c) BadNets erased by our NAD.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attention function is the most effective at erasing the trigger sample?",
    "answer": "A4sum",
    "rationale": "The attention maps for the A4sum function in (c) show the least amount of attention on the trigger sample, indicating that it is the most effective at erasing it.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2101.05930v2",
    "pdf_url": null
  },
  {
    "instance_id": "2a8abf12500d4bc088aa6e7a662e207b",
    "figure_id": "2009.01072v1-Figure3-1",
    "image_file": "2009.01072v1-Figure3-1.png",
    "caption": " Illustration of model structure",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which observed feature nodes are most likely to be associated with hidden class node 2?",
    "answer": "Observed feature nodes 4 and 5.",
    "rationale": "The figure shows that there are directed arrows from observed feature nodes 4 and 5 to hidden class node 2. This indicates that these observed feature nodes are likely to be associated with hidden class node 2.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.01072v1",
    "pdf_url": null
  },
  {
    "instance_id": "c0014ebcacbc4c968741a4a9ae99c8a0",
    "figure_id": "1905.08622v3-Figure8-1",
    "image_file": "1905.08622v3-Figure8-1.png",
    "caption": " Image generation conditioning on long encyclopedia documents using VHE-raster-scanGAN trained on (a) CUB-E and (b) Flower. Shown in the top part of each subplot are representative sentences taken from the long document that describes an unseen class; for the three rows of images shown in the bottom part, the first row includes three real images from the corresponding unseen class, and the other two rows include a total of six randomly generated images conditioning on the long encyclopedia document of the corresponding unseen class.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the two classes, Ball Moss or Barberton Daisy, is more likely to hinder tree growth?",
    "answer": "Ball Moss is more likely to hinder tree growth.",
    "rationale": "The passage states that Ball Moss can hinder tree growth, while there is no mention of Barberton Daisy hindering tree growth.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1905.08622v3",
    "pdf_url": null
  },
  {
    "instance_id": "350ca40158dc4c8793589a599049ce21",
    "figure_id": "2006.04492v2-Figure8-1",
    "image_file": "2006.04492v2-Figure8-1.png",
    "caption": " Rank correlation performance of our TSE (yellow), the sum over training accuracy, SoTAcc (blue), the sum over validation losses, SoVL (pink), the sum of validation accuracy, SoVAcc (green) as well as their summing over the recent E epoch counterparts (dash dot) for 5000 random architectures in NASBench-201 on three image datasets.",
    "figure_type": "** Plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " Which method has the highest rank correlation performance on the CIFAR10 dataset?",
    "answer": " TSE",
    "rationale": " The plot in Figure (a) shows that the yellow line, which represents TSE, is consistently higher than the other lines throughout the training process.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.04492v2",
    "pdf_url": null
  },
  {
    "instance_id": "c92509b7ddc04aa6992e1115cb0f353d",
    "figure_id": "2012.02258v1-Figure3-1",
    "image_file": "2012.02258v1-Figure3-1.png",
    "caption": " LSMerkle tree sample state and example operations.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What happens to the WedgeChain Buffer when a block is added?",
    "answer": "The WedgeChain Buffer stores the new block.",
    "rationale": "In Figure (b), we see that the block \"x4, w3\" is added to the WedgeChain Buffer.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.02258v1",
    "pdf_url": null
  },
  {
    "instance_id": "3b62e7822e2447ba859e16b67ccaeccb",
    "figure_id": "2002.07017v2-Figure3-1",
    "image_file": "2002.07017v2-Figure3-1.png",
    "caption": " Left: mean average precision (mAP) of the classifier trained on different multi-view representations for the MIR-Flickr task. Right: comparing the performance for different values of β and percentages of given labeled examples (from 1% up to 100%). Each model uses encoders of comparable size, producing a 1024 dimensional representation. † results from Wang et al. (2016).",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method achieves the highest mAP score when trained on all labeled examples?",
    "answer": "MV-InfoMax.",
    "rationale": "The right panel of the figure shows that MV-InfoMax has the highest mAP score of all the methods shown when trained on 100% of the labeled examples.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.07017v2",
    "pdf_url": null
  },
  {
    "instance_id": "6cf4da5815a747a5b6ffd0ddd7406c5a",
    "figure_id": "2109.04404v1-Figure10-1",
    "image_file": "2109.04404v1-Figure10-1.png",
    "caption": " Average correlation (Spearman’s ρ) with human judgements on each word similarity dataset, with and without postprocessing for GPT-2",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which post-processing technique is most effective in improving the correlation between GPT-2 and human judgments on the simverb-3500 dataset?",
    "answer": "Standardization",
    "rationale": "The figure shows that the standardized curve is the highest of all the post-processing techniques for the simverb-3500 dataset.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.04404v1",
    "pdf_url": null
  },
  {
    "instance_id": "33a312b2cbdc4c6f9590eface6c07f29",
    "figure_id": "1812.00020v2-Figure4-1",
    "image_file": "1812.00020v2-Figure4-1.png",
    "caption": " (a) With appropriate method like Quadriflow, we can get the surface parameterization aligning to shape features with negligible distortions. (b) Harmonic parameterizations leads to high distortion in the scale. (c) Geometry images [15] result in high distortion in the orientation.",
    "figure_type": "Schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which parameterization method is the best for preserving shape features with minimal distortion?",
    "answer": "QuadriFlow parameterization.",
    "rationale": "The figure shows that QuadriFlow parameterization (a) aligns well with the shape features of the camel head, while harmonic parameterization (b) and geometry images (c) introduce significant distortions in scale and orientation, respectively.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.00020v2",
    "pdf_url": null
  },
  {
    "instance_id": "6d37b3e6281e4fbe94802f24bfda45d4",
    "figure_id": "1903.12626v1-Figure3-1",
    "image_file": "1903.12626v1-Figure3-1.png",
    "caption": " The distributions of confidence scores of positive examples from four seen classes of DBpedia in Phase 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four classes of DBpedia in Phase 1 has the highest median confidence score?",
    "answer": "Artist",
    "rationale": "The box plot for \"Artist\" has the highest median line, which represents the median confidence score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1903.12626v1",
    "pdf_url": null
  },
  {
    "instance_id": "2b69c5c4e2674e0480c279f3dfc209d8",
    "figure_id": "2306.02867v1-Figure1-1",
    "image_file": "2306.02867v1-Figure1-1.png",
    "caption": " The more effective the finetuning procedure is, the less difference there is between SPLADE middle trained models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best when there is no distillation and only one negative sample is used?",
    "answer": "BERT MS CLS",
    "rationale": "The figure shows that the BERT MS CLS model has the highest MRR@10 score when there is no distillation and only one negative sample is used.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.02867v1",
    "pdf_url": null
  },
  {
    "instance_id": "ceef9e96e9874be2b320498608e9ef28",
    "figure_id": "1909.13404v1-Figure8-1",
    "image_file": "1909.13404v1-Figure8-1.png",
    "caption": " Results for the architectures sampled in the search space experiments. Left: Relation between number of parameters and validation accuracy at 25 epochs. Right: Relation between time to complete 25 epochs of training and validation accuracy.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which search space had the best trade-off between accuracy and training time?",
    "answer": "Nasnet.",
    "rationale": "The Nasnet search space achieved the highest accuracy for a given training time. This can be seen in the right-hand plot, where the Nasnet points are clustered towards the top right corner.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1909.13404v1",
    "pdf_url": null
  },
  {
    "instance_id": "7ad0677387b541329861202a88f101f4",
    "figure_id": "2012.07988v1-Figure5-1",
    "image_file": "2012.07988v1-Figure5-1.png",
    "caption": " Difference detection performances with different ensemble sizes: I = J ∈ {1,3,5,7}.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best for all ensemble sizes?",
    "answer": "Skip-GANomaly",
    "rationale": "The figure shows the AUROC for different anomaly detection methods with different ensemble sizes. The Skip-GANomaly method has the highest AUROC for all ensemble sizes, indicating that it performs best.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.07988v1",
    "pdf_url": null
  },
  {
    "instance_id": "5fb581ba681d4bf781128b8e32ca84bb",
    "figure_id": "2210.09789v3-Figure2-1",
    "image_file": "2210.09789v3-Figure2-1.png",
    "caption": " The test accuracy with respect to the number of layers on all the graph benchmark datasets. From the top left to the bottom, we show: PubMed, Coauthor CS, Coauthor Physics, Amazon Computers, and Amazon Photo. The accuracy is averaged over 5 random train/validation/test splits and 5 random weight initialization of the best configuration per split.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs the best on the Coauthor CS dataset?",
    "answer": "Our(GCN)",
    "rationale": "The plot for the Coauthor CS dataset shows that the Our(GCN) model has the highest accuracy for all numbers of layers.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.09789v3",
    "pdf_url": null
  },
  {
    "instance_id": "63d1925a1854491a97e861e6f29e4d4b",
    "figure_id": "2306.10563v1-Figure8-1",
    "image_file": "2306.10563v1-Figure8-1.png",
    "caption": " Confusion matrix of viseme-phoneme mapping in (a) Online Balanced Clustering, (b) Online Balanced Clustering + AMIE (without adversarial learning) and (c) Online Balanced Clustering + AMIE.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method results in the most accurate viseme-phoneme mapping?",
    "answer": "Online Balanced Clustering + AMIE.",
    "rationale": "The confusion matrix in (c) shows the most diagonal structure, indicating that the predicted visemes are more likely to match the actual phonemes.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.10563v1",
    "pdf_url": null
  },
  {
    "instance_id": "9112b0b0895242e1bcbe495c577b6297",
    "figure_id": "2012.02374v1-Figure9-1",
    "image_file": "2012.02374v1-Figure9-1.png",
    "caption": " Comparing performance of D-NetPAD in Experiment-1 and Experiment-3 to evaluate the efficacy of the proposed method, CIT-GAN, in generating synthetic PA samples that represent the real PA distribution across various PA domains.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which experiment shows better performance in terms of true detection rate for a given false detection rate?",
    "answer": "Experiment-1",
    "rationale": "The figure shows the performance of D-NetPAD in Experiment-1 and Experiment-3. The true detection rate is plotted against the false detection rate. For a given false detection rate, the true detection rate for Experiment-1 is higher than that of Experiment-3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.02374v1",
    "pdf_url": null
  },
  {
    "instance_id": "191ff2f1dbff419fa73667eb1b69133a",
    "figure_id": "2110.08173v3-Figure8-1",
    "image_file": "2110.08173v3-Figure8-1.png",
    "caption": " Confusion matrices of expert annotated scores versus the extracted UMLS answers. Five annotation score levels: 5-Perfectly answer the query; 4- Similar to the gold answer, could somehow be the answer; 3-Related to the query but not correct; 2-Same domain or slight relation; 1-Completely unrelated.",
    "figure_type": "** \n\nTable",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " \n\nWhat percentage of UMLS answers were considered \"Perfectly answer the query\" by human experts? ",
    "answer": " \n\n26.67% ",
    "rationale": " \n\nThe top left cell of the \"Top 1\" confusion matrix shows that 4 out of 15 UMLS answers were given a score of 5 by human experts, which corresponds to \"Perfectly answer the query.\" 4/15 = 0.2667 or 26.67%. ",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.08173v3",
    "pdf_url": null
  },
  {
    "instance_id": "650e38e5ccd24e05afa19b8951520e90",
    "figure_id": "2205.00904v3-Figure3-1",
    "image_file": "2205.00904v3-Figure3-1.png",
    "caption": " Sensitivity analysis of PUDA w.r.t. different settings of positive class prior πp on FB15k-237 dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which metric is most sensitive to changes in the positive class prior πp?",
    "answer": "Hit@10",
    "rationale": "The plot shows that Hit@10 has the largest change in value as πp varies.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.00904v3",
    "pdf_url": null
  },
  {
    "instance_id": "fab164cbab1b40ae9d7fe3ce5e1f5d8c",
    "figure_id": "1811.09845v3-Figure16-1",
    "image_file": "1811.09845v3-Figure16-1.png",
    "caption": " When GeNeVA-GAN is provided with an initial image different from the background image used during training, it still adds the desired object with the right properties at the correct location. The model was not trained in this setting and the success of this experiment demonstrates that it has learnt to preserve the existing canvas, understand the existing objects, and add new objects with the correct relationships to existing objects.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following objects was added to the scene in image (b)?",
    "answer": "A cyan cube.",
    "rationale": "The left image in (b) shows the original scene, and the right image shows the scene after the cyan cube has been added.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1811.09845v3",
    "pdf_url": null
  },
  {
    "instance_id": "4dd65eb959a746c1b926f36e38aa963c",
    "figure_id": "1904.02870v1-Figure9-1",
    "image_file": "1904.02870v1-Figure9-1.png",
    "caption": " Visual comparisons of the super-resolution results for video Fan on ×4 upscaling factor.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which super-resolution algorithm produced the highest PSNR value?",
    "answer": "FSTRN",
    "rationale": "The PSNR value is shown below each image. FSTRN has the highest PSNR value of 35.08.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.02870v1",
    "pdf_url": null
  },
  {
    "instance_id": "a0965cfd755f41799c3363db58e14265",
    "figure_id": "2303.07677v2-Figure2-1",
    "image_file": "2303.07677v2-Figure2-1.png",
    "caption": " Visualization of Grad-CAM and Guided-Backpropagation for ResNet50 on ImageNet. (c, f): visualization of pre-trained model. (d, g): visualization of SR-init on the lowest accuracy drop model. (e, h): visualization of SR-init on the highest accuracy drop model. The top-1 accuracy of different models is marked below the visual results.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method, Grad-CAM or Guided-Backpropagation, provides a more detailed and accurate visualization of the features that the model is using to classify the images?",
    "answer": "Grad-CAM.",
    "rationale": "Grad-CAM produces heatmaps that highlight the regions of the image that are most important for the model's classification decision. These heatmaps are more detailed and accurate than the visualizations produced by Guided-Backpropagation, which tend to be more blurry and less informative.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.07677v2",
    "pdf_url": null
  },
  {
    "instance_id": "c014bfd7350541699c8b24b89fd31cd8",
    "figure_id": "2204.08396v1-Figure5-1",
    "image_file": "2204.08396v1-Figure5-1.png",
    "caption": " Comparison of MoE-based Transformers with different numbers of experts. Lower perplexity indicates better performance.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three MoE-based Transformers performs the best when the number of experts is 32?",
    "answer": "StableMoE",
    "rationale": "The figure shows that StableMoE has the lowest perplexity when the number of experts is 32. Lower perplexity indicates better performance.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2204.08396v1",
    "pdf_url": null
  },
  {
    "instance_id": "890cd64bf54146b3a49c6251e06cb934",
    "figure_id": "2305.16391v2-Figure4-1",
    "image_file": "2305.16391v2-Figure4-1.png",
    "caption": " Pilot misspecifications lead to inconsistent model performance in Opt-Sampling.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which pilot specification leads to the most inconsistent model performance in Opt-Sampling?",
    "answer": "W&D",
    "rationale": "The box plot for W&D in Opt-Sampling has the largest range, indicating that the model performance is more variable for this pilot specification than for the others.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.16391v2",
    "pdf_url": null
  },
  {
    "instance_id": "7fbc795bbe6d4e9385e2133a7fd16644",
    "figure_id": "2209.15162v3-Figure13-1",
    "image_file": "2209.15162v3-Figure13-1.png",
    "caption": " F1 of image encoder probes trained on CC3M and evaluated on COCO. We find that F1 of captions by object category tend to follow those of probe performance. Notably the BEIT probe is much worse at transferring from CC3M to COCO, and the captioning F1 tends to be consistently higher which makes it difficult to draw conclusions for this model. Generally, it appears the ability to encode lexical information into the image representation entails being able to transfer that information to the LM with a linear map.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the best probe performance on the COCO dataset?",
    "answer": "CLIP",
    "rationale": "The figure shows the F1 score of image encoder probes trained on CC3M and evaluated on COCO. CLIP has the highest F1 score for both probe and caption.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.15162v3",
    "pdf_url": null
  },
  {
    "instance_id": "5452181664d64d5cb93a45258c50f8f9",
    "figure_id": "1902.03045v1-Figure2-1",
    "image_file": "1902.03045v1-Figure2-1.png",
    "caption": " Classification performance on controlled UCI datasets with p ranging from 0.1 to 0.7 (r = 1).",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm performed the best on the glass dataset?",
    "answer": "PALOC",
    "rationale": "The figure shows the classification accuracy of different algorithms on the glass dataset for different values of p. PALOC has the highest accuracy for all values of p.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1902.03045v1",
    "pdf_url": null
  },
  {
    "instance_id": "c0d667561f7641138251c41ec82b7eba",
    "figure_id": "2112.11701v3-Figure7-1",
    "image_file": "2112.11701v3-Figure7-1.png",
    "caption": " Performance comparison and ablation tests: Average episode rewards over 400 timestep (1 min) trajectories for different methods, with standard error over 5 different random seeds, paired with the proxy human HProxy. The hashed bars with the slash (/) show results with the starting position of the agents switched. Figure (a) shows the performance comparison among MEP, SP and PBT. Figure (b) shows the ablation tests, where we use MEPα=0 and MEPβ=0 to denote the MEP model without the population entropy reward and without the prioritized sampling mechanism, respectively. Figure (c) shows the performance comparison with Maximum Population Diversity (MPD) with or without clipping the importance weights that are greater than 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in the Counter Circle scenario?",
    "answer": "PBT + H_proxy",
    "rationale": "In Figure (a), the bar for PBT + H_proxy in the Counter Circle scenario is the highest, indicating that this method achieved the highest average reward per episode.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2112.11701v3",
    "pdf_url": null
  },
  {
    "instance_id": "ffe81ed9914747679719e742518f6cfc",
    "figure_id": "2010.08188v2-Figure6-1",
    "image_file": "2010.08188v2-Figure6-1.png",
    "caption": " Change of LPIPS at different sampling rates. VidODE outperforms the baselines at all sampling rates and shows relatively small declines in performance as inputs are sparsely drawn.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best at the highest sampling rate?",
    "answer": "Vid-ODE.",
    "rationale": "The plot shows that Vid-ODE has the highest LPIPS score at all sampling rates, including the highest sampling rate of 100%.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.08188v2",
    "pdf_url": null
  },
  {
    "instance_id": "c79f84d87d6e405ca3be1ccc36a7153e",
    "figure_id": "2008.12905v1-Figure6-1",
    "image_file": "2008.12905v1-Figure6-1.png",
    "caption": " (a) Distribution of the order-to-vehicle ratio across timeslots. (b) Comparison with Reyes. (c-e) Comparison with Greedy. (f-g) Number of accumulation windows where the time taken for assignment is above ∆ = 3 minutes across (e) all slots, and (f) peak slots. (h) Average running times of FOODMATCH,KM, and greedy. (i-k) Improvement over KM across timeslots in (i) XDT, (j) O/KM, and (k) WT.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which city has the highest average number of orders per vehicle?",
    "answer": "City B",
    "rationale": "This can be seen in Figure (a), which shows the distribution of the order-to-vehicle ratio across timeslots. City B has the highest peak in this figure, indicating that it has the highest average number of orders per vehicle.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2008.12905v1",
    "pdf_url": null
  },
  {
    "instance_id": "d0379573e760478ab4fa6f16ddf30898",
    "figure_id": "2009.11027v1-Figure4-1",
    "image_file": "2009.11027v1-Figure4-1.png",
    "caption": " Histogram of number of entities in each sentence for the Russian-to-English source corpus.",
    "figure_type": "** plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": " What is the most common number of entities in a sentence in the Russian-to-English source corpus?",
    "answer": " 1",
    "rationale": " The histogram shows that the bar for 1 entity is the highest, which means that sentences with 1 entity are the most frequent.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2009.11027v1",
    "pdf_url": null
  },
  {
    "instance_id": "3d79c1eefdf5492788a25d9ff6f57891",
    "figure_id": "2306.00335v2-Figure2-1",
    "image_file": "2306.00335v2-Figure2-1.png",
    "caption": " Links between corresponding cliques in CTF1, CTF1,a and CTF2 for the example shown in Figure 1b. All links (C,C′, C̃) are marked with dashed magenta lines and the link variables corresponding to each link are marked in magenta color.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between CTF1 and CTF1,a?",
    "answer": "CTF1,a is a sub-clique of CTF1.",
    "rationale": "This can be seen in the figure by noting that all of the nodes in CTF1,a are also present in CTF1. Additionally, the figure shows that the link variables between the corresponding nodes in CTF1 and CTF1,a are all equal.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.00335v2",
    "pdf_url": null
  },
  {
    "instance_id": "2bba100fd9bf457ea69fee50241cb77d",
    "figure_id": "2108.13298v2-Figure4-1",
    "image_file": "2108.13298v2-Figure4-1.png",
    "caption": " Qini curves of uplift modeling on Y (top) and R (bottom) for 3 discount types (normalized axes)",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which discount type is the most effective at increasing the probability of purchase?",
    "answer": "Discount A",
    "rationale": "The top plot shows the uplift in purchase probability for each discount type. Discount A has the highest uplift in purchase probability for all values of the fraction of the population targeted.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2108.13298v2",
    "pdf_url": null
  },
  {
    "instance_id": "5049571214214bf58c9fb398cbbaff7b",
    "figure_id": "2304.14394v2-Figure4-1",
    "image_file": "2304.14394v2-Figure4-1.png",
    "caption": " AUC scores of different attributes on LaSOT [16]",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which tracker performs best on the \"Overall\" attribute?",
    "answer": "DiMP.",
    "rationale": "The figure shows the AUC scores of different trackers on various attributes. The \"Overall\" attribute is represented by the grey line. The DiMP tracker has the highest AUC score on this line, indicating that it performs best on this attribute.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2304.14394v2",
    "pdf_url": null
  },
  {
    "instance_id": "df0ecedd08f64bdf97f4c1a15c9543c5",
    "figure_id": "2206.02511v1-Figure4-1",
    "image_file": "2206.02511v1-Figure4-1.png",
    "caption": " Normalized classification error (NCE) when tuning ResNet on three vision problems.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method consistently achieved the lowest NCE across all three datasets?",
    "answer": "Our method with GP",
    "rationale": "The figure shows the NCE for different methods on three datasets. The green line, which represents our method with GP, is consistently lower than the other lines, indicating that it achieves the lowest NCE.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.02511v1",
    "pdf_url": null
  },
  {
    "instance_id": "4b703338aee542fd9be25f4b6090d258",
    "figure_id": "2004.07070v2-Figure3-1",
    "image_file": "2004.07070v2-Figure3-1.png",
    "caption": " Results of diagnostic and RSA analytical methods applied to the RNN-ASR model. The score is RER for the diagnostic methods and Pearson’s r for RSA.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which analytical method, diagnostic or RSA, shows a stronger correlation between the trained and random models?",
    "answer": "Diagnostic.",
    "rationale": "The figure shows that the score for the trained and random models are more similar for the diagnostic methods than for the RSA methods. This suggests that the diagnostic methods are better at capturing the differences between the trained and random models.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2004.07070v2",
    "pdf_url": null
  },
  {
    "instance_id": "6ce917d4333141a4a2ff68782c704c59",
    "figure_id": "2103.14794v1-Figure1-1",
    "image_file": "2103.14794v1-Figure1-1.png",
    "caption": " The lighting layout of our setup. A side view of the setup (left) and the vertical-cross parameterization of all lights with 4,096 LEDs on each face (right).",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many LEDs are used in total in this setup?",
    "answer": "20,480",
    "rationale": "The figure shows that there are six faces to the lighting setup, and each face has 4,096 LEDs. Therefore, the total number of LEDs is 6 x 4,096 = 20,480.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.14794v1",
    "pdf_url": null
  },
  {
    "instance_id": "099ef9d21ca94ee694e7b2479bf71a3a",
    "figure_id": "1806.03195v2-Figure1-1",
    "image_file": "1806.03195v2-Figure1-1.png",
    "caption": " General repairing scheme",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What does the red node represent in the general repairing scheme?",
    "answer": "The red node represents the damage to be repaired.",
    "rationale": "The figure shows a schematic of a general repairing scheme. The blue nodes represent the original state of the system, and the red node represents the damage to be repaired. The arrow pointing to the red node indicates that the damage is being repaired.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1806.03195v2",
    "pdf_url": null
  },
  {
    "instance_id": "034a3db1b1fc4a1685a16ae84f964f5b",
    "figure_id": "2106.05933v2-Figure108-1",
    "image_file": "2106.05933v2-Figure108-1.png",
    "caption": " Sparsity over layers for wav2vec-base finetuned for Turkish tr at 50% sparsity.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which layer has the highest sparsity?",
    "answer": "Layer 11.",
    "rationale": "The figure shows the sparsity for each layer of the wav2vec-base model. The x-axis shows the layer number, and the y-axis shows the sparsity percentage. The bar for layer 11 is the tallest, indicating that it has the highest sparsity.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.05933v2",
    "pdf_url": null
  },
  {
    "instance_id": "3977392416fe449992e2e3c126ce23dc",
    "figure_id": "2203.15458v1-Figure6-1",
    "image_file": "2203.15458v1-Figure6-1.png",
    "caption": " Comparison of our proposed method with stateof-the-art methods on NYU dataset. Left: mean joint error per hand joint. Right: the percentage of success frames over different error thresholds.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the lowest mean joint error for the wrist?",
    "answer": "V2V",
    "rationale": "The left plot shows the mean joint error for each hand joint. The V2V bar is the shortest for the wrist, indicating that it has the lowest mean joint error.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2203.15458v1",
    "pdf_url": null
  },
  {
    "instance_id": "1a092ca9e0f34d28b3f0f4ece4fb5652",
    "figure_id": "2110.00053v1-Figure2-1",
    "image_file": "2110.00053v1-Figure2-1.png",
    "caption": " Comparison of matchings between the first and last image of the CMU house sequence obtained by several methods. The colour of the dots indicates the ground truth correspondence, and the lines show the obtained matchings (green: correct, red: wrong). Overall, our approach obtains the best matchings, see also Fig. 1.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performed the best in matching the features between the first and last images of the CMU house sequence?",
    "answer": "Our approach",
    "rationale": "The figure shows the matchings obtained by several methods, with green lines indicating correct matches and red lines indicating wrong matches. Our approach has the fewest red lines, indicating that it made the fewest errors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.00053v1",
    "pdf_url": null
  },
  {
    "instance_id": "5bda6a124d754dd1a45bd1342a855b13",
    "figure_id": "2103.01171v2-Figure5-1",
    "image_file": "2103.01171v2-Figure5-1.png",
    "caption": " Per-station cost vs. marginal cost of domains with uniform distribution over the goals of the worker.",
    "figure_type": "plot.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which strategy performs the best when the per station cost is high?",
    "answer": "Baseline.",
    "rationale": "The figure shows that the Baseline strategy has the lowest marginal cost when the per station cost is high.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.01171v2",
    "pdf_url": null
  },
  {
    "instance_id": "27dcb5f835e04da0a3e25903c5305568",
    "figure_id": "2012.11782v2-Figure1-1",
    "image_file": "2012.11782v2-Figure1-1.png",
    "caption": " Features and the causal DAG of our synthetic credit loan approval dataset. The task is to predict whether an individual’s credit loan will be approved. We labeled each individual depending on one’s values of “Income” and “HealthStatus”.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between \"Education\" and \"Income\"?",
    "answer": "Education has a positive causal effect on Income.",
    "rationale": "The causal DAG shows that there is a directed edge from \"Education\" to \"JobSkill\" and another directed edge from \"JobSkill\" to \"Income\". This indicates that Education influences JobSkill, which in turn influences Income.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.11782v2",
    "pdf_url": null
  },
  {
    "instance_id": "f326109b45e1499f869d0870091a908e",
    "figure_id": "2102.07945v4-FigureC.2-1",
    "image_file": "2102.07945v4-FigureC.2-1.png",
    "caption": "Figure C.2: Average output conductance and F1 score against ground-truth conductance, on kuniform hypergraphs with k = 5. The error bars show variation over 50 runs using different seed nodes. Both the ground-truth and the target conductances are computed using cardinality-based cut-cost.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the following models has the highest F1 score when the ground-truth conductance is 0.12?",
    "answer": "C-HFD",
    "rationale": "The F1 score is shown on the y-axis of the right-hand plot. When the ground-truth conductance is 0.12, the C-HFD line is the highest.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2102.07945v4",
    "pdf_url": null
  },
  {
    "instance_id": "a56e88dac6b841539e147d720b9f23b3",
    "figure_id": "2109.07556v1-Figure2-1",
    "image_file": "2109.07556v1-Figure2-1.png",
    "caption": " Mediator Z with no direct effects of X on Y .",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Is the effect of X on Y direct or indirect?",
    "answer": "Indirect.",
    "rationale": "The figure shows that there is no direct path from X to Y. The only path from X to Y goes through Z, which means that the effect of X on Y is mediated by Z.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.07556v1",
    "pdf_url": null
  },
  {
    "instance_id": "5009ff532543406da0a308f17363369d",
    "figure_id": "1906.05423v2-Figure12-1",
    "image_file": "1906.05423v2-Figure12-1.png",
    "caption": " Various evaluation scores for all baselines on the MNIST dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the four models performed the best according to the Wasserstein metric?",
    "answer": "VCAE",
    "rationale": "The VCAE line is the lowest in both the Wass pixel and Wass conv plots, indicating that it has the lowest Wasserstein metric values.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.05423v2",
    "pdf_url": null
  },
  {
    "instance_id": "f2cbffde0c114df5a0341f5c28922c8b",
    "figure_id": "2210.12566v2-Figure16-1",
    "image_file": "2210.12566v2-Figure16-1.png",
    "caption": " Comparison of DQN, DQN with a distributional C51 critic, and the distributional IQN agent. Without decoupling, we do not observe benefits of distributional critics in these domains. Slower convergence on Walker Walk with distributional representations could indicate that the associated increased parameter count translates to a more difficult optimization problem.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which agent performs the best on the Quadruped Run task?",
    "answer": "IQN (3 Bin)",
    "rationale": "The figure shows that the IQN (3 Bin) agent achieves the highest episode return on the Quadruped Run task.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.12566v2",
    "pdf_url": null
  },
  {
    "instance_id": "a703bb11c3714ed6a1dd13404a65c76d",
    "figure_id": "2005.12210v1-Figure1-1",
    "image_file": "2005.12210v1-Figure1-1.png",
    "caption": " Performance comparison varying k .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model has the lowest MSE across all datasets?",
    "answer": "HFT.",
    "rationale": "The red line representing HFT is consistently the lowest line across all three datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.12210v1",
    "pdf_url": null
  },
  {
    "instance_id": "db83b3092fb84ed39986b29177d5b2b6",
    "figure_id": "2109.01396v1-Figure2-1",
    "image_file": "2109.01396v1-Figure2-1.png",
    "caption": " (a) KenLM scores (horizontal dashed lines are the scores for the references); (b) proportion of tokens of different frequency ranks in model translations. En-Ru.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which n-gram model has the highest KenLM score in stage 1?",
    "answer": "The 5-gram model.",
    "rationale": "In Figure (a), the 5-gram model has the highest curve (green), which means it has the highest KenLM score.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2109.01396v1",
    "pdf_url": null
  },
  {
    "instance_id": "23ca3daf433146b185baefadbb0d318f",
    "figure_id": "2111.01007v1-Figure20-1",
    "image_file": "2111.01007v1-Figure20-1.png",
    "caption": " Uncurated Results for Pokemon (10242). The images are selected randomly given one global random seed. We recommend zooming in for comparison.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which GAN generated the images with the highest recall?",
    "answer": "Projected GAN",
    "rationale": "The Projected GAN has the highest recall score of 0.215. This is shown in the bottom right corner of the image.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2111.01007v1",
    "pdf_url": null
  },
  {
    "instance_id": "5fab579b436446ffbb0f6dcbea6cd580",
    "figure_id": "2012.00857v3-Figure4-1",
    "image_file": "2012.00857v3-Figure4-1.png",
    "caption": " Dependency relation weights learnt on different datasets. Row i constains relation weights for all attention heads in the i-th transformer layer. p represents the parent relation. d represents the dependent relation. We observe a clearer preference for each attention head in the model trained on BLLIP-SM. This probably due to BLLIP-SM has signficantly more training data. It’s also interesting to notice that the first layer tend to focus on parent relations.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model tends to focus more on parent relations in the first layer?",
    "answer": "The model trained on BLLIP-SM.",
    "rationale": "In the figure, the first layer of the model trained on BLLIP-SM has higher weights for the parent relation (p) than the dependent relation (d) for most attention heads. This is not as evident in the model trained on PTB.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2012.00857v3",
    "pdf_url": null
  },
  {
    "instance_id": "94f909a8eb6d43ab9e0c581723c5316d",
    "figure_id": "2306.06779v1-Figure3-1",
    "image_file": "2306.06779v1-Figure3-1.png",
    "caption": " Transition probability for noise simulation, e.g., the correct option ‘>’ is corrupted into ‘<’ or ‘=’ equally by the same probability.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the probability that the correct option \">\" is corrupted into \"=\"?",
    "answer": "0.5",
    "rationale": "The figure shows that the transition probability from \">\" to \"=\" is 0.5. This means that there is a 50% chance that the correct option \">\" will be corrupted into \"=\".",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.06779v1",
    "pdf_url": null
  },
  {
    "instance_id": "84743c0c4f16482b900c14ae23849102",
    "figure_id": "1908.03245v1-Figure8-1",
    "image_file": "1908.03245v1-Figure8-1.png",
    "caption": " Runtime comparison of different dehazing methods.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dehazing method is the fastest?",
    "answer": "AOD-Net.",
    "rationale": "The figure shows the runtime of different dehazing methods, and AOD-Net has the lowest runtime of 0.08 seconds.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1908.03245v1",
    "pdf_url": null
  },
  {
    "instance_id": "883c72aa0bd74ff6a46b97c1c7993d8e",
    "figure_id": "2305.11449v1-Figure4-1",
    "image_file": "2305.11449v1-Figure4-1.png",
    "caption": " We fine-tune only the last one/two/three layers and record the performance gap on the validation set every hundred updates. We then plot those curves in this figure.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset and optimizer combination resulted in the highest accuracy?",
    "answer": "NER dataset and Adamw optimizer",
    "rationale": "The figure shows that the NER dataset and Adamw optimizer combination resulted in the highest accuracy, as the blue line in the NER dataset plot is higher than the other lines in the other plots.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2305.11449v1",
    "pdf_url": null
  },
  {
    "instance_id": "a3759c2894cc4ff4aa180383ff0ec0c5",
    "figure_id": "2309.07974v1-Figure8-1",
    "image_file": "2309.07974v1-Figure8-1.png",
    "caption": " Three different data modalities available from our data generator for a single scene. (A) visual: not used in our models since we’re primarily interested in learning memory representations. (B) text: flattened text used by the Sequence + GPT-2 model. (C) relational: structured object and property nodes used by the Structured + Transformer model.",
    "figure_type": "Schematic.",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which data modality is used by the Sequence + GPT-2 model?",
    "answer": "Text.",
    "rationale": "The caption states that the Sequence + GPT-2 model uses flattened text.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2309.07974v1",
    "pdf_url": null
  },
  {
    "instance_id": "2af3cf256d2c4579a7061edccb1d8ea1",
    "figure_id": "2003.10401v1-Figure5-1",
    "image_file": "2003.10401v1-Figure5-1.png",
    "caption": " Distribution of route activating probabilities in dynamic networks. Most of the paths tend to be preserved without budget constraints in Dynamic-Raw. Given resource budgets, different proportions of routes will be closed in Dynamic-A, B, and C.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the dynamic networks has the highest proportion of routes preserved?",
    "answer": "Dynamic-Raw",
    "rationale": "The figure shows the distribution of route activating probabilities in different dynamic networks. The x-axis shows the gate probability, and the y-axis shows the proportion of routes preserved. The Dynamic-Raw curve is the highest, indicating that it has the highest proportion of routes preserved.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2003.10401v1",
    "pdf_url": null
  },
  {
    "instance_id": "379ffa26cd5746d5b0283bdcbde73e4a",
    "figure_id": "2210.08069v1-Figure6-1",
    "image_file": "2210.08069v1-Figure6-1.png",
    "caption": " Pictorial aid for Theorem 4.2. By only considering the edges on the upper convex hull (above the dotted line), we sort the edges according to slope and step along them until we reach the end of the upper convex hull. The convex lower hull can be computed easily by central symmetry of zonotopes.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many edges are on the upper convex hull of the zonotope?",
    "answer": "5",
    "rationale": "The upper convex hull is the portion of the zonotope above the dotted line. There are 5 edges on this portion of the zonotope.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2210.08069v1",
    "pdf_url": null
  },
  {
    "instance_id": "23b090f1c3e1462995320de55fa8d9e6",
    "figure_id": "1904.09658v4-Figure8-1",
    "image_file": "1904.09658v4-Figure8-1.png",
    "caption": " Distribution of estimated uncertainty on different datasets. Here, “Uncertainty” refers to the harmonic mean of σ across all feature dimensions. Note that the estimated uncertainty is proportional to the complexity of the datasets. Best viewed in color.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which dataset has the highest average uncertainty?",
    "answer": "LFW",
    "rationale": "The LFW distribution has the highest peak and is shifted to the right compared to the other two datasets. This indicates that the LFW dataset has a higher average uncertainty than the IJB-A and IJB-S datasets.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1904.09658v4",
    "pdf_url": null
  },
  {
    "instance_id": "27cebb838d344fa7aeead61867b3d4e7",
    "figure_id": "2303.01526v2-Figure5-1",
    "image_file": "2303.01526v2-Figure5-1.png",
    "caption": " SAFF object segmentations show balanced quality while recovering a volumetric scene representation (e). Basic DINO-ViT produces low-quality segmentations and misses objects. A state-of-the-art 2D video learning method [19] sometimes has edge detail (Umbrella, legs) but othertimes misses detail and objects (Balloon NBoard).",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produces the most accurate object segmentations?",
    "answer": "SAFF",
    "rationale": "The figure shows that SAFF object segmentations are more accurate than those produced by DINO-ViT and ProposeReduce. For example, in the \"Dynamic Board\" image, SAFF correctly segments the person, while DINO-ViT and ProposeReduce do not. In the \"Balloon NBoard\" image, SAFF correctly segments the balloon and the person, while ProposeReduce only segments the balloon and DINO-ViT only segments the person.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2303.01526v2",
    "pdf_url": null
  },
  {
    "instance_id": "6ee2f380f1814d80b8743264817711cc",
    "figure_id": "2306.06529v2-Figure1-1",
    "image_file": "2306.06529v2-Figure1-1.png",
    "caption": " (a) The number of failures of graph neural networks, with varying hidden dimension and activation, to achieve WL separation on the 600 graphs from the TUDataset [30]. Analytic activations succeed on all graphs, as Theorem 6.3 predicts. (b) The normalized smallest singular value of multiset functions induced by piecewise-linear ReLU-networks and analytic SiLU-networks. Piecewise-linear networks have singularities on squares intersecting the diagonal, leading to non-injectivity. Analytic networks are moment injective, but have singularities on the diagonal, which leads to a non-Lipschitz inverse. See the end of Section 5 for more details.",
    "figure_type": "table and plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which activation function is more likely to be injective for a network with hidden dimension 100?",
    "answer": "SiLU",
    "rationale": "The table in (a) shows that the SiLU activation function has zero failures for all hidden dimensions, while the ReLU activation function has one failure for a hidden dimension of 100. This suggests that the SiLU activation function is more likely to be injective for a network with hidden dimension 100.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2306.06529v2",
    "pdf_url": null
  },
  {
    "instance_id": "0bdfd2c99d1e435094be86eb6b4483d9",
    "figure_id": "1711.04076v2-Figure4-1",
    "image_file": "1711.04076v2-Figure4-1.png",
    "caption": " (Left) Clusters of SnapBuddy data set where each cluster is a distinct linear function that explains time based on size of profile image. (Right) Learning decision tree model to explain clusters in Figure 4 based on SnapBuddy function calls.",
    "figure_type": "plot and schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which filter function is the most computationally expensive for larger images?",
    "answer": "The KaleidoscopeFilter.filter function.",
    "rationale": "The figure on the right shows the decision tree model that was learned to explain the clusters in the figure on the left. The KaleidoscopeFilter.filter function is the first filter function that is applied to the image, and it is only applied to images that are larger than a certain size. This means that the KaleidoscopeFilter.filter function is only applied to the images in the blue and green clusters, which are the two clusters with the largest images. The figure on the left shows that the blue and green clusters have the highest execution times, which means that the KaleidoscopeFilter.filter function is the most computationally expensive for larger images.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1711.04076v2",
    "pdf_url": null
  },
  {
    "instance_id": "ff8fbd229f8546969c940ad246fa9858",
    "figure_id": "1808.04768v3-Figure1-1",
    "image_file": "1808.04768v3-Figure1-1.png",
    "caption": " Hypothesized relationship between skip interval ∆t and error accumulation rate L ∆t .",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the optimal skip interval that minimizes the error accumulation rate?",
    "answer": "The optimal skip interval is Δtopt.",
    "rationale": "The figure shows that the error accumulation rate is minimized when the skip interval is Δtopt.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1808.04768v3",
    "pdf_url": null
  },
  {
    "instance_id": "fd7933e82b874ecea5b2d14a9cd816ca",
    "figure_id": "2103.10559v2-Figure1-1",
    "image_file": "2103.10559v2-Figure1-1.png",
    "caption": " A challenging example consists of large motion, severe occlusion and non-stationary finer details. From top to bottom: the overlaid two inputs, the ground-truth middle frame, the frame generated by AdaCoF [32], the frame generated by the 10× compressed AdaCoF, and the frame generated by our method. The compressed AdaCoF even outperforms the full one in this case.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method produced the frame with the highest PSNR and SSIM?",
    "answer": "Ours.",
    "rationale": "The PSNR and SSIM values are shown below each frame. The frame generated by our method has the highest PSNR (33.96) and SSIM (0.98).",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2103.10559v2",
    "pdf_url": null
  },
  {
    "instance_id": "62fec57c4fc54ffca6b9308bd1d41dfb",
    "figure_id": "1906.04279v3-Figure5-1",
    "image_file": "1906.04279v3-Figure5-1.png",
    "caption": " Comparison with curriculum learning. We compare HGG with the original HER, HER+GOID with two threshold values.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which algorithm has the highest median success rate after 8000 episodes?",
    "answer": "HGG",
    "rationale": "The figure shows the median success rate for different algorithms over the course of 8000 episodes. The HGG curve is consistently higher than the other curves, indicating that it has the highest median success rate.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.04279v3",
    "pdf_url": null
  },
  {
    "instance_id": "4755be4db01b4a3ea9e0e83d8c7171a3",
    "figure_id": "2006.10965v1-Figure8-1",
    "image_file": "2006.10965v1-Figure8-1.png",
    "caption": " Image explanation metric (segment AUC) versus top and bottom % of attributions retained for different attribution methods on ResNet152 over the MS COCO test set. These plots expand the analysis of Table 2.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which attribution method has the highest segment AUC when only the top 10% of attributions are retained?",
    "answer": "ArchAttribute",
    "rationale": "The plot shows that the blue line, which represents ArchAttribute, is the highest at the 10% mark on the x-axis.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.10965v1",
    "pdf_url": null
  },
  {
    "instance_id": "a6011711f7ee4f5f9bcb949fd80bb8d8",
    "figure_id": "2206.13764v1-Figure4-1",
    "image_file": "2206.13764v1-Figure4-1.png",
    "caption": " Comparing the distribution of the orders of the generated feature interactions from HIRS, HIRS without s-Infomax and Infomin, and HIRS without 𝐿0 regularization for MovieLens 1M.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method generated the most feature interactions of order 12?",
    "answer": "HIRS without L0 regularization.",
    "rationale": "The purple bar corresponding to \"w/o L0\" is the highest at order 12.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2206.13764v1",
    "pdf_url": null
  },
  {
    "instance_id": "3698c61f7f1a40189487c5660d95dc30",
    "figure_id": "2307.15049v2-Figure3-1",
    "image_file": "2307.15049v2-Figure3-1.png",
    "caption": " Analysis of change in mask weights M when finetuning to downstream tasks with ∇Lce. Over 11 datasets, the mean change in the MHSA layers is significantly higher than MLP over.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which type of layer in the model experiences a greater change in mask weights when fine-tuning to downstream tasks?",
    "answer": "MHSA layers",
    "rationale": "The figure shows the mean change in mask weights for MHSA and MLP layers across 11 datasets. The mean change for MHSA layers is significantly higher than that for MLP layers, as indicated by the difference in the height of the bars.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2307.15049v2",
    "pdf_url": null
  },
  {
    "instance_id": "0be1b7e3d81d4609a9df1a124992c00f",
    "figure_id": "2209.08503v3-Figure4-1",
    "image_file": "2209.08503v3-Figure4-1.png",
    "caption": " The normalized reprojection error comparison between our proposed method NW-RSBA and NM-RSBA [2] along the degeneracy process.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has a lower reprojection error?",
    "answer": "NW-RSBA has a lower reprojection error.",
    "rationale": "The plot shows that the NW-RSBA curve is below the NM-RSBA curve, indicating that NW-RSBA has a lower reprojection error for all levels of degeneracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.08503v3",
    "pdf_url": null
  },
  {
    "instance_id": "d293ff2ff0764d4bbb3fb54f24d10361",
    "figure_id": "2211.10277v2-Figure3-1",
    "image_file": "2211.10277v2-Figure3-1.png",
    "caption": " Performance comparison on few-shot learning, i.e., 1-/2-/4-/8-/16-shot, on 11 benchmark datasets. The top-left is the averaged accuracy over the 11 datasets. The full numerical results can be found in the supplementary.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method performs best on average across all datasets when the number of labeled training examples per class is 16?",
    "answer": "Ours",
    "rationale": "The top-left plot shows the average accuracy over all datasets. When the number of labeled training examples per class is 16, the \"Ours\" method has the highest accuracy.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.10277v2",
    "pdf_url": null
  },
  {
    "instance_id": "f93885b33e2a43a0a7fa64064703b144",
    "figure_id": "1907.03880v1-Figure5-1",
    "image_file": "1907.03880v1-Figure5-1.png",
    "caption": " Swarm performance P (N,κ) for the 64× 32 scenario.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three algorithms performed the best?",
    "answer": "GP_DPO",
    "rationale": "The GP_DPO line is the highest on the graph, indicating that it collected the most blocks.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1907.03880v1",
    "pdf_url": null
  },
  {
    "instance_id": "145c5e1f2f854ad78c28f99c10576b69",
    "figure_id": "1812.02766v1-Figure13-1",
    "image_file": "1812.02766v1-Figure13-1.png",
    "caption": " Qualitative results: Caltech256. Extends Figure 6 in the main paper. GT labels are underlined, correct knockoff top-1",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which image in the test set has the highest predicted probability for the correct label?",
    "answer": "The image of the tomato.",
    "rationale": "The image of the tomato has a predicted probability of 0.94 for the correct label \"Tomato\", which is the highest predicted probability for any image in the test set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.02766v1",
    "pdf_url": null
  },
  {
    "instance_id": "7a738c831cdb4c4e8d089818ae70910e",
    "figure_id": "2110.11258v1-Figure3-1",
    "image_file": "2110.11258v1-Figure3-1.png",
    "caption": " Plot of Eξr(w) (points) for w ∈ {wO, wOe, wb} in the autoregressive regime with d = bγnc, r2 = 1, σ2 = 1, n = 2000, ρ = 0.5.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which interpolator has the lowest risk for all values of γ?",
    "answer": "Wb",
    "rationale": "The plot shows that the black dots, which represent Wb, are consistently lower than the other two interpolators for all values of γ.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2110.11258v1",
    "pdf_url": null
  },
  {
    "instance_id": "59ded8a2ab164e08b9bc1d4aa0a96734",
    "figure_id": "2010.11344v2-Figure5-1",
    "image_file": "2010.11344v2-Figure5-1.png",
    "caption": " The learning curves on the validation set. Equivariant models converge faster using fewer samples than the non-equivariant models.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model requires the least amount of training data to achieve a DE@3s of 7 on the validation set?",
    "answer": "ProgECCO",
    "rationale": "The plot shows the DE@3s on the validation set as a function of the number of training samples. The green line, which represents the ProgECCO model, reaches a DE@3s of 7 with the least amount of training data.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2010.11344v2",
    "pdf_url": null
  },
  {
    "instance_id": "9c067328f21b439bb6fbe84141549496",
    "figure_id": "1910.00610v1-Figure1-1",
    "image_file": "1910.00610v1-Figure1-1.png",
    "caption": " An example of an ideal conversation model with dynamic knowledge graphs.",
    "figure_type": "schematic",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the relationship between Jin-Xi and Feng, Ruozhao?",
    "answer": "Enemy.",
    "rationale": "The knowledge graph shows a directed edge labeled \"EnemyOf\" from Jin-Xi to Feng, Ruozhao.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1910.00610v1",
    "pdf_url": null
  },
  {
    "instance_id": "7c71ee6892fa4e92bd1b8bdf46994a1f",
    "figure_id": "1812.01210v2-Figure7-1",
    "image_file": "1812.01210v2-Figure7-1.png",
    "caption": " Higher SSIM score from Oursroigan indicates we preserve more perceptual structures than SepConv [21].",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three methods preserves the most perceptual structures according to the SSIM score?",
    "answer": "Oursroigan.",
    "rationale": "The caption states that Oursroigan has a higher SSIM score than SepConv, which indicates that it preserves more perceptual structures.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1812.01210v2",
    "pdf_url": null
  },
  {
    "instance_id": "605ef8f4b5fa4ed2a2dd83a374a07898",
    "figure_id": "2209.07007v2-Figure9-1",
    "image_file": "2209.07007v2-Figure9-1.png",
    "caption": " Generated images in CelebA (Liu et al., 2015). We show 100 images sampled from the generative model pθ(x) without conducting cherry-picking.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the models shown in the figure appears to generate the most realistic faces?",
    "answer": "GWAE (NP, λw = 1, λp = 10, λz = 0.00001)",
    "rationale": "The figure shows that the GWAE model generates faces that are more realistic and less blurry than the other models. The faces generated by the GWAE model also have more natural skin tones and hair colors.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2209.07007v2",
    "pdf_url": null
  },
  {
    "instance_id": "b6f1d8c3f92f48f1a5254a74736385d2",
    "figure_id": "2308.14616v1-Figure12-1",
    "image_file": "2308.14616v1-Figure12-1.png",
    "caption": " Direct optimization on a 2563 grid.",
    "figure_type": "Photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "What is the resolution of the grid used to create this image?",
    "answer": "256^3",
    "rationale": "The caption states that the image was created using direct optimization on a 256^3 grid.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2308.14616v1",
    "pdf_url": null
  },
  {
    "instance_id": "56c4e40396ac4df38c2b77dc6c3b455d",
    "figure_id": "2006.12982v2-Figure8-1",
    "image_file": "2006.12982v2-Figure8-1.png",
    "caption": " Eigenfunctions of ∆2 for the product manifold S3×S3, without restriction to symmetric matrices. The first nontrivial eigenfunction is the expected projection matrix, while the next eight eigenfunctions are all skew-symmetric – four per manifold.",
    "figure_type": "other",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "How many eigenfunctions are shown in the image?",
    "answer": "9",
    "rationale": "The image shows a grid of 9 smaller images, each representing an eigenfunction of ∆2 for the product manifold S3×S3.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2006.12982v2",
    "pdf_url": null
  },
  {
    "instance_id": "db6e2998d23048bbafac687bf44635cc",
    "figure_id": "2211.16869v1-Figure5-1",
    "image_file": "2211.16869v1-Figure5-1.png",
    "caption": " Errors of normal estimation on the PCPNet dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which method has the highest error in normal estimation on the PCPNet dataset?",
    "answer": "PCPNet",
    "rationale": "The figure shows the errors of normal estimation for different methods on the PCPNet dataset. The error values are shown in the top right corner of each subfigure. PCPNet has the highest error value of 29.95.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2211.16869v1",
    "pdf_url": null
  },
  {
    "instance_id": "64635fc1dc914057b6f710fb984d3a26",
    "figure_id": "2106.03747v2-Figure3-1",
    "image_file": "2106.03747v2-Figure3-1.png",
    "caption": " Histogram of the kernel target alignment over 50 runs (left) and task model alignment (right) for d = 7.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which kernel alignment has the highest value for the task-model alignment?",
    "answer": "q",
    "rationale": "The right plot shows the task-model alignment for different kernels. The green line, which represents q, has the highest value for the task-model alignment.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2106.03747v2",
    "pdf_url": null
  },
  {
    "instance_id": "543de017857049fa9e07a638068d1080",
    "figure_id": "2205.12486v2-Figure2-1",
    "image_file": "2205.12486v2-Figure2-1.png",
    "caption": " ROUGE-1 (F1) scores for different values of sampling factor (sf ) and number of samples per document (nd), evaluated on the GovReport test set. BARTlarge is an end-to-end baseline, which is equivalent to nd = 1 and sf = 1.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best in terms of ROUGE-1 F1 score?",
    "answer": "BART-large (end-to-end)",
    "rationale": "The figure shows the ROUGE-1 F1 scores for different models and configurations. BART-large (end-to-end) has the highest score of all the models, regardless of the number of samples per document.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2205.12486v2",
    "pdf_url": null
  },
  {
    "instance_id": "007e2714995d4e6989951ed5d0b2d1dc",
    "figure_id": "1906.08207v5-Figure1-1",
    "image_file": "1906.08207v5-Figure1-1.png",
    "caption": " The convergence of the proposed bound optimizers for minimizing several fair-clustering objectives in (4): Fair K-means, Fair Ncut and Fair K-median. The plots are based on the Synthetic dataset.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the three objectives converges the fastest?",
    "answer": "Fair K-median converges the fastest.",
    "rationale": "The plot for Fair K-median shows that the objective function value decreases rapidly in the first few iterations and then levels off. The plots for Fair K-means and Fair Ncut show that the objective function values decrease more gradually.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "1906.08207v5",
    "pdf_url": null
  },
  {
    "instance_id": "3a2bffcd167f44c58a40ffa86dd7793c",
    "figure_id": "2002.04992v2-Figure2-1",
    "image_file": "2002.04992v2-Figure2-1.png",
    "caption": " F1-score as a function of update steps for different models on the TIMIT validation set.",
    "figure_type": "plot",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which model performs best on the TIMIT validation set?",
    "answer": "SegFeat + Phn + Bin",
    "rationale": "The figure shows the F1-score as a function of update steps for different models on the TIMIT validation set. The SegFeat + Phn + Bin model has the highest F1-score, which indicates that it performs best on the TIMIT validation set.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2002.04992v2",
    "pdf_url": null
  },
  {
    "instance_id": "04d4cf755047491abdd83242eb4f0b0c",
    "figure_id": "2005.09704v1-Figure9-1",
    "image_file": "2005.09704v1-Figure9-1.png",
    "caption": " Test results on places2 validation datasets with input size of 1024 × 1024.",
    "figure_type": "photograph(s)",
    "compound": null,
    "figs_numb": null,
    "qa_pair_type": null,
    "question": "Which of the inpainting methods produces the most realistic results for the image of the beach?",
    "answer": "Ours.",
    "rationale": "The image in column (h) is the most visually similar to the ground truth image in column (b). The other methods produce artifacts or blurry results in the area where the object was removed.",
    "answer_options": [],
    "venue": null,
    "categories": null,
    "source_dataset": "spiqa",
    "paper_id": "2005.09704v1",
    "pdf_url": null
  }
]